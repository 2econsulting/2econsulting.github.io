"site","date","headline","url_address","text"
"mastery",2018-12-14,"How to Improve Deep Learning Model Robustness by Adding Noise","https://machinelearningmastery.com/how-to-improve-deep-learning-model-robustness-by-adding-noise/","Adding noise to an underconstrained neural network model with a small training dataset can have a regularizing effect and reduce overfitting. Keras supports the addition of Gaussian noise via a separate layer called the GaussianNoise layer. This layer can be used to add noise to an existing model. In this tutorial, you will discover how to add noise to deep learning models in Keras in order to reduce overfitting and improve model generalization. After completing this tutorial, you will know: Let¡¯s get started. How to Improve Deep Learning Model Robustness by Adding NoisePhoto by Michael Mueller, some rights reserved. This tutorial is divided into three parts; they are: Keras supports the addition of noise to models via the GaussianNoise layer. This is a layer that will add noise to inputs of a given shape. The noise has a mean of zero and requires that a standard deviation of the noise be specified as a parameter. For example: The output of the layer will have the same shape as the input, with the only modification being the addition of noise to the values. The GaussianNoise can be used in a few different ways with a neural network model. Firstly, it can be used as an input layer to add noise to input variables directly. This is the traditional use of noise as a regularization method in neural networks. Below is an example of defining a GaussianNoise layer as an input layer for a model that takes 2 input variables. Noise can also be added between hidden layers in the model. Given the flexibility of Keras, the noise can be added before or after the use of the activation function. It may make more sense to add it before the activation; nevertheless, both options are possible. Below is an example of a GaussianNoise layer that adds noise to the linear output of a Dense layer before a rectified linear activation function, perhaps a more appropriate use of noise between hidden layers. Noise can also be added after the activation function, much like using a noisy activation function. One downside of this usage is that the resulting values may be out-of-range from what the activation function may normally provide. For example, a value with added noise may be less than zero, whereas the relu activation function will only ever output values 0 or larger. Let¡¯s take a look at how noise regularization can be used with some common network types. The example below adds noise between two Dense fully connected layers. The example below adds noise after a pooling layer in a convolutional network. The example below adds noise between an LSTM recurrent layer and a Dense fully connected layer. Now that we have seen how to add noise to neural network models, let¡¯s look at a case study of adding noise to an overfit model to reduce generalization error. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this section, we will demonstrate how to use noise regularization to reduce overfitting of an MLP on a simple binary classification problem. This example provides a template for applying noise regularization to your own neural network for classification and regression problems. We will use a standard binary classification problem that defines two two-dimensional concentric circles of observations, one semi-circle for each class. Each observation has two input variables with the same scale and a class output value of either 0 or 1. This dataset is called the ¡°circles¡± dataset because of the shape of the observations in each class when plotted. We can use the make_circles() function to generate observations from this problem. We will add noise to the data and seed the random number generator so that the same samples are generated each time the code is run. We can plot the dataset where the two variables are taken as x and y coordinates on a graph and the class value is taken as the color of the observation. The complete example of generating the dataset and plotting it is listed below. Running the example creates a scatter plot showing the concentric circles shape of the observations in each class. We can see the noise in the dispersal of the points making the circles less obvious. Scatter Plot of Circles Dataset with Color Showing the Class Value of Each Sample This is a good test problem because the classes cannot be separated by a line, e.g. are not linearly separable, requiring a nonlinear method such as a neural network to address. We have only generated 100 samples, which is small for a neural network, providing the opportunity to overfit the training dataset and have higher error on the test dataset, a good case for using regularization. Further, the samples have noise, giving the model an opportunity to learn aspects of the samples that don¡¯t generalize. We can develop an MLP model to address this binary classification problem. The model will have one hidden layer with more nodes than may be required to solve this problem, providing an opportunity to overfit. We will also train the model for longer than is required to ensure the model overfits. Before we define the model, we will split the dataset into train and test sets, using 30 examples to train the model and 70 to evaluate the fit model¡¯s performance. Next, we can define the model. The hidden layer uses 500 nodes in the hidden layer and the rectified linear activation function. A sigmoid activation function is used in the output layer in order to predict class values of 0 or 1. The model is optimized using the binary cross entropy loss function, suitable for binary classification problems and the efficient Adam version of gradient descent. The defined model is then fit on the training data for 4,000 epochs and the default batch size of 32. We will also use the test dataset as a validation dataset. We can evaluate the performance of the model on the test dataset and report the result. Finally, we will plot the performance of the model on both the train and test set each epoch. If the model does indeed overfit the training dataset, we would expect the line plot of accuracy on the training set to continue to increase and the test set to rise and then fall again as the model learns statistical noise in the training dataset. We can tie all of these pieces together; the complete example is listed below. Running the example reports the model performance on the train and test datasets. We can see that the model has better performance on the training dataset than the test dataset, one possible sign of overfitting. Your specific results may vary given the stochastic nature of the neural network and the training algorithm. Because the model is severely overfit, we generally would not expect much, if any, variance in the accuracy across repeated runs of the model on the same dataset. A figure is created showing line plots of the model accuracy on the train and test sets. We can see that expected shape of an overfit model where test accuracy increases to a point and then begins to decrease again. Line Plots of Accuracy on Train and Test Datasets While Training Showing an Overfit The dataset is defined by points that have a controlled amount of statistical noise. Nevertheless, because the dataset is small, we can add further noise to the input values. This will have the effect of creating more samples or resampling the domain, making the structure of the input space artificially smoother. This may make the problem easier to learn and improve generalization performance. We can add a GaussianNoise layer as the input layer. The amount of noise must be small. Given that the input values are within the range [0, 1], we will add Gaussian noise with a mean of 0.0 and a standard deviation of 0.01, chosen arbitrarily. The complete example with this change is listed below. Running the example reports the model performance on the train and test datasets. Your results will vary, given both the stochastic nature of the learning algorithm and the stochastic nature of the noise added to the model. Try running the example a few times. In this case, we may see a small lift in performance of the model on the test dataset, with no negative impact on the training dataset. We clearly see the impact of the added noise on the evaluation of the model during training as graphed on the line plot. The noise cases the accuracy of the model to jump around during training, possibly due to the noise introducing points that conflict with true points from the training dataset. Perhaps a lower input noise standard deviation would be more appropriate. The model still shows a pattern of being overfit, with a rise and then fall in test accuracy over training epochs. Line Plot of Train and Test Accuracy With Input Layer Noise An alternative approach to adding noise to the input values is to add noise between the hidden layers. This can be done by adding noise to the linear output of the layer (weighted sum) before the activation function is applied, in this case a rectified linear activation function. We can also use a larger standard deviation for the noise as the model is less sensitive to noise at this level given the presumably larger weights from being overfit. We will use a standard deviation of 0.1, again, chosen arbitrarily. The complete example with Gaussian noise between the hidden layers is listed below. Running the example reports the model performance on the train and test datasets. Your results will vary, given both the stochastic nature of the learning algorithm and the stochastic nature of the noise added to the model. Try running the example a few times. In this case, we can see a marked increase in the performance of the model on the hold out test set. We can also see from the line plot of accuracy over training epochs that the model no longer appears to show the properties of being overfit. Line Plot of Train and Test Accuracy With Hidden Layer Noise We can also experiment and add the noise after the outputs of the first hidden layer pass through the activation function. The complete example is listed below. Running the example reports the model performance on the train and test datasets. Surprisingly, we see little difference in the performance of the model. Again, we can see from the line plot of accuracy over training epochs that the model no longer shows sign of overfitting. Line Plot of Train and Test Accuracy With Hidden Layer Noise (alternate) This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to add noise to deep learning models in Keras in order to reduce overfitting and improve model generalization. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Better Deep Learning It provides self-study tutorials on topics like: weight decay, batch normalization, dropout, model stacking and much more¡¦ Skip the Academics. Just Results. Click to learn more. Thanks Jason, nicely explained. Really enjoyed it. Thanks. Comment  Name (required)  Email (will not be published) (required)  Website"
"mastery",2018-12-12,"Train Neural Networks With Noise to Reduce Overfitting","https://machinelearningmastery.com/train-neural-networks-with-noise-to-reduce-overfitting/","Training a neural network with a small dataset can cause the network to memorize all training examples, in turn leading to poor performance on a holdout dataset. Small datasets may also represent a harder mapping problem for neural networks to learn, given the patchy or sparse sampling of points in the high-dimensional input space. One approach to making the input space smoother and easier to learn is to add noise to inputs during training. In this post, you will discover that adding noise to a neural network during training can improve the robustness of the network, resulting in better generalization and faster learning. After reading this post, you will know: Let¡¯s get started. Train Neural Networks With Noise to Reduce OverfittingPhoto by John Flannery, some rights reserved. This tutorial is divided into five parts; they are: Small datasets can introduce problems when training large neural networks. The first problem is that the network may effectively memorize the training dataset. Instead of learning a general mapping from inputs to outputs, the model may learn the specific input examples and their associated outputs. This will result in a model that performs well on the training dataset, and poor on new data, such as a holdout dataset. The second problem is that a small dataset provides less opportunity to describe the structure of the input space and its relationship to the output. More training data provides a richer description of the problem from which the model may learn. Fewer data points means that rather than a smooth input space, the points may represent a jarring and disjointed structure that may result in a difficult, if not unlearnable, mapping function. It is not always possible to acquire more data. Further, getting a hold of more data may not address these problems. One approach to improving generalization error and to improving the structure of the mapping problem is to add random noise. Many studies [¡¦] have noted that adding small amounts of input noise (jitter) to the training data often aids generalization and fault tolerance. <U+2014> Page 273, Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999. At first, this sounds like a recipe for making learning more challenging. It is a counter-intuitive suggestion to improving performance because one would expect noise to degrade performance of the model during training. Heuristically, we might expect that the noise will ¡®smear out¡¯ each data point and make it difficult for the network to fit individual data points precisely, and hence will reduce over-fitting. In practice, it has been demonstrated that training with noise can indeed lead to improvements in network generalization. <U+2014> Page 347, Neural Networks for Pattern Recognition, 1995. The addition of noise during the training of a neural network model has a regularization effect and, in turn, improves the robustness of the model. It has been shown to have a similar impact on the loss function as the addition of a penalty term, as in the case of weight regularization methods. It is well known that the addition of noise to the input data of a neural network during training can, in some circumstances, lead to significant improvements in generalization performance. Previous work has shown that such training with noise is equivalent to a form of regularization in which an extra term is added to the error function. <U+2014> Training with Noise is Equivalent to Tikhonov Regularization, 2008. In effect, adding noise expands the size of the training dataset. Each time a training sample is exposed to the model, random noise is added to the input variables making them different every time it is exposed to the model. In this way, adding noise to input samples is a simple form of data augmentation. Injecting noise in the input to a neural network can also be seen as a form of data augmentation. <U+2014> Page 241, Deep Learning, 2016. Adding noise means that the network is less able to memorize training samples because they are changing all of the time, resulting in smaller network weights and a more robust network that has lower generalization error. The noise means that it is as though new samples are being drawn from the domain in the vicinity of known samples, smoothing the structure of the input space. This smoothing may mean that the mapping function is easier for the network to learn, resulting in better and faster learning. ¡¦ input noise and weight noise encourage the neural-network output to be a smooth function of the input or its weights, respectively. <U+2014> The Effects of Adding Noise During Backpropagation Training on a Generalization Performance, 1996. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course The most common type of noise used during training is the addition of Gaussian noise to input variables. Gaussian noise, or white noise, has a mean of zero and a standard deviation of one and can be generated as needed using a pseudorandom number generator. The addition of Gaussian noise to the inputs to a neural network was traditionally referred to as ¡°jitter¡± or ¡°random jitter¡± after the use of the term in signal processing to refer to to the uncorrelated random noise in electrical circuits. The amount of noise added (eg. the spread or standard deviation) is a configurable hyperparameter. Too little noise has no effect, whereas too much noise makes the mapping function too challenging to learn. This is generally done by adding a random vector onto each input pattern before it is presented to the network, so that, if the patterns are being recycled, a different random vector is added each time. <U+2014> Training with Noise is Equivalent to Tikhonov Regularization, 2008. The standard deviation of the random noise controls the amount of spread and can be adjusted based on the scale of each input variable. It can be easier to configure if the scale of the input variables has first been normalized. Noise is only added during training. No noise is added during the evaluation of the model or when the model is used to make predictions on new data. The addition of noise is also an important part of automatic feature learning, such as in the case of autoencoders, so-called denoising autoencoders that explicitly require models to learn robust features in the presence of noise added to inputs. We have seen that the reconstruction criterion alone is unable to guarantee the extraction of useful features as it can lead to the obvious solution ¡°simply copy the input¡± or similarly uninteresting ones that trivially maximizes mutual information. [¡¦] we change the reconstruction criterion for a both more challenging and more interesting objective: cleaning partially corrupted input, or in short denoising. <U+2014> Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion, 2010. Although additional noise to the inputs is the most common and widely studied approach, random noise can be added to other parts of the network during training. Some examples include: The addition of noise to the layer activations allows noise to be used at any point in the network. This can be beneficial for very deep networks. Noise can be added to the layer outputs themselves, but this is more likely achieved via the use of a noisy activation function. The addition of noise to weights allows the approach to be used throughout the network in a consistent way instead of adding noise to inputs and layer activations. This is particularly useful in recurrent neural networks. Another way that noise has been used in the service of regularizing models is by adding it to the weights. This technique has been used primarily in the context of recurrent neural networks. [¡¦] Noise applied to the weights can also be interpreted as equivalent (under some assumptions) to a more traditional form of regularization, encouraging stability of the function to be learned. <U+2014> Page 242, Deep Learning, 2016. The addition of noise to gradients focuses more on improving the robustness of the optimization process itself rather than the structure of the input domain. The amount of noise can start high at the beginning of training and decrease over time, much like a decaying learning rate. This approach has proven to be an effective method for very deep networks and for a variety of different network types. We consistently see improvement from injected gradient noise when optimizing a wide variety of models, including very deep fully-connected networks, and special-purpose architectures for question answering and algorithm learning. [¡¦] Our experiments indicate that adding annealed Gaussian noise by decaying the variance works better than using fixed Gaussian noise <U+2014> Adding Gradient Noise Improves Learning for Very Deep Networks, 2015. Adding noise to the activations, weights, or gradients all provide a more generic approach to adding noise that is invariant to the types of input variables provided to the model. If the problem domain is believed or expected to have mislabeled examples, then the addition of noise to the class label can improve the model¡¯s robustness to this type of error. Although, it can be easy to derail the learning process. Adding noise to a continuous target variable in the case of regression or time series forecasting is much like the addition of noise to the input variables and may be a better use case. This section summarizes some examples where the addition of noise during training has been used. Lasse Holmstrom studied the addition of random noise both analytically and experimentally with MLPs in the 1992 paper titled ¡°Using Additive Noise in Back-Propagation Training.¡± They recommend first standardizing input variables then using cross-validation to choose the amount of noise to use during training. If a single general-purpose noise design method should be suggested, we would pick maximizing the cross-validated likelihood function. This method is easy to implement, is completely data-driven, and has a validity that is supported by theoretical consistency results Klaus Gref, et al. in their 2016 paper titled ¡°LSTM: A Search Space Odyssey¡± used a hyperparameter search for the standard deviation for Gaussian noise on the input variables for a suite of sequence prediction tasks and found that it almost universally resulted in worse performance. Additive Gaussian noise on the inputs, a traditional regularizer for neural networks, has been used for LSTM as well. However, we find that not only does it almost always hurt performance, it also slightly increases training times. Alex Graves, et al. in their groundbreaking 2013 paper titled ¡°Speech recognition with deep recurrent neural networks¡± that achieved then state-of-the-art results for speech recognition added noise to the weights of LSTMs during training. ¡¦ weight noise [was used] (the addition of Gaussian noise to the network weights during training). Weight noise was added once per training sequence, rather than at every timestep. Weight noise tends to ¡®simplify¡¯ neural networks, in the sense of reducing the amount of information required to transmit the parameters, which improves generalisation. In a prior 2011 paper that studies different types of static and adaptive weight noise titled ¡°Practical Variational Inference for Neural Networks,¡± Graves recommends using early stopping in conjunction with the addition of weight noise with LSTMs. ¡¦ in practice early stopping is required to prevent overfitting when training with weight noise. This section provides some tips for adding noise during training with your neural network. Noise can be added to training regardless of the type of problem that is being addressed. It is appropriate to try adding noise to both classification and regression type problems. The type of noise can be specialized to the types of data used as input to the model, for example, two-dimensional noise in the case of images and signal noise in the case of audio data. Adding noise during training is a generic method that can be used regardless of the type of neural network that is being used. It was a method used primarily with multilayer Perceptrons given their prior dominance, but can be and is used with Convolutional and Recurrent Neural Networks. It is important that the addition of noise has a consistent effect on the model. This requires that the input data is rescaled so that all variables have the same scale, so that when noise is added to the inputs with a fixed variance, it has the same effect. The also applies to adding noise to weights and gradients as they too are affected by the scale of the inputs. This can be achieved via standardization or normalization of input variables. If random noise is added after data scaling, then the variables may need to be rescaled again, perhaps per mini-batch. You cannot know how much noise will benefit your specific model on your training dataset. Experiment with different amounts, and even different types of noise, in order to discover what works best. Be systematic and use controlled experiments, perhaps on smaller datasets across a range of values. Noise is only added during the training of your model. Be sure that any source of noise is not added during the evaluation of your model, or when your model is used to make predictions on new data. This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered that adding noise to a neural network during training can improve the robustness of the network resulting in better generalization and faster learning. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Better Deep Learning It provides self-study tutorials on topics like: weight decay, batch normalization, dropout, model stacking and much more¡¦ Skip the Academics. Just Results. Click to learn more. Thank you so much for your great article. If there is an example of using this technique to improve performance, it will be very helpful. Yes, I will post one in a few days. Thank you so much. I am looking forward to your new post. Comment  Name (required)  Email (will not be published) (required)  Website"
"mastery",2018-12-10,"How to Stop Training Deep Neural Networks At the Right Time Using Early Stopping","https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/","A problem with training neural networks is in the choice of the number of training epochs to use. Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold out validation dataset. In this tutorial, you will discover the Keras API for adding early stopping to overfit deep learning neural network models. After completing this tutorial, you will know: Let¡¯s get started. How to Stop Training Deep Neural Networks At the Right Time With Using Early StoppingPhoto by Ian D. Keating, some rights reserved. This tutorial is divided into six parts; they are: Callbacks provide a way to execute code and interact with the training model process automatically. Callbacks can be provided to the fit() function via the ¡°callbacks¡± argument. First, callbacks must be instantiated. Then, one or more callbacks that you intend to use must be added to a Python list. Finally, the list of callbacks is provided to the callback argument when fitting the model. Early stopping requires that a validation dataset is evaluated during training. This can be achieved by specifying the validation dataset to the fit() function when training your model. There are two ways of doing this. The first involves you manually splitting your training data into a train and validation dataset and specifying the validation dataset to the fit() function via the validation_data argument. For example: Alternately, the fit() function can automatically split your training dataset into train and validation sets based on a percentage split specified via the validation_split argument. The validation_split is a value between 0 and 1 and defines the percentage amount of the training dataset to use for the validation dataset. For example: In both cases, the model is not trained on the validation dataset. Instead, the model is evaluated on the validation dataset at the end of each training epoch. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course The loss function chosen to be optimized for your model is calculated at the end of each epoch. To callbacks, this is made available via the name ¡°loss.¡± If a validation dataset is specified to the fit() function via the validation_data or validation_split arguments, then the loss on the validation dataset will be made available via the name ¡°val_loss.¡± Additional metrics can be monitored during the training of the model. They can be specified when compiling the model via the ¡°metrics¡± argument to the compile function. This argument takes a Python list of known metric functions, such as ¡®mse¡® for mean squared error and ¡®acc¡® for accuracy. For example: If additional metrics are monitored during training, they are also available to the callbacks via the same name, such as ¡®acc¡® for accuracy on the training dataset and ¡®val_acc¡® for the accuracy on the validation dataset. Or, ¡®mse¡® for mean squared error on the training dataset and ¡®val_mse¡® on the validation dataset. Keras supports the early stopping of training via a callback called EarlyStopping. This callback allows you to specify the performance measure to monitor, the trigger, and once triggered, it will stop the training process. The EarlyStopping callback is configured when instantiated via arguments. The ¡°monitor¡± allows you to specify the performance measure to monitor in order to end training. Recall from the previous section that the calculation of measures on the validation dataset will have the ¡®val_¡® prefix, such as ¡®val_loss¡® for the loss on the validation dataset. Based on the choice of performance measure, the ¡°mode¡± argument will need to be specified as whether the objective of the chosen metric is to increase (maximize or ¡®max¡®) or to decrease (minimize or ¡®min¡®). For example, we would seek a minimum for validation loss and a minimum for validation mean squared error, whereas we would seek a maximum for validation accuracy. By default, mode is set to ¡®auto¡® and knows that you want to minimize loss or maximize accuracy. That is all that is needed for the simplest form of early stopping. Training will stop when the chosen performance measure stops improving. To discover the training epoch on which training was stopped, the ¡°verbose¡± argument can be set to 1. Once stopped, the callback will print the epoch number. Often, the first sign of no further improvement may not be the best time to stop training. This is because the model may coast into a plateau of no improvement or even get slightly worse before getting much better. We can account for this by adding a delay to the trigger in terms of the number of epochs on which we would like to see no improvement. This can be done by setting the ¡°patience¡± argument. The exact amount of patience will vary between models and problems. Reviewing plots of your performance measure can be very useful to get an idea of how noisy the optimization process for your model on your data may be. By default, any change in the performance measure, no matter how fractional, will be considered an improvement. You may want to consider an improvement that is a specific increment, such as 1 unit for mean squared error or 1% for accuracy. This can be specified via the ¡°min_delta¡± argument. Finally, it may be desirable to only stop training if performance stays above or below a given threshold or baseline. For example, if you have familiarity with the training of the model (e.g. learning curves) and know that once a validation loss of a given value is achieved that there is no point in continuing training. This can be specified by setting the ¡°baseline¡± argument. This might be more useful when fine tuning a model, after the initial wild fluctuations in the performance measure seen in the early stages of training a new model are past. The EarlyStopping callback will stop training once triggered, but the model at the end of training may not be the model with best performance on the validation dataset. An additional callback is required that will save the best model observed during training for later use. This is the ModelCheckpoint callback. The ModelCheckpoint callback is flexible in the way it can be used, but in this case we will use it only to save the best model observed during training as defined by a chosen performance measure on the validation dataset. Saving and loading models requires that HDF5 support has been installed on your workstation. For example, using the pip Python installer, this can be achieved as follows: You can learn more from the h5py Installation documentation. The callback will save the model to file, which requires that a path and filename be specified via the first argument. The preferred loss function to be monitored can be specified via the monitor argument, in the same way as the EarlyStopping callback. For example, loss on the validation dataset (the default). Also, as with the EarlyStopping callback, we must specify the ¡°mode¡± as either minimizing or maximizing the performance measure. Again, the default is ¡®auto,¡¯ which is aware of the standard performance measures. Finally, we are interested in only the very best model observed during training, rather than the best compared to the previous epoch, which might not be the best overall if training is noisy. This can be achieved by setting the ¡°save_best_only¡± argument to True. That is all that is needed to ensure the model with the best performance is saved when using early stopping, or in general. It may be interesting to know the value of the performance measure and at what epoch the model was saved. This can be printed by the callback by setting the ¡°verbose¡± argument to ¡°1¡°. The saved model can then be loaded and evaluated any time by calling the load_model() function. Now that we know how to use the early stopping and model checkpoint APIs, let¡¯s look at a worked example. In this section, we will demonstrate how to use early stopping to reduce overfitting of an MLP on a simple binary classification problem. This example provides a template for applying early stopping to your own neural network for classification and regression problems. We will use a standard binary classification problem that defines two semi-circles of observations, one semi-circle for each class. Each observation has two input variables with the same scale and a class output value of either 0 or 1. This dataset is called the ¡°moons¡± dataset because of the shape of the observations in each class when plotted. We can use the make_moons() function to generate observations from this problem. We will add noise to the data and seed the random number generator so that the same samples are generated each time the code is run. We can plot the dataset where the two variables are taken as x and y coordinates on a graph and the class value is taken as the color of the observation. The complete example of generating the dataset and plotting it is listed below. Running the example creates a scatter plot showing the semi-circle or moon shape of the observations in each class. We can see the noise in the dispersal of the points making the moons less obvious. Scatter Plot of Moons Dataset With Color Showing the Class Value of Each Sample This is a good test problem because the classes cannot be separated by a line, e.g. are not linearly separable, requiring a nonlinear method such as a neural network to address. We have only generated 100 samples, which is small for a neural network, providing the opportunity to overfit the training dataset and have higher error on the test dataset: a good case for using regularization. Further, the samples have noise, giving the model an opportunity to learn aspects of the samples that don¡¯t generalize. We can develop an MLP model to address this binary classification problem. The model will have one hidden layer with more nodes than may be required to solve this problem, providing an opportunity to overfit. We will also train the model for longer than is required to ensure the model overfits. Before we define the model, we will split the dataset into train and test sets, using 30 examples to train the model and 70 to evaluate the fit model¡¯s performance. Next, we can define the model. The hidden layer uses 500 nodes and the rectified linear activation function. A sigmoid activation function is used in the output layer in order to predict class values of 0 or 1. The model is optimized using the binary cross entropy loss function, suitable for binary classification problems and the efficient Adam version of gradient descent. The defined model is then fit on the training data for 4,000 epochs and the default batch size of 32. We will also use the test dataset as a validation dataset. This is just a simplification for this example. In practice, you would split the training set into train and validation and also hold back a test set for final model evaluation. We can evaluate the performance of the model on the test dataset and report the result. Finally, we will plot the loss of the model on both the train and test set each epoch. If the model does indeed overfit the training dataset, we would expect the line plot of loss (and accuracy) on the training set to continue to increase and the test set to rise and then fall again as the model learns statistical noise in the training dataset. We can tie all of these pieces together; the complete example is listed below. Running the example reports the model performance on the train and test datasets. We can see that the model has better performance on the training dataset than the test dataset, one possible sign of overfitting. Your specific results may vary given the stochastic nature of the neural network and the training algorithm. Because the model is severely overfit, we generally would not expect much, if any, variance in the accuracy across repeated runs of the model on the same dataset. A figure is created showing line plots of the model loss on the train and test sets. We can see that expected shape of an overfit model where test accuracy increases to a point and then begins to decrease again. Reviewing the figure, we can also see flat spots in the ups and downs in the validation loss. Any early stopping will have to account for these behaviors. We would also expect that a good time to stop training might be around epoch 800. Line Plots of Loss on Train and Test Datasets While Training Showing an Overfit Model We can update the example and add very simple early stopping. As soon as the loss of the model begins to increase on the test dataset, we will stop training. First, we can define the early stopping callback. We can then update the call to the fit() function and specify a list of callbacks via the ¡°callback¡± argument. The complete example with the addition of simple early stopping is listed below. Running the example reports the model performance on the train and test datasets. We can also see that the callback stopped training at epoch 200. This is too early as we would expect an early stop to be around epoch 800. This is also highlighted by the classification accuracy on both the train and test sets, which is worse than no early stopping. Reviewing the line plot of train and test loss, we can indeed see that training was stopped at the point when validation loss began to plateau for the first time. Line Plot of Train and Test Loss During Training With Simple Early Stopping We can improve the trigger for early stopping by waiting a while before stopping. This can be achieved by setting the ¡°patience¡± argument. In this case, we will wait 200 epochs before training is stopped. Specifically, this means that we will allow training to continue for up to an additional 200 epochs after the point that validation loss started to degrade, giving the training process an opportunity to get across flat spots or find some additional improvement. The complete example with this change is listed below. Running the example, we can see that training was stopped much later, in this case after epoch 1,000. Your specific results may differ given the stochastic nature of training neural networks. We can also see that the performance on the test dataset is better than not using any early stopping. Reviewing the line plot of loss during training, we can see that the patience allowed the training to progress past some small flat and bad spots. Line Plot of Train and Test Loss During Training With Patient Early Stopping We can also see that test loss started to increase again in the last approximately 100 epochs. This means that although the performance of the model has improved, we may not have the best performing or most stable model at the end of training. We can address this by using a ModelChecckpoint callback. In this case, we are interested in saving the model with the best accuracy on the test dataset. We could also seek the model with the best loss on the test dataset, but this may or may not correspond to the model with the best accuracy. This highlights an important concept in model selection. The notion of the ¡°best¡± model during training may conflict when evaluated using different performance measures. Try to choose models based on the metric by which they will be evaluated and presented in the domain. In a balanced binary classification problem, this will most likely be classification accuracy. Therefore, we will use accuracy on the validation in the ModelCheckpoint callback to save the best model observed during training. During training, the entire model will be saved to the file ¡°best_model.h5¡± only when accuracy on the validation dataset improves overall across the entire training process. A verbose output will also inform us as to the epoch and accuracy value each time the model is saved to the same file (e.g. overwritten). This new additional callback can be added to the list of callbacks when calling the fit() function. We are no longer interested in the line plot of loss during training; it will be much the same as the previous run. Instead, we want to load the saved model from file and evaluate its performance on the test dataset. The complete example with these changes is listed below. Running the example, we can see the verbose output from the ModelCheckpoint callback for both when a new best model is saved and from when no improvement was observed. We can see that the best model was observed at epoch 879 during this run. Your specific results may vary given the stochastic nature of training neural networks. Again, we can see that early stopping continued patiently until after epoch 1,000. Note that epoch 880 + a patience of 200 is not epoch 1044. Recall that early stopping is monitoring loss on the validation dataset and that the model checkpoint is saving models based on accuracy. As such, the patience of early stopping started at an epoch other than 880. In this case, we don¡¯t see any further improvement in model accuracy on the test dataset. Nevertheless, we have followed a good practice. Why not monitor validation accuracy for early stopping? This is a good question. The main reason is that accuracy is a coarse measure of model performance during training and that loss provides more nuance when using early stopping with classification problems. The same measure may be used for early stopping and model checkpointing in the case of regression, such as mean squared error. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered the Keras API for adding early stopping to overfit deep learning neural network models. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Better Deep Learning It provides self-study tutorials on topics like: weight decay, batch normalization, dropout, model stacking and much more¡¦ Skip the Academics. Just Results. Click to learn more. Thanks a lot, A very clear illustration as always Thanks, I¡¯m glad it helped. Comment  Name (required)  Email (will not be published) (required)  Website"
