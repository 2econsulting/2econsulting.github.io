"","site","date","headline","url_address","text","keyword"
"1","mastery",2018-12-28,"How to Develop a Weighted Average Ensemble for Deep Learning Neural Networks","https://machinelearningmastery.com/weighted-average-ensemble-for-deep-learning-neural-networks/","A modeling averaging ensemble combines the prediction from each model equally and often results in better performance on average than a given single model. Sometimes there are very good models that we wish to contribute more to an ensemble prediction, and perhaps less skillful models that may be useful but should contribute less to an ensemble prediction. A weighted average ensemble is an approach that allows multiple models to contribute to a prediction in proportion to their trust or estimated performance. In this tutorial, you will discover how to develop a weighted average ensemble of deep learning neural network models in Python with Keras. After completing this tutorial, you will know: Let¡¯s get started. How to Develop a Weighted Average Ensemble for Deep Learning Neural NetworksPhoto by Simon Matzinger, some rights reserved. This tutorial is divided into six parts; they are: Model averaging is an approach to ensemble learning where each ensemble member contributes an equal amount to the final prediction. In the case of regression, the ensemble prediction is calculated as the average of the member predictions. In the case of predicting a class label, the prediction is calculated as the mode of the member predictions. In the case of predicting a class probability, the prediction can be calculated as the argmax of the summed probabilities for each class label. A limitation of this approach is that each model has an equal contribution to the final prediction made by the ensemble. There is a requirement that all ensemble members have skill as compared to random chance, although some models are known to perform much better or much worse than other models. A weighted ensemble is an extension of a model averaging ensemble where the contribution of each member to the final prediction is weighted by the performance of the model. The model weights are small positive values and the sum of all weights equals one, allowing the weights to indicate the percentage of trust or expected performance from each model. One can think of the weight Wk as the belief in predictor k and we therefore constrain the weights to be positive and sum to one. <U+2014> Learning with ensembles: How over-fitting can be useful, 1996. Uniform values for the weights (e.g. 1/k where k is the number of ensemble members) means that the weighted ensemble acts as a simple averaging ensemble. There is no analytical solution to finding the weights (we cannot calculate them); instead, the value for the weights can be estimated using either the training dataset or a holdout validation dataset. Finding the weights using the same training set used to fit the ensemble members will likely result in an overfit model. A more robust approach is to use a holdout validation dataset unseen by the ensemble members during training. The simplest, perhaps most exhaustive approach would be to grid search weight values between 0 and 1 for each ensemble member. Alternately, an optimization procedure such as a linear solver or gradient descent optimization can be used to estimate the weights using a unit norm weight constraint to ensure that the vector of weights sum to one. Unless the holdout validation dataset is large and representative, a weighted ensemble has an opportunity to overfit as compared to a simple averaging ensemble. A simple alternative to adding more weight to a given model without calculating explicit weight coefficients is to add a given model more than once to the ensemble. Although less flexible, it allows a given well-performing model to contribute more than once to a given prediction made by the ensemble. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course We will use a small multi-class classification problem as the basis to demonstrate the weighted averaging ensemble. The scikit-learn class provides the make_blobs() function that can be used to create a multi-class classification problem with the prescribed number of samples, input variables, classes, and variance of samples within a class. The problem has two input variables (to represent the x and y coordinates of the points) and a standard deviation of 2.0 for points within each group. We will use the same random state (seed for the pseudorandom number generator) to ensure that we always get the same data points. The results are the input and output elements of a dataset that we can model. In order to get a feeling for the complexity of the problem, we can plot each point on a two-dimensional scatter plot and color each point by class value. The complete example is listed below. Running the example creates a scatter plot of the entire dataset. We can see that the standard deviation of 2.0 means that the classes are not linearly separable (separable by a line) causing many ambiguous points. This is desirable as it means that the problem is non-trivial and will allow a neural network model to find many different ¡°good enough¡± candidate solutions resulting in a high variance. Scatter Plot of Blobs Dataset With Three Classes and Points Colored by Class Value Before we define a model, we need to contrive a problem that is appropriate for the weighted average ensemble. In our problem, the training dataset is relatively small. Specifically, there is a 10:1 ratio of examples in the training dataset to the holdout dataset. This mimics a situation where we may have a vast number of unlabeled examples and a small number of labeled examples with which to train a model. We will create 1,100 data points from the blobs problem. The model will be trained on the first 100 points and the remaining 1,000 will be held back in a test dataset, unavailable to the model. The problem is a multi-class classification problem, and we will model it using a softmax activation function on the output layer. This means that the model will predict a vector with three elements with the probability that the sample belongs to each of the three classes. Therefore, we must one hot encode the class values before we split the rows into the train and test datasets. We can do this using the Keras to_categorical() function. Next, we can define and compile the model. The model will expect samples with two input variables. The model then has a single hidden layer with 25 nodes and a rectified linear activation function, then an output layer with three nodes to predict the probability of each of the three classes and a softmax activation function. Because the problem is multi-class, we will use the categorical cross entropy loss function to optimize the model and the efficient Adam flavor of stochastic gradient descent. The model is fit for 500 training epochs and we will evaluate the model each epoch on the test set, using the test set as a validation set. At the end of the run, we will evaluate the performance of the model on the train and test sets. Then finally, we will plot learning curves of the model accuracy over each training epoch on both the training and validation datasets. Tying all of this together, the complete example is listed below. Running the example first prints the shape of each dataset for confirmation, then the performance of the final model on the train and test datasets. Your specific results will vary (by design!) given the high variance nature of the model. In this case, we can see that the model achieved about 87% accuracy on the training dataset, which we know is optimistic, and about 81% on the test dataset, which we would expect to be more realistic. A line plot is also created showing the learning curves for the model accuracy on the train and test sets over each training epoch. We can see that training accuracy is more optimistic over most of the run as we also noted with the final scores. Line Plot Learning Curves of Model Accuracy on Train and Test Dataset over Each Training Epoch Now that we have identified that the model is a good candidate for developing an ensemble, we can next look at developing a simple model averaging ensemble. We can develop a simple model averaging ensemble before we look at developing a weighted average ensemble. The results of the model averaging ensemble can be used as a point of comparison as we would expect a well configured weighted average ensemble to perform better. First, we need to fit multiple models from which to develop an ensemble. We will define a function named fit_model() to create and fit a single model on the training dataset that we can call repeatedly to create as many models as we wish. We can call this function to create a pool of 10 models. Next, we can develop model averaging ensemble. We don¡¯t know how many members would be appropriate for this problem, so we can create ensembles with different sizes from one to 10 members and evaluate the performance of each on the test set. We can also evaluate the performance of each standalone model in the performance on the test set. This provides a useful point of comparison for the model averaging ensemble, as we expect that the ensemble will out-perform a randomly selected single model on average. Each model predicts the probabilities for each class label, e.g. has three outputs. A single prediction can be converted to a class label by using the argmax() function on the predicted probabilities, e.g. return the index in the prediction with the largest probability value. We can ensemble the predictions from multiple models by summing the probabilities for each class prediction and using the argmax() on the result. The ensemble_predictions() function below implements this behavior. We can estimate the performance of an ensemble of a given size by selecting the required number of models from the list of all models, calling the ensemble_predictions() function to make a prediction, then calculating the accuracy of the prediction by comparing it to the true values. The evaluate_n_members() function below implements this behavior. The scores of the ensembles of each size can be stored to be plotted later, and the scores for each individual model are collected and the average performance reported. Finally, we create a graph that shows the accuracy of each individual model (blue dots) and the performance of the model averaging ensemble as the number of members is increased from one to 10 members (orange line). Tying all of this together, the complete example is listed below. Running the example first reports the performance of each single model as well as the model averaging ensemble of a given size with 1, 2, 3, etc. members. Your results will vary given the stochastic nature of the training algorithm. On this run, the average performance of the single models is reported at about 80.4% and we can see that an ensemble with between five and nine members will achieve a performance between 80.8% and 81%. As expected, the performance of a modest-sized model averaging ensemble out-performs the performance of a randomly selected single model on average. Next, a graph is created comparing the accuracy of single models (blue dots) to the model averaging ensemble of increasing size (orange line). On this run, the orange line of the ensembles clearly shows better or comparable performance (if dots are hidden) than the single models. Line Plot Showing Single Model Accuracy (blue dots) and Accuracy of Ensembles of Increasing Size (orange line) Now that we know how to develop a model averaging ensemble, we can extend the approach one step further by weighting the contributions of the ensemble members. The model averaging ensemble allows each ensemble member to contribute an equal amount to the prediction of the ensemble. We can update the example so that instead, the contribution of each ensemble member is weighted by a coefficient that indicates the trust or expected performance of the model. Weight values are small values between 0 and 1 and are treated like a percentage, such that the weights across all ensemble members sum to one. First, we must update the ensemble_predictions() function to make use of a vector of weights for each ensemble member. Instead of simply summing the predictions across each ensemble member, we must calculate a weighted sum. We can implement this manually using for loops, but this is terribly inefficient; for example: Instead, we can use efficient NumPy functions to implement the weighted sum such as einsum() or tensordot(). Full discussion of these functions is a little out of scope so please refer to the API documentation for more information on how to use these functions as they are challenging if you are new to linear algebra and/or NumPy. We will use tensordot() function to apply the tensor product with the required summing; the updated ensemble_predictions() function is listed below. Next, we must update evaluate_ensemble() to pass along the weights when making the prediction for the ensemble. We will use a modest-sized ensemble of five members, that appeared to perform well in the model averaging ensemble. We can then estimate the performance of each individual model on the test dataset as a reference. Next, we can use a weight of 1/5 or 0.2 for each of the five ensemble members and use the new functions to estimate the performance of a model averaging ensemble, a so-called equal-weight ensemble. We would expect this ensemble to perform as well or better than any single model. Finally, we can develop a weighted average ensemble. A simple, but exhaustive approach to finding weights for the ensemble members is to grid search values. We can define a course grid of weight values from 0.0 to 1.0 in steps of 0.1, then generate all possible five-element vectors with those values. Generating all possible combinations is called a Cartesian product, which can be implemented in Python using the itertools.product() function from the standard library. A limitation of this approach is that the vectors of weights will not sum to one (called the unit norm), as required. We can force reach generated weight vector to have a unit norm by calculating the sum of the absolute weight values (called the L1 norm) and dividing each weight by that value. The normalize() function below implements this hack. We can now enumerate each weight vector generated by the Cartesian product, normalize it, and evaluate it by making a prediction and keeping the best to be used in our final weight averaging ensemble. Once discovered, we can report the performance of our weight average ensemble on the test dataset, which we would expect to be better than the best single model and ideally better than the model averaging ensemble. The complete example is listed below. Running the example first creates the five single models and evaluates their performance on the test dataset. Your specific results will vary given the stochastic nature of the learning algorithm. On this run, we can see that model 2 has the best solo performance of about 81.7% accuracy. Next, a model averaging ensemble is created with a performance of about 80.7%, which is reasonable compared to most of the models, but not all. Next, the grid search is performed. It is pretty slow and may take about twenty minutes on modern hardware. The process could easily be made parallel using libraries such as Joblib. Each time a new top performing set of weights is discovered, it is reported along with its performance on the test dataset. We can see that during the run, the process discovered that using model 2 alone resulted in a good performance, until it was replaced with something better. We can see that the best performance was achieved on this run using the weights that focus only on the first and second models with the accuracy of 81.8% on the test dataset. This out-performs both the single models and the model averaging ensemble on the same dataset. An alternate approach to finding weights would be a random search, which has been shown to be effective more generally for model hyperparameter tuning. An alternative to searching for weight values is to use a directed optimization process. Optimization is a search process, but instead of sampling the space of possible solutions randomly or exhaustively, the search process uses any available information to make the next step in the search, such as toward a set of weights that has lower error. The SciPy library offers many excellent optimization algorithms, including local and global search methods. SciPy provides an implementation of the Differential Evolution method. This is one of the few stochastic global search algorithms that ¡°just works¡± for function optimization with continuous inputs, and it works well. The differential_evolution() SciPy function requires that function is specified to evaluate a set of weights and return a score to be minimized. We can minimize the classification error (1 <U+2013> accuracy). As with the grid search, we most normalize the weight vector before we evaluate it. The loss_function() function below will be used as the evaluation function during the optimization process. We must also specify the bounds of the optimization process. We can define the bounds as a five-dimensional hypercube (e.g. 5 weights for the 5 ensemble members) with values between 0.0 and 1.0. Our loss function requires three parameters in addition to the weights, which we will provide as a tuple to then be passed along to the call to the<U+00A0>loss_function() each time a set of weights is evaluated. We can now call our optimization process. We will limit the total number of iterations of the algorithms to 1,000, and use a smaller than default tolerance to detect if the search process has converged. The result of the call to differential_evolution() is a dictionary that contains all kinds of information about the search. Importantly, the ¡®x¡® key contains the optimal set of weights found during the search. We can retrieve the best set of weights, then report them and their performance on the test set when used in a weighted ensemble. Tying all of this together, the complete example is listed below. Running the example first creates five single models and evaluates the performance of each on the test dataset. Your specific results will vary given the stochastic nature of the learning algorithm. We can see on this run that models 3 and 4 both perform best with an accuracy of about 82.2%. Next, a model averaging ensemble with all five members is evaluated on the test set reporting an accuracy of 81.8%, which is better than some, but not all, single models. The optimization process is relatively quick. We can see that the process found a set of weights that pays most attention to models 3 and 4, and spreads the remaining attention out among the other models, achieving an accuracy of about 82.4%, out-performing the model averaging ensemble and individual models. It is important to note that in these examples, we have treated the test dataset as though it were a validation dataset. This was done to keep the examples focused and technically simpler. In practice, the choice and tuning of the weights for the ensemble would be chosen by a validation dataset, and single models, model averaging ensembles, and weighted ensembles would be compared on a separate test set. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop a weighted average ensemble of deep learning neural network models in Python with Keras. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Better Deep Learning It provides self-study tutorials on topics like: weight decay, batch normalization, dropout, model stacking and much more¡¦ Skip the Academics. Just Results. Click to learn more. Great article Jason! Thank you.. Additionally using models whose error terms are not correlated yield better results..  One query: some suggest giving weights inversely proportional to RMSE or directly proportional to accuracy measures. Do you find weights derived from this method are similar to the weights derived from grid search? Or do they differ? Thanks in advance Jason. I prefer to use a global optimization algorithm to find robust weights. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): weight(31), model(28), value(12), result(9), ensemble(8), point(7), example(5), dot(4), function(4), prediction(4)"
"2","mastery",2018-12-26,"How to Reduce Variance in the Final Deep Learning Model With a Horizontal Voting Ensemble","https://machinelearningmastery.com/horizontal-voting-ensemble/","Predictive modeling problems where the training dataset is small relative to the number of unlabeled examples are challenging. Neural networks can perform well on these types of problems, although they can suffer from high variance in model performance as measured on a training or hold-out validation datasets. This makes choosing which model to use as the final model risky, as there is no clear signal as to which model is better than another toward the end of the training run. The horizontal voting ensemble is a simple method to address this issue, where a collection of models saved over contiguous training epochs towards the end of a training run are saved and used as an ensemble that results in more stable and better performance on average than randomly choosing a single final model. In this tutorial, you will discover how to reduce the variance of a final deep learning neural network model using a horizontal voting ensemble. After completing this tutorial, you will know: Let¡¯s get started. How to Reduce Variance in the Final Deep Learning Model With a Horizontal Voting EnsemblePhoto by Fatima Flores, some rights reserved. This tutorial is divided into five parts; they are: Ensemble learning combines the predictions from multiple models. A challenge when using ensemble learning when using deep learning methods is that given the use of very large datasets and large models, a given training run may take days, weeks, or even months. Training multiple models may not be feasible. An alternative source of models that may contribute to an ensemble are the state of a single model at different points during training. Horizontal voting is an ensemble method proposed by Jingjing Xie, et al. in their 2013 paper ¡°Horizontal and Vertical Ensemble with Deep Representation for Classification.¡± The method involves using multiple models from the end of a contiguous block of epochs before the end of training in an ensemble to make predictions. The approach was developed specifically for those predictive modeling problems where the training dataset is relatively small compared to the number of predictions required by the model. This results in a model that has a high variance in performance during training. In this situation, using the final model or any given model toward the end of the training process is risky given the variance in performance. ¡¦ the error rate of classification would first decline and then tend to be stable with the training epoch grows. But when size of labeled training set is too small, the error rate would oscillate [¡¦] So it is difficult to choose a ¡°magic¡± epoch to obtain a reliable output. <U+2014> Horizontal and Vertical Ensemble with Deep Representation for Classification, 2013. Instead, the authors suggest using all of the models in an ensemble from a contiguous block of epochs during training, such as models from the last 200 epochs. The result are predictions by the ensemble that are as good as or better than any single model in the ensemble. To reduce the instability, we put forward a method called Horizontal Voting. First, networks trained for a relatively stable range of epoch are selected. The predictions of the probability of each label are produced by standard classifiers with top level representation of the selected epoch, and then averaged. <U+2014> Horizontal and Vertical Ensemble with Deep Representation for Classification, 2013. As such, the horizontal voting ensemble method provides an ideal method for both cases where a given model requires vast computational resources to train, and/or cases where final model selection is challenging given the high variance of training due to the use of a relatively small training dataset. Now that are we are familiar with horizontal voting, we can implement the procedure. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course We will use a small multi-class classification problem as the basis to demonstrate a horizontal voting ensemble. The scikit-learn class provides the make_blobs() function that can be used to create a multi-class classification problem with the prescribed number of samples, input variables, classes, and variance of samples within a class. The problem has two input variables (to represent the x and y coordinates of the points) and a standard deviation of 2.0 for points within each group. We will use the same random state (seed for the pseudorandom number generator) to ensure that we always get the same data points. The results are the input and output elements of a dataset that we can model. In order to get a feeling for the complexity of the problem, we can graph each point on a two-dimensional scatter plot and color each point by class value. The complete example is listed below. Running the example creates a scatter plot of the entire dataset. We can see that the standard deviation of 2.0 means that the classes are not linearly separable (can be separated by a line) causing many ambiguous points. This is desirable as it means that the problem is non-trivial and will allow a neural network model to find many different ¡°good enough¡± candidate solutions resulting in a high variance. Scatter Plot of Blobs Dataset With Three Classes and Points Colored by Class Value Before we define a model, we need to contrive a problem that is appropriate for a horizontal voting ensemble. In our problem, the training dataset is relatively small. Specifically, there is a 10:1 ratio of examples in the training dataset to the holdout dataset. This mimics a situation where we may have a vast number of unlabeled examples and a small number of labeled examples with which to train a model. We will create 1,100 data points from the blobs problem. The model will be trained on the first 100 points and the remaining 1,000 will be held back in a test dataset, unavailable to the model. The problem is a multi-class classification problem, and we will model it using a softmax activation function on the output layer. This means that the model will predict a vector with three elements with the probability that the sample belongs to each of the three classes. Therefore, we must one hot encode the class values, ideally before we split the rows into the train, test, and validation datasets so that it is a single function call. Next, we can define and combine the model. The model will expect samples with two input variables. The model then has a single hidden layer with 25 nodes and a rectified linear activation function, then an output layer with three nodes to predict the probability of each of the three classes and a softmax activation function. Because the problem is multi-class, we will use the categorical cross entropy loss function to optimize the model and the efficient Adam flavor of stochastic gradient descent. The model is fit for 1,000 training epochs and we will evaluate the model each epoch on the training set, using the test set as a validation set. At the end of the run, we will evaluate the performance of the model on the train, validation, and test sets. Then finally, we will plot learning curves of the model accuracy over each training epoch on both the training and validation datasets. Tying all of this together, the complete example is listed below. Running the example first prints the shape of each dataset for confirmation, then the performance of the final model on the train and test datasets. Your specific results will vary (by design!) given the high variance nature of the model. In this case, we can see that the model achieved about 85% accuracy on the training dataset that we know is optimistic, and about 80% on the test dataset, which we would expect to be more realistic. A line plot is also created showing the learning curves for the model accuracy on the train and test sets over each training epoch. We can see that training accuracy is more optimistic over the whole run as we also noted with the final scores. We can see that the accuracy of the model has high variance on the training dataset as compared to the test set, as we would expect. The variance in the model highlights the fact that choosing the model at the end of the run or any model from about epoch 800 is challenging as accuracy on the training dataset has a high variance. We also see a muted version of the variance on the test dataset. Line Plot Learning Curves of Model Accuracy on Train and Test Dataset over Each Training Epoch Now that we have identified that the model is a good candidate for a horizontal voting ensemble, we can begin to implement the technique. There may be many ways to implement a horizontal voting ensemble. Perhaps the simplest is to manually drive the training process, one epoch at a time, then save models at the end of the epoch if we have exceeded an upper limit on the number of epochs. For example, with our test problem, we will train the model for 1,000 epochs and perhaps save models from epoch 950 onwards (e.g. between and including epochs 950 and 999). Models can be saved to file using the save() function on the model and specifying a filename that includes the epoch number. To avoid clutter with our source files, we will save all models under a new ¡°models/¡± folder in the current working directory. Note, saving and loading neural network models in Keras requires that you have the h5py library installed. You can install this library using pip as follows: Tying all of this together, the complete example of fitting the model on the training dataset and saving all models from the last 50 epochs is listed below. Running the example creates the ¡°models/¡± folder and saves 50 models into the directory. Note, to re-run this example, you must delete the ¡°models/¡± directory so that the script can recreate it. Now that we have created the models, we can use them in a horizontal voting ensemble. First, we need to load the models into memory. This is reasonable as the models are small. If you are trying to develop a horizontal voting ensemble with very large models, it might be easier to load models one at a time, make a prediction, then load the next model and repeat the process. The function load_all_models() below will load models from the ¡°models/¡± directory. It takes the start and end epochs as arguments so that you can experiment with different groups of models saved over contiguous epochs. We can call the function to load all of the models. We can then reverse the list of models so that the models at the end of the run are at the beginning of the list. This will be helpful later when we test voting ensembles of different sizes, including models sequentially from the end of the run backward through training epochs, in case the best models really were at the end of the run. Next, we can evaluate each saved model on the test dataset, as well as a voting ensemble of the last n contiguous models from training. We want to know how well each model actually performed on the test dataset and, importantly, the distribution of model performance on the test dataset, so that we know how well (or poorly) an average model chosen from the end of the run would perform in practice. We don¡¯t know how many members to include in the horizontal voting ensemble. Therefore, we can test different numbers of contiguous members, working backward from the final model. First, we need a function to make a prediction with a list of ensemble members. Each member predicts the probabilities for each of the three output classes. The probabilities are added and we use an argmax to select the class with the most support. The ensemble_predictions() function below implements this voting based prediction scheme. Next, we need a function to evaluate a subset of the ensemble members of a given size. The subset needs to be selected, predictions made, and the performance of the ensemble estimated by comparing the predictions to the expected values. The evaluate_n_members() function below implements this ensemble size evaluation. We can now enumerate through different sized horizontal voting ensembles from 1 to 50. Each member is evaluated alone, then the ensemble of that size is evaluated and scores are recorded. At the end of the evaluations, we report the distribution of scores of single models on the test dataset. The average score is what we would expect on average if we picked any of the saved models as a final model. Finally, we can plot the scores. The scores of each standalone model are plotted as blue dots and line plot is created for each ensemble of contiguous models (orange). Our expectation is that a fair sized ensemble will outperform a randomly selected model and that there is a point of diminishing returns in choosing the ensemble size. The complete example is listed below. First, the 50 saved models are loaded into memory. Next, the performance of each single model is evaluated on the holdout test dataset, and the ensemble of that size (1, 2, 3, etc.) is created and evaluated on the holdout test dataset. Roughly, we can see that the ensemble appears to outperform most single models, consistently achieving accuracy around 81.8%. Next, the distribution of the accuracy of single models is reported. We can see that picking any of the saved models at random would result in a model with the accuracy of 81.6% on average with a reasonably tight standard deviation of 0.3%. Your specific results may vary given the stochastic nature of the algorithm, but the general behavior should be the same. We would require that a horizontal ensemble out-perform this average in order to be useful. Finally, a graph is created summarizing the performance of each single model (blue dot) and the ensemble of each size from 1 to 50 members. We can see from the blue dots that there is no structure to the models over the epochs, e.g. if the last models during training were better, there would be a downward trend in accuracy from left to right. We can see that as we add more ensemble members, the better the performance of the horizontal voting ensemble in the orange line. We can see a flattening of performance on this problem perhaps between 23 and 33 epochs; that might be a good choice. Line Plot Showing Single Model Accuracy (blue dots) vs Accuracy of Ensembles of Varying Size With a Horizontal Voting Ensemble This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to reduce the variance of a final deep learning neural network model using a horizontal voting ensemble. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Better Deep Learning It provides self-study tutorials on topics like: weight decay, batch normalization, dropout, model stacking and much more¡¦ Skip the Academics. Just Results. Click to learn more. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): model(41), epoch(14), point(8), prediction(7), dataset(5), score(5), example(4), result(4), curve(3), dot(3)"
"3","mastery",2018-12-24,"How to Create a Random-Split, Cross-Validation, and Bagging Ensemble for Deep Learning in Keras","https://machinelearningmastery.com/how-to-create-a-random-split-cross-validation-and-bagging-ensemble-for-deep-learning-in-keras/","Ensemble learning are methods that combine the predictions from multiple models. It is important in ensemble learning that the models that comprise the ensemble are good, making different prediction errors. Predictions that are good in different ways can result in a prediction that is both more stable and often better than the predictions of any individual member model. One way to achieve differences between models is to train each model on a different subset of the available training data. Models are trained on different subsets of the training data naturally through the use of resampling methods such as cross-validation and the bootstrap, designed to estimate the average performance of the model generally on unseen data. The models used in this estimation process can be combined in what is referred to as a resampling-based ensemble, such as a cross-validation ensemble or a bootstrap aggregation (or bagging) ensemble. In this tutorial, you will discover how to develop a suite of different resampling-based ensembles for deep learning neural network models. After completing this tutorial, you will know: Let¡¯s get started. How to Create a Random-Split, Cross-Validation, and Bagging Ensembles for Deep Learning in KerasPhoto by Gian Luca Ponti, some rights reserved. This tutorial is divided into six parts; they are: Combining the predictions from multiple models can result in more stable predictions, and in some cases, predictions that have better performance than any of the contributing models. Effective ensembles require members that disagree. Each member must have skill (e.g. perform better than random chance), but ideally, perform well in different ways. Technically, we can say that we prefer ensemble members to have low correlation in their predictions, or prediction errors. One approach to encourage differences between ensembles is to use the same learning algorithm on different training datasets. This can be achieved by repeatedly resampling a training dataset that is in turn used to train a new model. Multiple models are fit using slightly different perspectives on the training data and, in turn, make different errors and often more stable and better predictions when combined. We can refer to these methods generally as data resampling ensembles. A benefit of this approach is that resampling methods may be used that do not make use of all examples in the training dataset. Any examples that are not used to fit the model can be used as a test dataset to estimate the generalization error of the chosen model configuration. There are three popular resampling methods that we could use to create a resampling ensemble; they are: Perhaps the most widely used resampling ensemble method is bootstrap aggregation, more commonly referred to as bagging. The resampling with replacement allows more difference in the training dataset, biasing the model and, in turn, resulting in more difference between the predictions of the resulting models. Resampling ensemble models makes some specific assumptions about your project: Neural network models are remarkably flexible, therefore the lift in performance provided by a resampling ensemble is not always possible given that individual models trained on all available data can perform so well. As such, the sweet spot for using a resampling ensemble is the case where there is a requirement for a robust estimate of performance and multiple models can be fit to calculate the estimate, but there is also a requirement for one (or more) of the models created during the estimate of performance to be used as the final model (e.g. a new final model cannot be fit on all available training data). Now that we are familiar with resampling ensemble methods, we can work through an example of applying each method in turn. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course We will use a small multi-class classification problem as the basis to demonstrate a model resampling ensembles. The scikit-learn class provides the make_blobs() function that can be used to create a multi-class classification problem with the prescribed number of samples, input variables, classes, and variance of samples within a class. We use this problem with 1,000 examples, with input variables (to represent the x and y coordinates of the points) and a standard deviation of 2.0 for points within each group. We will use the same random state (seed for the pseudorandom number generator) to ensure that we always get the same 1,000 points. The results are the input and output elements of a dataset that we can model. In order to get a feeling for the complexity of the problem, we can plot each point on a two-dimensional scatter plot and color each point by class value. The complete example is listed below. Running the example creates a scatter plot of the entire dataset. We can see that the standard deviation of 2.0 means that the classes are not linearly separable (separable by a line) causing many ambiguous points. This is desirable as it means that the problem is non-trivial and will allow a neural network model to find many different ¡°good enough¡± candidate solutions resulting in a high variance. Scatter Plot of Blobs Dataset with Three Classes and Points Colored by Class Value We will define a Multilayer Perceptron neural network, or MLP, that learns the problem reasonably well. The problem is a multi-class classification problem, and we will model it using a softmax activation function on the output layer. This means that the model will predict a vector with 3 elements with the probability that the sample belongs to each of the 3 classes. Therefore, the first step is to one hot encode the class values. Next, we must split the dataset into training and test sets. We will use the test set both to evaluate the performance of the model and to plot its performance during training with a learning curve. We will use 90% of the data for training and 10% for the test set. We are choosing a large split because it is a noisy problem and a well-performing model requires as much data as possible to learn the complex classification function. Next, we can define and combine the model. The model will expect samples with two input variables. The model then has a single hidden layer with 50 nodes and a rectified linear activation function, then an output layer with 3 nodes to predict the probability of each of the 3 classes, and a softmax activation function. Because the problem is multi-class, we will use the categorical cross entropy loss function to optimize the model and the efficient Adam flavor of stochastic gradient descent. The model is fit for 50 training epochs and we will evaluate the model each epoch on the test set, using the test set as a validation set. At the end of the run, we will evaluate the performance of the model on both the train and the test sets. Then finally, we will plot learning curves of the model accuracy over each training epoch on both the training and test datasets. The complete example is listed below. Running the example first prints the performance of the final model on the train and test datasets. Your specific results will vary (by design!) given the high variance nature of the model. In this case, we can see that the model achieved about 83% accuracy on the training dataset and about 86% accuracy on the test dataset. The chosen split of the dataset into train and test sets means that the test set is small and not representative of the broader problem. In turn, performance on the test set is not representative of the model; in this case, it is optimistically biased. A line plot is also created showing the learning curves for the model accuracy on the train and test sets over each training epoch. We can see that the model has a reasonably stable fit. Line Plot Learning Curves of Model Accuracy on Train and Test Dataset Over Each Training Epoch The instability of the model and the small test dataset mean that we don¡¯t really know how well this model will perform on new data in general. We can try a simple resampling method of repeatedly generating new random splits of the dataset in train and test sets and fit new models. Calculating the average of the performance of the model across each split will give a better estimate of the model¡¯s generalization error. We can then combine multiple models trained on the random splits with the expectation that performance of the ensemble is likely to be more stable and better than the average single model. We will generate 10 times more sample points from the problem domain and hold them back as an unseen dataset. The evaluation of a model on this much larger dataset will be used as a proxy or a much more accurate estimate of the generalization error of a model for this problem. This extra dataset is not a test dataset. Technically, it is for the purposes of this demonstration, but we are pretending that this data is unavailable at model training time. So now we have 5,000 examples to train our model and estimate its general performance. We also have 50,000 examples that we can use to better approximate the true general performance of a single model or an ensemble. Next, we need a function to fit and evaluate a single model on a training dataset and return the performance of the fit model on a test dataset. We also need the model that was fit so that we can use it as part of an ensemble. The evaluate_model() function below implements this behavior. Next, we can create random splits of the training dataset and fit and evaluate models on each split. We can use the train_test_split() function from the scikit-learn library to create a random split of a dataset into train and test sets. It takes the X and y arrays as arguments and the ¡°test_size¡± specifies the size of the test dataset in terms of a percentage. We will use 10% of the 5,000 examples as the test. We can then call the evaluate_model() to fit and evaluate a model. The returned accuracy and model can then be added to lists for later use. In this example, we will limit the number of splits, and in turn, the number of fit models to 10. After fitting and evaluating the models, we can estimate the expected performance of a given model with the chosen configuration for the domain. We don¡¯t know how many of the models will be useful in the ensemble. It is likely that there will be a point of diminishing returns, after which the addition of further members no longer changes the performance of the ensemble. Nevertheless, we can evaluate different ensemble sizes from 1 to 10 and plot their performance on the unseen holdout dataset. We can also evaluate each model on the holdout dataset and calculate the average of these scores to get a much better approximation of the true performance of the chosen model on the prediction problem. Finally, we can compare and calculate a more robust estimate of the general performance of an average model on the prediction problem, then plot the performance of the ensemble size to accuracy on the holdout dataset. Tying all of this together, the complete example is listed below. Running the example first fits and evaluates 10 models on 10 different random splits of the dataset into train and test sets. From these scores, we estimate that the average model fit on the dataset will achieve an accuracy of about 83% with a standard deviation of about 1.9%. We then evaluate the performance of each model on the unseen dataset and the performance of ensembles of models from 1 to 10 models. From these scores, we can see that a more accurate estimate of the performance of an average model on this problem is about 82% and that the estimated performance is optimistic. A lot of the difference between the accuracy scores is happening in the fractions of percent. A graph is created showing the accuracy of each individual model on the unseen holdout dataset as blue dots and the performance of an ensemble with a given number of members from 1-10 as an orange line and dots. We can see that using an ensemble of 4-to-8 members, at least in this case, results in an accuracy that is better than most of the individual runs (orange line is above many blue dots). Line Plot Showing Single Model Accuracy (blue dots) vs Accuracy of Ensembles of Varying Size for Random-Split Resampling The graph does show some individual models can perform better than an ensemble of models (blue dots above the orange line), but we are unable to choose these models. Here, we demonstrate that without additional data (e.g. the out of sample dataset) that an ensemble of 4-to-8 members will give better on average performance than a randomly selected train-test model. More repeats (e.g. 30 or 100) may result in a more stable ensemble performance. A problem with repeated random splits as a resampling method for estimating the average performance of model is that it is optimistic. An approach designed to be less optimistic and is widely used as a result is the k-fold cross-validation method. The method is less biased because each example in the dataset is only used one time in the test dataset to estimate model performance, unlike random train-test splits where a given example may be used to evaluate a model many times. The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. The average of the scores of each model provides a less biased estimate of model performance. A typical value for k is 10. Because neural network models are computationally very expensive to train, it is common to use the best performing model during cross-validation as the final model. Alternately, the resulting models from the cross-validation process can be combined to provide a cross-validation ensemble that is likely to have better performance on average than a given single model. We can use the KFold class from scikit-learn to split the dataset into k folds. It takes as arguments the number of splits, whether or not to shuffle the sample, and the seed for the pseudorandom number generator used prior to the shuffle. Once the class is instantiated, it can be enumerated to get each split of indexes into the dataset for the train and test sets. Once the scores are calculated on each fold, the average of the scores can be used to report the expected performance of the approach. Now that we have collected the 10 models evaluated on the 10 folds, we can use them to create a cross-validation ensemble. It seems intuitive to use all 10 models in the ensemble, nevertheless, we can evaluate the accuracy of each subset of ensembles from 1 to 10 members as we did in the previous section. The complete example of analyzing the cross-validation ensemble is listed below. Running the example first prints the performance of each of the 10 models on each of the folds of the cross-validation. The average performance of these models is reported as about 82%, which appears to be less optimistic than the random-splits approach used in the previous section. Next, each of the saved models is evaluated on the unseen holdout set. The average of these scores is also about 82%, highlighting that, at least in this case, the cross-validation estimation of the general performance of the model was reasonable. A graph of single model accuracy (blue dots) and ensemble size vs accuracy (orange line) is created. As in the previous example, the real difference between the performance of the models is in the fractions of percent in model accuracy. The orange line shows that as the number of members increases, the accuracy of the ensemble increases to a point of diminishing returns. We can see that, at least in this case, using four or more of the models fit during cross-validation in an ensemble gives better performance than almost all individual models. We can also see that a default strategy of using all models in the ensemble would be effective. Line Plot Showing Single Model Accuracy (blue dots) vs Accuracy of Ensembles of Varying Size for Cross-Validation Resampling A limitation of random splits and k-fold cross-validation from the perspective of ensemble learning is that the models are very similar. The bootstrap method is a statistical technique for estimating quantities about a population by averaging estimates from multiple small data samples. Importantly, samples are constructed by drawing observations from a large data sample one at a time and returning them to the data sample after they have been chosen. This allows a given observation to be included in a given small sample more than once. This approach to sampling is called sampling with replacement. The method can be used to estimate the performance of neural network models. Examples not selected in a given sample can be used as a test set to estimate the performance of the model. The bootstrap is a robust method for estimating model performance. It does suffer a little from an optimistic bias, but is often almost as accurate as k-fold cross-validation in practice. The benefit for ensemble learning is that each model is that each data sample is biased, allowing a given example to appear many times in the sample. This, in turn, means that the models trained on those samples will be biased, importantly in different ways. The result can be ensemble predictions that can be more accurate. Generally, use of the bootstrap method in ensemble learning is referred to as bootstrap aggregation or bagging. We can use the resample() function from scikit-learn to select a subsample with replacement. The function takes an array to subsample and the size of the resample as arguments. We will perform the selection in rows indices that we can in turn use to select rows in the X and y arrays. The size of the sample will be 4,500, or 90% of the data, although the test set may be larger than 10% as given the use of resampling, more than 500 examples may have been left unselected. It is common to use simple overfit models like unpruned decision trees when using a bagging ensemble learning strategy. Better performance may be seen with over-constrained and overfit neural networks. Nevertheless, we will use the same MLP from previous sections in this example. Additionally, it is common to continue to add ensemble members in bagging until the performance of the ensemble plateaus, as bagging does not overfit the dataset. We will again limit the number of members to 10 as in previous examples. The complete example of bootstrap aggregation for estimating model performance and ensemble learning with a Multilayer Perceptron is listed below. Running the example prints the model performance on the unused examples for each bootstrap sample. We can see that, in this case, the expected performance of the model is less optimistic than random train-test splits and is perhaps quite similar to the finding for k-fold cross-validation. Perhaps due to the bootstrap sampling procedure, we see that the actual performance of each model is a little worse on the much larger unseen holdout dataset. This is to be expected given the bias introduced by the sampling with replacement of the bootstrap. The created line plot is encouraging. We see that after about four members that the bagged ensemble achieves better performance on the holdout dataset than any individual model. No doubt, given the slightly lower average performance of individual models. Line Plot Showing Single Model Accuracy (blue dots) vs Accuracy of Ensembles of Varying Size for Bagging This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop a suite of different resampling-based ensembles for deep learning neural network models. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Better Deep Learning It provides self-study tutorials on topics like: weight decay, batch normalization, dropout, model stacking and much more¡¦ Skip the Academics. Just Results. Click to learn more. Dear Sir, Thanks so much for your tutorials. They have helped me a lot and i respect your work. One question for the above tutorial: Is it possible to use the Ensemble methods mentioned above with CNN, RNN and not MLP. thanks Yes! Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): model(45), ensemble(10), example(10), prediction(10), split(10), dot(8), score(8), set(8), method(7), point(6)"
"4","vidhya",2018-12-26,"The 25 Best Data Science Projects on GitHub from 2018 that you Should Not Miss","https://www.analyticsvidhya.com/blog/2018/12/best-data-science-machine-learning-projects-github/","

New Year's Grand Sale - 40% Discount On All Courses (Use Coupon: HNY2019) Click To Enroll Today !

 What¡¯s the best platform for hosting your code, collaborating with team members, and also acts as an online resume to showcase your coding skills? Ask any data scientist, and they¡¯ll point you towards GitHub. It has been a truly revolutionary platform in recent years and has changed the landscape of how we host and even do coding. But that¡¯s not all. It acts as a learning tool as well. How, you ask? I¡¯ll give you a hint <U+2013> open source! The world¡¯s leading tech companies open source their projects on GitHub by releasing the code behind their popular algorithms. 2018 saw a huge spike in such releases, with the likes of Google and Facebook leading the way. The best part about these releases is that the researchers behind the code also provide pretrained models so folks like you and I don¡¯t have to waste time building difficult models from scratch. Additionally, we regularly see the top trending repositories aimed towards coders and developers <U+2013> this includes resources like cheatsheets, video links, e-books, research paper links, among other things. No matter which level you are at in your professional career (beginner, established or advanced), you will always find something new to learn on GitHub. 2018 was a transcendent one in a lot of data science sub-fields, as we will shortly see. Natural Language Processing (NLP) was easily the most talked about domain within the community with the likes of ULMFiT and BERT being open-sourced. In my quest to bring the best to our awesome community, I ran a monthly series throughout the year where I hand-picked the top 5 projects every data scientist should know about. You can check out the entire collection below: There will be some overlap here with my article covering the biggest breakthroughs in AI and ML in 2018. Do check out that article as well <U+2013> it is essentially a list of all the major developments I feel everyone in this field needs to know about. As a bonus, there are predictions from experts as well <U+2013> not something you want to miss. <U+0001F642> And now, get ready to explore new projects in your quest to attain data science stardom in 2019 and scroll down! Simply click on each project title to head over to the code repository on GitHub. Let¡¯s get the ball rolling with a look at the top projects in terms of tools, libraries and frameworks. Since we are speaking about a software repository platform, it feels right to open things with this section. Technology is advancing rapidly and computational costs are lower than ever, so we¡¯re being treated to one massive release after another. Can we call this the golden age of coding in machine learning? That is an open question, but one thing we can all agree on <U+2013> it¡¯s a great time to be a programmer in data science. In this section (and the article overall), I have tried to diversify the languages as much as possible, but Python inevitably ruled the roost.  How about all you .NET developers wanting to learn a bit of machine learning to complement your existing skills? Here¡¯s the perfect repository to get that idea started! ML.NET, a Microsoft project, is an open-source machine learning framework that allows you design and develop models in .NET. You can even integrate existing ML models into your application, all without requiring explicit knowledge of how ML models are developed. ML.NET is actually used in multiple Microsoft products, like Windows, Bing Search, MS Office, among others. ML.NET runs on Windows, Linux and MacOS. Machine learning in the browser! A fictional thought a few years back, a stunning reality now. A lot of us in this field are welded to our favorite IDEs, but TensorFlow.js has the potential to change your habits. It¡¯s become a very popular release since it¡¯s release earlier this year and continues to amaze with its flexibility. As the repository states, there are primarily three major features of TensorFlow.js: If you¡¯re familiar with Keras, the high-level layers API will seem quite familiar. There are plenty of examples available on the GitHub repository, so check those out to quicken your learning curve. What a year it has been for PyTorch. It has won the hearts and now projects of data scientists and ML researchers around the globe. It is easy to grasp, flexible, and is already being implemented across high profile researches (as you¡¯ll see later in this article). The latest version (v1.0) already powers many Facebook products and services at scale, including performing 6 billion text translations a day. If you¡¯ve been wondering when to start dabbling with PyTorch, the time is NOW. If you¡¯re new to this field, ensure you check out Faizan Shaikh¡¯s guide to getting started with PyTorch. While not strictly a tool or framework, this repository is a gold mine for all data scientists. Most of us struggle with reading through a paper and then implementing it (at least I do). There are a lot of moving parts that don¡¯t seem to work on our machines. And that¡¯s where ¡®Papers with Code¡¯ comes in. As the name suggests, they have a code implementation of all the major papers that have been released in the last 6 years or so. It is a mind-blowing collection that you will find yourself fawning over. They have even added code from papers presented at NIPS (NeurIPS) 2018. Get yourself over there now! Thanks to falling computational costs and a surge of breakthroughs from the top researchers (something tells me those two might be linked), deep learning is accessible to more people than ever before. And within deep learning, computer vision projects are ubiquitous <U+2013> most of the repositories you¡¯ll see in this section will cover one computer vision technique or another.  It is simply the hottest field in deep learning right now and will continue to be so for the foreseeable future. Whether it¡¯s object detection or pose estimation, there¡¯s a repository for seemingly all computer vision tasks. Never a better time to get acquainted with these developments <U+2013> a lot of job openings might come your way soon. Detectron made a HUGE splash when it was launched in early 2018. Developed by Facebook¡¯s AI Research team (FAIR), it implements state-of-the-art object detection frameworks. It is (surprise, surprise) written in Python and has helped enable multiple projects, including DensePose (which we will talk about soon). This repository contains the code and over 70 pretrained models. Too good an opportunity pass up, would¡¯t you agree? Object detection in images is awesome, but what about doing it in videos? And not just that, can we extend this concept and translate the style of one video to another? Yes, we can! It is a really cool concept and NVIDIA have been generous enough to release the PyTorch implementation for you to play around with. The repository contains videos of how the technique looks, the full research paper, and of course the code. The Cityscapes dataset, available publicly post registration, is used in NVIDIA¡¯s examples. One of my favorite projects from 2018. Training a deep learning model in 18 minutes? While not having access to high-end computational resources? Believe me, it¡¯s already been done. Fast.ai¡¯s Jeremy Howard and his team of students built a model on the popular ImageNet dataset that even outperformed Google¡¯s approach. I encourage you to at least go through this project to get a sense of how these researchers structured their code. Not everyone has access to multiple GPUs (or even one) so this was quite a win for the minnows. Another research paper collection repository! It¡¯s always helpful to know how your subject of choice has evolved over a span of multiple years, and this one-stop shop will help you do just that for object detection. It¡¯s a comprehensive collection of papers from 2014 till date, and even include code wherever possible. The above image shows how object detection frameworks have evolved and transformed in the last five years. Quite fascinating, isn¡¯t it? There¡¯s even a 2019 entry included, so you have quite a lot of catching up to do. Let¡¯s turn our attention to the field of pose detection. I came across this concept this year itself and have been fascinated with it ever since. That above image captures the essence of this repository <U+2013> dense human pose estimation in the wild. The code to train and evaluate your own DensePose-RCNN model is included here. There are notebooks available as well to visualize the DensePose COCO dataset. Pretty good place to kick off your pose estimation learning. The above image (taken from a video) really piqued my interest.<U+00A0>I covered the release of the research paper<U+00A0>back in August and have continued to be in awe of this technique. This technique enables us to transfer the motion between human objects in different videos. The video I mentioned is available within the repository <U+2013> it will blow your mind! This repository further contains the PyTorch implementation of this approach. The amount of intricate details this approach is capable of picking up and replicating is incredible. I¡¯m sure most of you must have come across a GAN application (even if you perhaps didn¡¯t realize it at the time). GANs, or Generative Adversarial Networks, were introduced by Ian Goodfellow back in 2014 and have caught fire since. They specilize in performing creative tasks, especially artistic ones. Check out this amazing introductory guide by Faizan Shaikh to the world of GANs, along with an implementation in Python. We saw a plethora of GAN based projects in 2018 and hence I wanted to create a separate section for this. Let¡¯s start off with one of my favorites. I want you to take a moment to just admire the above images. Can you tell which one was done by a human and which one by a machine? I certainly couldn¡¯t. Here, the first frame is the input image (original) and the third frame has been generated by this technique. Amazing, right? The algorithm adds an external object of your choosing to any image and manages to make it look like nothing touched it. Make sure you check out the code and try to implement it on a different set of images yourself. It¡¯s really, really fun. What if I gave you an image and asked you to extend the boundaries by imagining what it would look like when the entire scene was captured? You would understandably turn to some image editing software. But here¡¯s the awesome news <U+2013> you can achieve it in a few lines of code! This project is a Keras implementation of Stanford¡¯s Image Outpainting paper (incredibly cool and illustrated paper <U+2013> this is how most research papers should be!).<U+00A0>You can either build a model from scratch or use the one provided by this repository¡¯s author. Deep learning wonders never cease to amaze. If you haven¡¯t got a handle on GANs yet, try out this project. Pioneered by researchers from MIT¡¯s CSAIL division, it helped you visualize and understand GANs. You can explore what your GAN model has learned by inspecting and manipulating it¡¯s neurons. I would like to point you towards the official MIT project page, which has plenty of resources to get you familiar with the concept, including a video demo. This algorithm enables you to change the facial expression of any person in an image. It¡¯s as exciting as it is concerning. The images above inside the green border at the originals, the rest have been generated by GANimation. The link contains a beginner¡¯s guide, data preparation resources, prerequisites, and the Python code. As the author mentioned, do NOT use it for immoral purposes. This project is quite similar to the Deep Painterly Harmonization one we saw earlier. But it deserved a mention given it came from NVIDIA themselves. As you can see in the image above, the FastPhotoStyle algorithm requires two inputs <U+2013> a style photo and a content photo. The algorithm then works in one of two ways to generate the output <U+2013> it either uses photorealistic image stylization code or uses semantic label maps. The computer vision field has the potential to overshadow other work in deep learning but I wanted to highlight a few projects outside it. Audio processing is another field where deep learning has started to make it¡¯s mark. It¡¯s not just limited to generating music, you can do tasks like audio classification, fingerprinting, segmentation, tagging, etc. There is a lot that¡¯s still yet to be explored and who knows, perhaps you could use these projects to pioneer your way to the top. Here are two intuitive articles to help you get acquainted with this line of work: And here comes NVIDIA again. WaveGlow is a flow-based network capable of generating really high quality audio. It is essentially a single network for speech synthesis. This repository includes a PyTorch implementation of WaveGlow along with a pre-trained model which you can download. The researchers have also listed down the steps you can follow if you want to train your own model from scratch. Want to discover your own planet? That might perhaps be overstating things a bit, but this AstroNet repository will definitely get you close. The Google Brain team discovered two new planets in December 2017 by applying AstroNet. It¡¯s a deep neural network meant for working with astronomical data. It goes to show the far-ranging applications of machine learning and was a truly monumental development. And now the team behind the technology has open sourced the entire code (hint: the model is based on CNNs!) that powers AstroNet. Who doesn¡¯t love visualizations? But it can get a tad bit intimidating to imagine how a deep learning model works <U+2013> there are too many moving parts involved. But VisualDL does a great job mitigating those challenges by designing specific deep learning jobs. VisualDL currently supports the below components for visualizing jobs (you can see examples of each in the repository): Surprised to see NLP so down in this list? That¡¯s primarily because I covered almost all the major open source releases in this article. I highly recommend checking out that list to stay on top of your NLP game. The frameworks I have mentioned here include ULMFiT, Google¡¯s BERT, ELMo, and Facebook¡¯s PyText. I will briefly mention BERT and a couple of other respositories here as I found them very helpful. I couldn¡¯t possibly let this section pass by without mentioning BERT. Google AI¡¯s release has smashed records on it¡¯s way to winning the hearts of NLP enthusiasts and experts alike.<U+00A0>Following ULMFiT and ELMo, BERT really blew away the competition with it¡¯s performance. It obtained state-of-the-art results on 11 NLP tasks. Apart from the official Google repository I have linked to above, a PyTorch implementation of BERT is worth checking out. Whether it marks a new era of not in NLP we will soon find out. It often helps to know how well your model is performing against a certain benchmark. For NLP, and specifically deep text matching models, I have found the MatchZoo toolkit quite reliable.<U+00A0>Potential tasks related to MatchZoo include: MatchZoo 2.0 is currently under development so expect to see a lot more being added to this already useful toolkit. This repository was created by none other than Sebastian Ruder. The aim of this project is to track the latest progress in NLP. This includes both datasets and state-of-the-art models. Any NLP technique you¡¯ve ever wanted to know more about <U+2013> there¡¯s a good chance it¡¯ll already be present here. The repository covers both traditional and core NLP tasks such as reading comprehension and parts-of-speech tagging. It¡¯s mandatory to star/bookmark this repository if you¡¯re even vaguely interested in this field. What an year for AutoML. With industries look to integrate machine learning into their core mission, the need to data science specialists continues to grow. There is currently a massive gap between the demand and the supply. This gap could potentially be filled by AutoML tools. These tools are designed for those people who do not have data science expertise. While there are certainly some incredible tools out there, most of them are priced significantly higher than most individuals can afford. So our amazing open source community came to the rescue in 2018, with two high profile releases. This made quite a splash upon it¡¯s release a few months ago. And why wouldn¡¯t it? Deep learning has been long considered a very specialist field, so a library that can automate most tasks came as a welcome sign. Quoting from their official site, ¡°The ultimate goal of AutoML is to provide easily accessible deep learning tools to domain experts with limited data science or machine learning background¡±. You can install this library from pip: The repository contains a simple example to give you a sense of how the whole thing works. You¡¯re welcome, deep learning enthusiasts. <U+0001F642> AdaNet is a framework for automatically learning high-quality models without requiring programming expertise. Since it¡¯s a Google invention, the framework is based on TensorFlow. You can build ensemble models using AdaNet, and even extend it¡¯s use to training a neural network. The GitHub page contains the code, an example, the API documentation, and other things to get your hands dirty. Trust me, AutoML is the next big thing in our field. Since I already covered a few reinforcement learning releases in my 2018 overview article, I will keep this section fairly brief. My hope in including a RL section where I can is to foster a discussion among our community and to hopefully accelerate research in this field. First, make sure you check out OpenAI¡¯s Spinning Up repository, an exhaustive educational resource for beginners. Then head over to Google¡¯s Dopamine page. It is a research framework for accelerating research in this still nascent field. Now let¡¯s look at a couple of other resources as well. If you follow a few researchers on social media, you must have come across the above images in video form. A stick human running across a terrain, or trying to stand up, or some such sort. That, dear reader, is reinforcement learning in action. Here is a signature example of it <U+2013> a framework to train a simulated humanoid to imitate multiple motion skills. You can get the code, examples, and a step-by-step run-through on the above link. This repository is a collection of reinforcement learning algorithms from Richard Sutton and Andrew Barto¡¯s book and other research papers. These algorithms are presented in the form of Python notebooks. As the author of this repo mentioned, you will only truly learn if you implement the learning as you go along. It¡¯s a complex topic, and giving up or reading the resources like a storybook will lead you nowhere. And that bring us to the end of our journey for 2018. What a year! It was a joyful ride putting this article together and I learned a lot of new stuff along the way. I would love to hear your feedback on this article. Which repository have you used? Which one did you find the most useful? And which one(s) did I miss out on? Use the comments section below and let me know. Good m orning: Yes I am very interest in this course. My question is what is the cost?
Thank you","Keyword(freq): model(10), project(10), researcher(7), task(7), resource(6), image(5), release(5), tool(5), example(4), framework(4)"
