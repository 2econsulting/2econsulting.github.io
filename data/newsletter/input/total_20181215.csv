"","site","date","headline","url_address","text","keyword"
"1","datacamp",2018-12-10,"Enter the #DataFramedChallenge for a chance to be on an upcoming podcast segment.","https://www.datacamp.com/community/blog/dataframed-challenge","At the start of Season 2, we¡¯ll randomly select the sender of one of these tweets to chat with Hugo on a podcast segment. The more tweets you send, the more chances you'll have to win. Don't just tweet the same quotation more than once as we'll delete duplicates ;). You can find the podcast episodes on DataCamp, YouTube, and iTunes.","Keyword(freq): tweet(2), chance(1), duplicate(1), episode(1), itune(1), NA(NA), NA(NA), NA(NA), NA(NA), NA(NA)"
"2","mastery",2018-12-14,"How to Improve Deep Learning Model Robustness by Adding Noise","https://machinelearningmastery.com/how-to-improve-deep-learning-model-robustness-by-adding-noise/","Adding noise to an underconstrained neural network model with a small training dataset can have a regularizing effect and reduce overfitting. Keras supports the addition of Gaussian noise via a separate layer called the GaussianNoise layer. This layer can be used to add noise to an existing model. In this tutorial, you will discover how to add noise to deep learning models in Keras in order to reduce overfitting and improve model generalization. After completing this tutorial, you will know: Let¡¯s get started. How to Improve Deep Learning Model Robustness by Adding NoisePhoto by Michael Mueller, some rights reserved. This tutorial is divided into three parts; they are: Keras supports the addition of noise to models via the GaussianNoise layer. This is a layer that will add noise to inputs of a given shape. The noise has a mean of zero and requires that a standard deviation of the noise be specified as a parameter. For example: The output of the layer will have the same shape as the input, with the only modification being the addition of noise to the values. The GaussianNoise can be used in a few different ways with a neural network model. Firstly, it can be used as an input layer to add noise to input variables directly. This is the traditional use of noise as a regularization method in neural networks. Below is an example of defining a GaussianNoise layer as an input layer for a model that takes 2 input variables. Noise can also be added between hidden layers in the model. Given the flexibility of Keras, the noise can be added before or after the use of the activation function. It may make more sense to add it before the activation; nevertheless, both options are possible. Below is an example of a GaussianNoise layer that adds noise to the linear output of a Dense layer before a rectified linear activation function, perhaps a more appropriate use of noise between hidden layers. Noise can also be added after the activation function, much like using a noisy activation function. One downside of this usage is that the resulting values may be out-of-range from what the activation function may normally provide. For example, a value with added noise may be less than zero, whereas the relu activation function will only ever output values 0 or larger. Let¡¯s take a look at how noise regularization can be used with some common network types. The example below adds noise between two Dense fully connected layers. The example below adds noise after a pooling layer in a convolutional network. The example below adds noise between an LSTM recurrent layer and a Dense fully connected layer. Now that we have seen how to add noise to neural network models, let¡¯s look at a case study of adding noise to an overfit model to reduce generalization error. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this section, we will demonstrate how to use noise regularization to reduce overfitting of an MLP on a simple binary classification problem. This example provides a template for applying noise regularization to your own neural network for classification and regression problems. We will use a standard binary classification problem that defines two two-dimensional concentric circles of observations, one semi-circle for each class. Each observation has two input variables with the same scale and a class output value of either 0 or 1. This dataset is called the ¡°circles¡± dataset because of the shape of the observations in each class when plotted. We can use the make_circles() function to generate observations from this problem. We will add noise to the data and seed the random number generator so that the same samples are generated each time the code is run. We can plot the dataset where the two variables are taken as x and y coordinates on a graph and the class value is taken as the color of the observation. The complete example of generating the dataset and plotting it is listed below. Running the example creates a scatter plot showing the concentric circles shape of the observations in each class. We can see the noise in the dispersal of the points making the circles less obvious. Scatter Plot of Circles Dataset with Color Showing the Class Value of Each Sample This is a good test problem because the classes cannot be separated by a line, e.g. are not linearly separable, requiring a nonlinear method such as a neural network to address. We have only generated 100 samples, which is small for a neural network, providing the opportunity to overfit the training dataset and have higher error on the test dataset, a good case for using regularization. Further, the samples have noise, giving the model an opportunity to learn aspects of the samples that don¡¯t generalize. We can develop an MLP model to address this binary classification problem. The model will have one hidden layer with more nodes than may be required to solve this problem, providing an opportunity to overfit. We will also train the model for longer than is required to ensure the model overfits. Before we define the model, we will split the dataset into train and test sets, using 30 examples to train the model and 70 to evaluate the fit model¡¯s performance. Next, we can define the model. The hidden layer uses 500 nodes in the hidden layer and the rectified linear activation function. A sigmoid activation function is used in the output layer in order to predict class values of 0 or 1. The model is optimized using the binary cross entropy loss function, suitable for binary classification problems and the efficient Adam version of gradient descent. The defined model is then fit on the training data for 4,000 epochs and the default batch size of 32. We will also use the test dataset as a validation dataset. We can evaluate the performance of the model on the test dataset and report the result. Finally, we will plot the performance of the model on both the train and test set each epoch. If the model does indeed overfit the training dataset, we would expect the line plot of accuracy on the training set to continue to increase and the test set to rise and then fall again as the model learns statistical noise in the training dataset. We can tie all of these pieces together; the complete example is listed below. Running the example reports the model performance on the train and test datasets. We can see that the model has better performance on the training dataset than the test dataset, one possible sign of overfitting. Your specific results may vary given the stochastic nature of the neural network and the training algorithm. Because the model is severely overfit, we generally would not expect much, if any, variance in the accuracy across repeated runs of the model on the same dataset. A figure is created showing line plots of the model accuracy on the train and test sets. We can see that expected shape of an overfit model where test accuracy increases to a point and then begins to decrease again. Line Plots of Accuracy on Train and Test Datasets While Training Showing an Overfit The dataset is defined by points that have a controlled amount of statistical noise. Nevertheless, because the dataset is small, we can add further noise to the input values. This will have the effect of creating more samples or resampling the domain, making the structure of the input space artificially smoother. This may make the problem easier to learn and improve generalization performance. We can add a GaussianNoise layer as the input layer. The amount of noise must be small. Given that the input values are within the range [0, 1], we will add Gaussian noise with a mean of 0.0 and a standard deviation of 0.01, chosen arbitrarily. The complete example with this change is listed below. Running the example reports the model performance on the train and test datasets. Your results will vary, given both the stochastic nature of the learning algorithm and the stochastic nature of the noise added to the model. Try running the example a few times. In this case, we may see a small lift in performance of the model on the test dataset, with no negative impact on the training dataset. We clearly see the impact of the added noise on the evaluation of the model during training as graphed on the line plot. The noise cases the accuracy of the model to jump around during training, possibly due to the noise introducing points that conflict with true points from the training dataset. Perhaps a lower input noise standard deviation would be more appropriate. The model still shows a pattern of being overfit, with a rise and then fall in test accuracy over training epochs. Line Plot of Train and Test Accuracy With Input Layer Noise An alternative approach to adding noise to the input values is to add noise between the hidden layers. This can be done by adding noise to the linear output of the layer (weighted sum) before the activation function is applied, in this case a rectified linear activation function. We can also use a larger standard deviation for the noise as the model is less sensitive to noise at this level given the presumably larger weights from being overfit. We will use a standard deviation of 0.1, again, chosen arbitrarily. The complete example with Gaussian noise between the hidden layers is listed below. Running the example reports the model performance on the train and test datasets. Your results will vary, given both the stochastic nature of the learning algorithm and the stochastic nature of the noise added to the model. Try running the example a few times. In this case, we can see a marked increase in the performance of the model on the hold out test set. We can also see from the line plot of accuracy over training epochs that the model no longer appears to show the properties of being overfit. Line Plot of Train and Test Accuracy With Hidden Layer Noise We can also experiment and add the noise after the outputs of the first hidden layer pass through the activation function. The complete example is listed below. Running the example reports the model performance on the train and test datasets. Surprisingly, we see little difference in the performance of the model. Again, we can see from the line plot of accuracy over training epochs that the model no longer shows sign of overfitting. Line Plot of Train and Test Accuracy With Hidden Layer Noise (alternate) This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to add noise to deep learning models in Keras in order to reduce overfitting and improve model generalization. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Better Deep Learning It provides self-study tutorials on topics like: weight decay, batch normalization, dropout, model stacking and much more¡¦ Skip the Academics. Just Results. Click to learn more. Thanks Jason, nicely explained. Really enjoyed it. Thanks. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): value(7), circle(5), dataset(5), kera(5), layer(5), model(5), sample(5), epoch(4), observation(4), point(4)"
"3","mastery",2018-12-12,"Train Neural Networks With Noise to Reduce Overfitting","https://machinelearningmastery.com/train-neural-networks-with-noise-to-reduce-overfitting/","Training a neural network with a small dataset can cause the network to memorize all training examples, in turn leading to poor performance on a holdout dataset. Small datasets may also represent a harder mapping problem for neural networks to learn, given the patchy or sparse sampling of points in the high-dimensional input space. One approach to making the input space smoother and easier to learn is to add noise to inputs during training. In this post, you will discover that adding noise to a neural network during training can improve the robustness of the network, resulting in better generalization and faster learning. After reading this post, you will know: Let¡¯s get started. Train Neural Networks With Noise to Reduce OverfittingPhoto by John Flannery, some rights reserved. This tutorial is divided into five parts; they are: Small datasets can introduce problems when training large neural networks. The first problem is that the network may effectively memorize the training dataset. Instead of learning a general mapping from inputs to outputs, the model may learn the specific input examples and their associated outputs. This will result in a model that performs well on the training dataset, and poor on new data, such as a holdout dataset. The second problem is that a small dataset provides less opportunity to describe the structure of the input space and its relationship to the output. More training data provides a richer description of the problem from which the model may learn. Fewer data points means that rather than a smooth input space, the points may represent a jarring and disjointed structure that may result in a difficult, if not unlearnable, mapping function. It is not always possible to acquire more data. Further, getting a hold of more data may not address these problems. One approach to improving generalization error and to improving the structure of the mapping problem is to add random noise. Many studies [¡¦] have noted that adding small amounts of input noise (jitter) to the training data often aids generalization and fault tolerance. <U+2014> Page 273, Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999. At first, this sounds like a recipe for making learning more challenging. It is a counter-intuitive suggestion to improving performance because one would expect noise to degrade performance of the model during training. Heuristically, we might expect that the noise will ¡®smear out¡¯ each data point and make it difficult for the network to fit individual data points precisely, and hence will reduce over-fitting. In practice, it has been demonstrated that training with noise can indeed lead to improvements in network generalization. <U+2014> Page 347, Neural Networks for Pattern Recognition, 1995. The addition of noise during the training of a neural network model has a regularization effect and, in turn, improves the robustness of the model. It has been shown to have a similar impact on the loss function as the addition of a penalty term, as in the case of weight regularization methods. It is well known that the addition of noise to the input data of a neural network during training can, in some circumstances, lead to significant improvements in generalization performance. Previous work has shown that such training with noise is equivalent to a form of regularization in which an extra term is added to the error function. <U+2014> Training with Noise is Equivalent to Tikhonov Regularization, 2008. In effect, adding noise expands the size of the training dataset. Each time a training sample is exposed to the model, random noise is added to the input variables making them different every time it is exposed to the model. In this way, adding noise to input samples is a simple form of data augmentation. Injecting noise in the input to a neural network can also be seen as a form of data augmentation. <U+2014> Page 241, Deep Learning, 2016. Adding noise means that the network is less able to memorize training samples because they are changing all of the time, resulting in smaller network weights and a more robust network that has lower generalization error. The noise means that it is as though new samples are being drawn from the domain in the vicinity of known samples, smoothing the structure of the input space. This smoothing may mean that the mapping function is easier for the network to learn, resulting in better and faster learning. ¡¦ input noise and weight noise encourage the neural-network output to be a smooth function of the input or its weights, respectively. <U+2014> The Effects of Adding Noise During Backpropagation Training on a Generalization Performance, 1996. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course The most common type of noise used during training is the addition of Gaussian noise to input variables. Gaussian noise, or white noise, has a mean of zero and a standard deviation of one and can be generated as needed using a pseudorandom number generator. The addition of Gaussian noise to the inputs to a neural network was traditionally referred to as ¡°jitter¡± or ¡°random jitter¡± after the use of the term in signal processing to refer to to the uncorrelated random noise in electrical circuits. The amount of noise added (eg. the spread or standard deviation) is a configurable hyperparameter. Too little noise has no effect, whereas too much noise makes the mapping function too challenging to learn. This is generally done by adding a random vector onto each input pattern before it is presented to the network, so that, if the patterns are being recycled, a different random vector is added each time. <U+2014> Training with Noise is Equivalent to Tikhonov Regularization, 2008. The standard deviation of the random noise controls the amount of spread and can be adjusted based on the scale of each input variable. It can be easier to configure if the scale of the input variables has first been normalized. Noise is only added during training. No noise is added during the evaluation of the model or when the model is used to make predictions on new data. The addition of noise is also an important part of automatic feature learning, such as in the case of autoencoders, so-called denoising autoencoders that explicitly require models to learn robust features in the presence of noise added to inputs. We have seen that the reconstruction criterion alone is unable to guarantee the extraction of useful features as it can lead to the obvious solution ¡°simply copy the input¡± or similarly uninteresting ones that trivially maximizes mutual information. [¡¦] we change the reconstruction criterion for a both more challenging and more interesting objective: cleaning partially corrupted input, or in short denoising. <U+2014> Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion, 2010. Although additional noise to the inputs is the most common and widely studied approach, random noise can be added to other parts of the network during training. Some examples include: The addition of noise to the layer activations allows noise to be used at any point in the network. This can be beneficial for very deep networks. Noise can be added to the layer outputs themselves, but this is more likely achieved via the use of a noisy activation function. The addition of noise to weights allows the approach to be used throughout the network in a consistent way instead of adding noise to inputs and layer activations. This is particularly useful in recurrent neural networks. Another way that noise has been used in the service of regularizing models is by adding it to the weights. This technique has been used primarily in the context of recurrent neural networks. [¡¦] Noise applied to the weights can also be interpreted as equivalent (under some assumptions) to a more traditional form of regularization, encouraging stability of the function to be learned. <U+2014> Page 242, Deep Learning, 2016. The addition of noise to gradients focuses more on improving the robustness of the optimization process itself rather than the structure of the input domain. The amount of noise can start high at the beginning of training and decrease over time, much like a decaying learning rate. This approach has proven to be an effective method for very deep networks and for a variety of different network types. We consistently see improvement from injected gradient noise when optimizing a wide variety of models, including very deep fully-connected networks, and special-purpose architectures for question answering and algorithm learning. [¡¦] Our experiments indicate that adding annealed Gaussian noise by decaying the variance works better than using fixed Gaussian noise <U+2014> Adding Gradient Noise Improves Learning for Very Deep Networks, 2015. Adding noise to the activations, weights, or gradients all provide a more generic approach to adding noise that is invariant to the types of input variables provided to the model. If the problem domain is believed or expected to have mislabeled examples, then the addition of noise to the class label can improve the model¡¯s robustness to this type of error. Although, it can be easy to derail the learning process. Adding noise to a continuous target variable in the case of regression or time series forecasting is much like the addition of noise to the input variables and may be a better use case. This section summarizes some examples where the addition of noise during training has been used. Lasse Holmstrom studied the addition of random noise both analytically and experimentally with MLPs in the 1992 paper titled ¡°Using Additive Noise in Back-Propagation Training.¡± They recommend first standardizing input variables then using cross-validation to choose the amount of noise to use during training. If a single general-purpose noise design method should be suggested, we would pick maximizing the cross-validated likelihood function. This method is easy to implement, is completely data-driven, and has a validity that is supported by theoretical consistency results Klaus Gref, et al. in their 2016 paper titled ¡°LSTM: A Search Space Odyssey¡± used a hyperparameter search for the standard deviation for Gaussian noise on the input variables for a suite of sequence prediction tasks and found that it almost universally resulted in worse performance. Additive Gaussian noise on the inputs, a traditional regularizer for neural networks, has been used for LSTM as well. However, we find that not only does it almost always hurt performance, it also slightly increases training times. Alex Graves, et al. in their groundbreaking 2013 paper titled ¡°Speech recognition with deep recurrent neural networks¡± that achieved then state-of-the-art results for speech recognition added noise to the weights of LSTMs during training. ¡¦ weight noise [was used] (the addition of Gaussian noise to the network weights during training). Weight noise was added once per training sequence, rather than at every timestep. Weight noise tends to ¡®simplify¡¯ neural networks, in the sense of reducing the amount of information required to transmit the parameters, which improves generalisation. In a prior 2011 paper that studies different types of static and adaptive weight noise titled ¡°Practical Variational Inference for Neural Networks,¡± Graves recommends using early stopping in conjunction with the addition of weight noise with LSTMs. ¡¦ in practice early stopping is required to prevent overfitting when training with weight noise. This section provides some tips for adding noise during training with your neural network. Noise can be added to training regardless of the type of problem that is being addressed. It is appropriate to try adding noise to both classification and regression type problems. The type of noise can be specialized to the types of data used as input to the model, for example, two-dimensional noise in the case of images and signal noise in the case of audio data. Adding noise during training is a generic method that can be used regardless of the type of neural network that is being used. It was a method used primarily with multilayer Perceptrons given their prior dominance, but can be and is used with Convolutional and Recurrent Neural Networks. It is important that the addition of noise has a consistent effect on the model. This requires that the input data is rescaled so that all variables have the same scale, so that when noise is added to the inputs with a fixed variance, it has the same effect. The also applies to adding noise to weights and gradients as they too are affected by the scale of the inputs. This can be achieved via standardization or normalization of input variables. If random noise is added after data scaling, then the variables may need to be rescaled again, perhaps per mini-batch. You cannot know how much noise will benefit your specific model on your training dataset. Experiment with different amounts, and even different types of noise, in order to discover what works best. Be systematic and use controlled experiments, perhaps on smaller datasets across a range of values. Noise is only added during the training of your model. Be sure that any source of noise is not added during the evaluation of your model, or when your model is used to make predictions on new data. This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered that adding noise to a neural network during training can improve the robustness of the network resulting in better generalization and faster learning. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Better Deep Learning It provides self-study tutorials on topics like: weight decay, batch normalization, dropout, model stacking and much more¡¦ Skip the Academics. Just Results. Click to learn more. Thank you so much for your great article. If there is an example of using this technique to improve performance, it will be very helpful. Yes, I will post one in a few days. Thank you so much. I am looking forward to your new post. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): network(16), variable(10), input(9), weight(9), example(5), type(5), model(4), point(4), sample(4), activation(3)"
"4","mastery",2018-12-10,"How to Stop Training Deep Neural Networks At the Right Time Using Early Stopping","https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/","A problem with training neural networks is in the choice of the number of training epochs to use. Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold out validation dataset. In this tutorial, you will discover the Keras API for adding early stopping to overfit deep learning neural network models. After completing this tutorial, you will know: Let¡¯s get started. How to Stop Training Deep Neural Networks At the Right Time With Using Early StoppingPhoto by Ian D. Keating, some rights reserved. This tutorial is divided into six parts; they are: Callbacks provide a way to execute code and interact with the training model process automatically. Callbacks can be provided to the fit() function via the ¡°callbacks¡± argument. First, callbacks must be instantiated. Then, one or more callbacks that you intend to use must be added to a Python list. Finally, the list of callbacks is provided to the callback argument when fitting the model. Early stopping requires that a validation dataset is evaluated during training. This can be achieved by specifying the validation dataset to the fit() function when training your model. There are two ways of doing this. The first involves you manually splitting your training data into a train and validation dataset and specifying the validation dataset to the fit() function via the validation_data argument. For example: Alternately, the fit() function can automatically split your training dataset into train and validation sets based on a percentage split specified via the validation_split argument. The validation_split is a value between 0 and 1 and defines the percentage amount of the training dataset to use for the validation dataset. For example: In both cases, the model is not trained on the validation dataset. Instead, the model is evaluated on the validation dataset at the end of each training epoch. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course The loss function chosen to be optimized for your model is calculated at the end of each epoch. To callbacks, this is made available via the name ¡°loss.¡± If a validation dataset is specified to the fit() function via the validation_data or validation_split arguments, then the loss on the validation dataset will be made available via the name ¡°val_loss.¡± Additional metrics can be monitored during the training of the model. They can be specified when compiling the model via the ¡°metrics¡± argument to the compile function. This argument takes a Python list of known metric functions, such as ¡®mse¡® for mean squared error and ¡®acc¡® for accuracy. For example: If additional metrics are monitored during training, they are also available to the callbacks via the same name, such as ¡®acc¡® for accuracy on the training dataset and ¡®val_acc¡® for the accuracy on the validation dataset. Or, ¡®mse¡® for mean squared error on the training dataset and ¡®val_mse¡® on the validation dataset. Keras supports the early stopping of training via a callback called EarlyStopping. This callback allows you to specify the performance measure to monitor, the trigger, and once triggered, it will stop the training process. The EarlyStopping callback is configured when instantiated via arguments. The ¡°monitor¡± allows you to specify the performance measure to monitor in order to end training. Recall from the previous section that the calculation of measures on the validation dataset will have the ¡®val_¡® prefix, such as ¡®val_loss¡® for the loss on the validation dataset. Based on the choice of performance measure, the ¡°mode¡± argument will need to be specified as whether the objective of the chosen metric is to increase (maximize or ¡®max¡®) or to decrease (minimize or ¡®min¡®). For example, we would seek a minimum for validation loss and a minimum for validation mean squared error, whereas we would seek a maximum for validation accuracy. By default, mode is set to ¡®auto¡® and knows that you want to minimize loss or maximize accuracy. That is all that is needed for the simplest form of early stopping. Training will stop when the chosen performance measure stops improving. To discover the training epoch on which training was stopped, the ¡°verbose¡± argument can be set to 1. Once stopped, the callback will print the epoch number. Often, the first sign of no further improvement may not be the best time to stop training. This is because the model may coast into a plateau of no improvement or even get slightly worse before getting much better. We can account for this by adding a delay to the trigger in terms of the number of epochs on which we would like to see no improvement. This can be done by setting the ¡°patience¡± argument. The exact amount of patience will vary between models and problems. Reviewing plots of your performance measure can be very useful to get an idea of how noisy the optimization process for your model on your data may be. By default, any change in the performance measure, no matter how fractional, will be considered an improvement. You may want to consider an improvement that is a specific increment, such as 1 unit for mean squared error or 1% for accuracy. This can be specified via the ¡°min_delta¡± argument. Finally, it may be desirable to only stop training if performance stays above or below a given threshold or baseline. For example, if you have familiarity with the training of the model (e.g. learning curves) and know that once a validation loss of a given value is achieved that there is no point in continuing training. This can be specified by setting the ¡°baseline¡± argument. This might be more useful when fine tuning a model, after the initial wild fluctuations in the performance measure seen in the early stages of training a new model are past. The EarlyStopping callback will stop training once triggered, but the model at the end of training may not be the model with best performance on the validation dataset. An additional callback is required that will save the best model observed during training for later use. This is the ModelCheckpoint callback. The ModelCheckpoint callback is flexible in the way it can be used, but in this case we will use it only to save the best model observed during training as defined by a chosen performance measure on the validation dataset. Saving and loading models requires that HDF5 support has been installed on your workstation. For example, using the pip Python installer, this can be achieved as follows: You can learn more from the h5py Installation documentation. The callback will save the model to file, which requires that a path and filename be specified via the first argument. The preferred loss function to be monitored can be specified via the monitor argument, in the same way as the EarlyStopping callback. For example, loss on the validation dataset (the default). Also, as with the EarlyStopping callback, we must specify the ¡°mode¡± as either minimizing or maximizing the performance measure. Again, the default is ¡®auto,¡¯ which is aware of the standard performance measures. Finally, we are interested in only the very best model observed during training, rather than the best compared to the previous epoch, which might not be the best overall if training is noisy. This can be achieved by setting the ¡°save_best_only¡± argument to True. That is all that is needed to ensure the model with the best performance is saved when using early stopping, or in general. It may be interesting to know the value of the performance measure and at what epoch the model was saved. This can be printed by the callback by setting the ¡°verbose¡± argument to ¡°1¡°. The saved model can then be loaded and evaluated any time by calling the load_model() function. Now that we know how to use the early stopping and model checkpoint APIs, let¡¯s look at a worked example. In this section, we will demonstrate how to use early stopping to reduce overfitting of an MLP on a simple binary classification problem. This example provides a template for applying early stopping to your own neural network for classification and regression problems. We will use a standard binary classification problem that defines two semi-circles of observations, one semi-circle for each class. Each observation has two input variables with the same scale and a class output value of either 0 or 1. This dataset is called the ¡°moons¡± dataset because of the shape of the observations in each class when plotted. We can use the make_moons() function to generate observations from this problem. We will add noise to the data and seed the random number generator so that the same samples are generated each time the code is run. We can plot the dataset where the two variables are taken as x and y coordinates on a graph and the class value is taken as the color of the observation. The complete example of generating the dataset and plotting it is listed below. Running the example creates a scatter plot showing the semi-circle or moon shape of the observations in each class. We can see the noise in the dispersal of the points making the moons less obvious. Scatter Plot of Moons Dataset With Color Showing the Class Value of Each Sample This is a good test problem because the classes cannot be separated by a line, e.g. are not linearly separable, requiring a nonlinear method such as a neural network to address. We have only generated 100 samples, which is small for a neural network, providing the opportunity to overfit the training dataset and have higher error on the test dataset: a good case for using regularization. Further, the samples have noise, giving the model an opportunity to learn aspects of the samples that don¡¯t generalize. We can develop an MLP model to address this binary classification problem. The model will have one hidden layer with more nodes than may be required to solve this problem, providing an opportunity to overfit. We will also train the model for longer than is required to ensure the model overfits. Before we define the model, we will split the dataset into train and test sets, using 30 examples to train the model and 70 to evaluate the fit model¡¯s performance. Next, we can define the model. The hidden layer uses 500 nodes and the rectified linear activation function. A sigmoid activation function is used in the output layer in order to predict class values of 0 or 1. The model is optimized using the binary cross entropy loss function, suitable for binary classification problems and the efficient Adam version of gradient descent. The defined model is then fit on the training data for 4,000 epochs and the default batch size of 32. We will also use the test dataset as a validation dataset. This is just a simplification for this example. In practice, you would split the training set into train and validation and also hold back a test set for final model evaluation. We can evaluate the performance of the model on the test dataset and report the result. Finally, we will plot the loss of the model on both the train and test set each epoch. If the model does indeed overfit the training dataset, we would expect the line plot of loss (and accuracy) on the training set to continue to increase and the test set to rise and then fall again as the model learns statistical noise in the training dataset. We can tie all of these pieces together; the complete example is listed below. Running the example reports the model performance on the train and test datasets. We can see that the model has better performance on the training dataset than the test dataset, one possible sign of overfitting. Your specific results may vary given the stochastic nature of the neural network and the training algorithm. Because the model is severely overfit, we generally would not expect much, if any, variance in the accuracy across repeated runs of the model on the same dataset. A figure is created showing line plots of the model loss on the train and test sets. We can see that expected shape of an overfit model where test accuracy increases to a point and then begins to decrease again. Reviewing the figure, we can also see flat spots in the ups and downs in the validation loss. Any early stopping will have to account for these behaviors. We would also expect that a good time to stop training might be around epoch 800. Line Plots of Loss on Train and Test Datasets While Training Showing an Overfit Model We can update the example and add very simple early stopping. As soon as the loss of the model begins to increase on the test dataset, we will stop training. First, we can define the early stopping callback. We can then update the call to the fit() function and specify a list of callbacks via the ¡°callback¡± argument. The complete example with the addition of simple early stopping is listed below. Running the example reports the model performance on the train and test datasets. We can also see that the callback stopped training at epoch 200. This is too early as we would expect an early stop to be around epoch 800. This is also highlighted by the classification accuracy on both the train and test sets, which is worse than no early stopping. Reviewing the line plot of train and test loss, we can indeed see that training was stopped at the point when validation loss began to plateau for the first time. Line Plot of Train and Test Loss During Training With Simple Early Stopping We can improve the trigger for early stopping by waiting a while before stopping. This can be achieved by setting the ¡°patience¡± argument. In this case, we will wait 200 epochs before training is stopped. Specifically, this means that we will allow training to continue for up to an additional 200 epochs after the point that validation loss started to degrade, giving the training process an opportunity to get across flat spots or find some additional improvement. The complete example with this change is listed below. Running the example, we can see that training was stopped much later, in this case after epoch 1,000. Your specific results may differ given the stochastic nature of training neural networks. We can also see that the performance on the test dataset is better than not using any early stopping. Reviewing the line plot of loss during training, we can see that the patience allowed the training to progress past some small flat and bad spots. Line Plot of Train and Test Loss During Training With Patient Early Stopping We can also see that test loss started to increase again in the last approximately 100 epochs. This means that although the performance of the model has improved, we may not have the best performing or most stable model at the end of training. We can address this by using a ModelChecckpoint callback. In this case, we are interested in saving the model with the best accuracy on the test dataset. We could also seek the model with the best loss on the test dataset, but this may or may not correspond to the model with the best accuracy. This highlights an important concept in model selection. The notion of the ¡°best¡± model during training may conflict when evaluated using different performance measures. Try to choose models based on the metric by which they will be evaluated and presented in the domain. In a balanced binary classification problem, this will most likely be classification accuracy. Therefore, we will use accuracy on the validation in the ModelCheckpoint callback to save the best model observed during training. During training, the entire model will be saved to the file ¡°best_model.h5¡± only when accuracy on the validation dataset improves overall across the entire training process. A verbose output will also inform us as to the epoch and accuracy value each time the model is saved to the same file (e.g. overwritten). This new additional callback can be added to the list of callbacks when calling the fit() function. We are no longer interested in the line plot of loss during training; it will be much the same as the previous run. Instead, we want to load the saved model from file and evaluate its performance on the test dataset. The complete example with these changes is listed below. Running the example, we can see the verbose output from the ModelCheckpoint callback for both when a new best model is saved and from when no improvement was observed. We can see that the best model was observed at epoch 879 during this run. Your specific results may vary given the stochastic nature of training neural networks. Again, we can see that early stopping continued patiently until after epoch 1,000. Note that epoch 880 + a patience of 200 is not epoch 1044. Recall that early stopping is monitoring loss on the validation dataset and that the model checkpoint is saving models based on accuracy. As such, the patience of early stopping started at an epoch other than 880. In this case, we don¡¯t see any further improvement in model accuracy on the test dataset. Nevertheless, we have followed a good practice. Why not monitor validation accuracy for early stopping? This is a good question. The main reason is that accuracy is a coarse measure of model performance during training and that loss provides more nuance when using early stopping with classification problems. The same measure may be used for early stopping and model checkpointing in the case of regression, such as mean squared error. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered the Keras API for adding early stopping to overfit deep learning neural network models. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Better Deep Learning It provides self-study tutorials on topics like: weight decay, batch normalization, dropout, model stacking and much more¡¦ Skip the Academics. Just Results. Click to learn more. Thanks a lot, A very clear illustration as always Thanks, I¡¯m glad it helped. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): callback(10), epoch(8), model(7), network(4), observation(4), problem(4), result(4), sample(4), set(4), dataset(3)"
"5","vidhya",2018-12-13,"WNS Hackathon Solutions by Top Finishers","https://www.analyticsvidhya.com/blog/2018/12/wns-hackathon-solutions-by-top-finishers/","How do you prefer learning a machine learning technique? First get to know how it works on paper and then apply it? Or get your hands dirty straight away by learning the practical side? I prefer the latter <U+2013> there¡¯s nothing like ingraining a concept by right away applying it and watching it in action. Participating in online hackathons, preparing and tuning our models, and competing against fellow top participants can help us evaluate our performance and understand the area where need to improve. There is always something new to learn, and someone¡¯s unique approach to learn from! At the end of each hackathon, we eagerly wait for the final rankings and look forward to the winning solutions so that we can learn, improve and prepare ourselves for the next hackathon (and perhaps the next project?). We recently conducted the WNS Analytics Wizard 2018, which received an overwhelming response <U+2013> 3800+ registrations, 364 teams, and more than 1,200 submissions! Here is a glimpse of the solutions provided by the folks who finished in the top echelons for the WNS online hackathon conducted on 14 <U+2013> 16 September, 2018. The WNS Analytics wizard is a one-of-its-kind online analytics hackathon conducted by WNS, a leading global business process management company. WNS offers business value to 350+ global clients by combining operational excellence with deep domain expertise in key industry verticals. This hackathon was aimed at giving budding data wizards the exciting opportunity to get a sneak peek into real-life business scenarios. We received more than 3800 registrations and a total of 1215 submissions during this 2-day online hackathon. The top finishers for WNS Hackathon are- Lets have a look at the problem statement for WNS Hackathon The problem statement for WNS Analytics Wizard was based on a real-life business use case. As a part of the WNS hackathon, participants were required to classify whether an employee should be promoted or not, by looking at employee¡¯s past and current performance. Train consisted of 54808 rows and test had 23490 rows. Raw dataset contained 12 features. An employee is first nominated for promotion (based on his/her previous performance) and then goes through a training process. The task is to predict whether a potential promotee in the test set will be promoted or not after the evaluation process. The multiple attributes provided to the participants include the employee¡¯s past performance, education level, years of experience, previous year¡¯s rating , number of trainings taken in the last year, training score etc. Now that we have an understanding about the problem statement, let¡¯s have a look at the approaches shared by the winners. Siddharth followed a well structured approach that helped him secure the first position in the competition. He has divided<U+00A0> the approach into three broad categories <U+2013> model selection, feature engineering and hyperparameter tuning. Siddharth has listed down the steps he followed <U+2013> Model Selection Feature Engineering and Missing value imputation Hyperparameter tuning: Here is the complete code for the above mentioned approach : Rank1 Solution The second position was grabbed by Nikita Churkin & Dmitrii Simakov. According to the team, their approach is based on a good validation scheme, careful feature selection and strong regularization. Below is a detailed explanation of the same. Validation <U+00A0>Feature Generation and Selection Model Building The code for rank 2 solution is shared here. harshsarda29 & maverick_kamakal participated in WNS hackathon as a team and secured the third position. They used a 5 fold stratified scheme for validation and prepared three models XGBoost, LightGBM, CatBoost. Here is a complete step by step approach followed by the team: Missing value imputation Categorical Encodings: Hyper-parameter Finding: Feature Engineering: Model Building: The final model was an ensemble of 29 models consisting of: Here is the link to the code for Rank 3 The solutions shared above is a proof that the winners have put in great efforts and truly deserve the rewards for the same. They came up with some innovative solutions and had a well structured approach. I hope you find these solutions useful and have learnt some key takeaways which you can implement in the upcoming hackathons! Register yourself in the upcoming hackathons at DataHack Platform. Link for rank 3 and rank 1 is same. Hi Ankur, Thanks for letting us know. We have updated the link. Are the data sets available somewhere? Hi Barna, The datasets are not accessible once the competition is over. Where to get the dataset of this solution.","Keyword(freq): solution(5), analytics(4), hackathon(3), model(3), participant(3), employee(2), registration(2), row(2), submission(2), winner(2)"
"6","vidhya",2018-12-10,"Building a Face Detection Model from Video using Deep Learning (Python Implementation)","https://www.analyticsvidhya.com/blog/2018/12/introduction-face-detection-video-deep-learning-python/","¡°Computer vision and machine learning have really started to take off, but for most people, the whole idea of what a computer is seeing when it¡¯s looking at an image is relatively obscure.¡± <U+2013> Mike Kreiger The wonderful field of Computer Vision has soared into a league of it¡¯s own in recent years. There are an impressive number of applications already in wide use around the world <U+2013> and we are just getting started! One of my favorite things in this field is the idea of our community embracing the concept of open source. Even the big tech giants are willing to share new breakthroughs and innovations with everyone so that the techniques do not remain a ¡°thing of the rich¡±. One such technology is face detection, which offers a plethora of potential applications in real-world use cases (if used correctly and ethically). In this article, I will show you how to build a capable face detection algorithm using open source tools. Here is a demo to get you excited and set the stage for what will follow: So, are you ready? Read on then! Note: If you want to understand the intricacies of computer vision, this course <U+2013> Computer Vision using Deep Learning <U+2013> is the perfect place to start. Let me pull up some awesome examples of applications where face detection techniques are being popularly used. I¡¯m sure you must have come across these use cases at some point and not realized what technique was being used behind the scenes! For instance, Facebook replaced manual image tagging with automatically generated tag suggestions for each picture that was uploaded to the platform. Facebook uses a simple face detection algorithm to analyze the pixels of faces in the image and compare it with relevant users. We¡¯ll learn how to build a face detection model ourselves, but before we get into the technical details of that, let¡¯s discuss some other use cases. We are becoming used to unlocking our phones with the latest ¡®face unlock¡¯ feature. This is a very small example of how a face detection technique is being used to maintain the security of personal data. <U+00A0>The same can be implemented on a larger scale, enabling cameras to capture images and detect faces. There are a few other lesser known applications of face detection in advertising, healthcare, banking, etc. Most of the companies, or even in many conferences, you are supposed to carry an ID card in order to get entry. But what if we could figure out a way so that you don¡¯t need to carry any ID card to get access? Face Detection helps in making this process smooth and easy. The person just looks at the camera and it will automatically detect whether he/she should be allowed to enter or not. Another interesting application of face detection could be to count the number of people attending an event (like a conference or concert). Instead of manually counting the attendees, we install a camera which can capture the images of the attendees and give us the total head count. This can help to automate the process and save a ton of manual effort. Pretty useful, isn¡¯t it? You can come up with many more applications like these <U+2013> feel free to share them in the comments section below. In this article, I will focus upon the practical application of face detection, and just gloss over upon how the algorithms in it actually work. If you want to know more about them, you go through this article. Now that you know the potential applications you can build with face detection techniques, let¡¯s see how we can implement this using the open source tools available to us. That¡¯s the advantage we have with our community <U+2013> the willingness to share and open source code is unparalleled across any industry. For this article specifically, here¡¯s what I have used and recommend using: Let¡¯s explore these points in a bit more detail to ensure everything is set up properly before we build our face detection model. The first thing you have to do is check if the webcam is setup correctly. A simple trick in Ubuntu <U+2013> see if the device has been registered by the OS. You can follow the steps given below: The code in this article is built using Python version 3.5. Although there are multiple ways to install Python, I would recommend using Anaconda <U+2013> the most popular Python distribution for data science. Here is a link to install Anaconda in your system. OpenCV (Open Source Computer Vision) is a library aimed at building computer vision applications. It has numerous pre-written functions for image processing tasks. To install OpenCV, do a pip install of the library: Now that you have setup your system, it¡¯s finally time to dive in to the actual implementation. First, we will quickly build our program, then break it down to understand what we did. First, create a file face_detector.py and then copy the code given below: Then, run this Python file by typing: If everything works correctly, a new window will pop up with real-time face detection running. To summarize, this is what our above code did: Simple, isn¡¯t it? If you want to go into more granular details, I have included the comments in each code section. You can always go back and review what we have done. The fun doesn¡¯t stop there! Another cool thing we can do <U+2013> build a complete use case around the above code. And you don¡¯t need to start from scratch. We can make just a few small changes to the code and we¡¯re good to go. Suppose, for example, you want to build an automated camera-based system to track where the speaker is in real-time. According to his position, the system rotates the camera so that the speaker is always in the middle of the video. How do we go about this? The first step is to build a system which identifies the person(s) in the video, and focuses on the location of the speaker. Let¡¯s see how we can implement this. For this article, I have taken a video from Youtube which shows a speaker talking during the DataHack Summit 2017 conference. First, we import the necessary libraries: Then, read the video and get the length: After that, we create an output file with the required resolution and frame rate which is similar to the input file. Load a sample image of the speaker to identify him in the video: All this completed, now we run a loop that will do the following: Let¡¯s see the code for this: The code would then give you an output like this: What a terrific thing face detection truly is. <U+0001F642> Congratulations! You now know how to build a face detection system for a number of potential use cases. Deep learning is such a fascinating field and I¡¯m so excited to see where we go next. In this article, we learned how you can leverage open source tools to build real-time face detection systems that have real-world usefulness. I encourage you to build plenty of such applications and try this on your own. Trust me, there¡¯s a lot to learn and it¡¯s just so much fun! As always, feel free to reach out if you have any queries/suggestions in the comment section below!","Keyword(freq): application(8), case(4), technique(3), tool(3), attendee(2), comment(2), detail(2), face(2), image(2), suggestion(2)"
