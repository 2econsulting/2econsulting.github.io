"","site","date","headline","url_address","text","keyword"
"1","kaggle",2018-08-09,"Data Notes: From Hate Speech to Russian Troll Tweets","http://blog.kaggle.com/2018/08/09/data-notes-from-hate-speech-to-russian-troll-tweets/","Enjoy these new, intriguing, and overlooked datasets and kernels. 1.<U+00A0><U+0001F92C> From Hate Speech to Russian Troll Tweets (link) 2.<U+00A0><U+0001F1F0> Data Science Trends on Kaggle (link) 3.<U+00A0><U+0001F45C>Fashion AC-GAN with Keras (link) 4.<U+00A0><U+0001F4C8> (Bio)statistics in R: Part #2 (link) 5.<U+00A0><U+0001F6F0> Segmenting Buildings in Satellite Images (link) 6.<U+00A0><U+26BD> World Cup 2018: The One That Nearly Came Home (link) 7.<U+00A0><U+0001F1EF><U+0001F1F5> The Best ""Izakaya"" Restaurant in Kyoto (link) 8.<U+00A0><U+0001F479> Dataset: Russian Troll Tweets (link) 9.<U+00A0><U+0001F4C8> Dataset: Political Propaganda on Facebook (link) 10.<U+00A0><U+0001F913> Dataset: Predict Pakistan Elections 2018 (link) Interested in deep learning? Try building a neural network from scratch! Copyright ¨Ï 2018 Kaggle, All rights reserved.","Keyword(freq): tweet(2), building(1), dataset(1), election(1), image(1), kernel(1), right(1), statistics(1), trend(1), NA(NA)"
"2","mastery",2018-08-10,"How to Develop a Skillful Machine Learning Time Series Forecasting Model","https://machinelearningmastery.com/how-to-develop-a-skilful-time-series-forecasting-model/","What do you do? This is a common situation; far more common than most people think. The problem can be reasonably well defined: So how do you tackle this problem? Unless you have been through this trial by fire, you may struggle. In all of these cases, you will benefit from working through the problem carefully and systematically. In this post, I want to give you a specific and actionable procedure that you can use to work through your time series forecasting problem. Let¡¯s get started. How to Develop a Skilful Time Series Forecasting ModelPhoto by Make it Kenya, some rights reserved. The goal of this process is to get a ¡°good enough¡± forecast model as fast as possible. This process may or may not deliver the best possible model, but it will deliver a good model: a model that is better than a baseline prediction, if such a model exists. Typically, this process will deliver a model that is 80% to 90% of what can be achieved on the problem. The process is fast. As such, it focuses on automation. Hyperparameters are searched rather than specified based on careful analysis. You are encouraged to test suites of models in parallel, rapidly getting an idea of what works and what doesn¡¯t. Nevertheless, the process is flexible, allowing you to circle back or go as deep as you like on a given step if you have the time and resources. This process is divided into four parts; they are: You will notice that the process is different from a classical linear work-through of a predictive modeling problem. This is because it is designed to get a working forecast model fast and then slow down and see if you can get a better model. What is your process for working through a new time series forecasting problem?
Share it below in the comments. The biggest mistake is skipping steps. For example, the mistake that almost all beginners make is going straight to modeling without a strong idea of what problem is being solved or how to robustly evaluate candidate solutions. This almost always results in a lot of wasted time. Slow down, follow the process, and complete each step. I recommend having separate code for each experiment that can be re-run at any time. This is important so that you can circle back when you discover a bug, fix the code, and re-run an experiment. You are running experiments and iterating quickly, but if you are sloppy, then you cannot trust any of your results. This is especially important when it comes to the design of your test harness for evaluating candidate models. Let¡¯s take a closer look at each step of the process. Define your time series problem. Some topics to consider and motivating questions within each topic are as follows: Answer each question even if you have to estimate or guess. Some useful tools to help get answers include: Update your answers to these questions as you learn more. Design a test harness that you can use to evaluate candidate models. This includes both the method used to estimate model skill and the metric used to evaluate predictions. Below is a common time series forecasting model evaluation scheme if you are looking for ideas: The test harness must be robust and you must have complete trust in the results it provides. An important consideration is to ensure that any coefficients used for data preparation are estimated from the training dataset only and then applied on the test set. This might include mean and standard deviation in the case of data standardization. Test many models using your test harness. I recommend carefully designing experiments to test a suite of configurations for standard models and letting them run. Each experiment can record results to a file, to allow you to quickly discover the top three to five most skilful configurations from each run. Some common classes of methods that you can design experiments around include the following: This list is based on a univariate time series forecasting problem, but you can adapt it for the specifics of your problem, e.g. use VAR/VARMA/etc. in the case of multivariate time series forecasting. Slot in more of your favorite classical time series forecasting methods and machine learning methods as you see fit. Order here is important and is structured in increasing complexity from classical to modern methods. Early approaches are simple and give good results fast; later approaches are slower and more complex, but also have a higher bar to clear to be skillful. The resulting model skill can be used in a ratchet. For example, the skill of the best persistence configuration provide a baseline skill that all other models must outperform. If an autoregression model does better than persistence, it becomes the new level to outperform in order for a method to be considered skilful. Ideally, you want to exhaust each level before moving on to the next. E.g. get the most out of Autoregression methods and use the results as a new baseline to define ¡°skilful¡± before moving on to Exponential Smoothing methods. I put deep learning at the end as generally neural networks are poor at time series forecasting, but there is still a lot of room for improvement and experimentation in this area. The more time and resources that you have, the more configurations that you can evaluate. For example, with more time and resources, you could: I also encourage you to include data preparation schemes as hyperparameters for model runs. Some methods will perform some basic data preparation, such as differencing in ARIMA, nevertheless, it is often unclear exactly what data preparation schemes or combinations of schemes are required to best present a dataset to a modeling algorithm. Rather than guess, grid search and decide based on real results. Some data preparation schemes to consider include: So much searching can be slow. Some ideas to speed up the evaluation of models include: At the end of the previous time step, you know whether your time series is predictable. If it is predictable, you will have a list of the top 5 to 10 candidate models that are skillful on the problem. You can pick one or multiple models and finalize them. This involves training a new final model on all available historical data (train and test). The model is ready for use; for example: If you have time, you can always circle back to the previous step and see if you can further improve upon the final model. This may be required periodically if the data changes significantly over time. This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered a simple four-step process that you can use to quickly discover a skilful predictive model for your time series forecasting problem. Did you find this process useful?
Let me know below. Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ...with just a few lines of python code Discover how in my new Ebook:Introduction to Time Series Forecasting With Python It covers self-study tutorials and end-to-end projects on topics like:Loading data, visualization, modeling, algorithm tuning, and much more... Skip the Academics. Just Results. Click to learn more. What is the cost/price to develop forecasting regression model? What do you mean exactly? Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning. Read More","Keyword(freq): model(9), result(8), method(7), question(4), resource(4), scheme(4), configuration(3), experiment(3), answer(2), approach(2)"
"3","mastery",2018-08-08,"Taxonomy of Time Series Forecasting Problems","https://machinelearningmastery.com/taxonomy-of-time-series-forecasting-problems/","The choice that you make directly impacts each step of the project from the design of a test harness to evaluate forecast models to the fundamental difficulty of the forecast problem that you are working on. It is possible to very quickly narrow down the options by working through a series of questions about your time series forecasting problem. By considering a few themes and questions within each theme, you narrow down the type of problem, test harness, and even choice of algorithms for your project. In this post, you will discover a framework that you can use to quickly understand and frame your time series forecasting problem. Let¡¯s get started. Taxonomy of Time Series Forecasting ProblemsPhoto by Adam Meek, some rights reserved. Time series forecasting involves developing and using a predictive model on data where there is an ordered relationship between observations. You can learn more about what time series forecasting is in this post: Before you get started on your project, you can answer a few questions and greatly improve your understanding of the structure of your forecast problem, the structure of the model requires, and how to evaluate it. The framework presented in this post is divided into seven parts; they are: I recommend working through this framework before starting any time series forecasting project. Your answers may not be crisp on the first time through and the questions may require to you study the data, the domain, and talk to experts and stakeholders. Update your answers as you learn more as it will help to keep you on track, avoid distractions, and develop the actual model that you need for your project. Generally, a prediction problem involves using past observations to predict or forecast one or more possible future observations. The goal is to guess about what might happen in the future. When you are required to make a forecast, it is critical to think about the data that you will have available to make the forecast and what you will be guessing about the future. We can summarize this as what are the inputs and outputs of the model when making a single forecast. The input data is not the data used to train the model. We are not at that point yet. It is the data used to make one forecast, for example the last seven days of sales data to forecast the next one day of sales data. Defining the inputs and outputs of the model forces you to think about what exactly is or may be required to make a forecast. You may not be able to be specific when it comes to input data. For example, you may not know whether one or multiple prior time steps are required to make a forecast. But you will be able to identify the variables that could be used to make a forecast. What are the inputs and outputs for a forecast? The input data can be further subdivided in order to better understand its relationship to the output variable. An input variable is endogenous if it is affected by other variables in the system and the output variable depends on it. In a time series, the observations for an input variable depend upon one another. For example, the observation at time t is dependent upon the observation at t-1; t-1 may depend on t-2, and so on. An input variable is an exogenous variable if it is independent of other variables in the system and the output variable depends upon it. Put simply, endogenous variables are influenced by other variables in the system (including themselves) whereas as exogenous variables are not and are considered as outside the system. Typically, a time series forecasting problem has endogenous variables (e.g. the output is a function of some number of prior time steps) and may or may not have exogenous variables. Often, exogenous variables are ignored given the strong focus on the time series. Explicitly thinking about both variable types may help to identify easily overlooked exogenous data or even engineered features that may improve the model. What are the endogenous and exogenous variables? Regression predictive modeling problems are those where a quantity is predicted. A quantity is a numerical value; for example a price, a count, a volume, and so on. A time series forecasting problem in which you want to predict one or more future numerical values is a regression type predictive modeling problem. Classification predictive modeling problems are those where a category is predicted. A category is a label from a small well-defined set of labels; for example {¡°hot¡±, ¡°cold¡±}, {¡°up¡±, ¡°down¡±}, and {¡°buy¡±, ¡°sell¡±} are categories. A time series forecasting problem in which you want to classify input time series data is a classification type predictive modeling problem. Are you working on a regression or classification predictive modeling problem? There is some flexibility between these types. For example, a regression problem can be reframed as classification and a classification problem can be reframed as regression. Some problems, like predicting an ordinal value, can be framed as either classification and regression. It is possible that a reframing of your time series forecasting problem may simplify it. What are some alternate ways to frame your time series forecasting problem? It is useful to plot each variable in a time series and inspect the plot looking for possible patterns. A time series for a single variable may not have any obvious pattern. We can think of a series with no pattern as unstructured, as in there is no discernible time-dependent structure. Alternately, a time series may have obvious patterns, such as a trend or seasonal cycles as structured. We can often simplify the modeling process by identifying and removing the obvious structures from the data, such as an increasing trend or repeating cycle. Some classical methods even allow you to specify parameters to handle these systematic structures directly. Are the time series variables unstructured or structured? A single variable measured over time is referred to as a univariate time series. Univariate means one variate or one variable. Multiple variables measured over time is referred to as a multivariate time series: multiple variates or multiple variables. Are you working on a univariate or multivariate time series problem? Considering this question with regard to inputs and outputs may add a further distinction. The number of variables may differ between the inputs and outputs, e.g. the data may not be symmetrical. For example, you may have multiple variables as input to the model and only be interested in predicting one of the variables as output. In this case, there is an assumption in the model that the multiple input variables aid and are required in predicting the single output variable. A forecast problem that requires a prediction of the next time step is called a one-step forecast model. Whereas a forecast problem that requires a prediction of more than one time step is called a multi-step forecast model. The more time steps to be projected into the future, the more challenging the problem given the compounding nature of the uncertainty on each forecasted time step. Do you require a single-step or a multi-step forecast? It is possible to develop a model once and use it repeatedly to make predictions. Given that the model is not updated or changed between forecasts, we can think of this model as being static. Conversely, we may receive new observations prior to making a subsequent forecast that could be used to create a new model or update the existing model. We can think of developing a new or updated model prior to each forecasts as a dynamic problem. For example, it if the problem requires a forecast at the beginning of the week for the week ahead, we may receive the true observation at the end of the week that we can use to update the model prior to making next weeks forecast. This would be a dynamic model. If we do not get a true observation at the end of the week or we do and choose to not re-fit the model, this would be a static model. We may prefer a dynamic model, but the constraints of the domain or limitations of a chosen algorithm may impose constraints that make this intractable. Do you require a static or a dynamically updated model? A time series where the observations are uniform over time may be described as contiguous. Many time series problems have contiguous observations, such as one observation each hour, day, month or year. A time series where the observations are not uniform over time may be described as discontiguous. The lack of uniformity of the observations may be caused by missing or corrupt values. It may also be a feature of the problem where observations are only made available sporadically or at increasingly or decreasingly spaced time intervals. In the case of non-uniform observations, specific data formatting may be required when fitting some models to make the observations uniform over time. Are your observations contiguous or discontiguous? This section lists some resources for further reading. To review, the themes and questions you can ask about your problem are as follows: Did you find this framework useful for your time series forecasting problem?
Let me know in the comments below. Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ...with just a few lines of python code Discover how in my new Ebook:Introduction to Time Series Forecasting With Python It covers self-study tutorials and end-to-end projects on topics like:Loading data, visualization, modeling, algorithm tuning, and much more... Skip the Academics. Just Results. Click to learn more. Is there a flowchart or any other tool that helps make sense of the answers to the questions above? I.e. once we know the answers to the questions above how do we then choose an appropriate model for use with our data? Great question James! The next post scheduled (Friday) will answer exactly this question. Nice post. In Multi-Step forecast I would add that we could predict de difference, integral, etc, of the variable to predict over the future time ¡¦  There is one thing I did not understand 100% (I think) that is : Multivariate VS Multivariable terms ?  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3518362/ https://www.researchgate.net/post/Are_the_terms_multivariate_and_multivariable_the_same For instance the CO2 ¡°multivariate¡± time serie you have as example, according to these links is  ¡°Multivariate¡± or  ¡°Multivariable¡± ?  great work Thanks. Hi Jason, What approach do you recommend to forecast Forex rates? I have historical data about USD-INR currency pair and I wish to predict short, medium and long term forecast of this rate. I don¡¯t know anything about forex. Perhaps try this process:https://machinelearningmastery.com/how-to-develop-a-skilful-time-series-forecasting-model/ Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning. Read More","Keyword(freq): variable(17), observation(13), question(9), input(5), output(5), answer(4), problem(4), step(3), comment(2), constraint(2)"
"4","mastery",2018-08-06,"11 Classical Time Series Forecasting Methods in Python (Cheat Sheet)","https://machinelearningmastery.com/time-series-forecasting-methods-in-python-cheat-sheet/","Before exploring machine learning methods for time series, it is a good idea to ensure you have exhausted classical linear time series forecasting methods. Classical time series forecasting methods may be focused on linear relationships, nevertheless, they are sophisticated and perform well on a wide range of problems, assuming that your data is suitably prepared and the method is well configured. In this post, will you will discover a suite of classical methods for time series forecasting that you can test on your forecasting problem prior to exploring to machine learning methods. The post is structured as a cheat sheet to give you just enough information on each method to get started with a working code example and where to look to get more information on the method. All code examples are in Python and use the Statsmodels library. The APIs for this library can be tricky for beginners (trust me!), so having a working code example as a starting point will greatly accelerate your progress. This is a large post; you may want to bookmark it. Let¡¯s get started. 11 Classical Time Series Forecasting Methods in Python (Cheat Sheet)Photo by Ron Reiring, some rights reserved. This cheat sheet demonstrates 11 different classical time series forecasting methods; they are: Did I miss your favorite classical time series forecasting method?
Let me know in the comments below. Each method is presented in a consistent manner. This includes: Each code example is demonstrated on a simple contrived dataset that may or may not be appropriate for the method. Replace the contrived dataset with your data in order to test the method. Remember: each method will require tuning to your specific problem. In many cases, I have examples of how to configure and even grid search parameters on the blog already, try the search function. If you find this cheat sheet useful, please let me know in the comments below. The autoregression (AR) method models the next step in the sequence as a linear function of the observations at prior time steps. The notation for the model involves specifying the order of the model p as a parameter to the AR function, e.g. AR(p). For example, AR(1) is a first-order autoregression model. The method is suitable for univariate time series without trend and seasonal components. The moving average (MA) method models the next step in the sequence as a linear function of the residual errors from a mean process at prior time steps. A moving average model is different from calculating the moving average of the time series. The notation for the model involves specifying the order of the model q as a parameter to the MA function, e.g. MA(q). For example, MA(1) is a first-order moving average model. The method is suitable for univariate time series without trend and seasonal components. We can use the ARMA class to create an MA model and setting a zeroth-order AR model. We must specify the order of the MA model in the order argument. The Autoregressive Moving Average (ARMA) method models the next step in the sequence as a linear function of the observations and resiudal errors at prior time steps. It combines both Autoregression (AR) and Moving Average (MA) models. The notation for the model involves specifying the order for the AR(p) and MA(q) models as parameters to an ARMA function, e.g. ARMA(p, q). An ARIMA model can be used to develop AR or MA models. The method is suitable for univariate time series without trend and seasonal components. The Autoregressive Integrated Moving Average (ARIMA) method models the next step in the sequence as a linear function of the differenced observations and residual errors at prior time steps. It combines both Autoregression (AR) and Moving Average (MA) models as well as a differencing pre-processing step of the sequence to make the sequence stationary, called integration (I). The notation for the model involves specifying the order for the AR(p), I(d), and MA(q) models as parameters to an ARIMA function, e.g. ARIMA(p, d, q). An ARIMA model can also be used to develop AR, MA, and ARMA models. The method is suitable for univariate time series with trend and without seasonal components. The Seasonal Autoregressive Integrated Moving Average (SARIMA) method models the next step in the sequence as a linear function of the differenced observations, errors, differenced seasonal observations, and seasonal errors at prior time steps. It combines the ARIMA model with the ability to perform the same autoregression, differencing, and moving average modeling at the seasonal level. The notation for the model involves specifying the order for the AR(p), I(d), and MA(q) models as parameters to an ARIMA function and AR(P), I(D), MA(Q) and m parameters at the seasonal level, e.g. SARIMA(p, d, q)(P, D, Q)m where ¡°m¡± is the number of time steps in each season (the seasonal period). A SARIMA model can be used to develop AR, MA, ARMA and ARIMA models. The method is suitable for univariate time series with trend and/or seasonal components. The Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX) is an extension of the SARIMA model that also includes the modeling of exogenous variables. Exogenous variables are also called covariates and can be thought of as parallel input sequences that have observations at the same time steps as the original series. The primary series may be referred to as endogenous data to contrast it from the exogenous sequence(s). The observations for exogenous variables are included in the model directly at each time step and are not modeled in the same way as the primary endogenous sequence (e.g. as an AR, MA, etc. process). The SARIMAX method can also be used to model the subsumed models with exogenous variables, such as ARX, MAX, ARMAX, and ARIMAX. The method is suitable for univariate time series with trend and/or seasonal components and exogenous variables. The Vector Autoregression (VAR) method models the next step in each time series using an AR model. It is the generalization of AR to multiple parallel time series, e.g. multivariate time series. The notation for the model involves specifying the order for the AR(p) model as parameters to a VAR function, e.g. VAR(p). The method is suitable for multivariate time series without trend and seasonal components. The Vector Autoregression Moving-Average (VARMA) method models the next step in each time series using an ARMA model. It is the generalization of ARMA to multiple parallel time series, e.g. multivariate time series. The notation for the model involves specifying the order for the AR(p) and MA(q) models as parameters to a VARMA function, e.g. VARMA(p, q). A VARMA model can also be used to develop VAR or VMA models. The method is suitable for multivariate time series without trend and seasonal components. The Vector Autoregression Moving-Average with Exogenous Regressors (VARMAX) is an extension of the VARMA model that also includes the modeling of exogenous variables. It is a multivariate version of the ARMAX method. Exogenous variables are also called covariates and can be thought of as parallel input sequences that have observations at the same time steps as the original series. The primary series(es) are referred to as endogenous data to contrast it from the exogenous sequence(s). The observations for exogenous variables are included in the model directly at each time step and are not modeled in the same way as the primary endogenous sequence (e.g. as an AR, MA, etc. process). The VARMAX method can also be used to model the subsumed models with exogenous variables, such as VARX and VMAX. The method is suitable for univariate time series without trend and seasonal components and exogenous variables. The Simple Exponential Smoothing (SES) method models the next time step as an exponentially weighted linear function of observations at prior time steps. The method is suitable for univariate time series without trend and seasonal components. The Holt Winter¡¯s Exponential Smoothing (HWES) also called the Triple Exponential Smoothing method models the next time step as an exponentially weighted linear function of observations at prior time steps, taking trends and seasonality into account. The method is suitable for univariate time series with trend and/or seasonal components. This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered a suite of classical time series forecasting methods that you can test and tune on your time series dataset. Did I miss your favorite classical time series forecasting method?
Let me know in the comments below. Did you try any of these methods on your dataset?
Let me know about your findings in the comments. Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ...with just a few lines of python code Discover how in my new Ebook:Introduction to Time Series Forecasting With Python It covers self-study tutorials and end-to-end projects on topics like:Loading data, visualization, modeling, algorithm tuning, and much more... Skip the Academics. Just Results. Click to learn more. Hi Jason, thanks for such an excellent and comprehensive post on time series. I sincerely appreciate your effort.  As you ask for the further topic, just wondering if I can request you for a specific topic I have been struggling to get an output. It¡¯s about Structural Dynamic Factor model ( SDFM) by Barigozzi, M., Conti, A., and Luciani, M. (Do euro area countries respond asymmetrically to the common monetary policy) and Mario Forni Luca Gambetti (The Dynamic Effects of Monetary Policy: A Structural Factor Model Approach). Would it be possible for you to go over and estimate these two models using Python or R? It¡¯s just a request from me and sorry if it doesn¡¯t go with your interest. Thanks for the suggestion. I¡¯ve not heard of that method before. I am working on Time series or Prediction  with neural network and SVR, I want to this in matlab by scratch can you give me the references of this materials
 Thank you in advance Sorry, I don¡¯t have any materials for matlab, it is only really used in universities. Hi Jason! From which editor do you import the python code into the webpage of your article? Or what kind of container it that windowed control used to display the python code? Great question, I explain the software I use for the website here:https://machinelearningmastery.com/faq/single-faq/what-software-do-you-use-to-run-your-website Thanks for all the things to try! I recently stumbled over some tasks where the classic algorithms like linear regression or decision trees outperformed even sophisticated NNs. Especially when boosted or averaged out with each other. Maybe its time to try the same with time series forecasting as I¡¯m not getting good results for some tasks with an LSTM. Always start with simple methods before trying more advanced methods.  The complexity of advanced methods just be justified by additional predictive skill. Hi Jason, Thanks for this nice post! You¡¯ve imported the sin function from math many times but have not used it. I¡¯d like to see more posts about GARCH, ARCH and co-integration models. Best,
Elie Thanks, fixed. I have a post on ARCH (and friends) scheduled. Will you consider writing a follow-up book on advanced time-series models soon? Yes, it is written. I am editing it now. The title will be ¡°Deep Learning for Time Series Forecasting¡±. CNNs are amazing at time series, and CNNs + LSTMs together are really great. will the new book cover classical time-series models like VAR, GARCH, ..? The focus is deep learning (MLP, CNN and LSTM) with tutorials on how to get the most from classical methods (Naive, SARIMA, ETS) before jumping into deep learning methods. I hope to have it done by the end of the month. This is great news! Don¡¯t you think that R is better suited than Python for classical time-series models? Perhaps generally, but not if you are building a system for operational use. I think Python is a better fit. Great to hear this news. May I ask if the book also cover the topic of multivariate and multistep? Yes, there are many chapters on multi-step and most chapters work with multivariate data. Sounds amazing that you finally <U+0001F609> are geting the new book out on time-series models <U+2013> when will it be available to buy? Thanks. I hope by the end of the month or soon after. I use Prophet.https://facebook.github.io/prophet/docs/quick_start.html Also, sometimes FastFourier Transformations gives a good result. Thanks. What are the typical application domain of these algos? Forecasting a time series across domains. Hi Jason!
Firstly I congratulate you for your blog. It is helping me a lot in my final work on my bachelor¡¯s degree in Statistics!
What are the assumptions for make forecasting on time series using Machine Learning algorithms? For example, it must to be stationary? Thanks! Gaussian error, but they work anyway if you violate assumptions. The methods like SARIMA/ETS try to make the series stationary as part of modeling (e.g. differencing). You may want to look at power transforms to make data more Gaussian. Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning. Read More","Keyword(freq): model(27), method(15), component(11), observation(11), step(10), variable(10), thank(8), parameter(7), comment(5), error(5)"
"5","vidhya",2018-08-11,"Independence Day Bonanza with Analytics Vidhya¡¯s Offers and Launches!","https://www.analyticsvidhya.com/blog/2018/08/independence-day-bonanza-analytics-vidhyas-offers-launches/","

INDEPENDENCE DAY SALE: Get FLAT 60% OFF on All Courses | Use COUPON CODE: IDAY60 |
OFFER ENDS: 15th Aug 23:59 hrs IST | 
Valid for first 60 users(daily) | Buy Now 

 India is celebrating its 72nd Independence Day and so are we! It¡¯s day to celebrate our past, enjoy the present and be optimistic about the future. What would be the best way to do this? The best way is to build things for future and enable more and more people to do so. That¡¯s what we will be doing this Independence Day. Let me tell what is in store. Dr. Avik Sarkar is the Head of the Data Science Cell as part of the Indian government¡¯s NITI Aayog initiative. He is involved in the effective use of artificial intelligence, big data and data science techniques for social good in areas of real-time governance, healthcare, agriculture monitoring, education, energy, etc. This exclusive podcast will feature Kunal in conversation with Dr. Avik about the state of the data science field in India, what and how NITI Aayog is using data science to work on various challenges in multiple Indian sectors, among other things. Happy listening! We will offer incredible deals on our existing as well as upcoming training courses<U+00A0>.<U+00A0>It all kicks off on 11th August midnight. Check them out. Whether you want to learn data science, machine learning, computer vision, MS Excel, or want to understand what Artificial Intelligence is from a business leader¡¯s perspective, we are offering each of these courses at flat 60% off. An unmissable offer! Analytics Vidhya¡¯s flagship event DataHack Summit 2018 is one of the most anticipated and eagerly awaited machine learning, deep learning artificial intelligence and IoT conferences this year. It will be held on November 22-24 in Bengaluru. These are just some of the sectors we will be exploring in our Independence Day article (to be published on 14th November). Data science is a powerful tool so why shouldn¡¯t we use it for the betterment of this wonderful nation? We will provide links to resources like open datasets which you can download and contribute back to the community. It¡¯s both an informational piece as well as a call to action. All aboard the ¡®Data Science for good¡¯ express!","Keyword(freq): sector(2), analytics(1), challenge(1), conference(1), dataset(1), deal(1), government(1), kick(1), leader(1), link(1)"
"6","vidhya",2018-08-09,"Ultimate guide to handle Big Datasets for Machine Learning using Dask (in Python)","https://www.analyticsvidhya.com/blog/2018/08/dask-big-datasets-machine_learning-python/","

INDEPENDENCE DAY SALE: Get FLAT 60% OFF on All Courses | Use COUPON CODE: IDAY60 |
OFFER ENDS: 15th Aug 23:59 hrs IST | 
Valid for first 60 users(daily) | Buy Now 

 Have you ever tried working with a large dataset on a 4GB RAM machine? It starts heating up while doing simplest of machine learning tasks? This is a common problem data scientists face when working with restricted computational resources. When I started my data science journey using python, I almost immediately realized that the existing libraries have certain limitations when it comes to handling large datasets.<U+00A0>Pandas and Numpy are great libraries but they are not always computationally efficient, especially when there are GBs of data to manipulate. So what can you do to get around this obstacle? This is where Dask weaves its magic! It works with Pandas dataframes and Numpy data structures to help you perform data wrangling and model building using large datasets on not-so-powerful machines. Once you start using Dask, you won¡¯t look back. In this article, we will look at what Dask is, how it works, and how you can use it for working on large datasets. We will also take up a dataset and put Dask to good use. Let¡¯s begin! Let me illustrate these aforementioned limitations with a simple example. Suppose you have 4 balls (of different colors) and you are asked to separate them within an hour (based on the color) into different buckets. What if you are given a hundred balls and you have to separate them in an hour¡¯s time? That would be a tedious task but still sounds feasible. Imagine you are given a thousand balls and an hour to separate them into buckets. It is impossible for an individual to complete the task within the given time (in this case, the data is huge and the resources are limited). How would you accomplish this? The best bet would be to ask a few other people for help. You can call 9 other friends, give each of them 100 balls and ask them to separate these based on the color. In this case, 10 people are simultaneously working on the assigned task and together would be able to complete it faster than a single person would have (here you had a huge amount of data which you distributed among a bunch of people). Currently we use common libraries like pandas, numpy and scikit-learn for data preprocessing and model building. These libraries are not scalable and work on a single CPU. Dask however can scale up to a cluster of machines. To sum up, pandas and numpy are like the individual trying to sort the balls alone, while the group of people working together represent<U+00A0>Dask. Python is one of the most popular programming languages today and is widely used by data scientists and analysts across the globe. There are common python libraries (numpy, pandas, sklearn) for performing data science tasks and these are easy to understand and implement. But when it comes to working with large datasets using these python libraries, the run time can become very high due to memory constraints. These libraries usually work well if the dataset fits into the existing RAM. But if we are given a large dataset to analyze (like 8/16/32<U+00A0> GB or beyond), it would be difficult to process and model it. Unfortunately, these popular libraries were not designed to scale beyond a single machine. It is like asking a single person to separate a thousand balls in a limited time frame, it¡¯s quite unfair to ask! What should one do when faced with a dataset larger than what a single machine can process? This is where Dask comes into the picture. It is a python library that can handle moderately large datasets on a single CPU by using multiple cores of machines or on a cluster of machines (distributed computing). If you are familiar with pandas and numpy, you will find working with Dask fairly easy. Dask is popularly known as a ¡®parallel computing¡¯ python library that has been designed to run across multiple systems. Your next question would understandably be <U+2013> what is parallel computing? As in our example of separating the balls, 10 people doing the job simultaneously can be considered analogous to parallel computation. In technical terms, parallel computation is performing multiple tasks (or computations) simultaneously, using more than one resource. Dask can efficiently perform parallel computations on a single machine using multi-core CPUs. For example, if you have a quad core processor, Dask can effectively use all 4 cores of your system simultaneously for processing. In order to use lesser memory during computations, Dask stores the complete data on the disk, and uses chunks of data (smaller parts, rather than the whole data) from the disk for processing. During the processing, the intermediate values generated (if any) are discarded as soon as possible, to save the memory consumption. In summary, Dask can run on a cluster of machines to process data efficiently as it uses all the cores of the connected machines. One interesting fact here is that it is not necessary that all machines should have the same number of cores. If one system has 2 cores while the other has 4 cores, Dask can handle these variations internally. Dask supports the Pandas dataframe and Numpy array data structures to analyze large datasets. Basically, Dask lets you scale pandas and numpy with minimum changes in your code format. How great is that? Before we go ahead and explore the various functionalities provided by Dask, we need to setup our system first. Dask can be installed with conda, with pip, or directly from the source. This section explores all three options. Dask is installed in Anaconda by default. You can update it using the following command: To install Dask using pip, simply use the below code in your command prompt/terminal window: To install Dask from source, follow these steps: 1. Clone the git repository 2. Use pip to install all dependencies Now that we are familiar with Dask and have set up our system, let us talk about the Dask interface before we jump over to the python code. Dask provides several user interfaces, each having a different set of parallel algorithms for distributed computing. For data science practitioners looking for scaling numpy, pandas and scikit-learn, following are the important user interfaces: The dataset used for implementation in this article is AV¡¯s<U+00A0>Black Friday practice problem . You can download the dataset from the given link and follow along with the code blocks below. Let¡¯s get started! A large numpy array is divided into smaller arrays which, when grouped together, form the Dask array. In simple words, Dask arrays are distributed numpy arrays! Every operation on a Dask array triggers operations on the smaller numpy arrays, each using a core on the machine. Thus all available cores are used simultaneously enabling computations on arrays which are larger than the memory size. Below is an image to help you understand what a Dask array looks like: As you can see, a number of numpy arrays are arranged into grids to form a Dask array. While creating a Dask array, you can specify the chunk size which defines the size of the numpy arrays. For instance, if you have 10 values in an array and you give the chunk size as 5, it will return 2 numpy arrays with 5 values each. In summary, below are a few important features of Dask arrays below: We will now have a look at some simple cases for creating arrays using Dask. As you can see here, I had 11 values in the array and I used the chunk size as 5.<U+00A0> This distributed my array into three chunks, where the first and second blocks have 5 values each and the third one has 1 value. Dask arrays support most of the numpy functions. For instance, you can use .sum() or .mean(), as we will do now. Here, we simply converted our numpy array into a Dask array and used .mean() to do the operation. In all the above codes, you must have noticed that we used .compute() to get the results. This is because when we simply use dask_array.mean(), Dask builds a graph of tasks to be executed. To get the final result, we use the<U+00A0>.compute() function which triggers the actual computations. We saw that multiple numpy arrays are grouped together to form a Dask array. Similar to a Dask array, a Dask dataframe consists of multiple smaller pandas dataframes. A large pandas dataframe splits row-wise to form multiple smaller dataframes. These smaller dataframes are present on a disk of a single machine, or multiple machines (thus allowing to store datasets of size larger than the memory). Each computation on a Dask dataframe parallelizes operations on the existing pandas dataframes. Below is an image that represents the structure of a Dask dataframe: The APIs offered by the Dask dataframe are very similar to that of the pandas dataframe. Now, let¡¯s perform some basic operations on Dask dataframes. Time to load up the Black Friday dataset you had downloaded earlier! The Black Friday dataset used here has 5,50,068 rows. On using Dask, the read time<U+00A0>reduced more than ten times as compared to using pandas! Dask ML provides scalable machine learning algorithms in python which are compatible with scikit-learn. Let us first understand how scikit-learn handles the computations and then we will look at how Dask performs these operations differently. A user can perform parallel computing using scikit-learn (on a single machine) by setting the parameter njobs = -1. Scikit-learn uses Joblib<U+00A0>to perform these parallel computations. Joblib is a library in python that provides support for parallelization. When you call the .fit() function, based on the tasks to be performed (whether it is a hyperparameter search or fitting a model), Joblib<U+00A0>distributes the task over the available cores. To understand Joblib<U+00A0>in detail, you can have a look at this documentation. Even though parallel computations can be performed using scikit-learn, it cannot be scaled to multiple machines. On the other hand, Dask works well on a single machine and can also be scaled up to a cluster of machines. Dask has a central task scheduler and a set of workers. The scheduler assigns tasks to the workers. Each worker is assigned a number of cores on which it can perform computations. The workers provide two functions: Below is an example that explains how a conversation between a scheduler and workers looks like (this has been given by one of the developers of Dask, Matthew Rocklin): The central task scheduler sends jobs (python functions) to lots of worker processes, either on the same machine or on a cluster: This should give you a clear idea about how Dask works. Now we will discuss about machine learning models and Dask-search CV! Dask-ML provides scalable machine learning in python which we will discuss in this section. Implementation for the same will be covered in section 6. Let us first get our systems ready. Below are the installation steps for Dask-ML. 1. Parallelize Scikit-Learn Directly As we have seen previously, sklearn provides parallel computing (on a single CPU) using Joblib. In order to parallelize multiple sklearn estimators, you can directly use Dask by adding a few lines of code (without having to make modifications in the existing code). The first step is to import client from dask.distributed. This command will create a local scheduler and worker on your machine. To read more about the Dask client, you can refer to<U+00A0>this document. The next step will be to instantiate dask joblib in the backend. You need to import parallel_backend from sklearn joblib like I have shown below. 2. Reimplement Algorithms with Dask Array For simple machine learning algorithms which use Numpy arrays, Dask ML re-implements these algorithms. Dask replaces numpy arrays with Dask arrays to achieve scalable algorithms. This has been implemented for: A.<U+00A0> Linear model example B. Pre-processing example C. Clustering example Hyperparameter tuning is an important step in model building and can greatly affect the performance of your model. Machine learning models have multiple hyperparameters and it is not easy to figure out which parameter would work best for a particular case. Performing this task manually is generally a tedious process. In order to simplify the process, sklearn provides Gridsearch for hyperparameter tuning. The user is required to give the values for parameters and Gridsearch gives you the best combination of these parameters. Consider an example where you choose a random forest technique to fit the dataset. Your model has three important tunable parameters <U+2013> parameter 1, parameter 2 and parameter 3. You set the values for these parameters as: Parameter 1 <U+2013> Bootstrap = True Parameter 2 <U+2013> max_depth <U+2013> [8, 9]
 Parameter 3 <U+2013> n_estimators : [50, 100 , 200]
 sklearn Gridsearch : For each combination of the parameters, sklearn Gridsearch executes the tasks, sometimes ending up repeating a single task multiple times. As you can see from the below graph, this is not exactly the most efficient method: Dask-Search CV:<U+00A0>Parallel to Gridsearch CV in sklearn, Dask provides a library called Dask-search CV (Dask-search CV is now included in Dask ML). It merges steps so that there are less repetitions. Below are the installation steps for Dask-search. The following graph explains the working of Dask-Search CV: We will implement what we have learned so far on the Black Friday dataset and see how it works. Data exploration and treatment is out of the scope of this article as I will only illustrate how to use Dask for a ML problem.<U+00A0>In case you are interested in these steps, you can check out the below mentioned articles: 1. Using a simple logistic regression model and making predictions This will give you the predictions on the given test set. 2. Using grid search and random forest algorithm to find the best set of parameters. On printing grid_search.best_params_ you will get the best combination of parameters for the given mode. I have varied only a few parameters here but once you are comfortable with using dask-search, I would suggest experimenting with more parameters while using multiple varying values for each parameter. One very common question that I have seen while exploring Dask is: How is Dask different from Spark and which one is preferred? There is no hard and fast rule that says one should use Dask (or Spark), but you can make your choice based on the features offered by them and whichever one suits your requirements more. Here are some important differences between Dask and Spark : I have recently started using Dask and am still exploring this amazing library. It is comforting to know that I don¡¯t have to explore a whole new tool in order to build my models when faced with large datasets. The best part about Dask is that it offers an interface very similar to pandas and there is a very slight (sometimes negligible) difference in the code. There are innumerable tasks that one can perform using Dask thanks to the drastic reduction in processing time. Go ahead and explore this library and share your experience in the comments section below. Thanks for sharing. It sounds like a promising library. Glad you liked it! Hello Aishwarya, That¡¯s a really awesome utility. Thanks for sharing it.  I would like to make an edit in Section 6.2 below
***************************************************************
# Instantiate the grid search model
grid_search = dcv.GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3)
***************************************************************
Here we need to ¡°import dask_searchcv as dcv¡± to make this command work.
And before that one has to install in the env if it¡¯s not available. Please update it for the benefit of others. Hi Nitin, Thanks for pointing it out. I missed that line with the code. Have updated the same in the article. Also, the installation steps for dask_searchcv are provided in the previous section. Good article. It would be an added value to the Dask if we added the comparison on runtime stats. Will give a try to use this python package to deal with the huge volume of data! Hi Jenarthanan, I actually did add a comparison on reading the file using dask and pandas. When pandas took 541 ms, dask took only 35.9 ms to read the file. Thank you very much for sharing this. I can see that Dask has got inherently array or data frame structures, which seems promising, but in terms of performance, how is it comparable with mpi library, which is also used for parallel programming? Hi Sahar, Comparing Dask and mpi library in terms of performance, MPI outperforms dask. In fact, Matthew Rocklin has said in an interview, ¡°Dask not going to out-compete MPI on super computers¡±. Nice artical, thanks! just a quick one in section ¡°Set up your system: Dask Installation¡± , we might want to specify how to install it in cluster.  Means what are the steps needed to be done for making dask work on more than one machine.  Cheers! Hi Sandeep, Thanks for the suggestion. Will update it soon. Hey, I had a problem executing this statement. Pl see screen shot below:
X.chunks
AttributeError Traceback (most recent call last)
in ()
1 #to see the size of each chunk
<U+2014>-> 2 X.chunks AttributeError: ¡®numpy.ndarray¡¯ object has no attribute ¡®chunks¡¯ Hi Anshul, Looks like X in your case is a numpy array. Convert it into a dask array and then execute X.chunks. i have just copy pasted ur code from section 5.1 till the point where i get this X.chunks error
Please elaborate what could be wrong. Hi,","Keyword(freq): panda(17), array(14), machine(10), computation(9), core(9), parameter(9), dataset(8), library(8), task(8), value(8)"
"7","vidhya",2018-08-09,"Infographic <U+2013> A Complete Guide on Getting Started with Deep Learning in Python","https://www.analyticsvidhya.com/blog/2018/08/infographic-complete-deep-learning-path/","

INDEPENDENCE DAY SALE: Get FLAT 60% OFF on All Courses | Use COUPON CODE: IDAY60 |
OFFER ENDS: 15th Aug 23:59 hrs IST | 
Valid for first 60 users(daily) | Buy Now 

 You seem to come across the term ¡®Deep Learning¡¯ everywhere these days. It¡¯s all pervasive and seems to be at the heart of all AI related research. It has even spawned new and never-thought-of-before innovations! But how can you learn it? There are way too many resources out there, spread in a very unstructured and not a very beginner friendly manner. You complete a course on one platform, move to another course on a different platform, and so on. You learn, but not in any logical or sequential manner. That¡¯s a bad idea.","Keyword(freq): innovation(1), resource(1), NA(NA), NA(NA), NA(NA), NA(NA), NA(NA), NA(NA), NA(NA), NA(NA)"
"8","vidhya",2018-08-05,"DataHack Radio Episode #6: Exploring Techniques and Strategy with Coursera¡¯s Head of Data Science, Emily Glassberg Sands","https://www.analyticsvidhya.com/blog/2018/08/datahack-radio-episode-4-coursera-data-science-emily-glassberg-sands/","

INDEPENDENCE DAY SALE: Get FLAT 60% OFF on All Courses | Use COUPON CODE: IDAY60 |
OFFER ENDS: 15th Aug 23:59 hrs IST | 
Valid for first 60 users(daily) | Buy Now 

 Have you ever wondered how Coursera uses data science? Sure we have all taken (or heard of) their courses but how does the platform make use of the amount of data that¡¯s generated? How does their pricing strategy work for certificates? How is their data science team structured? What tools and techniques do they use? These are just some of the questions answered in this phenomenal podcast. We had the pleasure of hosting Coursera¡¯s Head of Data Science, Emily Glassberg Sands, who gave us a detailed and thorough explanation of how Coursera functions behind the scenes. It makes for very fascinating listening and you will learn a whole bunch of data science and machine learning stuff throughout. In this article, we have listed a few snippets from Kunal¡¯s conversation with Emily. Click the above SoundCloud button to listen to the podcast! You can subscribe to DataHack Radio or listen to previous episodes on any of the below platforms: Emily joined Coursera over four years ago after successfully completing her Ph.D in Economics from Harvard University. Before that, she completed her undergraduate degree from Princeton University in Economics as well. She co-authored two papers during her Ph.D which make for very interesting reads (they are available on her LinkedIn profile and are very relevant for data scientists). She had not initially applied to be a data scientist at Coursera, instead opting to apply to the Partnerships division. She liked Coursera¡¯s mission and during her initial few months there, she worked with various machine learning folks (half of the original team was comprised of machine learning researchers from Stanford).<U+00A0>Emily was the first data science hire at Coursera and is now their Head of Data Science! Her research during her graduation years was exclusively in MATLAB. She trained herself in R and Python using books (courses back then weren¡¯t as good as they are now) before joining Coursera and learned quite a lot on the job as well. Emily elaborated on Coursera¡¯s growth since she joined in 2014. The platform has seen a rapid rise in both the number of users and the amount of content they have. The team has grown to over 300 people across the organization, with the product and engineering department making up half of that number. Emily¡¯s data science team is currently comprised of the below three roles, along with their functions: So which language(s) does Coursera¡¯s data science team use for day-to-day tasks? R and Python! Everyone on the team is expected to know and work with these two popular languages. For the content discovery aspect that we see on Coursera¡¯s platform, it¡¯s important for the data science team to establish the right metadata (understanding features of the content and Coursera¡¯s users). This is followed by building relatively lightweight logistic regression models to recommend the best content to the end user.<U+00A0>There are also a few black box models which the team experiments with. But the biggest boon has been what Emily calls ¡°skills graphs¡± or ¡°knowledge graphs¡±. This contains a bunch of metadata about content and learners. When you combine these two, you get very powerful predictions that Coursera leverages in it¡¯s end product. Building the graph however, is a bit complex. It maps about 40,000 skills to the available content types and this is what powers various machine learning models that work on recommendations. The team has been working on this knowledge graph since almost two years. At this point Emily offered some valuable insights into what a data scientist should expect <U+2013> it wasn¡¯t a one step process. They built out one part of the knowledge graph, tried it out in a practical situation, and then went ahead with building the rest, piece by piece. If you are interested in NLP or just want to know how a data science project functions from scratch, you should listen to this section in the podcast. Collaborative filtering, one of the most popular recommendation techniques, was initially being used by Coursera before they started using tons of metadata to refine and improve on it. If you have the amount of information and data that the team has collected over the years, why not use it to improve the experience for it¡¯s users? Emily wanted to improve the credential value of Coursera¡¯s certificates when she started working there. Her working hypothesis, before she had a look at the data, was that people in developing countries would value the certificate as it would help them in their job applications. The data revealed otherwise <U+2013> people in these countries were not taking these certificates. Using various econometric and data science techniques including regression analysis, fixed effects model, etc. she discovered that learners in developing markets were far more price sensitive than learners in developed markets. It sounds pretty intuitive now but was not something the team had thought of back then. Now came the question of how to price these certificates. Emily and her team ruled out trying A/B testing from the outset because of how unfair that could be for certain people. Instead, they went with a quasi experimental design. Emily took a bundle of developing markets and applied synthetic control methods and difference-in-difference analysis. Emily has described this in much more detail in the podcast and it makes for really interesting listening. This is a pretty common issue observed in today¡¯s MOOC dominated world. People start a course but leave halfway through due to various reasons. Emily discussed how Coursera divides their approach into two parts to handle this: After experimenting with a predictive model that identified at-risk learners, the team then built several advanced models to recognize the reason why learners were dropping off at certain points. They have built several ¡°intervention points¡± which help them understand where the user might need a nudge, or extra help, and then take action accordingly. On the instructor side of things, Coursera lets them do A/B testing to understand their audience. Two versions of a course are created at the backend and the instructor can edit one of those to make it a bit different. Then when learners enroll in the course, they are shown one of these two versions by a randomization method. ¡°Team building, for me, is about collecting a superstar team.¡± Emily looks at a number of components when building her data science team. I have mentioned a few crucial ones below: Analytics Vidhya has featured Emily in the ¡°most influential women in data science¡± list two years running now. She is an advocate of diversity in data science and enlightened us with her opinions on the subject in this podcast. Below are a few pointers she mentioned which hiring companies should take note of. When crafting a job description, she makes sure it¡¯s inclusive and gender neutral. She also believes it¡¯s important to invest in diversity early when you have a small team. Co-hosting events with the ¡®Women who Code¡¯ organization has also worked out really well for Coursera. Small things in the interview process also make a world of difference <U+2013> making sure that all candidates are asked the same questions, ensuring feedback from interviewers is not being shown to each other until they have submitted their own feedback, etc. There are a lot more really thoughtful points in the podcast. ¡°Both are tremendously valuable in isolation, and particularly valuable when combined.¡± Emily agreed with Kunal¡¯s assessment that people who comes from an economics background have more intuitive insights and hypothesis as compared to folks who come from computer science degrees. She mentioned that when taking these two separately, they have their own unique value. But they become truly valuable when they¡¯re combined.","Keyword(freq): coursera(6), learner(6), certificate(4), model(4), economic(3), market(3), point(3), technique(3), user(3), folk(2)"
