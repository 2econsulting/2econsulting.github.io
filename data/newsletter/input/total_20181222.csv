"","site","date","headline","url_address","text","keyword"
"1","kaggle",2018-12-19,"Hackathon Winner Interview: Penn State | Kaggle University Club","http://blog.kaggle.com/2018/12/19/hackathon-winner-interview-penn-state-kaggle-university-club/","We believe today¡¯s university students are tomorrow¡¯s leading data scientists. As such, we decided to launch Kaggle University Club <U+2014> a virtual community and Slack channel for existing data science clubs who want to compete in Kaggle competitions together. As our end-of-year event, we hosted our first-ever University Hackathon! 18 total kernels were submitted and the three top scoring teams won exclusive Kaggle swag and an opportunity to be featured here, on No Free Hunch. Please enjoy this profile from one of the top scoring university teams, ¡®Team NDL¡¯ from Penn State! To read more about the Hackathon and its grading criteria, see here: Winter ¡®18 Hackathon and to read this team¡¯s winning kernel, visit: Team NDL: Algorithms and Illnesses. Neil Ashtekar Major: Computer ScienceHometown: State College, PennsylvaniaAnticipated graduation: Spring 2020 What brought you to data science? I had read a lot about machine learning/artificial intelligence in the news, and I wanted to see what all the hype was about. So, I decided to complete Andrew Ng¡¯s machine learning class on Coursera. I learned a ton, and I really enjoyed the material. After finishing the class, I wanted to apply what I learned, so I turned to Kaggle. I started out with the basic competitions (Titanic, MNIST), then moved on to work with some more interesting datasets (Kobe Bryant Shot Selection, World Happiness Predictors).  What are your career aspirations after graduation? I want to get a job as a Machine Learning Engineer (not sure where!). --- William Wright Major: Mathematics Hometown: Dallas, TexasAnticipated graduation date: Spring 2019 What brought you to data science?I originally wanted to become a math professor, but after reading Smart People Should Build Things by Andrew Yang and Zero to One by Peter Thiel, I became more interested in pursuing a career involving technology. In his book, Yang claims the decisions we make in the next decade will decide whether society moves towards the future of Mad Max or Star Trek. This comment really stuck with me and inspired me to start learning python and to join Nittany Data Labs (the Penn State data science club). What are your career aspirations after graduation? I¡¯d love to work as a <U+00A0>data scientist or machine learning engineer. --- Izzi Oakes Major: Integrative Arts Anticipated graduation date: Fall 2020 What brought you to data science? I went to my university¡¯s first data science club meeting by random chance, and within five minutes I was hooked. This was about a year ago, and I had never programmed anything before and was in a completely unrelated major. I¡¯ve spent the past year grabbing any and all resources online I could find related to data science and devouring them, as well as moving towards studying higher level math and statistics. What are your career aspirations after graduation? I¡¯d like to be in a position where I do work related to some kind of intersection between machine learning and music / visual arts. 
How familiar was your team with Kaggle competitions prior to the Hackathon? A few of us had completed Kaggle competitions in the past, but they were mainly the beginner ones. This was our first time working on a competition as a team, as well as on a longer term project, as this competition lasted about a month.  How did your team work together on your Kernel? We started out working individually to explore and understand the data. After a week of exploration on our own, we met up to talk about our findings and ideas moving forward. At this point, we created a shared kernel and implemented our ideas in code. What was the most challenging part of the hackathon for you? Working with text data! None of us had any experience with natural language processing, so understanding how to represent the written review data was challenging.  What surprised you most about the competition? We were surprised by how well a very simple linear regression model worked with the problem. We had a long conversation about whether we should be using Neural Networks to solve the problem, and potentially why other approaches would work just as well. What advice would you give another student who wanted to compete in a Kaggle competition or even a hackathon? If you¡¯re just starting, definitely start with one of the beginner challenges. Try to work your way through it as much as you can by googling things if you get stuck, then begin looking through existing kernels people have once you¡¯re finished. These will give you great approaches to the problem, and you can begin on improving your own model. Also, try to build your way up to this if you¡¯re just starting. If you really don¡¯t feel like you¡¯re understanding anything you¡¯re doing, there are many great free ML courses and books online! Anything else? Thanks a lot for featuring us! You¡¯re welcome! Team NDL from Penn State University<U+00A0>(from left to right: Neil Ashtekar, Izzi Oakes, Suraj Dalsania, Will Wright, Ming Ju Li).","Keyword(freq): competition(4), aspiration(3), team(3), approach(2), art(2), idea(2), kernel(2), oake(2), algorithm(1), book(1)"
"2","datacamp",2018-12-18,"DataCamp Announces New Funding","https://www.datacamp.com/community/blog/datacamp-announces-new-funding","I¡¯m thrilled to announce a significant investment in DataCamp this month, led by Spectrum Equity, that will support our continued and rapid growth in 2019 and beyond. DataCamp enables companies to build data fluency across their organizations with hands-on courses taught by experts in the field. Spectrum Equity¡¯s investment will enable us to grow in ways that are critical to increasing data fluency by building the smartest data science and analytics learning platform out there. Looking ahead to 2019, we¡¯ll be laser-focused on three key areas of growth: Curriculum Expansion:
We¡¯re positioned for major expansion across DataCamp¡¯s data science and analytics curriculum, providing a much-needed career-long resource for learners at every level. In 2019, we¡¯re doubling the total number of Courses, Projects and Practice exercises on the platform. We¡¯ll continue to grow our Python and R content, while also adding content in critical-need areas like SQL, Spreadsheets, Data Engineering, Deep Learning and AI, and more. Product Innovation:
DataCamp¡¯s platform is getting smarter and more personalized. Users will gain insight into their current skill level, guidance on where they need to improve, and personalized recommendations for Courses and Projects to take next. We¡¯re also improving our interactive feedback system, so hints and correction messages will provide even more relevant feedback, exactly when you need it. DataCamp for Business:
DataCamp gives modern companies a scalable learning solution to build data science and analytics capabilities for better decision making. Companies will have access to LMS integrations and a reporting API, and we¡¯re making it easier to manage teams of all sizes on DataCamp with Team Manager roles. To learn more, read the full press release here.","Keyword(freq): analytics(3), company(3), datacamp(2), project(2), capability(1), exercise(1), expert(1), hint(1), integration(1), learner(1)"
"3","mastery",2018-12-21,"How to Reduce the Variance of Deep Learning Models in Keras Using Model Averaging Ensembles","https://machinelearningmastery.com/model-averaging-ensemble-for-deep-learning-neural-networks/","Deep learning neural network models are highly flexible nonlinear algorithms capable of learning a near infinite number of mapping functions. A frustration with this flexibility is the high variance in a final model. The same neural network model trained on the same dataset may find one of many different possible ¡°good enough¡± solutions each time it is run. Model averaging is an ensemble learning technique that reduces the variance in a final neural network model, sacrificing spread in the performance of the model for a confidence in what performance to expect from the model. In this tutorial, you will discover how to develop a model averaging ensemble in Keras to reduce the variance in a final model. After completing this tutorial, you will know: Let¡¯s get started. How to Reduce the Variance of Deep Learning Models in Keras With Model Averaging EnsemblesPhoto by John Mason, some rights reserved. This tutorial is divided into six parts; they are: Deep learning neural network models are nonlinear methods that learn via a stochastic training algorithm. This means that they are highly flexible, capable of learning complex relationships between variables and approximating any mapping function, given enough resources. A downside of this flexibility is that the models suffer high variance. This means that the models are highly dependent on the specific training data used to train the model and on the initial conditions (random initial weights) and serendipity during the training process. The result is a final model that makes different predictions each time the same model configuration is trained on the same dataset. This can be frustrating when training a final model for use in making predictions on new data, such as operationally or in a machine learning competition. The high variance of the approach can be addressed by training multiple models for the problem and combining their predictions. This approach is called model averaging and belongs to a family of techniques called ensemble learning. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course The simplest way to develop a model averaging ensemble in Keras is to train multiple models on the same dataset then combine the predictions from each of the trained models. Training multiple models may be resource intensive, depending on the size of the model and the size of the training data. You may have to train the models sequentially on the same hardware. For very large models, it may be worth training the models in parallel using cloud infrastructure such as Amazon Web Services. The number of models required for the ensemble may vary based on the complexity of the problem and model. A benefit of the approach is that you can continue to create models, add them to the ensemble, and evaluate their impact on the performance by making predictions on a holdout test set. For small models, you can train the models sequentially and keep them in memory for use in your experiment. For example: For large models, perhaps trained on different hardware, you can save each model to file. Models can then be loaded later. Small models can all be loaded into memory at the same time, whereas very large models may have to be loaded one at a time to make a prediction, then later to have the predictions combined. Once the models have been prepared, each model can be used to make a prediction and the predictions can be combined. In the case of a regression problem where each model is predicting a real-valued output, the values can be collected and the average calculated. In the case of a classification problem, there are two options. The first is to calculate the mode of the predicted integer class values. A downside of this approach is that for small ensembles or problems with a large number of classes, the sample of predictions may not be large enough for the mode to be meaningful. In the case of a binary classification problem, a sigmoid activation function is used on the output layer and the average of the predicted probabilities can be calculated much like a regression problem. In the case of a multi-class classification problem with more than two classes, a softmax activation function is used on the output layer and the sum of the probabilities for each predicted class can be calculated before taking the argmax to get the class value. These approaches for combining predictions of Keras models will work just as well for Multilayer Perceptron, Convolutional, and Recurrent Neural Networks. Now that we know how to average predictions from multiple neural network models in Keras, let¡¯s work through a case study. We will use a small multi-class classification problem as the basis to demonstrate a model averaging ensemble. The scikit-learn class provides the make_blobs() function that can be used to create a multi-class classification problem with the prescribed number of samples, input variables, classes, and variance of samples within a class. We use this problem with 500 examples, with input variables (to represent the x and y coordinates of the points) and a standard deviation of 2.0 for points within each group. We will use the same random state (seed for the pseudorandom number generator) to ensure that we always get the same 500 points. The results are the input and output elements of a dataset that we can model. In order to get a feeling for the complexity of the problem, we can graph each point on a two-dimensional scatter plot and color each point by class value. The complete example is listed below. Running the example creates a scatter plot of the entire dataset. We can see that the standard deviation of 2.0 means that the classes are not linearly separable (separable by a line) causing many ambiguous points. This is desirable as it means that the problem is non-trivial and will allow a neural network model to find many different ¡°good enough¡± candidate solutions resulting in a high variance. Scatter Plot of Blobs Dataset with Three Classes and Points Colored by Class Value Now that we have defined a problem, we can define a model to address it. We will define a model that is perhaps under-constrained and not tuned to the problem. This is intentional to demonstrate the high variance of a neural network model seen on truly large and challenging supervised learning problems. The problem is a multi-class classification problem, and we will model it using a softmax activation function on the output layer. This means that the model will predict a vector with 3 elements with the probability that the sample belongs to each of the 3 classes. Therefore, the first step is to one hot encode the class values. Next, we must split the dataset into training and test sets. We will use the test set both to evaluate the performance of the model and to plot its performance during training with a learning curve. We will use 30% of the data for training and 70% for the test set. This is an example of a challenging problem where we have more unlabeled examples than we do labeled examples. Next, we can define and compile the model. The model will expect samples with two input variables. The model then has a single hidden layer with 15 modes and a rectified linear activation function, then an output layer with 3 nodes to predict the probability of each of the 3 classes and a softmax activation function. Because the problem is multi-class, we will use the categorical cross entropy loss function to optimize the model and the efficient Adam flavor of stochastic gradient descent. The model is fit for 200 training epochs and we will evaluate the model each epoch on the test set, using the test set as a validation set. At the end of the run, we will evaluate the performance of the model on both the train and the test sets. Then finally, we will plot learning curves of the model accuracy over each training epoch on both the training and test dataset. The complete example is listed below. Running the example first prints the performance of the final model on the train and test datasets. Your specific results will vary (by design!) given the high variance nature of the model. In this case, we can see that the model achieved about 84% accuracy on the training dataset and about 76% accuracy on the test dataset; not terrible. A line plot is also created showing the learning curves for the model accuracy on the train and test sets over each training epoch. We can see that the model is not really overfit, but is perhaps a little underfit and may benefit from an increase in capacity, more training, and perhaps some regularization. All of these improvements of which we intentionally hold back to force the high variance for our case study. Line Plot Learning Curves of Model Accuracy on Train and Test Dataset Over Each Training Epoch It is important to demonstrate that the model indeed has a variance in its prediction. We can demonstrate this by repeating the fit and evaluation of the same model configuration on the same dataset and summarizing the final performance of the model. To do this, we first split the fit and evaluation of the model out as a function that we can call repeatedly. The evaluate_model() function below takes the train and test dataset, fits a model, then evaluates it, retuning the accuracy of the model on the test dataset. We can call this function 30 times, saving the test accuracy scores. Once collected, we can summarize the distribution scores, first in terms of the mean and standard deviation, assuming the distribution is Gaussian, which is very reasonable. We can then summarize the distribution both as a histogram to show the shape of the distribution and as a box and whisker plot to show the spread and body of the distribution. The complete example of summarizing the variance of the MLP model on the chosen blobs dataset is listed below. Running the example first prints the accuracy of each model on the test set, finishing with the mean and standard deviation of the sample of accuracy scores. The specifics of your sample may differ, but the summary statistics should be similar. In this case, we can see that the average of the sample is 77% with a standard deviation of about 1.4%. Assuming a Gaussian distribution, we would expect 99% of accuracy scores to fall between about 73% and 81% (i.e. 3 standard deviations above and below the mean). We can take the standard deviation of the accuracy of the model on the test set as an estimate for the variance of the predictions made by the model. A histogram of the accuracy scores is also created, showing a very rough Gaussian shape, perhaps with a longer right tail. A large sample and a different number of bins on the plot might better expose the true underlying shape of the distribution. Histogram of Model Test Accuracy Over 30 Repeats A box and whisker plot is also created showing a line at the median at about 76.5% accuracy on the test set and the interquartile range or middle 50% of the samples between about 78% and 76%. Box and Whisker Plot of Model Test Accuracy Over 30 Repeats The analysis of the sample of test scores clearly demonstrates a variance in the performance of the same model trained on the same dataset. A spread of likely scores of about 8 percentage points (81% <U+2013> 73%) on the test set could reasonably be considered large, e.g. a high variance result. We can use model averaging to both reduce the variance of the model and possibly reduce the generalization error of the model. Specifically, this would result in a smaller standard deviation on the holdout test set and a better performance on the training set. We can check both of these assumptions. First, we must develop a function to prepare and return a fit model on the training dataset. Next, we need a function that can take a list of ensemble members and make a prediction for an out of sample dataset. This could be one or more samples arranged in a two-dimensional array of samples and input features. Hint: you can use this function yourself for testing ensembles and for making predictions with ensembles on new data. We don¡¯t know how many ensemble members will be appropriate for this problem. Therefore, we can perform a sensitivity analysis of the number of ensemble members and how it impacts test accuracy. This means we need a function that can evaluate a specified number of ensemble members and return the accuracy of a prediction combined from those members. Finally, we can create a line plot of the number of ensemble members (x-axis) versus the accuracy of a prediction averaged across that many members on the test dataset (y-axis). The complete example is listed below. Running the example first fits 20 models on the same training dataset, which may take less than a minute on modern hardware. Then, different sized ensembles are tested from 1 member to all 20 members and test accuracy results are printed for each ensemble size. Finally, a line plot is created showing the relationship between ensemble size and performance on the test set. We can see that performance improves to about five members, after which performance plateaus around 76% accuracy. This is close to the average test set performance observed during the analysis of the repeated evaluation of the model. Line Plot of Ensemble Size Versus Model Test Accuracy Finally, we can update the repeated evaluation experiment to use an ensemble of five models instead of a single model and compare the distribution of scores. The complete example of a repeated evaluated five-member ensemble of the blobs dataset is listed below. Running the example may take a few minutes as five models are fit and evaluated and this process is repeated 30 times. The performance of each model on the test set is printed to provide an indication of progress. The mean and standard deviation of the model performance is printed at the end of the run. Your specific results may vary, but not by much. In this case, we can see that the average performance of a five-member ensemble on the dataset is 76%. This is very close to the average of 77% seen for a single model. The important difference is the standard deviation shrinking from 1.4% for a single model to 0.6% with an ensemble of five models. We might expect that a given ensemble of five models on this problem to have a performance fall between about 74% and about 78% with a likelihood of 99%. Averaging the same model trained on the same dataset gives us a spread for improved reliability, a property often highly desired in a final model to be used operationally. More models in the ensemble will further decrease the standard deviation of the accuracy of an ensemble on the test dataset given the law of large numbers, at least to a point of diminishing returns. This demonstrates that for this specific model and prediction problem, that a model averaging ensemble with five members is sufficient to reduce the variance of the model. This reduction in variance, in turn, also means a better on-average performance when preparing a final model. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop a model averaging ensemble in Keras to reduce the variance in a final model. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Better Deep Learning It provides self-study tutorials on topics like: weight decay, batch normalization, dropout, model stacking and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hello Jason, great post, as usual!. I am very interested in audio/sound processing and machine / deep learning application to it.  Image processing is well covered in ML domain. Unfortunately audio/sound is not. I wonder if by any chance you could kindly point to some knowledge source for the topic? Or even better any introduction blog post would be much appreciated <U+0001F609> I hope to cover the topic in the future, thanks for the suggestion. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): model(29), prediction(12), score(8), kera(6), point(6), sample(6), result(5), ensemble(4), variable(4), curve(3)"
"4","mastery",2018-12-19,"Ensemble Methods for Deep Learning Neural Networks to Reduce Variance and Improve Performance","https://machinelearningmastery.com/ensemble-methods-for-deep-learning-neural-networks/","Deep learning neural networks are nonlinear methods. They offer increased flexibility and can scale in proportion to the amount of training data available. A downside of this flexibility is that they learn via a stochastic training algorithm which means that they are sensitive to the specifics of the training data and may find a different set of weights each time they are trained, which in turn produce different predictions. Generally, this is referred to as neural networks having a high variance and it can be frustrating when trying to develop a final model to use for making predictions. A successful approach to reducing the variance of neural network models is to train multiple models instead of a single model and to combine the predictions from these models. This is called ensemble learning and not only reduces the variance of predictions but also can result in predictions that are better than any single model. In this post, you will discover methods for deep learning neural networks to reduce variance and improve prediction performance. After reading this post, you will know: Let¡¯s get started. Ensemble Methods to Reduce Variance and Improve Performance of Deep Learning Neural NetworksPhoto by University of San Francisco¡¯s Performing Arts, some rights reserved. This tutorial is divided into four parts; they are: Training deep neural networks can be very computationally expensive. Very deep networks trained on millions of examples may take days, weeks, and sometimes months to train. Google¡¯s baseline model [¡¦] was a deep convolutional neural network [¡¦] that had been trained for about six months using asynchronous stochastic gradient descent on a large number of cores. <U+2014> Distilling the Knowledge in a Neural Network, 2015. After the investment of so much time and resources, there is no guarantee that the final model will have low generalization error, performing well on examples not seen during training. ¡¦ train many different candidate networks and then to select the best, [¡¦] and to discard the rest. There are two disadvantages with such an approach. First, all of the effort involved in training the remaining networks is wasted. Second, [¡¦] the network which had best performance on the validation set might not be the one with the best performance on new test data. <U+2014> Pages 364-365, Neural Networks for Pattern Recognition, 1995. Neural network models are a nonlinear method. This means that they can learn complex nonlinear relationships in the data. A downside of this flexibility is that they are sensitive to initial conditions, both in terms of the initial random weights and in terms of the statistical noise in the training dataset. This stochastic nature of the learning algorithm means that each time a neural network model is trained, it may learn a slightly (or dramatically) different version of the mapping function from inputs to outputs, that in turn will have different performance on the training and holdout datasets. As such, we can think of a neural network as a method that has a low bias and high variance. Even when trained on large datasets to satisfy the high variance, having any variance in a final model that is intended to be used to make predictions can be frustrating. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course A solution to the high variance of neural networks is to train multiple models and combine their predictions. The idea is to combine the predictions from multiple good but different models. A good model has skill, meaning that its predictions are better than random chance. Importantly, the models must be good in different ways; they must make different prediction errors. The reason that model averaging works is that different models will usually not make all the same errors on the test set. <U+2014> Page 256, Deep Learning, 2016. Combining the predictions from multiple neural networks adds a bias that in turn counters the variance of a single trained neural network model. The results are predictions that are less sensitive to the specifics of the training data, choice of training scheme, and the serendipity of a single training run. In addition to reducing the variance in the prediction, the ensemble can also result in better predictions than any single best model. ¡¦ the performance of a committee can be better than the performance of the best single network used in isolation. <U+2014> Page 365, Neural Networks for Pattern Recognition, 1995. This approach belongs to a general class of methods called ¡°ensemble learning¡± that describes methods that attempt to make the best use of the predictions from multiple models prepared for the same problem. Generally, ensemble learning involves training more than one network on the same dataset, then using each of the trained models to make a prediction before combining the predictions in some way to make a final outcome or prediction. In fact, ensembling of models is a standard approach in applied machine learning to ensure that the most stable and best possible prediction is made. For example, Alex Krizhevsky, et al. in their famous 2012 paper titled ¡°Imagenet classification with deep convolutional neural networks¡± that introduced very deep convolutional neural networks for photo classification (i.e. AlexNet) used model averaging across multiple well-performing CNN models to achieve state-of-the-art results at the time. Performance of one model was compared to ensemble predictions averaged over two, five, and seven different models. Averaging the predictions of five similar CNNs gives an error rate of 16.4%. [¡¦] Averaging the predictions of two CNNs that were pre-trained [¡¦] with the aforementioned five CNNs gives an error rate of 15.3%. Ensembling is also the approach used by winners in machine learning competitions. Another powerful technique for obtaining the best possible results on a task is model ensembling. [¡¦] If you look at machine-learning competitions, in particular on Kaggle, you¡¯ll see that the winners use very large ensembles of models that inevitably beat any single model, no matter how good. <U+2014> Page 264, Deep Learning With Python, 2017. Perhaps the oldest and still most commonly used ensembling approach for neural networks is called a ¡°committee of networks.¡± A collection of networks with the same configuration and different initial random weights is trained on the same dataset. Each model is then used to make a prediction and the actual prediction is calculated as the average of the predictions. The number of models in the ensemble is often kept small both because of the computational expense in training models and because of the diminishing returns in performance from adding more ensemble members. Ensembles may be as small as three, five, or 10 trained models. The field of ensemble learning is well studied and there are many variations on this simple theme. It can be helpful to think of varying each of the three major elements of the ensemble method; for example: Let¡¯s take a closer look at each element in turn. The data used to train each member of the ensemble can be varied. The simplest approach would be to use k-fold cross-validation to estimate the generalization error of the chosen model configuration. In this procedure, k different models are trained on k different subsets of the training data. These k models can then be saved and used as members of an ensemble. Another popular approach involves resampling the training dataset with replacement, then training a network using the resampled dataset. The resampling procedure means that the composition of each training dataset is different with the possibility of duplicated examples allowing the model trained on the dataset to have a slightly different expectation of the density of the samples, and in turn different generalization error. This approach is called bootstrap aggregation, or bagging for short, and was designed for use with unpruned decision trees that have high variance and low bias. Typically a large number of decision trees are used, such as hundreds or thousands, given that they are fast to prepare. ¡¦ a natural way to reduce the variance and hence increase the prediction accuracy of a statistical learning method is to take many training sets from the population, build a separate prediction model using each training set, and average the resulting predictions. [¡¦] Of course, this is not practical because we generally do not have access to multiple training sets. Instead, we can bootstrap, by taking repeated samples from the (single) training data set. <U+2014> Pages 216-317, An Introduction to Statistical Learning with Applications in R, 2013. An equivalent approach might be to use a smaller subset of the training dataset without regularization to allow faster training and some overfitting. The desire for slightly under-optimized models applies to the selection of ensemble members more generally. ¡¦ the members of the committee should not individually be chosen to have optimal trade-off between bias and variance, but should have relatively smaller bias, since the extra variance can be removed by averaging. <U+2014> Page 366, Neural Networks for Pattern Recognition, 1995. Other approaches may involve selecting a random subspace of the input space to allocate to each model, such as a subset of the hyper-volume in the input space or a subset of input features. Training the same under-constrained model on the same data with different initial conditions will result in different models given the difficulty of the problem, and the stochastic nature of the learning algorithm. This is because the optimization problem that the network is trying to solve is so challenging that there are many ¡°good¡± and ¡°different¡± solutions to map inputs to outputs. Most neural network algorithms achieve sub-optimal performance specifically due to the existence of an overwhelming number of sub-optimal local minima. If we take a set of neural networks which have converged to local minima and apply averaging we can construct an improved estimate. One way to understand this fact is to consider that, in general, networks which have fallen into different local minima will perform poorly in different regions of feature space and thus their error terms will not be strongly correlated. <U+2014> When networks disagree: Ensemble methods for hybrid neural networks, 1995. This may result in a reduced variance, but may not dramatically improve generalization error. The errors made by the models may still be too highly correlated because the models all have learned similar mapping functions. An alternative approach might be to vary the configuration of each ensemble model, such as using networks with different capacity (e.g. number of layers or nodes) or models trained under different conditions (e.g. learning rate or regularization). The result may be an ensemble of models that have learned a more heterogeneous collection of mapping functions and in turn have a lower correlation in their predictions and prediction errors. Differences in random initialization, random selection of minibatches, differences in hyperparameters, or different outcomes of non-deterministic implementations of neural networks are often enough to cause different members of the ensemble to make partially independent errors. <U+2014> Pages 257-258, Deep Learning, 2016. Such an ensemble of differently configured models can be achieved through the normal process of developing the network and tuning its hyperparameters. Each model could be saved during this process and a subset of better models chosen to comprise the ensemble. Slightly inferiorly trained networks are a free by-product of most tuning algorithms; it is desirable to use such extra copies even when their performance is significantly worse than the best performance found. Better performance yet can be achieved through careful planning for an ensemble classification by using the best available parameters and training different copies on different subsets of the available database. <U+2014> Neural Network Ensembles, 1990. In cases where a single model may take weeks or months to train, another alternative may be to periodically save the best model during the training process, called snapshot or checkpoint models, then select ensemble members among the saved models. This provides the benefits of having multiple models trained on the same data, although collected during a single training run. Snapshot Ensembling produces an ensemble of accurate and diverse models from a single training process. At the heart of Snapshot Ensembling is an optimization process which visits several local minima before converging to a final solution. We take model snapshots at these various minima, and average their predictions at test time. <U+2014> Snapshot Ensembles: Train 1, get M for free, 2017. A variation on the Snapshot ensemble is to save models from a range of epochs, perhaps identified by reviewing learning curves of model performance on the train and validation datasets during training. Ensembles from such contiguous sequences of models are referred to as horizontal ensembles. First, networks trained for a relatively stable range of epoch are selected. The predictions of the probability of each label are produced by standard classifiers [over] the selected epoch[s], and then averaged. <U+2014> Horizontal and vertical ensemble with deep representation for classification, 2013. A further enhancement of the snapshot ensemble is to systematically vary the optimization procedure during training to force different solutions (i.e. sets of weights), the best of which can be saved to checkpoints. This might involve injecting an oscillating amount of noise over training epochs or oscillating the learning rate during training epochs. A variation of this approach called Stochastic Gradient Descent with Warm Restarts (SGDR) demonstrated faster learning and state-of-the-art results for standard photo classification tasks. Our SGDR simulates warm restarts by scheduling the learning rate to achieve competitive results [¡¦] roughly two to four times faster. We also achieved new state-of-the-art results with SGDR, mainly by using even wider [models] and ensembles of snapshots from SGDR¡¯s trajectory. <U+2014> SGDR: Stochastic Gradient Descent with Warm Restarts, 2016. A benefit of very deep neural networks is that the intermediate hidden layers provide a learned representation of the low-resolution input data. The hidden layers can output their internal representations directly, and the output from one or more hidden layers from one very deep network can be used as input to a new classification model. This is perhaps most effective when the deep model is trained using an autoencoder model. This type of ensemble is referred to as a vertical ensemble. This method ensembles a series of classifiers whose inputs are the representation of intermediate layers. A lower error rate is expected because these features seem diverse. <U+2014> Horizontal and vertical ensemble with deep representation for classification, 2013. The simplest way to combine the predictions is to calculate the average of the predictions from the ensemble members. This can be improved slightly by weighting the predictions from each model, where the weights are optimized using a hold-out validation dataset. This provides a weighted average ensemble that is sometimes called model blending. ¡¦ we might expect that some members of the committee will typically make better predictions than other members. We would therefore expect to be able to reduce the error still further if we give greater weight to some committee members than to others. Thus, we consider a generalized committee prediction given by a weighted combination of the predictions of the members ¡¦ <U+2014> Page 367, Neural Networks for Pattern Recognition, 1995. One further step in complexity involves using a new model to learn how to best combine the predictions from each ensemble member. The model could be a simple linear model (e.g. much like the weighted average), but could be a sophisticated nonlinear method that also considers the specific input sample in addition to the predictions provided by each member. This general approach of learning a new model is called model stacking, or stacked generalization. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. [¡¦] When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. <U+2014> Stacked generalization, 1992. There are more sophisticated methods for stacking models, such as boosting where ensemble members are added one at a time in order to correct the mistakes of prior models. The added complexity means this approach is less often used with large neural network models. Another combination that is a little bit different is to combine the weights of multiple neural networks with the same structure. The weights of multiple networks can be averaged, to hopefully result in a new single model that has better overall performance than any original model. This approach is called model weight averaging. ¡¦ suggests it is promising to average these points in weight space, and use a network with these averaged weights, instead of forming an ensemble by averaging the outputs of networks in model space <U+2014> Averaging Weights Leads to Wider Optima and Better Generalization, 2018. In summary, we can list some of the more common and interesting ensemble methods for neural networks organized by each element of the method that can be varied, as follows: There is no single best ensemble method; perhaps experiment with a few approaches or let the constraints of your project guide you. This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered ensemble methods for deep learning neural networks to reduce variance and improve prediction performance. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Better Deep Learning It provides self-study tutorials on topics like: weight decay, batch normalization, dropout, model stacking and much more¡¦ Skip the Academics. Just Results. Click to learn more. I truly enjoy your books. I  am thinking about purchasing this one. Thanks! Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): model(37), network(32), prediction(29), method(9), weight(9), ensemble(7), result(7), error(5), layer(5), input(4)"
"5","mastery",2018-12-17,"Introduction to Regularization to Reduce Overfitting of Deep Learning Neural Networks","https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/","Training a deep neural network that can generalize well to new data is a challenging problem. A model with too little capacity cannot learn the problem, whereas a model with too much capacity can learn it too well and overfit the training dataset. Both cases result in a model that does not generalize well. A modern approach to reducing generalization error is to use a larger model that may be required to use regularization during training that keeps the weights of the model small. These techniques not only reduce overfitting, but they can also lead to faster optimization of the model and better overall performance. In this post, you will discover the problem of overfitting when training neural networks and how it can be addressed with regularization methods. After reading this post, you will know: Let¡¯s get started. A Gentle Introduction to Regularization to Reduce Overfitting and Improve Generalization ErrorPhoto by jaimilee.beale, some rights reserved. This tutorial is divided into four parts; they are: The objective of a neural network is to have a final model that performs well both on the data that we used to train it (e.g. the training dataset) and the new data on which the model will be used to make predictions. The central challenge in machine learning is that we must perform well on new, previously unseen inputs <U+2014> not just those on which our model was trained. The ability to perform well on previously unobserved inputs is called generalization. <U+2014> Page 110, Deep Learning, 2016. We require that the model learn from known examples and generalize from those known examples to new examples in the future. We use methods like a train/test split or k-fold cross-validation only to estimate the ability of the model to generalize to new data. Learning and also generalizing to new cases is hard. Too little learning and the model will perform poorly on the training dataset and on new data. The model will underfit the problem. Too much learning and the model will perform well on the training dataset and poorly on new data, the model will overfit the problem. In both cases, the model has not generalized. Learning Curves A model fit can be considered in the context of the bias-variance trade-off. An underfit model has high bias and low variance. Regardless of the specific samples in the training data, it cannot learn the problem. An overfit model has low bias and high variance. The model learns the training data too well and performance varies widely with new unseen examples or even statistical noise added to examples in the training dataset. In order to generalize well, a system needs to be sufficiently powerful to approximate the target function. If it is too simple to fit even the training data then generalization to new data is also likely to be poor. [¡¦] An overly complex system, however, may be able to approximate the data in many different ways that give similar errors and is unlikely to choose the one that will generalize best ¡¦ <U+2014> Page 241, Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999. We can address underfitting by increasing the capacity of the model. Capacity refers to the ability of a model to fit a variety of functions; more capacity, means that a model can fit more types of functions for mapping inputs to outputs. Increasing the capacity of a model is easily achieved by changing the structure of the model, such as adding more layers and/or more nodes to layers. Because an underfit model is so easily addressed, it is more common to have an overfit model. An overfit model is easily diagnosed by monitoring the performance of the model during training by evaluating it on both a training dataset and on a holdout validation dataset. Graphing line plots of the performance of the model during training, called learning curves, will show a familiar pattern. For example, line plots of the loss (that we seek to minimize) of the model on train and validation datasets will show a line for the training dataset that drops and may plateau and a line for the validation dataset that drops at first, then at some point begins to rise again. As training progresses, the generalization error may decrease to a minimum and then increase again as the network adapts to idiosyncrasies of the training data. <U+2014> Page 250, Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999. A learning curve plot tells the story of the model learning the problem until a point at which it begins overfitting and its ability to generalize to the unseen validation dataset begins to get worse. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course There are two ways to approach an overfit model: A benefit of very deep neural networks is that their performance continues to improve as they are fed larger and larger datasets. A model with a near-infinite number of examples will eventually plateau in terms of what the capacity of the network is capable of learning. A model can overfit a training dataset because it has sufficient capacity to do so. Reducing the capacity of the model reduces the likelihood of the model overfitting the training dataset, to a point where it no longer overfits. The capacity of a neural network model, it¡¯s complexity, is defined by both it¡¯s structure in terms of nodes and layers and the parameters in terms of its weights. Therefore, we can reduce the complexity of a neural network to reduce overfitting in one of two ways: In the case of neural networks, the complexity can be varied by changing the number of adaptive parameters in the network. This is called structural stabilization. [¡¦] The second principal approach to controlling the complexity of a model is through the use of regularization which involves the addition of a penalty term to the error function. <U+2014> Page 332, Neural Networks for Pattern Recognition, 1995. For example, the structure could be tuned such as via grid search until a suitable number of nodes and/or layers is found to reduce or remove overfitting for the problem. Alternately, the model could be overfit and pruned by removing nodes until it achieves suitable performance on a validation dataset. It is more common to instead constrain the complexity of the model by ensuring the parameters (weights) of the model remain small. Small parameters suggest a less complex and, in turn, more stable model that is less sensitive to statistical fluctuations in the input data. Large weighs tend to cause sharp transitions in the [activation] functions and thus large changes in output for small changes in inputs. <U+2014> Page 269, Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999. It is more common to focus on methods that constrain the size of the weights in a neural network because a single network structure can be defined that is under-constrained, e.g. has a much larger capacity than is required for the problem, and regularization can be used during training to ensure that the model does not overfit. In such cases, performance can even be better as the additional capacity can be focused on better learning generalizable concepts in the problem. Techniques that seek to reduce overfitting (reduce generalization error) by keeping network weights small are referred to as regularization methods. More specifically, regularization refers to a class of approaches that add additional information to transform an ill-posed problem into a more stable well-posed problem. A problem is said to be ill-posed if small changes in the given information cause large changes in the solution. This instability with respect to the data makes solutions unreliable because small measurement errors or uncertainties in parameters may be greatly magnified and lead to wildly different responses. [¡¦] The idea behind regularization is to use supplementary information to restate an ill-posed problem in a stable form. <U+2014> Page 266, Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999. Regularization methods are so widely used to reduce overfitting that the term ¡°regularization¡± may be used for any method that improves the generalization error of a neural network model. Regularization is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error. Regularization is one of the central concerns of the field of machine learning, rivaled in its importance only by optimization. <U+2014> Page 120, Deep Learning, 2016. The simplest and perhaps most common regularization method is to add a penalty to the loss function in proportion to the size of the weights in the model. This will encourage the model to map the inputs to the outputs of the training dataset in such a way that the weights of the model are kept small. This approach is called weight regularization or weight decay and has proven very effective for decades for both simpler linear models and neural networks. A simple alternative to gathering more data is to reduce the size of the model or improve regularization, by adjusting hyperparameters such as weight decay coefficients ¡¦ <U+2014> Page 427, Deep Learning, 2016. Below is a list of five of the most common additional regularization methods. Most of these methods have been demonstrated (or proven) to approximate the effect of adding a penalty to the loss function. Each method approaches the problem differently, offering benefits in terms of a mixture of generalization performance, configurability, and/or computational complexity. This section outlines some recommendations for using regularization methods for deep learning neural networks. You should always consider using regularization, unless you have a very large dataset, e.g. big-data scale. Unless your training set contains tens of millions of examples or more, you should include some mild forms of regularization from the start. <U+2014> Page 426, Deep Learning, 2016. A good general recommendation is to design a neural network structure that is under-constrained and to use regularization to reduce the likelihood of overfitting. ¡¦ controlling the complexity of the model is not a simple matter of finding the model of the right size, with the right number of parameters. Instead, ¡¦ in practical deep learning scenarios, we almost always do find<U+2014>that the best fitting model (in the sense of minimizing generalization error) is a large model that has been regularized appropriately. <U+2014> Page 229, Deep Learning, 2016. Early stopping should almost universally be used in addition to a method to keep weights small during training. Early stopping should be used almost universally. <U+2014> Page 426, Deep Learning, 2016. Some more specific recommendations include: These recommendations would suit Multilayer Perceptrons and Convolutional Neural Networks. Some recommendations for recurrent neural nets include: There are no silver bullets when it comes to regularization and systematic experimentation is strongly encouraged. This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered the problem of overfitting when training neural networks and how it can be addressed with regularization methods. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Better Deep Learning It provides self-study tutorials on topics like: weight decay, batch normalization, dropout, model stacking and much more¡¦ Skip the Academics. Just Results. Click to learn more. Jo thanks fur the post. However, did you mistake loss and accuracy on your three plots about overfitting / underfitting and good fit? No, but they are maximizing (accuracy) rather than minimizing (loss). Perhaps that is confusing Couldn¡¯t underfitting be simply because of there not being enough data, instead of its being because of network capacity constraints? No, if there is not enough data you would overfit. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): network(12), method(9), weight(8), example(7), parameter(6), input(5), case(4), change(4), layer(4), node(4)"
"6","vidhya",2018-12-21,"DataHack Radio #14: Quantum Computing and Quantum Machine Learning with Dr. Mandaar Pande","https://www.analyticsvidhya.com/blog/2018/12/datahack-radio-quantum-machine-learning/","

New Year's Grand Sale - 40% Discount On All Courses (Use Coupon: HNY2019) Click To Enroll Today !

 Quantum computing and quantum machine learning <U+2013> most of us have come across these concepts at some point without getting the opportunity to delve deeper. But what if I told you that these could potentially disrupt the way we see and use technology? We are joined by Dr. Mandaar Pande in episode #14 of the DataHack Radio podcast, where he navigates us through the wonderfully complex world of quantum computing. Here¡¯s a mind-blowing fact to give you a taste of what to expect: ¡°The number of bits in a 300 qubit quantum computer will be more than the known atoms in the universe.¡±<U+00A0> <U+2013> Dr. Mandaar Pande Before I personally met Dr. Mandaar at DataHack Summit 2018 (where he also spoke on this subject), I only had a vague sense of what quantum computers are and the gigantic amount of power they can process. But as you¡¯ll soon find out, there¡¯s a lot more that goes on behind the scenes that one might never have thought of. I have briefly covered the main topics discussed in this episode but the true joy and knowledge lies in listening to Dr. Mandaar himself. Subscribe to DataHack Radio today and listen to this, as well as all previous episodes, on any of the below platforms: Dr. Mandaar holds a Ph.D degree in theoretical physics from the University of Hyderabad, with a specialization in non-linear optics. After completing his Ph.D in 1994, he took up a post as a lecturer at BITS Pilani for the next four years in the EEE department (electrical engineering). Experienced folks will recall that it was in the late ¡¯90s when IT started picking up steam in India, and Dr. Mandaar decided to take the plunge and explore other avenues outside of academia. He joined Tech Mahindra in 1998 as part of the modeling and simulation centre. There, he worked<U+00A0>in the capacity of Group Head and Principal Consultant in the area of Performance<U+00A0>Engineering and Management. Following a 12 year stint at Tech Mahindra, he spent 7 years at Wipro <U+2013> first as a Lead Architect, then as the Global Practice Head for Performance Engineering <U+2013> Quality Engineering and Testing. Dr. Mandaar¡¯s experience in this field, as you can tell, is incredibly rich and not many folks come close to rivalling his experience and know-how. But for Dr. Mandaar, it felt inevitable that he would return to academia at some point and he joined Symbiosis as the professor of IT last year. So where does his interest and passion for quantum computing and machine learning fit into the picture? Well, towards his final years with Wipro, he got some exposure to the digital way of working (and data science was a big part of that). He built up an interest while working there, and kept that up during his transition back to academia with Symbiosis. His Ph.D in quantum optics obviously helped while he pursued quantum computing. But what is this field exactly? And how does it tie into machine learning? Let¡¯s hear that from Dr. Mandaar himself: ¡°Quantum Computing is a field that is at the intersection of quantum physics, information science, as well as function theory. And one of the largest applications of quantum computing in the near future is going to be quantum machine learning.<U+00A0>¡° This is a tough one, and a question I have been wondering about ever since I heard about this subject. Below are the two key points Dr. Mandaar mentioned: There¡¯s a lot more to it, but if you don¡¯t possess a solid base in these two aspects, it¡¯s going to be next to impossible to make headway. The current version of digital devices we use, like computers and smartphones, use metal chips in them (which are based on integrated circuits that are in turn based on transistors). Whatever computing we do today, including the data we capture and the analysis we perform, is done using the 2 bits we see in these transistors <U+2013> 0 and 1. Any algorithm that we write eventually gets broken down into 0 and 1 at the machine level. At the most fundamental level, the physical principles that govern the nature in quantum computing follow the laws of quantum mechanics. So then what is quantum mechanics? It¡¯s a theory in physics that describes nature at the smallest level (atoms and molecules). At this really basic level, these particles behave very differently. Now, there are a couple of things that are very important to understand: This is just a taste of what Dr. Mandaar described in the podcast. He broke down this complex topic into easy-to-digest bits of information using examples. If you¡¯ve ever wondered how quantum computers work from the ground up, this section will feel like you¡¯ve hit the jackpot.","Keyword(freq): computer(3), atom(2), folk(2), mechanic(2), optic(2), physic(2), transistor(2), application(1), aspect(1), avenue(1)"
"7","vidhya",2018-12-19,"A Technical Overview of AI & ML (NLP, Computer Vision, Reinforcement Learning) in 2018 & Trends for 2019","https://www.analyticsvidhya.com/blog/2018/12/key-breakthroughs-ai-ml-2018-trends-2019/","

New Year's Grand Sale - 40% Discount On All Courses (Use Coupon: HNY2019) Click To Enroll Today !

 The last few years have been a dream run for Artificial Intelligence enthusiasts and machine learning professionals. These technologies have evolved from being a niche to becoming mainstream, and are impacting millions of lives today. Countries now have dedicated AI ministers and budgets to make sure they stay relevant in this race. The same has been true for a data science professional. A few years back <U+2013> you would have been comfortable knowing a few tools and techniques. Not anymore! There is so much happening in this domain and so much to keep pace with <U+2013> it feels mind boggling at times. This is why I thought of taking a step back and looking at the developments in some of the key areas in Artificial Intelligence from a data science practitioners¡¯ perspective. What were these breakthroughs? What happened in 2018 and what can be expected in 2019? Read this article to find out! P.S. As with any forecasts, these are my takes. These are based on me trying to connect the dots. If you have a different perspective <U+2013> I would love to hear it. Do let me know what you think might change in 2019. Making machines parse words and sentences has always seemed like a dream. There are way too many nuances and aspects of a language that even humans struggle to grasp at times. But 2018 has truly been a watershed moment for NLP. We saw one remarkable breakthrough after another <U+2013> ULMFiT, ELMO, OpenAI¡¯s Transformer and Google¡¯s BERT to name a few. The successful application of transfer learning (the art of being able to apply pretrained models to data) to NLP tasks has blown open the door to potentially unlimited applications. Our podcast with Sebastian<U+00A0>Ruder further cemented our belief in how far his field has traversed in recent times. As a side note, that¡¯s a<U+00A0>must-listen podcast for all NLP enthusiasts. Let¡¯s look at some of these key developments in a bit more detail. And if you¡¯re looking to learn the ropes in NLP and are looking for a place to get started, make sure you head over to this ¡®NLP using Python¡® course. It¡¯s as good a place as any to start your text-fuelled journey! Designed by Sebastian Ruder and fast.ai¡¯s Jeremy Howard, ULMFiT was the first framework that got the NLP transfer learning party started this year. For the uninitiated, it stands for Universal Language Model Fine-Tuning. Jeremy and Sebastian have truly put the word Universal in ULMFiT <U+2013> the framework can be applied to almost any NLP task! The best part about ULMFiT and the subsequent frameworks we¡¯ll see soon? You don¡¯t need to train models from scratch! These researchers have done the hard bit for you <U+2013> take their learning and apply it in your own projects. ULMFiT outperformed state-of-the-art methods in six text classification tasks. You can read this excellent tutorial by Prateek Joshi on how to get started with ULMFiT for any text classification problem. Want to take a guess at what ELMo stands for? It¡¯s short for Embeddings from Language Models. Pretty creative, eh? Apart from it¡¯s name resembling the famous Sesame Street character, ELMo grabbed the attention of the ML community as soon as it was released. ELMo uses language models to obtain embeddings for each word while also considering the context in which the word fits into the sentence or paragraph. Context is such a crucial aspect of NLP that most people failed to grasp before. ELMo uses bi-directional LSTMs to create the embeddings. Don¡¯t worry if that sounds like a mouthful <U+2013> check out this article to get a really simple overview of what LSTMs are and how they work. Like ULMFiT, ELMo significantly improves the performance of a wide variety of NLP tasks, like sentiment analysis and question answering. Read more about it here. Quite a few experts have claimed that the release of BERT marks a new era in NLP. Following ULMFiT and ELMo, BERT really blew away the competition with it¡¯s performance. As the original paper states, ¡°BERT is conceptually simple and empirically powerful¡±. BERT obtained state-of-the-art results on 11 (yes, 11!) NLP tasks. Check out their results on the SQuAD benchmark: Interested in getting started? You can use either the PyTorch implementation or Google¡¯s own TensorFlow code to try and replicate the results on your own machine. I¡¯m fairly certain you are wondering what BERT stands for at this point. <U+0001F642> It¡¯s<U+00A0>Bidirectional Encoder Representations from Transformers. Full marks if you got it right the first time. How could Facebook stay out of the race? They have open-sourced their own deep learning NLP framework called PyText. It was released earlier this week so I¡¯m still to experiment with it, but the early reviews are extremely promising. According to research published by FB, PyText has led to a 10% increase in accuracy of conversational models and reduced the training time as well. PyText is actually behind a few of Facebook¡¯s own products like the FB Messenger. So working on this adds some real-world value to your own portfolio (apart from the invaluable knowledge you¡¯ll gain obviously). You can try it out yourself by downloading the code from this GitHub repo. If you haven¡¯t heard of Google Duplex yet, where have you been?! Sundar Pichai knocked it out of the park with this demo and it has been in the headlines ever since: Since this is a Google product, there¡¯s a slim chance of them open sourcing the code behind it. But wow! That¡¯s a pretty awesome audio processing application to showcase. Of course it raises a lot of ethical and privacy questions, but that¡¯s a discussion for later in this article. For now, just revel in how far we have come with ML in recent years. Who better than Sebastian Ruder himself to provide a handle on where NLP is headed in 2019? Here are his thoughts: This is easily the most popular field right now in the deep learning space. I feel like we have plucked the low-hanging fruits of computer vision to quite an extent and are already in the refining stage. Whether it¡¯s image or video, we have seen a plethora of frameworks and libraries that have made computer vision tasks a breeze. We at Analytics Vidhya spent a lot of time this year working on democratizing these concepts. Check out our computer vision specific articles here, covering topics from object detection in videos and images to lists of pretrained models to get your deep learning journey started. Here¡¯s my pick of the best developments we saw in CV this year. And if you¡¯re curious about this wonderful field (actually going to become one of the hottest jobs in the industry soon), then go ahead and start your journey with our ¡®Computer Vision using Deep Learning¡¯ course. Ian Goodfellow designed GANs in 2014, and the concept has spawned multiple and diverse applications since. Year after year we see the original concept being tweaked to fit a practical use case. But one thing has remained fairly consistent till this year <U+2013> images generated by machines were fairly easy to spot. There would always be some inconsistency in the frame which made the distinction fairly obvious. But that boundary has started to seep away in recent months. And with the creation of BigGANs, that boundary could be removed permanently. Check out the below images generated using this method: Unless you take a microscope to it, you won¡¯t be able to tell if there¡¯s anything wrong with that collection. Concerning or exciting? I¡¯ll leave that up to you, but there¡¯s no doubt GANs are changing the way we perceive digital images (and videos). For the data scientists out there, these models were trained on the ImageNet dataset first and then the JFT-300M data to showcase that these models transfer well from one set to the other. I would also to direct you to the GAN Dissection page <U+2013> a really cool way to visualize and understand GANs. This was a really cool development. There is a very common belief that you need a ton of data along with heavy computational resources to perform proper deep learning tasks. That includes training a model from scratch on the ImageNet dataset. I understand that perception <U+2013> most of us thought the same before a few folks at fast.ai found a way to prove all of us wrong. Their model gave an accuracy of 93% in an impressive 18 minutes timeframe. The hardware they used, detailed in their<U+00A0>blog post, contained 16 public AWS cloud instances, each with 8 NVIDIA V100 GPUs. They built the algorithm using the fastai and PyTorch libraries. The total cost of putting the whole thing together came out to be just<U+00A0>$40!<U+00A0>Jeremy has described their approach, including techniques, in much more detail<U+00A0>here. A win for everyone! Image processing has come leaps and bounds in the last 4-5 years, but what about video? Translating methods from a static frame to a dynamic one has proved to be a little tougher than most imagined. Can you take a video sequence and predict what will happen in the next frame? It had been explored before but the published research had been vague, at best. NVIDIA decided to open source their approach earlier this year, and it was met with widespread praise. The goal of their vid2vid approach is to learn a mapping function from a given input video in order to produce an output video which depicts the contents of the input video with incredible precision. You can try out their PyTorch implementation available on their GitHub here. Like I mentioned earlier, we might see modifications rather than inventions in 2019. It might feel like more of the same <U+2013> self-driving cars, facial recognition algorithms, virtual reality, etc. Feel free to disagree with me here and add your point of view <U+2013> I would love to know what else we can expect next year that we haven¡¯t already seen. Drones, pending political and government approvals, might finally get the green light in the United States (India is far behind there). Personally, I would like to see a lot of the research being implemented in real-world scenarios. Conferences like CVPR and ICML portray the latest in this field but how close are those projects to being used in reality? Visual question answering and visual dialog systems could finally make their long-awaited debut soon. These systems lack the ability to generalize but the expectation is that we¡¯ll see an integrated multi-modal approach soon. Self-supervised learning came to the forefront this year. I can bet on that being used in far more studies next year. It¡¯s a really cool line of learning <U+2013> the labels are directly determined from the data we input, rather than wasting time labelling images manually. Fingers crossed! This section will appeal to all data science professionals. Tools and libraries are the bread and butter of data scientists. I have been part a part of plenty of debates about which tool is the best, which framework supersedes the other, which library is the epitome of economical computations, etc. I¡¯m sure quite a lot of you will be able to relate to this as well. But one thing we can all agree on <U+2013> we need to be on top of the latest tools in the field, or risk being left behind. The pace with which Python has overtaken everything else and planted itself as the industry leader is example enough of this. Of course a lot of this comes down to subjective choices (what tool is your organization using, how feasible is it to switch from the current framework to a new one, etc.), but if you aren¡¯t even considering the state-of-the-art out there, then I implore you to start NOW. So what made the headlines this year? Let¡¯s find out! What¡¯s all the hype about PyTorch? I¡¯ve mentioned it multiple times already in this article (and you¡¯ll see more instances later). I¡¯ll leave it to my colleague Faizan Shaikh to acquaint you with the framework. That¡¯s one of my favorite deep learning articles on AV <U+2013> a must-read! Given how slow TensorFlow can be at times, it opened the door for PyTorch to capture the deep learning market in double-quick time. Most of the code that I see open soruced on GitHub is a PyTorch implemnantation of the concept. It¡¯s not a coincidence <U+2013> PyTorch is super flexible and the latest version (v1.0) already powers many Facebook products and services at scale, including performing 6 billion text translations a day. PyTorch¡¯s adoption rate is only going to go up in 2019 so now is as good a time as any to get on board. Automated machine learning (or AutoML) has been gradually making inroads in the last couple of years. Companies like RapidMiner, KNIME, DataRobot and H2O.ai have released excellent products showcasing the immense potential of this service. Can you imagine working on a ML project where you only need to work with a drag-and-drop interface without coding? It¡¯s a scenario that¡¯s not too far off in the future. But apart from these companies, there was a significant release in the ML/DL space <U+2013> Auto Keras! It¡¯s an open source library for performing AutoML tasks. The idea behind it is to make deep learning accessible to domain experts who perhaps don¡¯t have a ML background. Make sure you check it out here. It is primed to make a huge run in the coming years. We¡¯ve been building and designing machine learning and deep learning models in our favorite IDEs and notebooks since we got into this line of work. How about taking a step out and trying something different? Yes, I¡¯m talking about performing deep learning in your web browser itself! This is now a reality thanks to the release of TensorFlow.js. That link has a few demos as well which demonstrate how cool this open source concept is. There are primarily three advantages/features of TensorFlow.js: I wanted to focus particularly on AutoML in this thread. Why? Because I feel it¡¯s going to be a real-game changer in the data science space in the next few years. But dont just take my word for it! Here¡¯s H2O.ai¡¯s Marios Michailidis, Kaggle Grandmaster, with his view of what to expect from AutoML in 2019: Machine learning continues its march into being one of the most important trends of the future <U+2013> of where the world is going towards to. This expansion has increased the demand for skilled applications in this space. Given its growth , it is imperative that automation is the key into utilising the data science resources as best as possible. The applications are limitless: Credit, insurance, fraud, computer vision, acoustics,sensors, recommenders, forecasting, NLP <U+2013> you name it. It is a privilege to be working in this space . The trends that will continue being important can be defined as: If I had to pick one field where I want to see more penetration, it would be reinforcement learning. Apart from the occasional headlines we see at irregular intervals, there hasn¡¯t yet been a game-changing breakthrough. The general perception I have seen in the community is that it¡¯s too math-heavy and there are no real industry applications to work on. While this is true to a certain extent, I would love to see more practical use cases coming out of RL next year. In my monthly GitHub and Reddit series, I tend to keep at least one repository or discussion on RL to at least foster a discussion around the topic. This might well be the next big thing to come out of all that research. OpenAI have released a really helpful toolkit to get beginners started with the field, which I have mentioned below. You can also check out this beginner-friendly introduction on the topic (it has been super helpful for me). If there¡¯s anything I have missed, would love to hear your thoughts on it. If research in RL has been slow, the educational material around it has been minimal (at best). But true to their word, OpenAI have open sourced some awesome material on the subject. They are calling this project ¡®Spinning Up in Deep RL¡¯ and you can read all about it here. It¡¯s actually quite a comprehensive list of resources on RL and they have attempted to keep the code and explanations as simple as possible. There is quite a lot of material which includes things like RL terminologies, how to grow into an RL research role, a list of important papers, a supremely well-documented code repository, and even a few exercised to get you started. No more procrastinating now <U+2013> if you were planning to get started with RL, your time has come! To accelerate research and get the community more involved in reinforcement learning, the Google AI team has open sourced Dopamine, a TensorFlow framework that aims to create research by making it more flexible and reproducible. You can find the entire training data along with the TensorFlow code (just 15 Python notebooks!) on this GitHub repository. Here¡¯s the perfect platform for performing easy experiments in a controlled and flexible environment. Sounds like a dream for any data scientist. Xander Steenbrugge, speaker at DataHack Summit 2018 and founder of the ArxivInsights channel, is quite the expert in reinforcement learning. Here are his thoughts on the current state of RL and what to expect in 2019: BONUS: Check out Xander¡¯s video about overcoming sparse rewards in Deep RL (the first challenge highlighted above). Imagine a world ruled by algorithms that dictate every action humans take. Not exactly a rosy scenario, is it? Ethics in AI is a topic we at Analytics Vidhya have always been keen to talk about. It becomes bogged down amid all the technical discussions when it should be considered along with those topics. Quite a few organizations were left with egg on their face this year with Facebook¡¯s Cambridge Analytica scandal and Google¡¯s internal rife about designing weapons headlining the list of scandals. But all of this led to the big tech companies penning down charters and guidelines they intend to follow. There isn¡¯t one out-of-the-box solution or one size fits all solution to handling the ethical aspect of AI. It requires a nuanced approach combined with a structured path put forward by the leadership. Let¡¯s see a couple of major moves that shook the landscape earlier this year. It was heartening to see the big corporations putting emphasis on this side of AI (even though the road that led to this point wasn¡¯t pretty). I want to direct your attention to the guidelines and principles released by a couple of these companies: These all essentially talk about fairness in AI and when and where to draw the line. Always a good idea to reference them when you¡¯re starting a new AI based project. GDPR, or the General Data Protection Regulation, has definitely had an impact on the way data is collected for building AI applications. GDPR came into play to ensure users have more control over their data (what information is collected and shared about them). So how does that affect AI? Well, if the data scientist does not have data (or enough of it), building any model becomes a non-starter. This has certainly put a spanner in the works of how social platforms and other sites used to work. GDPR will make for a fascinating case study down the line but for now, it has limited the usefulness of AI for a lot of platforms. This is a bit of a grey field. Like I mentioned, there¡¯s no one solution to it. We have to come together as a community to integrate ethics within AI projects. How can we make that happen? As Analytics Vidhya¡¯s Founder and CEO Kunal Jain highlighted in his talk at DataHack Summit 2018, we will need to pen down a framework which others can follow. I expect to see new roles being added in organizations that primarily deal with ethical AI. Corporate best practices will need to be re-structured and governance approaches re-drawn as AI becomes central to the company¡¯s vision. I also expect the Government to play a more active role in this regard with new or modified policies coming into play. 2019 will be a very interesting year, indeed. Impactful <U+2013> the only word that succinctly describes the amazing developments in 2018. I¡¯ve become an avid user of ULMFiT this year and I¡¯m looking forward to exploring BERT soon. Exciting times, indeed. I would love to hear from you as well! What developments did you find the most useful? Are you working on any project using the frameworks/tools/concepts we saw in this article? And what are your predictions for the coming year? I look forward to hearing your thoughts and ideas in the comments section below.","Keyword(freq): model(9), task(7), application(6), time(6), company(5), development(5), image(5), thought(4), analytics(3), embedding(3)"
"8","vidhya",2018-12-19,"Top Highlights from the Amazing Machine Learning Tutorials Presented at NeurIPS (NIPS) 2018","https://www.analyticsvidhya.com/blog/2018/12/top-highlights-tutorials-neurips-2018/","

New Year's Grand Sale - 40% Discount On All Courses (Use Coupon: HNY2019) Click To Enroll Today !

 NeurIPS (formerly called NIPS <U+2013> Neural Information Processing Systems) is one of the premier machine learning conferences in the world. Researchers from across the globe present their latest projects in this field, but getting past the review screening? Not so easy. Thousands of papers are submitted every year out of which only a handful make the final conference. The audience tickets for NeurIPS 2018 sold out within 12 minutes of the portal being opened! That might give you an inkling of how popular this annual conference is. For those who couldn¡¯t be there <U+2013> we are thrilled to present a quick summary of the best tutorials from NeurIPS 2018! This year¡¯s edition was held in Montreal, Canada between 2nd to 8th December. There were a variety of topics being showcased <U+2013> from fairness and transparency in AI to visualizing deep learning models. You can check out the full schedule here. There are hours and hours of videos, so our team went through all of them to bring you the best in the form of this article. Note: We have embedded the videos for most sessions as well. A couple of videos are not being embedded due to some technical issue with FB¡¯s video platform, and we have provided their direct links. While the summary is a good starting point, we encourage everyone to watch the videos as well <U+2013> this is a great chance to learn from the top minds in this field. Speakers: Frank Hutter and Joaquin Vanschoren (function(d, s, id) {  var js, fjs = d.getElementsByTagName(s)[0];  if (d.getElementById(id)) return;  js = d.createElement(s); js.id = id;  js.src = 'https://connect.facebook.net/mr_IN/sdk.js#xfbml=1&version=v3.2';  fjs.parentNode.insertBefore(js, fjs);}(document, 'script', 'facebook-jssdk')); Tutorial Session: Automatic Machine Learning Welcome back to the NeurIPS 2018 Tutorial SessionThis tutorial Automatic Machine Learning will cover the methods underlying the current state of the art in this fast-paced field.Persented by Frank Hutter (University of Freiburg) and Joaquin Vanschoren (Eindhoven University of Technology, OpenML) Neural Information Processing Systems <U+092F><U+093E><U+0902><U+0928><U+0940> <U+0935><U+0930> <U+092A><U+094B><U+0938><U+094D><U+091F> <U+0915><U+0947><U+0932><U+0947> <U+0938><U+094B><U+092E><U+0935><U+093E><U+0930>, <U+0969> <U+0921><U+093F><U+0938><U+0947><U+0902><U+092C><U+0930>, <U+0968><U+0966><U+0967><U+096E> Building an end-to-end machine learning model involves a number of steps, such as preprocessing data, creating features, selecting model, and tuning the hyperparameters. Automatic Machine Learning, or AutoML, aims to automate these processes <U+2013> this tutorial covers methods underlying the state-of-the-art in AutoML. Quite a relevant topic in today¡¯s environment. Frank Hutter kicked off the tutorial by discussing the various applications of deep learning and an expert¡¯s role in building a successful model. This can potentially be replaced by an AutoML service that tries to learn the features, architecture and parameters to use based on the raw data that we provide. Followed by this basic introduction to AutoML, Frank spoke about the types of hyperparameters and modern approaches to Hyperparameter Optimisation. This is broadly divided into three sub-topics: The next topic Frank covered was about Neural Architecture, which is again divided into three parts <U+2013> Search Space Design, Blackbox optimization and Beyond Blackbox optimization. After a short Q&A session with Frank, Joaquin<U+00A0>Vanschoren took over for the second half of the tutorial. His focus was mainly on Meta-Learning. He spoke about various approaches, configuration space design, surrogate model transfer and warm-started multi-task learning. Joaquin further discussed the Learning Pipeline followed by transfer learning and transfer features. He also spent some time discussing topics like gradient descent and LSTM meta-learner. Speakers: Deirdre Mulligan, Nitin Kohli, Joshua A. Kroll (function(d, s, id) {  var js, fjs = d.getElementsByTagName(s)[0];  if (d.getElementById(id)) return;  js = d.createElement(s); js.id = id;  js.src = 'https://connect.facebook.net/mr_IN/sdk.js#xfbml=1&version=v3.2';  fjs.parentNode.insertBefore(js, fjs);}(document, 'script', 'facebook-jssdk')); Tutorial Session: Common Pitfalls for Studying the Human Side of Machine Learning Welcome Back to the NeurIPS 2018 Tutorial Sessions.This tutorial Common Pitfalls for Studying the Human Side of Machine Learning will present common misconceptions machine learning researchers and practitioners hold.Presented by Deirdre Mulligan (UC Berkley), Nitin Kohli (UC Berkley) and Joshua A. Kroll (Princeton University) Neural Information Processing Systems <U+092F><U+093E><U+0902><U+0928><U+0940> <U+0935><U+0930> <U+092A><U+094B><U+0938><U+094D><U+091F> <U+0915><U+0947><U+0932><U+0947> <U+0938><U+094B><U+092E><U+0935><U+093E><U+0930>, <U+0969> <U+0921><U+093F><U+0938><U+0947><U+0902><U+092C><U+0930>, <U+0968><U+0966><U+0967><U+096E> Tutorial summary: Machine learning is being used in almost every domain in the industry and researchers all over the world are examining how it can affect people and society. Ethics, essentially, should be at the heart of every ML project. The main idea behind this tutorial was to put forward some common misconceptions machine learning researchers and practitioners hold when thinking about certain topics. This is a video we implore everyone to watch! Some of the terms, like fairness, accountability, transparency and interpretability, are often reused to represent different meanings which may cause an unnecessary misunderstanding. This tutorial examined how the same words can be used to refer to different ideas. The presenters also showcased a few case studies where these learnings are being applied to ML problems. The session started with focusing on the necessity of having certain definitions for the terms we use and how these terms carry different meanings for different people. We were introduced to the term ¡®sociotechnical¡¯. To explain the concept better, Nitin Kohli took the term ¡®Fairness¡¯ and showcased how it can come across differently for statisticians, computer scientists, or lawyers. The next example, quite naturally, was of the word ¡®Transparency¡¯. The presenters picked up some very common examples to differentiate the meaning of the word for someone working in the government sector, to a machine learning engineer. Post this, Nitin described the word ¡®Explanation¡¯ and its various types with suitable instances. They also spoke about the terms ¡®accountability¡¯ and ¡®interpretability¡¯ during the session and the Q&A that followed was pretty informative as well. Speakers: John Shawe-Taylor, Omar Rivasplata Tutorial summary: John Shawe-Taylor initiated the tutorial by giving an introduction to statistical learning theory (SLT) followed by some basic definitions and notations for terms used quite frequently, such as generalization gap, upper bound, etc. Both speakers provided a broad outline of the session where they listed down some important topics: After familiarizing the audience with important terminologies, Omar<U+00A0>Rivasplata took over the baton by discussing about First Generation SLT. He started with talking about the building blocks of a single function and then explains the finite function class and infinite function class. In the next few slides, Omar discussed the VM bounds along with the limitations to the VM framework. Once the audience had been familiarized with the first generation of SLT in the first half of the talk, John gave an overview of what comes after that <U+2013> the second generation of SLT. He elaborated on the different ways to make the bound function dependent and the techniques that can be used for detecting a benign distribution. We also saw the<U+00A0>Three Proof Techniques <U+2013> Covering numbers, Rademacher Complexity, and PAC Bayes Analysis. He compared the PAC-Bayes bounds with Bayesian Learning. This part of the talk is really interesting <U+2013> do watch the video to gain a deeper insight into it. The last section of the tutorial is a discussion over the <U+00A0>Next Generation SLT, where the speakers talks about Performance of neural networks and stability. Throughout the tutorial, the speakers explain all the concepts using plots and mathematical equations which makes the topics crystal clear. Speakers: Alex Graves and Marc¡¯Aurelio Ranzato (function(d, s, id) {  var js, fjs = d.getElementsByTagName(s)[0];  if (d.getElementById(id)) return;  js = d.createElement(s); js.id = id;  js.src = 'https://connect.facebook.net/mr_IN/sdk.js#xfbml=1&version=v3.2';  fjs.parentNode.insertBefore(js, fjs);}(document, 'script', 'facebook-jssdk')); Tutorial Sessions: Unsupervised Deep Learning Welcome Back to the NeurIPS 2018 Tutorial Sessions.This tutorial Unsupervised Deep Learning will cover in detail, the approach to simply 'predict everything' in the data, typically with a probabilistic model, which can be seen through the lens of the Minimum Description Length principle as an effort to compress the data as compactly as possible.Presented by Alex Graves (Google DeepMind) and Marc Aurelio Ranzato (Facebook) Neural Information Processing Systems <U+092F><U+093E><U+0902><U+0928><U+0940> <U+0935><U+0930> <U+092A><U+094B><U+0938><U+094D><U+091F> <U+0915><U+0947><U+0932><U+0947> <U+0938><U+094B><U+092E><U+0935><U+093E><U+0930>, <U+0969> <U+0921><U+093F><U+0938><U+0947><U+0902><U+092C><U+0930>, <U+0968><U+0966><U+0967><U+096E> A topic most of you will be curious to explore more! This tutorial is divided into two parts: In Part 1, Alex explained why we need unsupervised learning in the first place. Why can¡¯t we just provide the true labels for training the model? There are mainly three reasons for that: Targets in supervised learning contain very less information as compared to the input data. Using supervised learning, we are bounding the model to learn only a few bits of information. Unsupervised learning on the other hand, gives us an essentially unlimited supply of information to learn. So instead of learning the data points, the model learns the dataset. Unsupervised learning gives us more of a signal to learn from, but the learning objective is not entirely clear. Autoregressive neural networks can be used for density modelling which help to learn information from the data. Methods such as auto-encoding and predictive coding can yield useful latent representations. In Part 2, Marc discussed various applications of unsupervised learning which are based on other frameworks and principles. He explained how to learn representations and samples and how to map between two domains. Some of the tips for learning representations are: He also mentioned how to extract features in NLP using unsupervised learning. Some of the applications of learning how to map between two domains are: Unsupervised learning has tons of sub-areas like feature learning, learning to align domains, learning to generate samples, etc. The biggest challenges with unsupervised learning are: Speakers: Zico Kolter and Aleksander M<U+0105>dry (function(d, s, id) {  var js, fjs = d.getElementsByTagName(s)[0];  if (d.getElementById(id)) return;  js = d.createElement(s); js.id = id;  js.src = 'https://connect.facebook.net/mr_IN/sdk.js#xfbml=1&version=v3.2';  fjs.parentNode.insertBefore(js, fjs);}(document, 'script', 'facebook-jssdk')); Tutorial Session: Adversarial Robustness, Theory and Practice Welcome to NeurIPS 2018 Tutorial Sessions. This tutorial will survey some of the key challenges in this context and then focus on the topic of adversarial robustness: the widespread vulnerability of state-of-the-art deep learning models to adversarial misclassification (aka adversarial examples)Presented by J Zico Kolter (CMU/Bosch Center for AI) and Aleksander Madry (MIT) Neural Information Processing Systems <U+092F><U+093E><U+0902><U+0928><U+0940> <U+0935><U+0930> <U+092A><U+094B><U+0938><U+094D><U+091F> <U+0915><U+0947><U+0932><U+0947> <U+0938><U+094B><U+092E><U+0935><U+093E><U+0930>, <U+0969> <U+0921><U+093F><U+0938><U+0947><U+0902><U+092C><U+0930>, <U+0968><U+0966><U+0967><U+096E> In the tutorial, both researchers spoke about how machine learning predictions are mostly accurate, but at the same time, brittle as well. Intrigued? Adding just a little noise to the data can change the predictions drastically, resulting in a drop in performance. Trying data augmentation also does not help much in improving the performance. Some of the problems that the brittleness of machine learning can cause are: Zico and Aleksander proposed three commandments in order to make our machine learning model more secure: They talked about adversarial examples and verification, and how to train adversarially robust models. Zico further propounded on whether robust deep networks overfit or not. Even for training adversarial robust models, more data is required <U+2013> this is a known fact. Data Augmentation can be used to make the model robust. Adversarial training is also an ultimate version of data augmentation as we train on the most confusing version of the given training set. Some of the keypoints to make a model robust are: Finally, to summarize adversarial robustness: Apart from this, the advantages of using adversarial robust models are massive. The model becomes more semantically meaningful. We will be able to rely on it far more. And it leads to machine learning that is not only safe and secure, but also better. Sounds like a good bet to us! Speakers: Fernanda Viegas and Martin Wattenberg Visualization is a topic all of us can relate to at some level. Who among us hasn¡¯t done a thorough EDA before? Fernanda Viegas and Martin Wattenburg covered one of the most interesting and fundamental topics of machine learning <U+2013> visualization. They first spoke about what data visualization is, how it works and what are some of the best practices for it. The talk then focused on how visualization has been applied to machine learning till date. A special case of high dimensional data has also been covered in this tutorial. Data visualization is good for almost every field and some of its applications include: Using colors for visualization makes it more interpretable and even faster. Visualization makes calculations easier and less tedious (and who doesn¡¯t appreciate that?!). Some of the examples where it helps in calculation are: The tutorial also dove into the interpretability and model inspection facets of a ML project. Visualizing different layers of convolutional neural networks (CNNs) helps us to understand how it classifies images (this also helps in case the model is not performing well). We can interpret the model layer by layer and finally conclude where it is going wrong. They recommend using Jupyter notebooks for visualization which have libraries like matplotlib and plotly which have pre-built codes for most visualizations. Speaker: Susan Athey (function(d, s, id) {  var js, fjs = d.getElementsByTagName(s)[0];  if (d.getElementById(id)) return;  js = d.createElement(s); js.id = id;  js.src = 'https://connect.facebook.net/mr_IN/sdk.js#xfbml=1&version=v3.2';  fjs.parentNode.insertBefore(js, fjs);}(document, 'script', 'facebook-jssdk')); Tutorial Session: Counterfactual Inference Welcome back to the NeurIPS 2018 Tutorial SessionsThis tutorial Counterfactual Inference will review the literature that brings together recent developments in machine learning with methods for counterfactual inference.Presented by Susan Athey (Stanford University) Neural Information Processing Systems <U+092F><U+093E><U+0902><U+0928><U+0940> <U+0935><U+0930> <U+092A><U+094B><U+0938><U+094D><U+091F> <U+0915><U+0947><U+0932><U+0947> <U+0938><U+094B><U+092E><U+0935><U+093E><U+0930>, <U+0969> <U+0921><U+093F><U+0938><U+0947><U+0902><U+092C><U+0930>, <U+0968><U+0966><U+0967><U+096E> In this absorbing tutorial, Susan primarily spoke about estimating the magnitude of causal effects and how researchers¡¯ uncertainty about these magnitudes can be quantified. She brought up the developments in machine learning with methods for counterfactual inference. Next, Susan presented a cold fact <U+2013> there are gaps between what researchers are doing and what firms are applying. She mentioned three Counterfactual Inference approaches: In addition, Susan briefly mentioned the key challenges faced by causal inference, For example, lack of variation in the data, data insufficiency, and the practitioner¡¯s lack of knowledge about the model. Tech firms who are proactively conducting lots of experiments and interacting with experts on a large scale should focus more on learning about causal effects. Speaker: David Dunson (function(d, s, id) {  var js, fjs = d.getElementsByTagName(s)[0];  if (d.getElementById(id)) return;  js = d.createElement(s); js.id = id;  js.src = 'https://connect.facebook.net/mr_IN/sdk.js#xfbml=1&version=v3.2';  fjs.parentNode.insertBefore(js, fjs);}(document, 'script', 'facebook-jssdk')); Tutorial Session: Scalable Bayesian Inference Welcome to NeurlIPS 2018 Tutorial Sessions. This tutorial on Scalable Bayesian Inference will provide a practical overview of state-of-the-art approaches for analyzing massive data sets using Bayesian statistical methods.Presented by David Dunson (Duke) Neural Information Processing Systems <U+092F><U+093E><U+0902><U+0928><U+0940> <U+0935><U+0930> <U+092A><U+094B><U+0938><U+094D><U+091F> <U+0915><U+0947><U+0932><U+0947> <U+0938><U+094B><U+092E><U+0935><U+093E><U+0930>, <U+0969> <U+0921><U+093F><U+0938><U+0947><U+0902><U+092C><U+0930>, <U+0968><U+0966><U+0967><U+096E> Bayesian learning as a topic has fascinated us for a long time. The objective of this session was to motivate people to work more on Bayesian methods as these methods offer an attractive general approach for modeling complex data.<U+00A0>David Dunson gave an overview of the state-of-the-art approaches for analyzing huge datasets using Bayesian statistical methods. David explained how the Markov Chain Monte Carlo (MCMC) algorithm is becoming more and more scalable and faster thanks to the emerging rich and practical literature on the subject. Apart from that, he put quite an emphasis on tweaking the Bayesian paradigm to be more robust with respect to Big Data and scaling of Bayes to high-dimensional data (no. of features > no. of samples), which in itself is quite a hot topic. If you are interested in Bayesian statistics, then this is a must-watch video, and has the following key takeaways: Speakers: Survit Sra and Stefanie Jegalka (function(d, s, id) {  var js, fjs = d.getElementsByTagName(s)[0];  if (d.getElementById(id)) return;  js = d.createElement(s); js.id = id;  js.src = 'https://connect.facebook.net/mr_IN/sdk.js#xfbml=1&version=v3.2';  fjs.parentNode.insertBefore(js, fjs);}(document, 'script', 'facebook-jssdk')); Tutorial Session: Negative Dependence, Stable Polynomials and All That Welcome back to NeurIPS 2018 Tutorial Sessions.This tutorial Negative Dependence, Stable Polynomials and All That provides an introduction to a rapidly evolving topic: the theory of negative dependence and its numerous ramifications in machine learning.Presented by Survit Sra (MIT) and Stefanie Jegalka (MIT) Neural Information Processing Systems <U+092F><U+093E><U+0902><U+0928><U+0940> <U+0935><U+0930> <U+092A><U+094B><U+0938><U+094D><U+091F> <U+0915><U+0947><U+0932><U+0947> <U+0938><U+094B><U+092E><U+0935><U+093E><U+0930>, <U+0969> <U+0921><U+093F><U+0938><U+0947><U+0902><U+092C><U+0930>, <U+0968><U+0966><U+0967><U+096E> This tutorial gives an introduction to the topic: the theory of negative dependence. This can impact all aspects of machine learning, including both supervised and unsupervised learning. It is a rich mathematical toolbox which aids in tasks like anomaly detection, information maximization, experimental design, validation of black-box systems, architecture learning, fast MCMC sampling, and much more.","Keyword(freq): method(9), neurip(9), speaker(9), system(9), researcher(7), session(7), topic(7), feature(5), model(5), term(5)"
"9","vidhya",2018-12-17,"A Comprehensive Guide to Digital Marketing and Analytics Every Data Science Professional Must Read","https://www.analyticsvidhya.com/blog/2018/12/guide-digital-marketing-analytics/","

New Year's Grand Sale - 40% Discount On All Courses (Use Coupon: HNY2019) Click To Enroll Today !

 One of the biggest challenges of breaking into the field of digital analytics is that the landscape of digital marketing is extremely complex. It¡¯s a hard task finding professionals who know the best of both worlds <U+2013> digital marketing and data science. There is a serious shortage in the supply of adequate talent, while spending on digital marketing continues to rise unabated. This spending is quite prevalent in the developed economies. But here¡¯s the good news <U+2013> developing countries are starting to catch up, and are not far behind the curve. Check out the below chart and see for yourself the growth in the digital marketing spend share over time in India: Figure source : eMarketer I have seen positions open for years in the digital analytics space because of the shortage of such niche talent coupled with the immense growth of this field. Over the last few months, I have spent some time trying to understand the digital marketing landscape and how it integrates with our field of data science. In this article, I have shared my compiled notes that should simplify this complex world of digital marketing and help you understand how you can use your data science tools in this terrain. Excited yet? Good. Let¡¯s start with the most basic question and take it from there! Marketing is all about getting 4 things right <U+2013> reaching out to the right customer with the right product at the right time through the right channel. Marketing also needs a lot of testing to see which combination works for these 4 factors, so we need a channel that takes minimum time to hit the market. Traditional media channels like cable TV, flyers, radio, physical banners, etc. present a lot of challenges getting all of these 4 things right. To name a few: 1. Once printed or aired, an advertisement cannot be changed. 2. High time to market. This is actually a big challenge for any dynamic industry. A lot of offers require a quick response time (like Amazon Prime sales) and advertisers don¡¯t wish to announce the sale beforehand to make sure customers regularly come to their website to check out offers. 3. No way to accurately measure viewed ads. All we know is the response from ad campaigns. 4. Limited ways to reach out to a specific audience. 5. Even if we manage to reach the right customers, we might not really reach out at the right time. For instance, if we advertise during specific TV shows that are viewed by our target audience, we don¡¯t really know whether the target customer is looking at the TV at the time when our ad is being played. Similarly, if we send a mail to a specific prospect who is traveling at the time, it¡¯s a lost opportunity. 6. Given a low response from these channels, most of these traditional methods have a high cost per acquisition. Digital marketing is able to overcome all the challenges mentioned above.¡±How¡± will become clearer as we cover other related topics in this article. With internet penetration reaching 60%+ around the globe and rapidly increasing (see chart below), digital media is now an ideal place to catch your audience¡¯s attention. Figure Source : Research Gate Today, we can reach out to 4 billion+ internet users through digital media. Another survey (below) indicates 2 hr. 28 mins spent per day by adult internet users in India. Noting that average time spent by the total adult population is 1 hour, total internet user average should be somewhere between 1 hr to 2.5 hrs (say, 2 hours). An estimate of the ad opportunity available per day on the internet is 4 Bn*2 hours = 8 Bn hours/day. Even if we assume only 1% of this internet time can be monetized with ads, we are still talking about 80 Million hours/day! Figure source : eMarketer This exercise would have given you a sense of the magnitude of opportunity in digital marketing. And as an analyst, you would have definitely noticed the incremental curve for both internet users and the average time spent on the internet. Given the scale and precision of this channel, digital media presents multiple methods for advertising. Let¡¯s broadly classify these sub-channels of digital media. A data scientist can contribute literally in any domain and any industry. For instance, there are data science roles in creating new medicine, intelligent voice assistants like Google Home, targeting strategy, etc. For each of these roles, a data scientist might need minimal knowledge of the domain, but can still make strong contributions with the tools we have. For instance, to contribute towards building a new medicine, you need not know all the biology behind it. All you need to know is an objective function and the parameters you need to optimize. In these scenarios, you can easily break down the work into what the business person needs to tell and what you need to know already. However, digital analytics is slightly different. Given the fast changing dynamics in the industry, we are getting better data everyday. As an analyst, you need to decide: The barrier between a business person and an analyst becomes very blurry in this space. Here is how companies describe this new skill set they need for digital analytics: The T-shaped talent suffices for most data scientist roles in different domains. However, digital analytics professionals need the Pi-shaped talent to be a rockstar. If you already understand the world of data science, then this article will help you grasp the second aspect of the Pi-shaped talent requirement, the digital ecosystem, and open up a land of opportunities for you. In no way does this article capture everything under the digital marketing umbrella, but this is as good a place as any to get your journey started. We generally talk about 3 types of digital media <U+2013> owned media, paid media, and earned media. Here¡¯s a description of each: Data science has a strong application in each of these 3 types of media. Before we get into the actual applications of data science, let¡¯s first broadly break down the journey of a prospect into stages. The Different Stages of Customer Acquisition There are three journeys involved in customer acquisition. Our end objective of a successful campaign is to show ads to only those prospect that have a strong propensity of being a profitable customer. Looks simple, right? But in the real world, it really isn¡¯t! Here¡¯s why: As you might have guessed, it becomes very difficult to collaborate between all the three stages to reach ensure a successful campaign. Let¡¯s look at a a real-life scenario that will make this challenge a lot more clearer. Company X sells life insurance and acquires 50% of customers through paid digital media. A typical life insurance starts low on profit because of all the overhead costs involved, and gradually increases to maximum profit within 2 to 4 years. Year 5 onward, good customers start leaving the insurance to pick up better deals in the market, and hence the profit starts to decline. See this illustrative table that shows year wise profit: Just relying on the average can so often be deceiving. It seems like every acquisition is worth (lifetime value) $1,130 to your company, but all customers are different and as analysts, we try to find the most profitable segment. Now suppose you found that there is one attribute, total number of insurance policies held by the customer till date, that can strongly separate highly profitable customers from others. Here is how simple this rule can be: Now, here is the challenge. To stay compliant with the regulatory authority, you cannot deny a policy just because of profitability reasons. Hence, the only way you can craft your acquisition portfolio is if you can control who is looking at your ads. For a simplistic view, let¡¯s say 50% of your acquisition comes from your website directly and the remaining comes through Google¡¯s Paid Search. Acquisition coming directly to your site is hard to control as these prospects are already down the conversion funnel. So all you can do is control who looks at your paid search ads. Controlling paid search targeting looks straight forward <U+2013> just tell Google to show ads to those people with #insurance policy <= 2. But there are a few challenges in doing this: So you see the challenge <U+2013> the variable that is available for real-time targeting by ad server companies is not available with you to link profitability. Similarly, companies cannot make their profitability linked information available to Google to do specific targeting. Even though such direct targeting is difficult with paid media, we still have a lot of ways to get around this challenge. We will talk more about these targeting audience methods later in this article. Before we move on to targeting audience methods on digital media, let¡¯s get some jargon out of the way. The best way to understand these jargon terms is to put them in contrast against each other. When a common person thinks about Google, we think about the Search Engine (well, at least I do!). When we search for a keyword, we see a list of links. Advertisers have to pay for sponsored ads that appear at the top of the rankings. The list of links that come in organic search are free of cost and the rank is determined by Google¡¯s proprietary Page Rank algorithm. In the picture below, Adobe has paid for the sponsored ad, whereas the Google Analytics link below it is an organic search. The Google Search network is far bigger than just the search engine. It currently includes: What about Google Display Network (GDN)? In addition to the search network, Google also partners with 2 million sites that can publish ad banners on their site. GDN has a huge coverage of about 90% internet users. The biggest challenge with GDN is the low response rate because these visitors are not really looking out for your product. These visitors are simply looking at news, or the weather or are watching a video, etc. So, we classify such a visitor as a top-of-the-funnel prospect as he/she has a long way to go before making a purchase. Another challenge with this display network is attribution. Because a customer might view your ad today and make a purchase significantly later, attribution of these purchases becomes very subjective. GDN is very successful for brand awareness and multi-touch acquisitions (which requires a customer to see your product¡¯s ad multiple times before making the final purchase). You might see GDN ads on your favorite sites if they partner with Google. Here¡¯s an example: You should now have an understanding of Google¡¯s core business of advertising. This is the majority revenue generator for Google. Every company wants to partner with Google to advertise their product/service. But how does Google choose which ads have to be displayed and when? Let¡¯s review the Google Ad auction briefly before we jump to the digital marketing jargon. You¡¯ll be amazed to know how Google does this. Let¡¯s try to understand the logic through an illustrative example. I searched for the term ¡°insurance liberty mutual¡± on Google Search. At this point, Google hosts a live auction for this one instance ad inventory. This auction will be conducted in mili/microseconds and the winners will get the top positions on the sponsored ad slots. Is a high bid all you need to get the top spots? Of course not. Because my search was specifically for liberty mutual insurance, no other competitor should ideally get the top spot. Otherwise Google¡¯s customer will have a hard time getting to the relevant links (the core skill Google offers). Following is the result for an actual Google search of the term: As you can see, ¡°insurance-quote-instantly¡± also participated in this auction but did not win against liberty mutual. So how does Google bring in this dimension of relevancy to remove monetary bias? Google has a concept of quality score for each rank which is kind of a page rank score on organic search. This quality score is then multiplied by the bid to calculate the ad score. This ad score is finally used to rank order the ads for a search instance. Here is a simplistic view of what goes on behind the scenes and how advertisers are charged for their ads: Couple of things you should notice in this Google Auction model: CPC vs CPM bidding <U+2013> Quick note of the types of bidding we do on both search and display networks. CPC bidding is when the advertiser wants to bid on how much they are willing to pay for a click. CPM bidding is based on per 1000 impressions. It really depends on the type of objective an advertiser is looking to achieve. If the objective of the advertiser is branding, all that matters is impressions. However, if the advertiser wants customers to progress in the conversion funnel, they will generally bid on CPC. One exception here are multi-view purchases, where a customer generally takes a decision after looking at an ad many times. In such cases, the advertiser might bid on CPM even when the objective is conversion. Simply put <U+2013> Adwords is used by advertisers to post ads on Google¡¯s display and search networks. Adsense is used by publishers to monetize their content. Adwords manages the demand side of ad inventory whereas Adsense manages the supply side. The below picture illustrates this concept well: Even though Google does the heavy lifting for both the publisher and advertiser in terms of ad serving, it still provides many tools to match the right ad to the right placement. This is where analytics plays such a crucial role. Google delivers Ads to Ad Spaces with publishers through 3 methods: What levers do Publishers have to subset in the pool bidding for their ad space? What levers do Advertisers have to subset in the pool bidding for their ad space? We will cover this topic in our section on Google Analytics later. For now, just note that you have levers on Demographic and Behavioral attributes you can use to target ad inventory. Further, you can choose keywords to make a precise selection of the auction you wish to participate in. DoubleClick is an ad-serving company that was bought by Google for $3.1 billion in 2008. In simple words, DoubleClick for Publishers makes it easier for publishers to monitize their content. It also provides effective tracking of how your content is performing in context of advertisements. DoubleClick for Advertisers, on the other hand, helps advertisers optimize their Search and Display campaigns. Google recently rebranded DFP as Google Ad Manager. DFP comes out much stronger than AdSense when you wish to manage your ads beyond Google Display Network <U+2013> including Affiliated Brands, Real-time Ad exchanges, etc. Let¡¯s see a few examples of how we analysts use the ad tracking done by DFP. Suppose you own a travel blog and are using DFP to manage your ad inventory. There are 4 primary sections in your blog <U+2013> Food, Destinations, People and Latest Deals. On each blog page, you have 3 banners <U+2013> Leaderboard, Skyscraper and Square. The below picture will help you visualize each of these positions on your blog page: The below table covers the key metrics that DoubleClick publishes by default: If you use DFA with Google Analytics 360, you will get endless dimensions. However, Google Analytics 360 requires you to part with a significant amount, not something everyone can afford. So let¡¯s stick to some of the basic dimensions and what we can do with them. Here are a few dimensions which you can leverage to analyze your site¡¯s performance with respect to ad revenue: These dimensions are just examples of dimensions available to you for slicing and dicing information. Let¡¯s take a look at a dimension metric view to learn more. Starting with a basic view to see how each category is performing: Clearly, the food section brings in the majority of the revenue (50%+) even though the destination category gets most of the impressions. The destination category has both click through rate (CTR) and revenue per 1000 impressions at the lowest value, indicating our ads are not being optimized well for this category. Seems like quite a huge opportunity, right? Breaking down the destination category further by the traffic sources gives the following results: The above table shows that FB is the major source of traffic for our destination category. However, this source performs sub-optimal on both CTR and Revenue. This narrows down our search further. Are we presenting different information than what customers coming from Facebook are expecting? We can look out for this information by checking the bounce rate for this audience. We will hold that discussion for later. Let us also review how each banner type is performing across pages: Leaderboard (LB) has the maximum number of impressions, which makes sense as it comes on the top and should appear during most visits. Skyscraper has a low CTR and revenue per 1000 impressions, indicating these banners might not be completely visible (definitely scope of improvement there). We can further deep dive into this analysis with DoubleClick for effective targeting, but we¡¯ll keep that for a future article. DoubleClick is a suite of product provided by Google. The below diagram will help explain the types of services DoubleClick provides in the world of digital media: DoubleClick Search is primarily used by advertisers to manage their ads on multiple Search Engines, including Google Search Network. DoubleClick Bid Manager is used to manage Display Ads across the Google Display Network and Real-Time Bidding platforms. DoubleClick Ad Exchange is like the New York Stock Exchange where ad inventory is bought and sold. DoubleClick also provides campaign management services like DoubleClick Studio. Cookies are small text files that are placed on a visitor¡¯s machine through websites they browse. A cookie contains some key information that can be used when the visitor returns. For instance, a lot of sites use cookies to save ID and passwords. Others use them to refill the checkout cart when the visitor returns. These are primarily first-party cookies. Services like DoubleClick, LiveRamp, etc. place a 3rd party cookie that they use to track user activity across the web space. An important thing to note here is that you can only read those cookies that you have placed, because all cookies have unique properties. For example, Amazon cannot read a cookie that Wells Fargo has placed in a visitor¡¯s browser. Pixels (tags) are typically an invisible single pixel. These pixels fire, or a JAVA code executes (both mean the same thing), when the webpage is loaded and capture important information about the visitor. Pixels can also place a new cookie in a visitor¡¯s browser or check if there is an existing cookie already there. Websites need to embed this Java script in the site source code, which looks something like this: Tag containers can contain multiple pixels or tags. One tag can trigger another set of tags and so on. Hence, containers are used to make conditional decisions that can determine if a set of pixels should fire or not. Google Tag Manager (GTM):<U+00A0>By now, you would have realized that the digital world is all about maintaining these tags. Why? So you can measure campaigns and create new audiences for prospecting. Here is a simple example <U+2013> Tom runs a food blog. He actively markets on paid search using Adwords. He also re-markets his customer to return to his blog if the customer has not come back for some time. He additionally tracks his online traffic using Google Analytics. He even leverages DoubleClick to target display ads beyond Google Network. Imagine the number of tags Tom might have to put on his site <U+2013> one for Adwords, one for re-marketing, one for DoubleClick floodlight and some custom DMP/DSP tags for specific re-marketing campaigns. Handling so many tags within the source code of the site is extremely risky for Tom¡¯s blog. Google Tag Manager is a solution to this problem. Google Tag Manager provides a single Java script which Tom needs to put in his source code which will allow him to manage all the tags to his site straight from the GTM interface. Additionally, Tom does not need to inform his IT team when making small changes to the tags because tags and the site source code are two separate entities superficially linked by Google Tag Manager. Below is a list of tags that Google Tag Manager can manage for you: Data layer is another key concept you should know. This component is what makes Google Tag Manager such a powerful tool. Simply put, you can think of it as a bridge for data between your site and GTM. GTM can do a two-way communication with the data layer. GTM will then make this data available for all the tags sitting in it. Think of the additional capability this adds to the dynamics! Now your marketing tags and analytics tag can directly pull segments of visitors you have created with on-site and DMP data. This can help you achieve the same level of targetability on off-site as that of on-site. The below schematic will make the process clear: Quick note on Floodlight tags:<U+00A0>DoubleClick uses floodlight tags to note visitor activities and sales. They provide two types of tags <U+2013> FL Counter and FL Sales. FL Counter is primarily used to link some conversion to ad exposure. Hence, it is used to track subscriptions after an ad click from a visitor. FL Sales is primarily to store transactions with a dollar value. It can help you optimize your campaign on total ad spends instead of maximizing conversions. I must confess this is the most confusing and difficult aspect of digital marketing. Feel free to skip this section if it becomes too complex. I will try my best to make these concepts as simple as possible. All these terms are related to technologies used primarily in display media. So before getting into these technologies, let¡¯s first try to understand the display media ecosystem. Search media is always bought through programmatic buying, however, display media can be bought directly or programmatically. Programmatic buying is basically technology assisted buying of media. Consider the following scenario: Victor runs a very popular travel blog. He gets about a million visits a month. He now wants to monetize his blog by putting an ad banner at the bottom of the article in the square placement. Out of the million visits, 500k visits will be on blogs that are related to hotel stays and the remaining 500k visits on blogs related to other topics. Victor wants the hotel blogs to specifically have Hotel related ads. He leverages various ad selling options to sell all his ad spaces. The above list is in the priority order of how ad inventory is distributed. The below diagram provides more clarity on the hierarchy: To make this entire process possible, we use technologies like DSP, SSP and Ads Exchange. Here is a schematic of how it generally works: As you can clearly see from the diagram, Direct Buy is done without any trading involved. Programmatic Guaranteed Selling is through DSP to the Publisher. If the inventory is unsold in these two options, it will be made available as Preferred Ads buyer, before it goes into the pool of Ads Exchange. Finally, we go through the private auction and RTB stages respectively. Google DoubleClick Bid Manager is a Demand Side platform. However, Adwords is not truly a Demand Side platform because of multiple reasons <U+2013> it restricts the ad inventory to only the Google Display Network. There is no option for programmatic Direct Buy or private auctions. Data Management Platform is at the heart of analytics for Display Marketing. Let¡¯s try to understand it with an example. First, a quick concept <U+2013> Remarketing is a way to win back a prospect who has visited your website but has not made an immediate purchase or enquiry. You might have witnessed Amazon ads following you on the web <U+2013> that¡¯s remarketing.  XYZ co. is an e-commerce company that wants to create a niche audience for display marketing. They only want to use the display channel for customers: The remarketing ad will be an offer of 5% off on this Laptop A.<U+00A0> XYZ wants to publish these ads by placing bids on Ad Exchanges. How can XYZ execute such complex targeting? Short Answer <U+2013> through a Data Management Platform (DMP). If you want to know the technical details of how this can be done, continue reading this section. Otherwise, all you need to know is that DMP can make specific targeting possible. DMP also provides third-party data, like preferences, interests, etc. You can skip the rest of this section if you want to ignore the technical details. Here is what actually goes on behind the scenes: The above 11 steps happen in less than a mini-second! DMP is used for many different use cases, such as: This is the concept that fascinated and scared me the most. We already know that the entire web world is tracking visitors through cookies. Cookies have been around for a long time. Using cookie DoubleClick can really stitch a persona <U+2013> for instance, a visitor that reads AV blogs clicks on an online analytics course, but does not convert. The same visitor comes back to another university analytics course and as he/she was tagged as an analytics enthusiast, this new university gave him/her a discount of 20% on the course through on-site optimization. The visitor finally converts and complets the course in 60 days. And so on¡¦. So, essentially we know everything about the visitor. What¡¯s left? A very key information, which is, ¡°This visitor is John Bell¡±. What is so important about this new information? This is a Personal Identification Information (PII) and will never change over time. Cookies get deleted all the time. But if we can link these cookies to PII, we not only have a persona that can describe a small subset of the population, but we directly have an individual that only describes one person on the planet. This is exactly what an Identity Resolution does. Let¡¯s try to get a broad idea of how Identity Resolution works in the industry currently: Given all the background information, let¡¯s now talk about the various tools you have at your disposal that can process huge amounts of data coming from a number of channels. These help you analyze the data and implement your strategy in real-time. Even though there are many solutions in the market, most of the companies (big or small) are using one of the two <U+2013> Google Analytics or Adobe Analytics. Let¡¯s review them briefly by comparing a few key attributes: Both the tools have their pros and cons. If you are a small scale company, choosing Google Analytics is a no-brainer. Even if you are a large corporate, the choice is tricky because Adobe on one hand gives active support, but Google integrates seamlessly with well-managed ad inventory. Note that Adobe can also integrate third-party tools for ad targeting, but not all of them are well-managed and might be prone to bot attacks, thus wasting your ad spends. You might have realized that each concept we covered in this guide is closely linked with each other. A comprehensive view is very important to appreciate the entire digital ecosystem. Trust me, it was impossible to find all this information in one place. As a data scientist, we do the hard part of learning all these concepts on the job. So I decided to put everything in one place with the aim of helping any future analyst get up to speed really quickly and keep up with the pace of this dynamically evolving industry. With the knowledge provided in this article, you will not only understand how to build a successful digital analytics driven strategy, but will also start appreciating how your strategy fits into the broader world of digital marketing. This combination of knowledge is extremely rare in the industry because most professionals focus on only one of these aspects. What it takes to create a successful marketing campaign on digital media lies at the intersection of the digital ecosystem, marketing and analytics. Superb article. As a person who knows a bit of analytics and bit of Digital marketing, I would say you have summarized most of the digital marketing concepts from Google per se, with its relation to analytics. Your illustrations too are good, especially the one explaining how ad biding works. Looking forward to your article wherein you would mention about Social media analytics covering FB, Twitter, Snapchat etc","Keyword(freq): analytics(24), media(21), tag(16), cooky(9), customer(9), advertiser(8), visitor(7), impression(6), tool(6), adword(5)"
