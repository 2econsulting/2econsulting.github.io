"site","date","headline","url_address","text"
"datacamp",2018-07-25,"New Project: Visualizing Inequalities in Life Expectancy","https://www.datacamp.com/community/blog/project-visualizing-inequalities-life-expectancy","Here is the project link. Do women live longer than men? How long? Does it happen everywhere? Is life expectancy increasing? Everywhere? Which is the country with the lowest life expectancy? Which is the one with the highest? In this Project, you will answer all these questions by manipulating and visualizing United Nations life expectancy data using ggplot2. We recommend that you have completed Introduction to the Tidyverse and Chapter 2 of Cleaning Data in R prior to starting this Project. The dataset can be found here and contains the average life expectancies of men and women by country (in years). It covers four periods: 1985-1990, 1990-1995, 1995-2000, and 2000-2005."
"datacamp",2018-07-25,"Data Science at Doctors without Borders (Transcript)","https://www.datacamp.com/community/blog/data-science-doctors-without-borders","Here is a link to the podcast. Hugo:    Hi there Derek, and welcome to DataFramed. Derek:    Hi. How are you? Hugo:    I'm very well. And it's great to have you on the show. How are you? Derek:    I'm doing great. I'm excited to be on the show today. Hugo:    This is super exciting. I'm in Sydney, Australia at the moment. Whereabouts are you? Derek:    I am speaking to you from Southern Myanmar in a place called Dawei. Hugo:    And are you in an office there or whereabouts are you? Derek:    Currently in Dawei, I'm actually in an office. I'm in an office that's kind of attached to a health clinic here in Dawei. I work with Doctors Without Borders and we have been running an HIV clinic out of Dawei for the last 18 years. So that's kind of where I'm speaking to you from for the moment. Hugo:    Okay, great. What else are you doing in Myanmar with Doctors Without Borders? Derek:    Currently we have two projects. We have the HIV clinic in Dawei which has been operational for about 18 years, but we also recently have opened up a project in Northern Myanmar in Nagaland in a town called Lahai, and by comparison, that's only been up and running for about a year now. We don't necessarily do a lot of HIV up there, but we're doing more of supporting the health infrastructure up there and doing more general health support. So very, very different than what we do in Dawei. Hugo:    So these are two very different projects that you're involved with. I'd love for you to abstract from that a little bit and let me know, or tell us a bit more about your general role at MSF. For listeners out there, we'll be referring to Doctors Without Borders as Doctors Without Borders, Medecins Sans Frontieres, and MSF which is an acronym, interchangeably. Hopefully you can stick along with that. Hugo: So Derek, yes. Tell us more generally about your role at MSF. Derek:    I'm actually currently the epidemiologist here. Basically, I help out with a lot of operational research and a lot of monitoring and evaluation. In Dawei, where I am right now, we're doing a bit of operational research on Hepatitis C. We are currently trying to treat people for Hepatitis C in Dawei. Recently, the price for treatment has dropped considerably from tens of thousands of dollars for treatment down to a few hundred dollars to treat people. And so here, we're trying to sort of scale up the treatment for Hepatitis C because it's more prevalent here than in other places in the world.
    In Lahai, I do operational research as well but I do a lot of monitoring and evaluation so the projects of our doctors who go to Lahe township villages, we're kind of just monitoring the routine data that they collect there. But we also have recently done a big baseline health survey that assesses ... it's township-wide representative and it assesses the health of the several different villages there. Hugo:    Fantastic. And that's something that we're going to get into a lot later in this conversation. But before that, you said you worked as an epidemiologist which of course, involves a lot of statistics and thinking about data management, data provenance, all of these types of things. More generally, a lot of things which intersect with data science. And I'm wondering how you think about data science, how you got into it originally, and what your path has been to end up in Myanmar working for MSF. Derek:    I kind of have an interesting path to data science. Originally, I was actually a biochemistry major way, way, way, way back in undergrad decades ago. I guess I kind of realized it very early and ... I got lucky, and realized that working in a lab, while very challenging and super rewarding, is not very social. There were internships that I worked in where I used to run kind of like a mass spec machine. A mass spectrometer. And back in the day, these things were huge, like the size of a car. Basically, I would just be sitting in a basement and the only people I would see would be occasionally my supervisor, maybe one other person, and that would be for 8 to 10 hours a day. I was like, I can't do this. I have to do something a little more adventurous.
    And so I kind of got into public health, but I still wanted to do a lot of science. I still wanted to do a lot of research. And so epidemiology, it's a great mix. You take a lot of data, and you get to collect a lot of data, which is really fun, but you get to do that kind of in the field, so to speak. You don't necessarily have to be going to the Northern parts of remote areas to do it. You can do it in a clinic. You can do it with just the data that people collect. And then you analyze it, and then you look for patterns or outcomes, or how things are associated with one another. It's like puzzle work, mixed with being a little bit of Indiana Jones. It's great. I love it. I'm super, super lucky to be able to do this. Hugo:    For sure. And how did you actually get involved in MSF originally? Derek:    Initially, I was in grad school and I used to do a lot of HIV work, and STDs in general. I ended up getting very lucky and getting a chance to go to Malawi to look at ... Initially, I was looking at using an HIV drug called tenofovir as a first line HIV drug and for people who know about antiretrovirals and HIV drugs, that's how old I am. Tenofovir became a first line drug in 2010 in Malawi so that's a long time ago. So I just kind of got into that. I was like, hey, this is really great. I can do data science. I can do my own surveys. Collect my own data. Analyze it. Use a lot of statistics. But then get to travel and get to meet really interesting, really crazy people. It was really great.
    From there, when I finished school, I was thinking, well, what to do now? And so, I met somebody who actually worked on ... MSF has refugee boats in the Mediterranean, off of Italy, and he worked on one of the boats helping refugees. Basically, the boats that would come from Northern Africa with two, three, five hundred people, trying to get to Europe. MSF has these boats that kind of help. Basically, just bring people in to make sure that they're safe, they don't die, they don't drown. And he said it was the most intense experience he's ever had. And I was like, ""I'll join MSF. I'll see if I can look at the data and things they collect.""
    And it turns out that, because MSF has operations all over the world, they need a lot of help with data science because that knowledge that they gain helps formulate and helps shape their policies. Much in the same way that when you work in a lab, that you publish a paper, people are like, ""Oh, this molecule is related to another molecule. We're going to go with this."" And that helps shape science. You get to help shape health policy of sorts. Hugo:    For sure. And I love that you mentioned the data collection process is something that you're very passionate about and involved in, and that's something we'll get to, particularly with respect to the baseline health assessment you've been doing in Lahai township because I think a lot of working data scientists and a lot of our listeners, when they think about data science and data collection, they think clicks, and browser-based stuff, and stuff that's put in a database as opposed to getting there on the ground into remote areas and conducting the types of surveys you do. That's a little teaser for where we're headed.
    But before we get there, I think a lot of people have a sense of what Doctors Without Borders does, but I'd love a brief rundown from you on the work that Doctors Without Borders does from your position on the ground. Derek:    Doctors Without Borders basically is a kind of an impartial and neutral NGO that provides healthcare to anybody, particularly in humanitarian crises. Be that natural disasters like earthquakes or typhoons, or more like manmade disasters like in war zones. They provide humanitarian relief. And so what Doctors Without Borders does is they go to these areas and they basically provide humanitarian aid. Most of the projects for example that I'm a part of, tend to be very short term. Before I came to Myanmar, I was actually in a refugee camp in Northern Uganda doing actually a big household survey on mosquito nets, but in the refugee camp itself. So we were just living there and doing work there but it was short. I was only there for about four months. Whereas compared to a lot of development work where they can be in an area, for example like Red Cross or USAID, there's plenty of development NGO's, that can be in an area for years, for decades. So that's kind of what MSF does, is provide humanitarian aid to places in crises. Hugo:    Great. Given that context around what is essentially a mission statement of MSF, what are the biggest challenges that the organization faces that data science and analysis and analytics can help to solve? Derek:    Basically, getting to know, I would say, in my experience, getting to know the context of where a health clinic works, and how do you take that knowledge and then inform the policies that bring your decisions forward. Because MSF works in so many different areas and so many different countries, it's critical that they know about the context that they're working in, and every place is different. When I was in Uganda, that is completely different than what I'm doing here now at Myanmar. The people who help decide what to do in terms of projects, and in terms of policies, they need as much information as they can get.
    And so as an epidemiologist, my job is to basically do monitoring and evaluation of some of these projects, and then putting together internal reports that people can read and be then like, oh, okay, these people that are taking Hepatitis C drugs, for example, maybe 25% of them don't complete their treatment. What are we going to do about it? Or in the case of like when I was in Uganda, we were looking at passing out mosquito nets to people in the refugee camp but the problem with that also is that a lot of times, people will tend to misuse the nets. A lot of times you'll use it for fishing, and I didn't know this. I actually didn't know this until I started working there. Until we started collecting the data to look at this. The nets are really strong, and they're just perfect for catching fish. And they're already in a little basket type shape and you just throw it in the water and then they're actually really good for fishing, which is a total misuse of the nets.
    The survey that we did there, helped us form these educational campaigns. So we would actually go to different parts of the camp and be like, ""Hey guys. Don't go fishing with this. This is more for mosquitoes."" And so it really helped get people to understand to use the nets better, and that's the purpose of the data. So you get the data, and that actually helps shape what everybody is going to be doing. So it's cool in the sense that if you stick around a project long enough, you actually get to see your results be translated into action which is something that I know when I used to work in the States, sometimes you can publish a paper, and you just never see the results. And then it gets really depressing when you realize that only like three people have cited your paper and you're just like, ""Oh, okay."" Like what's the point? Hugo:    Yeah. As I said  before, it really seems like data science in this form is so different to what people think of when they think of tech data science for example. Maybe you could speak to the types of differences that are I think dominant in your mind for the respectives. Derek:    Yeah. It's different. It's funny because at its core, the statistics in study design are very, very similar and very much kind of the same. Which I think is very fascinating that you can take your survival analysis or your logistic regression or your cluster designs and you can apply it to schools in England or you can apply it to villages in Northern Myanmar. But at the same time, the data ... I'm getting used to finding messy data, and it's smaller data. Nowadays, people are used to having these gigabyte size datasets with hundreds and thousands and thousands of observations. Sometimes, like here, a lot of the times you might only get a couple hundred people and so you're not actually working with big data. And when you have problems with your variables, like if you have a lot of missing variables for example, you might have to start getting creative and do things like imputation, or you might have to drop a question in general, just be like, ""Oh, it didn't quite work out."" And so it's very different than using larger datasets. Hugo:    And even the data collection process is very different, right? In terms of what you do when you're running surveys, whether it be with pen and paper and then putting them into your spreadsheet or computer program or database later. Derek:    Yeah, the collecting the data, I love it. That's the fun part. The best part of it is when you actually get to go to the field. For example in Lahai, we actually got to take motorcycles and go from village to village. It was kind of like off-roading through the mountains in Northern Myanmar to go to village to village, and then you do these household based surveys when you get there. And so you're right there to collect the data and see how it's collected and see what questions work, what questions don't. It gives you a better feel of where the data comes from. It's nice actually that you get to actually see the birth of your data. That sounds sappy but it's kind of like how your data is generated- Hugo:    No, that's a wonderful- Derek:    Yeah. Hugo:    Yeah. Derek:    Probably the most nerdy thing I've ever said. Hugo:    You understand the data provenance in that sense and you know what all your units are. It's not like you're being handed a CSV or pinging an API where you may not actually have the correct assumptions about your data. Derek:    Exactly, exactly. It's interesting. In MSF, I've done surveys where we use electronic means of data collection. I've used both Epi Info on a smart phone and a program called Dharma. It's a service that's actually started by an ex-MSF staff. She worked in the Ebola outbreak. Not the one that's currently going on but the one a couple years ago, and was like, ""We need better data collection tools. We need almost real-time information about what's going on because things, and outbreaks like Ebola, change so fast."" They were actually an epidemiologist and they actually ended up just creating this program that allowed for, not only data collection, but it also projected just trends and statistics on the dashboard. It was really good.
    But at the same time also, like in Lahai, because our teams are out for a week at a time, and it was very rural and rugged, we resorted to paper based surveys which are horrible. I think from here on out I think I'm just going to have to go with the electronic data collection route. Because with paper based surveys, there's no checks and balances. So if you're interviewing somebody and they say their age is really 10 years old, but you put down 100, there's no little automatic check that will be like ""beep,"" like, oh, that doesn't make any sense. Hugo:    No ability to test your data. Derek:    And then also, you have to enter the data which takes double the amount of time. So not only are you collecting the data, but then you have to get somebody to enter the data which takes forever. So electronic data, I'm glad MSF is adopting smartphone tech for entering and collecting data. It makes things a lot easier. Hugo:    For sure. But as you said, if you're in a region where you may not have access to electricity for a certain number of days, there are only so many battery packs you can carry, right? Derek:    Yeah, yeah. That's the problem. It was decided finally that because we're spending about a week in the field at a time ... So the Lahai survey went over the course of about two months, and people would go out for a week, come back for a day or two to rest, and then we would go out again for a week. But these are areas with no electricity, no cellphone coverage for the vast majority of it, until you start getting to the border of India. And then India has some pretty great cellphone coverage, or at least in that region. But you can't charge your battery pack or your phone and then, if you actually drop phone, or something happens to your phone, you lose a whole week of data out of a two week survey process. And it would be very hard to go back and redo it. So we decided to go with paper surveys and then we just had waterproof folders.
    So it has its pros and cons. I tend to be a little bit more tech oriented and I was like, ""But we could've had solar chargers or we could've had four battery packs. We could've gone out for two days at a time."" But in the end, we ended up doing paper. Hugo:    For sure. And you very kindly sent me through the study protocol for this baseline health assessment in Lahai township and there were so ... My eyes were stuck to this PDF while I was reading it. There were so many interesting things in there, particularly with respect to the data collection process. One thing that stood out to me was that it actually said that the local Naga dialect is a non-written language. So I presume you had translators there who were speaking Naga to the locals and then writing down the data in a different language or ... Can you just give me the rundown on that? Derek:    It's interesting and it actually comes up a lot when we do surveys in remote areas. Particularly areas with several different languages or different dialects. What ended up happening is we recruited ... we had about 34 people for this study between the drivers and the data collectors. About 12 data collectors and the rest were drivers because we had to carry supplies on motorbikes. We actually needed quite a bit of motorbikes to just carry tents and to carry food and to carry water and things. The data collectors themselves were actually recruited locally and they speak Burmese, they speak Myanmar, but when you get to parts of Nagaland, like you were saying, they speak Naga, Nagamese, which is like an umbrella term for a lot of local village languages, and a lot of these languages are not written down.
    The paper surveys themselves were translated into Burmese. The people we hired, they spoke Burmese but they also spoke their own village dialect. And so they'd be speaking in their own dialect but then translating it back to Burmese. Usually that causes a lot of problems in surveys, particularly if you're asking about complex things or behavioral questions. But this survey was a lot of health questions and so a lot of was things like, have you been coughing for over two weeks? Yes, no. Is there blood in your cough? Yes or no. So we didn't have to worry too much about the translational differences but that does become a problem. That definitely does become quite a problem if you start asking more sensitive questions. In a lot of refugee camps, if you start asking people about why did they flee? Why did they run away? That's actually quite a different question and it's open ended. And that requires that you actually have the proper translator or people speaking the same language as the person you're interviewing otherwise your data gets to be a little funky and not representative of what you're doing. Hugo:    Now I'd like to kind of step back a bit and talk about the baseline health assessment project in Lahai township as a whole. Maybe you can give us the rundown as to the motivation behind it and how it played out in practice. Derek:    Yeah. Hugo:    And what is it? Derek:    When MSF decides to open up a new project, they first do what's called an exploratory mission. They'll have a couple doctors and a couple logistics people. They'll go to an area and they'll do a quick assessment to see if there's any sort of glaring health needs. In Lahai for example, they went up there and they found that the access to healthcare was incredibly poor. Most people couldn't access a health clinic if they wanted to. And at the same time, there was a lot of just basic infectious diseases that would go untreated. There was this idea to, okay, now that we know that there's this general need, we need to be more specific. So usually what happens is, after an exploratory mission, then do a baseline health assessment which is a much more formal scientific health assessment of the needs in a particular area. That's kind of where the epidemiology comes into play because you end up designing a survey to collect the data for that area. And then what type of information you want to collect.
    So the baseline health assessment had a couple different parts. There's your basic demographics. There's health seeking behavior like where do you go if you are sick? How often do you go to a doctor? But then there was also assessments on malnutrition for children. We assess the nutritional status of children under five years old and then we also tried to assess the vaccination status. And so there's a couple different components to this survey that was pretty long, but it's a very cursory, very general overview.
    One of the things we found in this for example, is that there's a lot of respiratory illness. And because this is all self-reported, just things that they mention, we try to get to what do they think it is but people don't go to a hospital and there's just no way to really tell what it is unless you go to a doctor. So we ask all these questions about it but in the end it all becomes still fairly basic information captured in a very specific kind of scientific way.
    For example, for this particular baseline survey, the villages in Naga, in Lahai, they're spread out and in the mountains so it's very clustered. So a village might be an hour and a half drive away from the next village but really, it would only be about 30 kilometers away. But it just takes you a long time because you're literally driving on a dirt path. The problem is, within the Lahai township, there's about 107 villages. How do you sample enough villages to represent Lahai township and then within those villages, how do you sample enough houses to make sure you're actually capturing the information in the village? So you end up with this two stage cluster design to your survey which is pretty neat actually. It's different than health surveys I've done before.
    In Uganda, it's a big refugee camp and it was split into ... Well, where we were in Northern Uganda, the refugee camp was split into six different parts, and we almost pretty much did sort of random sampling in each of the parts. So we didn't do as much of a clustered survey design. It was a little easier to do. That's kind of how the data collection process happens with that.
    It was kind of neat because it actually ... we mixed a little bit of old tech and new tech to this. The WHO, the World Health Organization, has recommendations for cluster designs, how to sample it, dating all the way back to the 70s where, basically when you get to a village and you're trying to randomly select villages, you throw a pen up in the air and then wherever it lands, where that pen is pointing, that's the house you go to because they didn't have all these smartphones and all this tech.
    Now what we did, it's kind of nuts, that you can actually get satellite imagery of different villages in Lahai township and so we put that into QGIS, drew the borders around the villages, and randomly dropped points into that picture. Then we're like, ""Okay, that point is closer to this house. That's the house you're going to go to."" And so instead of doing the pen method from back in the 70s, we actually did this GIS way of selecting the households within the village. So it was kind of cool actually. It was pretty neat. I got to work on some GIS work which was pretty good. Hugo:    That's really cool. How many people are there in Lahai township and how many villages and how many people did you end up interviewing over what time scale? I've just asked you four questions actually so that's far too much, but just trying to get a general idea of quantities here. Derek:    Lahai township is one of three parts of the Nagaland area that's in Myanmar. The majority of Nagaland is actually over the Indian border but there's three sections within the Myanmar side and Lahai township is one of them. Lahai township itself, it's in the mountains and there's not a lot of people that live there. It's about 120,000 people. Lahai town is kind of the center of it and that's about 3,000 people. There's about 107 villages that are in the township and these villages will move every couple of years. Something we actually had to do, that we were told to do beforehand is when we would select a village, we actually had to send a team out there to make sure that village was still there. And it happened once, we selected 30 villages to represent this township, it's called ground-truthing, where you just go out there and you're like, ""Okay, does this village exist? Is there enough houses to do this survey?""
    It happened once where a village relocated 10-15 kilometers away because they had water problems. The stream that was providing water to this village dried up a year or two ago and so everybody just sort of migrated, pretty much to find water. That's kind of the context of Lahai is that it's mountainous and it's sparsely populated. The baseline survey itself took about two months. We did it in 30 villages and we chose those villages based on population size, or as close to population size as we could get. So we had basically kind of the size of population in the village in a Myanmar census and then the chance of the village being selected was weighted on the size. For example, Lahai town itself was chosen twice because it's up 3,000 people whereas the next biggest town we went to had about 700 people. So we chose the 30 villages and then within each of those villages, we did 30 households. Hugo:    So that's around 900 you surveyed in total? Derek:    Yeah, about 900 houses. The average household size came out to be about seven people. The interquartile range was between five and nine people per household. There's a common practice of intergenerational living, like kind of joint families. So you'll have younger children with the mom and the dad and then you'll have the grandparents and sometimes you also have aunts and uncles that live in a house as well. Household sizes can actually be quite big. It ended up being a little over 5,000 people that we would represent. We didn't interview all 5,000. You just took one member of the household and then just asked questions about everybody to that one person. That way, not everybody had to be present. Hugo:    How do you choose which member of the household? Derek:    It actually came down to a little bit of cultural acceptance. Usually it was the father of the household or the male figure. A lot of times, though, people do a lot of agricultural work and so if it was in the middle of the day, a lot of people would already be out in the field. What ended up actually happening more often than not, was a lot of times we did get a lot of female household heads that would answer for the survey. We also got a lot of grandparents that answered for the survey. The only requirement really to be considered a head of household, was that you had to be over 18 and you had to have been able to answer questions for everybody in the household. That was the only inclusion criteria to be deemed the head person to answer the survey. Hugo:    How are the insights that are gained from this baseline health assessment turned into actionables and deliverables for MSF? Derek:    Currently, actually, because we found a lot of respiratory illness with this survey, we're actually helping to put in what's called a GeneXpert machine at Lahai township hospital and so it's to help test for tuberculosis. The old school way of testing for TB would be to hack up your lungs and then you spit your sputum onto a slide and then you stain it, then you put it on a slide and then you have to have a trained health professional to look at that slide and be like, ""Oh, that's your microbacteria. You have TB."" Which is not very sensitive or specific. It's got a sensitivity & specificity in the 50s or 60s so it's pretty crap. But with this technology, the GeneXpert machine, it's highly accurate and it basically can give you test results in a couple hours. But it's expensive and it requires a bit of a constant electrical supply and we wouldn't have put it in unless we knew that there was a high amount of respiratory illness within the area. Hugo:    And is this type of actionable developed from the insights gained from the health assessment similar to other projects at MSF? For example, your work in Uganda at the refugee camp? Derek:    It's pretty similar. A lot of the operational research MSF does, it's all geared towards doing actionable results. One of the surveys we did was on mosquito net use because there's a lot of malaria at the time. And Malaria is cyclical. So when the rainy season came about, that's when you would see spikes of malaria. We did this assessment on net use right before the rainy season and be like, ""Hey, does everybody have bed nets? Or how do you use bed nets?"" The results from that kind of led to mass mosquito net distribution, and also the educational campaign to make sure you use mosquito nets right.
    A couple months after the survey, we ended up actually passing out a whole bunch of mosquito nets. It ended up being close to 13, 14 thousand mosquito nets for the area. It's a great way to see your data in action, and it's great. That's probably the number one thing I like about doing epidemiology for MSF is that if you're lucky, and you stick around long enough, you get to see your results turn into something actionable. Hugo:    So Derek, something I've been thinking a lot about recently is how we don't necessarily have good models for ... or we haven't settled on a global for how data scientists, statisticians, statistical modelers, are embedded in organizational structures in businesses. So I'm wondering how data science is embedded in the organizational structure of MSF. Derek:    Data has always been there in forms of like internal reports, but the operational research aspect of it is actually somewhat new. And it's funny because you can kind of look at ... like in the early days, MSF never really published a lot of scientific data but then all of a sudden there's actually kind of became two big data hubs in MSF. So there's Epicentre in Paris, which is a big repository of a lot of the data that MSF collects, and they also provide a lot of support when it comes to designing various studies, just in terms of study design. And then there's The Mason Unit in England which does the same thing. The Epicentre tends to focus a lot of the French speaking countries and The Mason Unit tends to take a lot of the English speaking countries. But these are two big data repositories so MSF has actually become quite serious in the last five to seven years. More like seven years, on how data is pretty much the best way to inform your policy decisions. It's hard to argue with the numbers.
    And before, especially with a lot of the work that MSF does, a lot of can be a little controversial, and a lot of it can be quite risky. So when you have the hard science and the hard numbers in data there, it adds a lot of weight to your policy decisions. For example, with the Ebola crisis that happened a couple years ago, it was very important to have real-time data of where cases were clustering because that's where you would send your health workers. Not just your doctors but educational people. Like your health promoters to be like, ""Oh, if somebody is bleeding, bring them to a doctor but don't touch the blood. It's a good way to prevent the transmission of Ebola."" But you have to do that fairly fast because Ebola, the incubation time is only a handful of days and then the mortality rate of it, it actually would kill people in about two weeks time. So you had to act really fast. And so the data that you collected was the best way to inform your decisions, otherwise you'll just be arguing over what you heard from people in the village at the time and it would take forever to do something. Hugo:    Something you mentioned earlier was that you see more and more abilities for tech to be used in the work MSF does and the type of surveys you've been doing. What else is there in the future of data science at MSF which isn't there or hasn't been discussed in this conversation? Derek:    Something that I kind of want to see be used a little more, particularly when it comes to health promotion, and something that I'm kind of playing with a little bit is network analysis. So social network analysis. Particularly for health promotion, in addition to outbreak, outbreak epidemiology. That's kind of what most people think about when they think about a network analysis and infectious disease, but it's also a great way to find who in the community has the most influence, and who do you really want to target with your health promotion behaviors.
    So if you're trying to get people to, like something fun or something silly like brush your teeth more, who do you really want to talk to the most? Do you want to talk to the children? Do you want to talk to the moms? Or maybe it's the grandparents that happen to have the most sway. By doing a network analysis, you can see who has the most connections and then you can see the strengths of those connections, and rather than target an entire village and be like, ""Hey, everybody. Brush your teeth."" And have a big rock concert on it, you can just target a handful of people knowing that they would just spread the message to a good chunk of the community. Hugo:    So it almost hurts me to say this but what we're looking for are influencers, right? This is influencer culture. Derek:    Yeah, yeah. Exactly. Instead of taking the influencers of like fashion and stuff, you can kind of give it a little bit of a health bend and be like, ""All right. Maybe if the cool guy were to brush his teeth in public, maybe he has the biggest influence."" But it's getting to know who has these influences in a way that it's really hard to do with traditional data collection methods. So you can find, with a lot of odds ratios and p-values and you can find the strength and magnitude of associations for one variable and another, but you can't really tell, okay, that's just the relation between those things but what outside of it influences those factors as well? Hugo:    For sure. I like the idea that you're thinking about network theory and network analysis in this respect because as you were saying, network analysis is thought about a lot in terms of infectious epidemiology but we know that it has a huge role to play in even non-infectious epidemiology. I think one of the common examples is, people don't contract obesity from each other but if you have a network and you're connected to more people with obesity then you've got a higher likelihood of having it yourself at some point. Derek:    Oh, exactly. That's actually a really great example of it. Obesity and dietary habits are greatly influenced by your friends and network. I can definitely speak from experience. I used to smoke cigarettes for about six years, seven years, and all my friends smoked cigarettes. And it wasn't really until I moved from ... I grew up in Boston, but moved from Boston to Philadelphia and just got an entirely new friend circle where nobody smoked and everybody was healthy and I was like, ""Huh, maybe I should quit smoking."" But I mean, if I had never changed my friend circle, I'd still be smoking two packs a day. Hugo:    So, we haven't talked much about the technical stuff that we love so much. You did mention that for sampling you do a two stage cluster sampling methodology. We¡¯ve also been discussing network analysis. But I'm wondering what's one of your favorite data sciencey techniques or methodologies. Just something you love to do when playing with data. Derek:    That's a great question. That's really good. I don't want to sound lame and kind of basic but logistic regression. Hugo:    Right on. Derek:    That's super simplistic but odds ratios are great because everybody understands them, they're easy to calculate, and you'd be surprised how much data you can get with a binary response. Like, do you like mangos? Yes, no. Like, do you use condoms? Yes, no. Are you an injection drug user? Did you go to a doctor last week? Yes, no. Anything that is kind of binary. And then by extension, you can do multivariate logistic regression type.
    You can do it with different levels of categories as well to get different odds. But that works out really well actually. Not only collect and analyze data but when you present it, so if data science is really going to lead the way in helping to change health policy in this case, you have to be able to communicate your results to people who might not be as like minded. So while I didn't get into coding about R and all this data management and things and it's great, and I love it. I know the project coordinator for Lahai, for example, that doesn't like it at all. Unless you can put it in a graph, their attention span kind of wanes after like three minutes. And so, logistic regression is probably a great way to communicate a lot of results, in my experience. Hugo:    I agree. I agree completely. I've said this time and time again on the podcast, but for people who are non-technical, you can show them that a 10% increase in this feature results in this probability in the outcome. And in that sense, it's interpretable which gives you massive gains. Derek:    Exactly. That, kind of what you just said, is a great way to just explain it. It's short, easy to understand. Just something like that. The odds of this risk factor increasing your risk of contracting HIV are such and such. However, kind of behind the scenes, is the way you sample, the way you collect that data to make sure your analysis is correct. And that gets to be quite complicated. For example, the two stage cluster design thing, using randomly assigned GPS points, that's somewhat complicated and it takes a little bit to do. But in the end, you try to distill it down to things that everybody can kind of digest. Hugo:    So Derek, my last question for you is, do you have a final call to action for all our listeners out there? Derek:    Yeah, yeah. I guess I'm a little different than a lot of people that come on this podcast in the sense that I don't always use massive data sets or I'm not always crunching out numbers in very clean data, but at the same time, I think people should get into data science and get into more field work out there. There's definitely a place for people to get out of the office, collect their own data, analyze it, and then actually have actionable results. Don't let the fear sitting in a cubicle somewhere just pumping out code dissuade you from doing data science. Data science is for everybody. Hugo:    Fantastic, Derek. It's been such a pleasure having you on the show. Derek:    This is great. I had a real fun time. Really good."
"datacamp",2018-07-25,"Chatbots, Conversational Software & Data Science (Transcript)","https://www.datacamp.com/community/blog/chatbots-conversational-software-data-science","Here is a link to the podcast. Hugo:    Hi there Alan and welcome to DataFramed. Alan:    Hey, it's great to be here. Hugo:    It's great to have you on the show. I want to jump in and discuss your OG Media post from April 2016, which really got the ball rolling for everything you're working in now. You opened this post with the following statement, ""We don't know how to build conversational software yet."" I feel like you meant ""we"" as a society and community of tool builders. Conversational software includes ChatBots. I'm wondering now, two odd years later, is it still the case that we don't know how to build these types of things yet? Alan:    I would say probably yes, I still agree with that statement, but we definitely made some progress. It's still very much early days for building great natural language interfaces for computers. Hugo:    Tell me a bit about the progress you've seen in the past two years. Alan:    Maybe a bit of context as well where that blog post came from. We built a few SlackBots because Slack was one of the first platforms that opened for this. Alex, my co-founder and I had ... Actually we had a couple of chatbots that companies were paying for. They were both around actually making data science more accessible, turning natural language queries into SQL and then running those queries on a database. Anyway, we had some experience building these things and looked around and thought, ""Wow. There really aren't any great developer tools for how to build conversational software."" Then we saw that Facebook Messenger platform was about to open up and that was right around this time. I thought, this is just, this is not going to go well. Hugo:    What you mean by Facebook Messenger platform opening up is opening up in order to have conversational software in it? Alan:    Exactly. Letting developers build chatbots and that anyone with a Facebook account could then chat to. We thought, ""Well, we have now quite a bit of experience. We know it¡¯s really hard to do this well and these are not going to be great experiences for people."" I think that is actually pretty much how it panned out. There's a lot of hard work involved in getting things working well even in a narrow domain. Doing something with just kind of open ended conversation is definitely nowhere in sight. Alan:    I think we've made some good progress in terms of libraries and tools that people can use that make it easier to build something that does work, even though we've definitely not cracked this problem by any means. Hugo:    What are the most pressing things that we haven't cracked with respect to building conversational software? Alan:    There are a number of things. The two things you need for building conversational software ... The first is understanding messages that people are sending to you. That's called NLU, or natural language understanding. What that usually means is classifying a short piece of text as belonging to one of N intents. Those are class labels for things like a greeting or saying good bye or asking for some specific things that your chatbot can do. Hugo:    Like looking for a hotel or something like that? Alan:    Exactly. Like, ""I'm looking for a hotel."" Then, the other part is pulling out some structured data. That's usually called entities. I'm looking for a hotel in San Francisco and then knowing that San Francisco is what you want to use in your query. That's NLU. I think it's fine to call it that as so long as you remember it's absolutely not true. The computer doesn't understand anything. It's just able to classify text into one of these buckets and then pull out some entities. Hugo:    Yeah. This is a large concern in general with terms like artificial intelligence, that kind of anthropomorphism involved in nomenclature and the way we name stuff. It's dangerous, especially when it permeates common language. Alan:    Definitely. Definitely. I think there are lots of complications and limitations to that, even that simple model of the intention of what a short message means, being a universal single label. This always means this. You can only represent its meaning by a single one hot vector. That's obviously a very limited model of understanding what people can express. It's a good starting point and lets you build on top of that, but of course, it's not ... When we think about the future, I think that's not how we're going to represent the meaning of short messages.Hugo:    We have NLU as being a huge challenge. What was the other one you were going to speak to? Alan:    Yeah, the other one then is dialogue management. If somebody says ""yes"" in the middle of the conversation, the way you respond, of course, depends on the context. It depends on what happened before. It's not enough to just map each of these intents or each of the outputs of your NLU system to the same action. You need to always build up some kind of statefulness. Alan:    Then, the question is how do you handle that in a way that doesn't break and is actually maintainable? That's a big part of what we've been working on for the last couple of years. That's, actually, I think the biggest part that was really missing back in 2016 was a reasonable way of dealing with that complexity because the way that people were doing it, basically manually writing a lot of rules, just doesn't work and it doesn't scale. It causes a lot of headaches. Hugo:    Right. I like this idea of scaling. Because as you say, writing a bunch of rules is, I suppose, the most bare-bones naive way to think about writing conversational software. I imagine you can have a set of nested ""if-elses"" to try and deal with everything, or a subset of everything. Then, as soon as a new use case comes up, you then need to nest them even further. This is something which definitively does not scale. Alan:    Right. Exactly. If you nest them deep enough, then it counts as deep learning.Hugo:    That's hilarious. Alan:    In general, you have ... I've tried this plenty of times. Even building a relative simple bot that it was a banking chatbot that we just built for fun to see ¡°How badly does this work if you really just do it with rules?¡±You could do a couple of things like check your balance and transfer some cash to people and everything. I think it came out at over 500 rules just for doing this simple stuff. Of course, then when you want to update something or something goes wrong when you add a new rule, it clashes with the old ones. Then, you go, ""Oh, no."" Then, you have to go and try to reason about all these rules and figure out why it is that something broke or that something clashed. Alan:    There's an asymmetry there, which is interesting, because in the middle of conversation ... The kind of cliche example by now is what do we want? chatbots. When do we want them? Sorry, I didn't understand your request. Hugo:    Okay. That's provided a great teaser into a lot of the through lines that we're going to talk about with respect to NLU, scaling, a lot of different use cases of these types of chatbots and conversational software. Before all that, I'd like to find out a bit about you and Rasa. Maybe you can tell me a bit about yourself, what you do, and what Rasa does and what Rasa is. Alan:    Yeah, Rasa is two different things. Rasa is a company. It's a start-up. It's also a pair of open source libraries. There's Rasa NLU, which does language understanding, so parsing short messages. Then, there's Rasa Core, which does dialogue management. I'm a maintainer of both of those libraries. The aim of them is really to expand chatbots and conversational software beyond the answering simple questions, FAQ style, one input, one output kind of ... Turning it into a real conversation and building that in a way that scales.Alan:    Where I see us as a company and also where we come from was around 2016, when we said, ""Okay, nobody knows how to do this."" Actually there's a lot of research on how to use machine learning to overcome some of these problems. There's a lot of great papers written on it, but there weren't any libraries that developers could use to actually implement those ideas. Where we see our role is really in that big gap between arXiv and github, let's say. Something that's actually well maintained, has lots of tests, has people responding to issues, has support, gets updated regularly, and ... We do a lot of applied research in this field and we publish papers. We work together with universities, but it's always very strictly applied. Then, the primary output is always to put out some new code that people can do that does something better that they couldn't do before.Alan:    Where I see us as a company and also where we come from was around 2016, when we said, ""Okay, nobody knows how to do this."" Actually there's a lot of research on how to use machine learning to overcome some of these problems. There's a lot of great papers written on it, but there weren't any libraries that developers could use to actually implement those ideas. Where we see our role is really in that big gap between arXiv and github, let's say. Something that's actually well maintained, has lots of tests, has people responding to issues, has support, gets updated regularly, and ... We do a lot of applied research in this field and we publish papers. We work together with universities, but it's always very strictly applied. Then, the primary output is always to put out some new code that people can do that does something better that they couldn't do before.Alan:    One recent example was we completely changed how we do intent classification. We shipped a new model, which threw all the old assumptions out the window and just said, ""Okay, now we're going to learn word embeddings in a supervised way for this task."" That lets us do things like building up hierarchical representations of meaning, understanding that a message can contain multiple meanings, because sometimes people just say multiple things. We do a lot of that. Then, the primary output is a piece of code people can use. Then, if we write up a paper, that's a nice component.Hugo:    This speaks to the open source side of Rasa. You said that Rasa is two things?Alan:    Yeah, so the other side is a company. The first year that we operated, we basically did a bunch of consulting work on top of the open source. That was really great, because building stuff with it yourself, it keeps you really honest about its limitations, helps you understand your customers. Then, after we did that for a year, that was obviously very nice and we bootstrapped the company, then we said, ""Actually, we think this is scalable product we can build here for an enterprise version."" We talked to a lot of these big companies that we've been consulting for. There was a clear need for an enterprise package for more features and a different product. We thought, ""Okay, that's something we want to take a bet on."" For the last six months now, we've stopped all the consulting work. We raised some venture capital and really just went full on building out the enterprise version.Alan:    Then, still all the machine learning stuff goes into the open source. A big part of what we believe in is you can't build up a competitive advantage by having secret implementations of algorithms lurking around. The machine learning stuff needs to be open and needs to be tweakable. People need to be able to play with it. There's just so much nonsense around in the AI space that it's better to just say, ""No fairy dust. There's no magic. It's just stats. Go look at the code. All the machine learning's open source. You can see exactly what it does. You can tweak it for your own purposes.""Hugo:    Yeah. I'm sure all the different interplays between your company and your open source development are really exciting. For example, correct me if I'm wrong, but you've recently hired two machine learning researchers. Alan:    Yeah. We're only 10 full-time people, but two of those are full-time on ML research. The measure that we really care about is how quickly can we take a new idea, like this little trick actually works, and then put it in production. The great thing about having the open source community is that whenever we have a new idea or something that kind of works, there are just thousands of people who are just ready to check out the master branch and see what it does on their data sets. That feedback loop is really awesome. You can't do that if you build things that are closed source product. You don't get that kind of insight. Hugo:    That's really awesome. I suppose we should say also that it's a Python library, right? Alan:    It is. It's written in Python, but also we obviously work a lot with large companies. Our main enterprise customers are Fortune 500. They're, let's say, not mostly on Python. They mostly write in .NET or Java or C#. Then, there's a large chatbot developer community that uses JavaScript. What we did was we made sure that the libraries, even though they're written in Python, you can use them without using any Python. You can consume everything over http API. That means that you don't have to be running a Python stack yourself to use these libraries. You can spin them up in docker containers and use them and deploy them to production without having to actually write in Python yourself. Hugo:    That's really cool. I may be putting the cart before the horse, or the chatbot before the company to extend the analogy into absurdity, but there is a fantastic DataCamp course that you've created and I facilitated last year on building conversational software with Python and using Rasa. There's also a lovely interplay, for those people who know a bit about the Python data science landscape, a lovely interplay between Rasa, scikit-learn, and SpaCy, for example. Alan:    Exactly. There's no point in reinventing the wheel. A lot of really great libraries out there for doing different things ... For example, in Rasa Core, which is the dialogue manager, you can plug in different back ends to implement your model in and actually do the machine learning part. One way you can think about Rasa Core is it does all the hard work to get that conversation into the kind of XY pair format that you think about when you think about machine learning. Then, you can plug in whatever classifier you like. Of course, we have some good ones implemented already. You can implement your stuff in TensorFlow or Keras. Actually, I think the Keras API was a big inspiration for that. Just saying, ""Okay, can we just abstract over all the things that aren't important for understanding the problem and really just present the API that makes sense to you?"" Then, we don't need to build our own autodiff library, because why should we? You can use TensorFlow for that. That also runs on GPU, et cetera.Alan:    Similarly for doing things like part of speech tagging. There's a lot of things you can do with SpaCy and it makes more sense to build on top of libraries like that. You can choose different back ends you want to use and implement some lower level functionality for both Rasa NLU and Rasa Core. Hugo:    That's fantastic. I want to in a second to think about what type of use cases, which verticals and industries, you see most interested in conversational software. Before that, I just want to step back a bit. What's a good working definition of a chatbot or conversational software? Are they the same thing? That's a relatively ill-formed question, but maybe you can speak to that. Alan:    Yeah. I'd say that the things that we're interested in are not just strictly chatbots. Some people would call some of these things virtual assistants. I think that voice is extremely important. Anything where you interact with the computer through natural language is something that we're interested in. I would call all of that conversational software. Hugo:    That's different to just a chatbot, right? Alan:    I think chatbots mean different things. In some groups of people, a chatbot strictly means actually something that does chit chat, can't actually do anything for you, it's just there having an interesting conversation. That's actually less what we're interested in. Small talk is not something that we really focus on. I'm more interested in purposeful conversations that actually do something. Then, there's another kind of definition that says a chatbot is on these mobile messaging apps, right? Facebook Messenger or Slack or WhatsApp or Telegram or any kind of application that lives inside of one of those apps, even whether it's chat or button-based or it shows you a little web view. All of those are also chatbots. It's ill-defined as most terms are. Hugo:    This may be a relatively naive question, but I think naive questions can give us a lot of insight. Why should we care about chatbots or conversational software?Alan:    I think it's a great question. I think one of the main reasons I'm really excited about it is that it makes computers usable by people who aren't experts. That can be on a very banal level or it can be on a more sophisticated level. For example, my grandfather never used a computer in his life. He knew what the word ""internet"" was, but he never knew what it meant and he never had an experience of it. Then, I think about my parents, who are to some degree computer-literate, but also certainly not the same as my generation. Then, I think of how quickly technology is progressing.Alan:    If we all believe that tech goes exponentially fast, then if my grandfather was behind, imagine how far behind we're going to be when we're old. I think a big step that we need to take is how do we, rather than have the need for computer literacy, how does the computer adapt to deal with how we already think about things? It's sometimes for really stuff. Just asking a simple question and getting an answer, rather than having to open up a document and find where it is or something. Then, sometimes it's for more sophisticated things. I always like to think how many Google searches do you think there are every year for, ""How do I do X in software Y?"" Then, imagine that you didn't have to Google that. You just had to say to the software, ""How do I do X?""Alan:    Photoshop is something I know is very powerful. I have no idea how to use it, but I could express some of the simple things I'd like to get done. ""Add a blue border around this image"" or something. Why can't I just engage with computers by saying what I want and just having it happen? I think it's powerful in a lot of different ways, especially for making tech more inclusive. It's really important.Hugo:    No, that's great. Because that also speaks to a very personal, individual power. Not the type of power when you're working with corporations or businesses achieving business goals, but really on the ground empowering. Alan:    Yeah. Yeah, absolutely. Hugo:    Which verticals and industries are currently most interested in using conversation software? Alan:    My sample is definitely biased, because I work on Rasa. The companies that we know about and approach us are the ones that find us important enough that they want to invest their own engineers into building this out. They realize it's strategically important. They want to build up these capabilities. I'm sure there are other industries where it's really relevant, but the considerations are different. They don't necessarily want to run the tech or own the tech in-house. Where we see the strongest pull from industries in financial services, so insurance and banking, and another one, which is automotive. I think that makes a lot of sense. I remember when I got the Amazon Echo. I had it for about six months. I thought it was cool. It was fun, but wasn't a game-changer. Alan:    Then, I went to a conference in the south of Spain. I rented a car and I had to drive three hours from the airport. I remember just driving on the motorway in a foreign country with my phone clipped to the dashboard. I was tapping on it change the map and to change the song on Spotify. I thought, ""This is so dangerous. Why can't I just talk to the car and say, 'Play this song' the same was I was used to doing with the Echo at home?"" I think automotive is a really valid use case, especially for voice interactions. Hugo:    For sure. That's very clear where the value can be delivered. The example you just gave was a paradigm, I think. How about in insurance and banking? What kind of value can conversational software deliver there? Alan:    I think it's on different levels. If you think about what is your main interface to your bank, to the data that you have with the bank, and how do you engaged with it? Some banks have really nice mobile apps and you can just do everything there. One other interface that a lot of people fall back to is just going to the branch and talking to a human. Why do you do that? Because you have questions, right? You can't ask a question of an app generally. You can do the things that were suggested to you that the developers decided to implement, but you can't say the things that you don't know or ask for the things that haven't been implemented. Asking for advice, understanding things a bit more in depth. Those are all things where there's a lot of value to be added, especially around insurance where the actual product that you buy, the policy, can be very complicated and hard to understand. Talking to a human is also tedious. You have to take time out of your day to engage with them.Alan:    If you have a 24/7 agent that you can chat with, they can answer most of your questions, it's actually really pretty powerful. Hugo:    Having some sort of intelligent conversational software for banking in that sense, for questions and FAQs and that type of stuff, would stop me from becoming infuriated when I call the bank. You end up in one of those graphs that directs you downwards and you just keep shouting representative like 15 times. Press 0 for whatever, because it makes no sense. I also have the added challenge, maybe you can speak to this, that I'm an Australian living in the United States most of the year round. I have to put on a fake American accent to be understood by the automated telephone system of Bank of America. Alan:    I can definitely understand that. I've seen a lot of that also. My girlfriend's from Scotland. Her accent can be misunderstood both by humans and by software. Also, non-native English speakers understanding language. Understanding speech, there's been some great successes, but it's certainly not a solved problem or commodity by any means.Alan:    It's not something where we try and compete. You've got to focus on something and it's not something that we do. It's a certainly really interesting problem. Hugo:    Hey, I remember you telling me some time ago about an insurance chatbot, but it wasn't voice. It was an SMS chatbot. Maybe you could tell us a bit about these types of use cases. Alan:    Yeah. That, actually, was a really interesting one. It's a large insurance company in Switzerland]. A really big company. It's 160 years old. They wanted to do something to engage people whose house insurance policies were about to expire. These are five year policies and they run out. They have a large customer base. Because employing people in Switzerland is very expensive, it actually wasn't literally worth their time to have an agent call up all these people and ask if they want to renew their policy. What they did was they built a chatbot, actually with Rasa, and it went out over SMS and engaged with these people. It said, ""Hey, your policy is due to expire. Is your living situation still the same?"" It would ask them questions. Has anything changed? Maybe you moved. Maybe you got a dog. Maybe you got a more expensive car. Or whatever it is. If the person wanted to renew and they collected all the data they needed, they'd say, ""Okay, here's your quote. Is that cool?"" If they agreed to it, it would just actually finalize the policy and the policy would be in the post with them within a couple of days.Alan:    It's really end-to-end automated. That's also what I mean by conversations that really do something. It's nice to answer questions, but it's a lot more powerful ... You can say, ""Okay, it's now done."" It's on the way. I think one of the interesting things is it challenged a lot of assumptions around what chatbots are and what they're useful for. People think about it firstly as a customer service thing and saving costs, which is certainly relevant. If you can actually increase revenue, that's really compelling. This was actively reaching out to these people over SMS. It was a 30% conversion rate, which is really astonishing in terms of getting people to buy a new five-year policy.Alan:    The other thing is that people think about chatbots and they think about generation Z and messaging apps, when actually the first person to buy an insurance policy over this chatbot was a 55 year old Swiss lady. That's all really interesting. I think it also speaks to the fact that it does make tech more accessible for a larger group of people. You can just speak to these systems. You let your customers speak to you how they think about the problem, rather than forcing them into your paradigm, which is this software that you've built for them to  with your company. Hugo:    Exactly. You've spoken to some really interesting use cases in insurance, banking, the automotive industry. Are there any other industries that aren't as interested in conversational software as you think they should be? Alan:    That's a really good question. I am not sure I have a good answer to it. I have personal frustrations with things like telecom companies getting your internet set up in your flat and all that kind of stuff, where it could certainly be a lot more useful if you could have a 24/7 automated support that you chat to and get things done. I'm not sure if I can think of off-the-shelf of examples of industries that are really neglecting this. We see, actually, a really big rise of companies reaching out telling us that they're using Rasa and sometimes asking about the enterprise or sometimes just have some questions. It's actually much more diverse than the use cases I listed. Definitely the dominant ones are things like financial services and automotive. Hugo:    Okay. Great. As you mentioned, expressed your frustrations with calling up telcos for example, I actually remembered government, I think is probably a great example. Alan:    Oh, yeah. Hugo:    The IRS and the tax system. I actually ended up ... I mentioned my frustration with those telephone trees that you get sent down, like, ""Press 1 for whatever"". I ended up, somehow, when I called the IRS several years ago, on a loop. I thought maybe you wouldn't be allowed loops in these graphs, but I ended up on a ... I was about to swear. I ended up on a loop in one of these conversations. I ended up hanging up. There's definitely room for a lot more aspects of conversational software in these types of places. Alan:    Yeah. I think when the experience of someone who's in a phone tree is, ""How do I most quickly get to speak to a human?"" It's not, ""How do I most quickly get the thing done?"" Just because we have such low expectations of these things actually ... Because they're mostly just pre-qualifying what you're problem is to send you to the correct person. Or that when the person talks to you, they already know half the information that they need and they don't have to collect that. It's really optimization on their part. It's really not optimization on the point of you as a customer getting your problem solved quickly. Hugo:    For sure. I think this has provided a lot of insight for our listeners into conversational software, where it's being used, the ins and outs. I'm sure a lot of them are eager to hear about where they can get started. I think both technical and non-technical listeners alike would be interested in hearing about how they can get started with conversational software. Maybe you can give a few pointers? Alan:    Yeah, definitely. Also partly because there was this big boom when Facebook Messenger opened up, there are actually some really nice online tools that you can just point and click and build a little prototype of a chatbot. Dialogue Flow is one example. It's owned by Google. Chat Fuel is another. You don't have to really write any code. You can design a prototype of your chatbot. You don't have to set up a server or anything. You can very quickly get something you can try out and just get a feel for how something like that would work. Alan:    Then, if you want to go beyond that, there's lots of good resources on understanding the tech behind it. Building something you can really maintain and scale. There's, of course, the DataCamp course that we created that really covers the fundamentals. What are you really doing? What are you really going on? Demystifying this concept. Which if you've never worked in NLP or anything, it seems really bizarre and almost impossible. How do you build computer programs and understand language? It seems so inconceivably hard. We go into a lot of fundamentals there.Alan:    Then, if you want to build more advanced things, you can either then build everything from the ground up, which is always an interesting, especially for a learning experience, an interesting approach. Or you can use something like Rasa, an open source library, where a lot of the heavy-lifting is done for you. You can then get started, build something out, then really tweak parameters to get better performance on your data set. Give it to real people and iterate. I'd tell you the best way, if you want to tinker, build a chatbot first. If you want to actually solve a problem, and you actually understand what people do, then actually the best way is not to have a chatbot, but just pretend to be a chatbot yourself. Just say that it's a bot and ask people to talk to it. Then, answer them yourself.Alan:    If people don't like the experience with your brain behind it, then they're definitely not going to like the experience with an artificial machine learning system behind it. It's a great way to validate if what you're doing actually makes sense and really get some inspiration for all the things you didn't think about.Alan:    One mantra that we have, and it's really one of the principles that we use to build our products, is that real conversations are more important than hypothetical ones. I'm less interested in giving people tools to design hypothetical situations and think about all possible conversations that people can have. It's more important to look at real conversations that people do have and learning from them. That's what Rasa Core is all about is learning to have conversations from real data. Hugo:    Absolutely. There's so much in there. I want to recap a couple of the takeaways. The first takeaway for me, of course, is take this DataCamp course. I do think it provides a wonderful introduction. In the first chapter, you get to build a chatbot, which is based around one of the early chatbots, the ELIZA chatbot, which is incredible. You do a bunch of natural language understanding. Then, you get to build a virtual assistant. It's a personal assistant that helps you plan trips, which is really cool. I think you've got a lot of insight into this course into everything we've been talking about.Hugo:    The second takeaway, I think, is you need to think about what the purpose of your chatbot is. For example, a lot of people, I think have a misconception that chatbots need to sound like humans, for example. The question is if you've got a virtual assistant who's going to help you build a trip, do you care whether it sounds like a human at all? Or do you want it to do the job you want it to do, right? Alan:    Yeah, I do have to agree with you. I do think there's a big, important topic as well around the design and writing good copy and being empathetic and doing things like active listening. That's a whole orthogonal set of problems kind of to what we're really tackling. It is important. Of course, yeah, the question is actually do you solve the problem? Do you actually do something for people?Alan:    I end up using, at least a couple of times a month, some product that I just completely hate, but I have to use it, because it's the only thing that gets the job done that I need. I think also in start-ups, if you think about product market fit, you almost want to put buyers in people's way. You almost want to have your first version of your product be really crappy. Maybe even intentionally so, just to see people persevere, because it really solves a problem for them. Hugo:    What are the most common misconceptions surrounding conversational software that you want to correct? Alan:    I think the first one is really artificial intelligence, maybe even artificial general intelligence. That you need to wait for DeepMind or OpenAI to solve these grand problems. Then, you'll be able to download a brain from the internet. That's going to magically solve all your problems. You can build stuff now with the techniques that exist. You've just got to put in a bit of work. You don't need a degree in statistics or computer science or anything like that to build these things. You can definitely self-teach. You can build something that's really useful and adds a lot of value with what's out there now. You shouldn't even try, I think, to build a do everything kind of system.Alan:    I think another one is that only Facebook, Google, Amazon, Apple have the tech to build conversational AI. It's just not true. We see that a lot. There's also some academic papers published, where they compare Rasa to some of these closed-source tools from Google and Microsoft. It actually does very well, because there's no fairy dust. I don't want to pitch Rasa too much, but the way Microsoft and IBM think about this problem is we have magical algorithms in the cloud. Upload your data here and we will turn it into gold. I think that's just nonsense. You can build a lot of great things, even better things, with opensource tools because you have full control. You can tweak things. You can customize things for your use case. It's definitely not something that's only in the domain of the big tech companies.Alan:    Then, another one that we already spoke to a little bit is that this generational bias that it's only something that's fun and light and for Millennials. It's actually something for the Boomers and the older generations as well. It's a really important piece of tech now. Hugo:    One thing you mentioned was this idea of product market fit. I want to zoom in on where Rasa Core came from and how it emerged. I want to approach it from, I suppose, the idea that we discussed at the start of this conversation about needing to have conversational software that scales. Maybe you could tell us about the landscape before Rasa Core and then how Rasa Core emerged in relation to the idea of scaling this software.Alan:    Yeah, definitely. Before Rasa Core came out, there was some nice cloud APIs for doing the NLU part. You send it a sentence and it sends you back its interpretation of what that means, what's the intent, what entities are in there. Then, the question is what do you do in response to that? You have things like yes and no. Of course, the response of that depends on what happened before. What do you do? You write you some rules. You write out a rule for, ""Okay, if the last thing we asked was this and the person says yes, then proceed down this direction. If the last thing the person said was no, then go down this path."" That's great. Then, you say, ""Okay, we now need to maintain that state over multiple turns in a conversation."" You don't just build point A to point B. You build a state machine.Alan:    You say, ""Okay, this person is currently in this state."" Now, they've said this. I've moved into this state, where they're now ready to make a purchase or they're asking about this topic or whatever. That, of course, works to an extent. Then, you deploy this and you give it to people. You ask them a yes or no question. One example is, ""Should we just send that to your home address?"" You've maybe built a branch for yes and you've built a branch for no. Then, the person responds, ""Oh, what home address do you have on record for me?"" There's always an edge case that you haven't thought of. Which is fine. Of course, all software has edge cases. Then the question is how do you deal with that? If that's an important thing that lots of people say, you need to answer that to them. You can either then add another nested if statement in your logic, or you can add another state to your state machine, which then is explaining some deeper information. Then, probably you can get into that state from multiple different other states. You have one use states, but then you have order N squared ways of getting in and out, because you have all the other states.Alan:    That very, very quickly becomes unmanageable. You have this bag of rules about how people navigate this state space. Then, whenever you want to change something, it clashes with old rules or breaks something else. The thing is that mid-conversation, it's absolutely trivial to know if a chat box said the wrong thing. You can give that to a four year old and say, ""No, that's nonsense. That doesn't make sense."" It's really hard to then figure out from that big state machine why did that happen? Why did that go wrong? Reversing the logic. Alan:    We said, ""Okay, why don't we do it completely differently?"" We don't build out that state machine at all and we just say, ""Okay, here's a conversation that went wrong."" Say what should have happened instead. Add that to your training data. Then, train the machine learning model that learns how to have these conversations. Rather than having a fixed, solid representation of this state machine, you learn a continuous fuzzy representation. A continuous vector that represents that state. You learn that such that you can do these conversations that you've had, you've seen. Then, you can measure how well it can generalize some of these patterns.Alan:    There's a recent paper that we just finished. I'll make public very soon. Where we actually study how well you can take these general patterns, like answering a clarification question like, ""Oh, what address do you have on file for me?"" Then, reusing that in different context and even different domains. That's what we've always wanted to do with Rasa Core is say, ""Throw away your state machine. Don't try and anticipate and write rules for every possible conversation."" Because it's basically impossible to build a flow chart and reason about every conversation that everybody could have, because it's just a combinatorially big space. Don't do that. Just have real conversations and learn from that. That's where Rasa Core came from.Alan:    There was lots of research on doing machine learning-based dialogue management. We certainly didn't embed them. We had to do things a little bit differently from how the academic world was doing it. They were doing a lot of work on reinforcement learning. There were some technical reasons why that didn't make sense for people getting started with Rasa Core. The short version is basically you have to implement a reward function. That's not a trivial thing to do. We said, ""Okay, we don't go for reinforcement learning. We do everything supervised learning."" We let people build up these conversations interactively by talking to the system. That's what we've been doing with Rasa Core is taking some of the ideas from the literature. The ones that we think are most applicable.Alan:    Then, straying from the literature, where we think it makes sense to. Then, building a library that lets people who don't have a PhD in statistical dialogue systems actually build machine learning-based dialogue and not have to build these unmaintainable state machines. Hugo:    That's great. This movement from state machines to the importance of real conversations and machine learning. Implementations in machine learning algorithms, which learn for more and more conversations and more training data, real data is incredible. So what happens next? What is the future of conversational software look like, thinking about it in these terms.Alan:    That's really interesting. I think for developers, it's a really cool time to be in this. We engaged a lot with our community. That's probably my favorite thing about open source is we have literally thousands and thousands of developers who use our software. We have over 100 contributors to the code base, who are just people using it for their own purposes and contribute back. This is a real sense of excitement. People are building and inventing new things. I kind of think about it like being a web developer in the 90s. If you build websites now or you have friends in development, it's well understood what you have to do. None of that's really been invented yet for conversational AI.Alan:    I mean, some of the kind of early versions of it are there. As a developer, this is a green field. I get to invent a lot of stuff. That's really fun. There's still lots of challenges. It's not, by any means, a solved problem. That's why we invest so heavily in research and breaking through the limitations of what we see currently in our own libraries and then what people do in academia. Pushing beyond that and shipping it into the libraries.Alan:    On the consumer side, the future is lots of little magical moments, where something just works. I had a great one literally yesterday. I was on Google Analytics. It's not something I look at very often. Maybe once every few months, because I was working on our documentation. I wanted to know how many people view our documentation on a mobile device. I'm not an expert in Google Analytics, so I would have to go and build some filter. They have this cool feature where you can just ask a question. I literally just typed into the box, ""How many of our users at nlu.rasa.com, our documentation, are a mobile client?"" It just answered, ""6%."" I thought, ""Amazing. I didn't have to do anything and it worked the first time.Alan:    Those magical moments are really cool, especially fun because that was one of the first use cases we worked on in early 2016 that we actually had some paying customers for. Then, to see that deployed at scale in the wild was really exciting. I think a lot of really magical moments that we'll all experience over the following years, but we won't have anytime soon is the do everything magical assistant that replaces a human butler or something. Hugo:    Yeah. It's great to hear your experience as a user as well, as someone who develops a lot of these things also. You spoke to the future challenge, or current or future challenge, of scaling to multiple use cases. Are there any other big challenges facing conversational software development? Alan:    Yes, I do think the biggest one is, ""Okay, you have something which works in a narrow domain. How do you extend it to more domains? We recently saw this demo from Google Duplex. Primarily very, very impressive, text-to-speech, the speech synthesis. Also a very nice functioning dial-up system that can handle quite a bit of complexity. They're very open in their post about the software. It only works because it's very limited. It works on restaurants and hairdresser appointments. Then, the question is, ""Okay. How do you build the next 100 use cases?"" If you look at Amazon, I think there's 6,000 engineers working on Alexa. That's a big effort.Alan:    Then, the other part is really not just around the technical challenge of building it, but it's also the conceptual challenge for programmers to build software, where the core logic is learn from data. That's very different from calling an image-recognition API. You send it a picture and it tells you that there's an apple in the picture and then you have statements like, ""If apple say apple"" or something. The way the conversations go is learned from real conversations that people have had and that you've checked and annotated and fixed and learned from. Alan:    Managing that training data become a way of programming. A lot of product management needs to be rented there in terms of how do you actually do that? That's really interesting. That's obviously something that we spent a lot of time thinking about. I think we have some cool things in the pipeline as well. I'll have some great things to show there in the future. Hugo:     Great. I look forward to it, hearing about those future developments. Alan, my last question is there is do you have a final call to action for all our listeners out there? Alan:    Yeah. All of Rasa is open source so go check it out. Rasa.com is the website. If you search ""Rasa Core"" or ""Rasa NLU"", Google should show you that the documentation, the GitHub repos, and build something. Try it out. Let us know how it goes.Alan:    I'm really curious. People always come up with infinitely creative use cases. Yeah, if you have any problems, let us know.Hugo:    Fantastic. Alan, it's been an absolute pleasure having you on the show."
