"site","date","headline","url_address","text"
"vidhya",2018-08-01,"The Best Machine Learning GitHub Repositories & Reddit Threads from July 2018","https://www.analyticsvidhya.com/blog/2018/08/best-machine-learning-github-repositories-reddit-threads-july-2018/","Did you ever imagine you could become an artist without knowing how to paint or even hold a paintbrush? This is what you can do now, thanks to computer vision techniques. And what¡¯s even better, the ML community is so awesome that the code to do this has been open sourced! This is the power of GitHub and why I encourage all data scientists, aspiring or established, to use it regularly. GitHub has been at the heart of open source data science and machine learning.<U+00A0>Whether you are contributing to an existing repository or building one of your own, you are sure to gain a ton of knowledge. There are some really cool repositories below <U+2013> deep learning and GANs specific, natural language processing (NLP) related text matching, and computer vision (as mentioned above) to extend and re-imagine existing images. There¡¯s something here for everyone! Coming to Reddit, we have selected a mix of deep learning and artificial intelligence related discussions. These will help you assess and understand the current state of certain technologies in the industry and where we might be headed in the near future. You can check out the top GitHub repositories and top Reddit discussions (from April onwards) for the first 6 months of the year below: This is one of the coolest repositories I have covered in this series. ¡®Inpainting¡¯ has been a trending concept recently but this technique, designed by a couple of researchers from Stanford, does the opposite.<U+00A0>¡®Outpainting¡¯ extends the use of GANs for inpainting to estimate and imagine what the existing image might look like beyond what can be seen. Then the algorithm expands the image beyond it¡¯s existing boundaries. The results, as you can see in the image above, are outstanding. This repository is an open source implementation using Keras in Python. You can either build a model from scratch or use the one provided by this repository¡¯s author. Either way, try it out! Be sure to check out Analytics Vidhya¡¯s article on this approach here. This repository does what it says <U+2013> it¡¯s a TensorFlow implementation of various text classification models. What I liked about this repository is that it contains links to each model that has been discussed. This provides an understanding of what you are doing, which is extremely helpful. The models implemented here are: While not strictly a library created last month, this repository got a big update recently. MatchZoo is basically a toolkit for text matching. It has been created in order to design, compare and share the various deep text matching models. Potential tasks MatchZoo can do are document retrieval, conversational response ranking, question answering, and paraphrase identification, among others. Some of the deep matching methods out there are DRMM, MatchPyramid, MV-LSTM, aNMM, DUET, etc. Check out the repository to get details on how to install and take advantage of this extremely useful library. Does the above ensemble of faces get you excited about this repository? The image inside the green border is the original one, the rest of the images use GANimation to anatomically change the facial expressions of the subject(s). This is a slightly complex approach but is something you must explore if you are interested in deep learning. The authors have provided everything you need to get started <U+2013> a beginner¡¯s guide, prerequisites, data preparation resources, and of course, the Python code. What are you waiting for? Dig in! This excellent repository contains Python codes for various experiments conducted as part of the ¡®here¡® paper. This was presented at the International Conference on Machine Learning 2018 last month. It¡¯s a fascinating case study for anybody interested in deep learning and especially GANs. Why I have included this repository is because it gives you a really good idea of the level of research and thinking that goes into papers that are accepted and presented at top class ML conferences. You can also view the best papers from ICML 2018 here. <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> Source: Wikipedia If you are a newcomer to deep learning, this instantly becomes a must-read thread for you. Plenty of DL experts have provided their views (and a plethora of links) on recently published papers that you should read and implement. This reinforces what you¡¯ve learned and has the additional advantage of keeping you up-to-date with a breakthrough technique. If you are a deep learning veteran, this will either refresh your concepts or teach you about all that¡¯s happening in this diverse field. You can never get enough knowledge so I encourage you to check out all the resources provided. You should also read through all the opinions provided by other data scientists which will add to your own perspective. The title of this thread is enough to grab a data scientist¡¯s attention. This discussion spawned from a Twitter debate on how science is being used by the big technology organizations. While the debate started from a pessimistic viewpoint, it jumped to more positive or assertive views from people who have worked with these companies. You will not only learn how science is defined and used at Google Brain, et all, but also what fellow data science people think about the current state of science in the industry. If you want to get into the research side of machine learning, you need to know the theory behind how things work. Thin includes topics like core mathematics, probability, etc. This thread lists down some of the more advanced books on various machine learning concepts. There are tons and tons of suggestions (almost a 100 comments!) in there along with links so you cannot complain about lack of resources. From advanced ML to introduction to reinforcement learning, this thread is a goldmine of top notch resources. This has been an ongoing discussions since decades, and has gained even more prominence with the recent interest in ML and AI. The concern is real despite experts doing their best to allay fears. Go through this thread end-to-end <U+2013> it contains opinions from AI enthusiasts and experts about how they see AI impacting jobs in different countries. There are also plenty of statistics and links shared which help in gauging where AI is headed. Make sure you contribute with your valuable opinion to the overall discussion as well. The more you put yourself out there, the more confident you will be in your data science skin. Data visualization is a critical aspect of any machine learning project. But it has it¡¯s own standalone applications as well, like dashboards, reports, etc. Business intelligence is a thriving field these days and as more folks get into it, they need to be aware of some of the most common mistakes people make. The given image is a great illustration of this."
"vidhya",2018-07-30,"Comprehensive Hands on Guide to Twitter Sentiment Analysis with dataset and code","https://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/","Natural Language Processing (NLP) is a hotbed of research in data science these days and one of the most common applications of NLP is sentiment analysis. From opinion polls to creating entire marketing strategies, this domain has completely reshaped the way businesses work, which is why this is an area every data scientist must be familiar with. Thousands of text documents can be processed for sentiment (and other features including named entities, topics, themes, etc.) in seconds, compared to the hours it would take a team of people to manually complete the same task. In this article, we will learn how to solve the Twitter Sentiment Analysis Practice Problem. We will do so by following a sequence of steps needed to solve a general sentiment analysis problem. We will start with preprocessing and cleaning of the raw text of the tweets. Then we will explore the cleaned text and try to get some intuition about the context of the tweets. After that, we will extract numerical features from the data and finally use these feature sets to train models and identify the sentiments of the tweets. This is one of the most interesting challenges in NLP so I¡¯m very excited to take this journey with you! Let¡¯s go through the problem statement once as it is very crucial to understand the objective before working on the dataset. The problem statement is as follows: The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets. Formally, given a training sample of tweets and labels, where label ¡®1¡¯ denotes the tweet is racist/sexist and label ¡®0¡¯ denotes the tweet is not racist/sexist, your objective is to predict the labels on the given test dataset. Note: The evaluation metric from this practice problem is F1-Score. Personally, I quite like this task because hate speech, trolling and social media bullying have become serious issues these days and a system that is able to detect such texts would surely be of great use in making the internet and social media a better and bully-free place. Let¡¯s look at each step in detail now. Take a look at the pictures below depicting two scenarios of an office space <U+2013> one is untidy and the other is clean and organized.<U+00A0> You are searching for a document in this office space. In which scenario are you more likely to find the document easily? Of course, in the less cluttered one because each item is kept in its proper place. The data cleaning exercise is quite similar. If the data is arranged in a structured format then it becomes easier to find the right information. The preprocessing of the text data is an essential step as it makes the raw text ready for mining, i.e., it becomes easier to extract information from the text and apply machine learning algorithms to it. If we skip this step then there is a higher chance that you are working with noisy and inconsistent data. The objective of this step is to clean noise those are less relevant to find the sentiment of tweets such as punctuation, special characters, numbers, and terms which don¡¯t carry much weightage in context to the text.  In one of the later stages, we will be extracting numeric features from our Twitter text data. This feature space is created using all the unique words present in the entire data. So, if we preprocess our data well, then we would be able to get a better quality feature space. Let¡¯s first read our data and load the necessary libraries. You can download the datasets from here. Let¡¯s check the first few rows of the train dataset. The data has 3 columns id, label, and tweet. label is the binary target variable and tweet contains the tweets that we will clean and preprocess.  Initial data cleaning requirements that we can think of after looking at the top 5 records: As mentioned above, the tweets contain lots of twitter handles (@user), that is how a Twitter user acknowledged on Twitter. We will remove all these twitter handles from the data as they don¡¯t convey much information. For our convenience, let¡¯s first combine train and test set. This saves the trouble of performing the same steps twice on test and train. Given below is a user-defined function to remove unwanted text patterns from the tweets. It takes two arguments, one is the original string of text and the other is the pattern of text that we want to remove from the string. The function returns the same input string but without the given pattern. We will use this function to remove the pattern ¡®@user¡¯ from all the tweets in our data. Now let¡¯s create a new column tidy_tweet, it will contain the cleaned and processed tweets. Note that we have passed ¡°@[\w]*¡± as the pattern to the remove_pattern function. It is actually a regular expression which will pick any word starting with ¡®@¡¯. As discussed, punctuations, numbers and special characters do not help much. It is better to remove them from the text just as we removed the twitter handles. Here we will replace everything except characters and terms starting with # (twitter trends). We have to be a little careful here in selecting the length of the words which we want to remove. So, I have decided to remove all the words having length 3 or less. For example, terms like ¡°hmm¡±, ¡°oh¡± are of very little use. It is better to get rid of them. Let¡¯s take another look at the first few rows of the combined dataframe. You can see the difference between the raw tweets and the cleaned tweets (tidy_tweet) quite clearly. Only the important words in the tweets have been retained and the noise (numbers, punctuations, and special characters) has been removed. Now we will tokenize all the cleaned tweets in our dataset. Tokens are individual terms or words, and tokenization is the process of splitting a string of text into tokens. Stemming is a rule-based process of stripping the suffixes (¡°ing¡±, ¡°ly¡±, ¡°es¡±, ¡°s¡± etc) from a word. For example, For example <U+2013> ¡°play¡±, ¡°player¡±, ¡°played¡±, ¡°plays¡± and ¡°playing¡± are the different variations of the word <U+2013> ¡°play¡±. Now let¡¯s stitch these tokens back together. It can easily be done using nltk¡¯s<U+00A0>MosesDetokenizer function. In this section, we will explore the cleaned tweets text. Exploring and visualizing data, no matter whether its text or any other data, is an essential step in gaining insights. Here explore the hashtags and the most frequent words in the data. Do not limit yourself to only these methods, feel free to explore the data as much as possible. Before we begin exploration, we must think and ask questions related to the data in hand. A few probable questions are as follows: Now I want to see how well the given sentiments are distributed across the train dataset. One way to accomplish this task is by understanding the common words by plotting wordclouds. A wordcloud is a visualization wherein the most frequent words appear in large size and the less frequent words appear in smaller sizes. Let¡¯s visualize all the words our data using the wordcloud plot.  We can see most of the words are positive or neutral. With<U+00A0>happy<U+00A0>and<U+00A0>love<U+00A0>being the most frequent ones. It doesn¡¯t give us any idea about the words associated with the racist/sexist tweets. Hence, we will plot separate wordclouds for both the classes(racist/sexist or not) in our train data. We can see most of the words are positive or neutral. With happy, smile,<U+00A0>and love being the most frequent ones. Hence, most of the frequent words are compatible with the sentiment which is non racist/sexists tweets. Similarly, we will plot the word cloud for the other sentiment. Expect to see negative, racist, and sexist terms. As we can clearly see, most of the words have negative connotations. So, it seems we have a pretty good text data to work on. Next we will the hashtags/trends in our twitter data. Hashtags in twitter are synonymous with the ongoing trends on twitter at any particular point in time. We should try to check whether these hashtags add any value to our sentiment analysis task, i.e., they help in distinguishing tweets into the different sentiments.  For instance, given below is a tweet from our dataset:<U+00A0> The tweet seems sexist in nature and the hashtags in the tweet convey the same feeling. We will store all the trend terms in two separate lists <U+2014> one for non-racist/sexist tweets and the other for racist/sexist tweets. Now that we have prepared our lists of hashtags for both the sentiments, we can plot the top n hashtags. So, first let¡¯s check the hashtags in the non-racist/sexist tweets. Non-Racist/Sexist Tweets All these hashtags are positive and it makes sense. I am expecting negative terms in the plot of the second list. Let¡¯s check the most frequent hashtags appearing in the racist/sexist tweets. Racist/Sexist Tweets As expected, most of the terms are negative with a few neutral terms as well. So, it¡¯s not a bad idea to keep these hashtags in our data as they contain useful information.<U+00A0>Next, we will try to extract features from the tokenized tweets. To analyze a preprocessed data, it needs to be converted into features. Depending upon the usage, text features can be constructed using assorted techniques <U+2013> Bag-of-Words, TF-IDF, and Word Embeddings. In this article, we will be covering only Bag-of-Words and TF-IDF. Bag-of-Words is a method to represent text into numerical features. Consider a corpus (a collection of texts) called C of D documents {d1,d2¡¦..dD} and N unique tokens extracted out of the corpus C. The N tokens (words) will form a list, and the size of the bag-of-words matrix M will be given by D X N. Each row in the matrix M contains the frequency of tokens in document D(i). Let us understand this using a simple example. Suppose we have only 2 document D1: He is a lazy boy. She is also lazy. D2: Smith is a lazy person. The list created would consist of all the unique tokens in the corpus C.  = [¡®He¡¯,¡¯She¡¯,¡¯lazy¡¯,¡¯boy¡¯,¡¯Smith¡¯,¡¯person¡¯] Here, D=2, N=6 The matrix M of size 2 X 6 will be represented as <U+2013> Now the columns in the above matrix can be used as features to build a classification model. Bag-of-Words features can be easily created using sklearn¡¯s CountVectorizer function. We will set the parameter max_features = 1000 to select only top 1000 terms ordered by term frequency across the corpus. This is another method which is based on the frequency method but it is different to the bag-of-words approach in the sense that it takes into account, not just the occurrence of a word in a single document (or tweet) but in the entire corpus. TF-IDF works by penalizing the common words by assigning them lower weights while giving importance to words which are rare in the entire corpus but appear in good numbers in few documents. Let¡¯s have a look at the important terms related to TF-IDF: We are now done with all the pre-modeling stages required to get the data in the proper form and shape. Now we will be building predictive models on the dataset using the two feature set <U+2014> Bag-of-Words and TF-IDF. We will use logistic regression to build the models. It predicts the probability of occurrence of an event by fitting data to a logit function. The following equation is used in Logistic Regression: Read this article to know more about Logistic Regression.  Note: If you are interested in trying out other machine learning algorithms like RandomForest, Support Vector Machine, or XGBoost, there is a full-fledged course on Sentiment Analysis coming up shortly for you at https://trainings.analyticsvidhya.com. Output: 0.53 We trained the logistic regression model on the Bag-of-Words features and it gave us an F1-score of 0.53 for the validation set. Now we will use this model to predict for the test data. The public leaderboard F1 score is 0.567. Now we will again train a logistic regression model but this time on the TF-IDF features. Let¡¯s see how it performs. Output: 0.544 The validation score is 0.544 and the public leaderboard F1 score is 0.564. So, by using the TF-IDF features, the validation score has improved and the<U+00A0>public leaderboard score is more or less the same.  If you are interested to learn about more advanced techniques for Sentiment Analysis, we have a well laid out free course for you on the same problem releasing shortly at https://trainings.analyticsvidhya.com/. The course will have advanced techniques like word2vec model for feature extraction, more machine learning algorithms, model fine-tuning and much more. In the course, you will learn the following things: In this article, we learned how to approach a sentiment analysis problem. We started with preprocessing and exploration of data. Then we extracted features from the cleaned text using Bag-of-Words and TF-IDF. Finally, we were able to build a couple of models using both the feature sets to classify the tweets. Did you find this article useful? Do you have any useful trick? Did you use any other method for feature extraction? Feel free to discuss your experiences in comments below or on the discussion portal and we¡¯ll be more than happy to discuss. Hello"
