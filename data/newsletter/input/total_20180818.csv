"","site","date","headline","url_address","text","keyword"
"1","mastery",2018-08-17,"A Gentle Introduction to SARIMA for Time Series Forecasting in Python","https://machinelearningmastery.com/sarima-for-time-series-forecasting-in-python/","Although the method can handle data with a trend, it does not support time series with a seasonal component. An extension to ARIMA that supports the direct modeling of the seasonal component of the series is called SARIMA. In this tutorial, you will discover the Seasonal Autoregressive Integrated Moving Average, or SARIMA, method for time series forecasting with univariate data containing trends and seasonality. After completing this tutorial, you will know: Let¡¯s get started. A Gentle Introduction to SARIMA for Time Series Forecasting in PythonPhoto by Mario Micklisch, some rights reserved. This tutorial is divided into four parts; they are: Autoregressive Integrated Moving Average, or ARIMA, is a forecasting method for univariate time series data. As its name suggests, it supports both an autoregressive and moving average elements. The integrated element refers to differencing allowing the method to support time series data with a trend. A problem with ARIMA is that it does not support seasonal data. That is a time series with a repeating cycle. ARIMA expects data that is either not seasonal or has the seasonal component removed, e.g. seasonally adjusted via methods such as seasonal differencing. For more on ARIMA, see the post: An alternative is to use SARIMA. Seasonal Autoregressive Integrated Moving Average, SARIMA or Seasonal ARIMA, is an extension of ARIMA that explicitly supports univariate time series data with a seasonal component. It adds three new hyperparameters to specify the autoregression (AR), differencing (I) and moving average (MA) for the seasonal component of the series, as well as an additional parameter for the period of the seasonality. A seasonal ARIMA model is formed by including additional seasonal terms in the ARIMA [¡¦] The seasonal part of the model consists of terms that are very similar to the non-seasonal components of the model, but they involve backshifts of the seasonal period. <U+2014> Page 242, Forecasting: principles and practice, 2013. Configuring a SARIMA requires selecting hyperparameters for both the trend and seasonal elements of the series. There are three trend elements that require configuration. They are the same as the ARIMA model; specifically: There are four seasonal elements that are not part of ARIMA that must be configured; they are: Together, the notation for an SARIMA model is specified as: Where the specifically chosen hyperparameters for a model are specified; for example: Importantly, the m parameter influences the P, D, and Q parameters. For example, an m of 12 for monthly data suggests a yearly seasonal cycle. A P=1 would make use of the first seasonally offset observation in the model, e.g. t-(m*1) or t-12. A P=2, would use the last two seasonally offset observations t-(m * 1), t-(m * 2). Similarly, a D of 1 would calculate a first order seasonal difference and a Q=1 would use a first order errors in the model (e.g. moving average). A seasonal ARIMA model uses differencing at a lag equal to the number of seasons (s) to remove additive seasonal effects. As with lag 1 differencing to remove a trend, the lag s differencing introduces a moving average term. The seasonal ARIMA model includes autoregressive and moving average terms at lag s. <U+2014> Page 142, Introductory Time Series with R, 2009. The trend elements can be chosen through careful analysis of ACF and PACF plots looking at the correlations of recent time steps (e.g. 1, 2, 3). Similarly, ACF and PACF plots can be analyzed to specify values for the seasonal model by looking at correlation at seasonal lag time steps. For more on interpreting ACF/PACF plots, see the post: Seasonal ARIMA models can potentially have a large number of parameters and combinations of terms. Therefore, it is appropriate to try out a wide range of models when fitting to data and choose a best fitting model using an appropriate criterion ¡¦ <U+2014> Pages 143-144, Introductory Time Series with R, 2009. Alternately, a grid search can be used across the trend and seasonal hyperparameters. For more on grid searching ARIMA parameters, see the post: The SARIMA time series forecasting method is supported in Python via the Statsmodels library. To use SARIMA there are three steps, they are: Let¡¯s look at each step in turn. An instance of the SARIMAX class can be created by providing the training data and a host of model configuration parameters. The implementation is called SARIMAX instead of SARIMA because the ¡°X¡± addition to the method name means that the implementation also supports exogenous variables. These are parallel time series variates that are not modeled directly via AR, I, or MA processes, but are made available as a weighted input to the model. Exogenous variables are optional can be specified via the ¡°exog¡± argument. The trend and seasonal hyperparameters are specified as 3 and 4 element tuples respectively to the ¡°order¡± and ¡°seasonal_order¡± arguments. These elements must be specified. These are the main configuration elements. There are other fine tuning parameters you may want to configure. Learn more in the full API: Once the model is created, it can be fit on the training data. The model is fit by calling the fit() function. Fitting the model returns an instance of the SARIMAXResults class. This object contains the details of the fit, such as the data and coefficients, as well as functions that can be used to make use of the model. Many elements of the fitting process can be configured, and it is worth reading the API to review these options once you are comfortable with the implementation. Once fit, the model can be used to make a forecast. A forecast can be made by calling the forecast() or the predict() functions on the SARIMAXResults object returned from calling fit. The forecast() function takes a single parameter that specifies the number of out of sample time steps to forecast, or assumes a one step forecast if no arguments are provided. The predict() function requires a start and end date or index to be specified. Additionally, if exogenous variables were provided when defining the model, they too must be provided for the forecast period to the predict() function. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered the Seasonal Autoregressive Integrated Moving Average, or SARIMA, method for time series forecasting with univariate data containing trends and seasonality. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ...with just a few lines of python code Discover how in my new Ebook:Introduction to Time Series Forecasting With Python It covers self-study tutorials and end-to-end projects on topics like:Loading data, visualization, modeling, algorithm tuning, and much more... Skip the Academics. Just Results. Click to learn more. How to configure multiple seasons in SARIMA? Good question, you might need to develop a custom model instead. Whats the difference between SARIMA model and the X-12 ARIMA model? Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning. Read More","Keyword(freq): element(8), hyperparameter(5), parameter(5), step(4), term(4), plot(3), variable(3), argument(2), model(2), question(2)"
"2","mastery",2018-08-15,"15 Statistical Hypothesis Tests in Python (Cheat Sheet)","https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/","Although there are hundreds of statistical hypothesis tests that you could use, there is only a small subset that you may need to use in a machine learning project. In this post, you will discover a cheat sheet for the most popular statistical hypothesis tests for a machine learning project with examples using the Python API. Each statistical test is presented in a consistent way, including: Note, when it comes to assumptions such as the expected distribution of data or sample size, the results of a given test are likely to degrade gracefully rather than become immediately unusable if an assumption is violated. Generally, data samples need to be representative of the domain and large enough to expose their distribution to analysis. In some cases, the data can be corrected to meet the assumptions, such as correcting a nearly normal distribution to be normal by removing outliers, or using a correction to the degrees of freedom in a statistical test when samples have differing variance, to name two examples. Finally, there may be multiple tests for a given concern, e.g. normality. We cannot get crisp answers to questions with statistics; instead, we get probabilistic answers. As such, we can arrive at different answers to the same question by considering the question in different ways. Hence the need for multiple different tests for some questions we may have about data. Let¡¯s get started. Statistical Hypothesis Tests in Python Cheat SheetPhoto by davemichuda, some rights reserved. This tutorial is divided into four parts; they are: This section lists statistical tests that you can use to check if your data has a Gaussian distribution. Tests whether a data sample has a Gaussian distribution. Assumptions Interpretation Python Code More Information Tests whether a data sample has a Gaussian distribution. Assumptions Interpretation Python Code More Information Tests whether a data sample has a Gaussian distribution. Assumptions Interpretation More Information This section lists statistical tests that you can use to check if two samples are related. Tests whether two samples have a monotonic relationship. Assumptions Interpretation Python Code More Information Tests whether two samples have a monotonic relationship. Assumptions Interpretation Python Code More Information Tests whether two samples have a monotonic relationship. Assumptions Interpretation Python Code More Information Tests whether two categorical variables are related or independent. Assumptions Interpretation Python Code More Information This section lists statistical tests that you can use to compare data samples. Tests whether the means of two independent samples are significantly different. Assumptions Interpretation Python Code More Information Tests whether the means of two paired samples are significantly different. Assumptions Interpretation Python Code More Information Tests whether the means of two or more independent samples are significantly different. Assumptions Interpretation Python Code More Information Tests whether the means of two or more paired samples are significantly different. Assumptions Interpretation Python Code Currently not supported in Python. More Information Tests whether the distributions of two independent samples are equal or not. Assumptions Interpretation Python Code More Information Tests whether the distributions of two paired samples are equal or not. Assumptions Interpretation Python Code More Information Tests whether the distributions of two or more independent samples are equal or not. Assumptions Interpretation Python Code More Information Tests whether the distributions of two or more paired samples are equal or not. Assumptions Interpretation Python Code More Information This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered the key statistical hypothesis tests that you may need to use in a machine learning project. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. Did I miss an important statistical test or key assumption for one of the listed tests?
Let me know in the comments below. ¡¦by writing lines of code in python Discover how in my new Ebook:Statistical Methods for Machine Learning It provides self-study tutorials on topics like:Hypothesis Tests, Correlation, Nonparametric Stats, Resampling, and much more¡¦ Skip the Academics. Just Results. Click to learn more. hi, the list looks good.  a few omissions. fishers exact test and Bernards test (potentially more power than a fishers exact test) one note on the anderson darling test. the use of p values to determine GoF has been discouraged in some fields . Excellent note, thanks Jonathan. Indeed, I think it was a journal of psychology that has adopted ¡°estimation statistics¡± instead of hypothesis tests in reporting results. Very Very Good and Useful Article Thanks, I¡¯m happy to hear that. Hi, thanks for this nice overview.  Some of these tests, like friedmanchisquare, expect that the quantity of events is the group to remain the same over time. But in practice this is not allways the case. Lets say there are 4 observations on a group of 100 people, but the size of the response from this group changes over time with n1=100, n2=95, n3=98, n4=60 respondants.
n4 is smaller because some external factor like bad weather.
What would be your advice on how to tackle this different ¡®respondants¡¯ sizes over time? Good question. Perhaps check the literature for corrections to the degrees of freedom for this situation? Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning.","Keyword(freq): test(26), assumption(17), sample(15), distribution(4), mean(4), question(4), answer(3), list(3), result(3), thank(3)"
"3","mastery",2018-08-13,"How to Reduce Variance in a Final Machine Learning Model","https://machinelearningmastery.com/how-to-reduce-model-variance/","A final machine learning model is one trained on all available data and is then used to make predictions on new data. A problem with most final models is that they suffer variance in their predictions. This means that each time you fit a model, you get a slightly different set of parameters that in turn will make slightly different predictions. Sometimes more and sometimes less skillful than what you expected. This can be frustrating, especially when you are looking to deploy a model into an operational environment. In this post, you will discover how to think about model variance in a final model and techniques that you can use to reduce the variance in predictions from a final model. After reading this post, you will know: Let¡¯s get started. How to Reduce Variance in a Final Machine Learning ModelPhoto by Kirt Edblom, some rights reserved. Once you have discovered which model and model hyperparameters result in the best skill on your dataset, you¡¯re ready to prepare a final model. A final model is trained on all available data, e.g. the training and the test sets. It is the model that you will use to make predictions on new data were you do not know the outcome. The final model is the outcome of your applied machine learning project. To learn more about preparing a final model, see the post: The bias-variance trade-off is a conceptual idea in applied machine learning to help understand the sources of error in models. The bias-variance tradeoff is a conceptual tool to think about these sources of error and how they are always kept in balance. More bias in an algorithm means that there is less variance, and the reverse is also true. You can learn more about the bias-variance tradeoff in this post: You can control this balance. Many machine learning algorithms have hyperparameters that directly or indirectly allow you to control the bias-variance tradeoff. For example, the k in k-nearest neighbors is one example. A small k results in predictions with high variance and low bias. A large k results in predictions with a small variance and a large bias. Most final models have a problem: they suffer from variance. Each time a model is trained by an algorithm with high variance, you will get a slightly different result. The slightly different model in turn will make slightly different predictions, for better or worse. This is a problem with training a final model as we are required to use the model to make predictions on real data where we do not know the answer and we want those predictions to as good as possible. We want to the best possible version of the model that we can get. We want the variance to play out in our favor. If we can¡¯t achieve that, at least we want the variance to not fall against us when making predictions. There are two common sources of variance in a final model: The first type we introduced above. The second type impacts those algorithms that harness randomness during learning. Three common examples include: You can measure both types of variance in your specific model using your training data. Often, the combined variance is estimated by running repeated k-fold cross-validation on a training dataset then calculating the variance or standard deviation of the model skill. If we want to reduce the amount of variance in a prediction, we must add bias. Consider the case of a simple statistical estimate of a population parameter, such as estimating the mean from a small random sample of data. A single estimate of the mean will have high variance and low bias. This is intuitive because if we repeated this process 30 times and calculated the standard deviation of the estimated mean values, we would see a large spread. The solutions for reducing the variance are also intuitive. Repeat the estimate on many different small samples of data from the domain and calculate the mean of the estimates, leaning on the central limit theorem. The mean of the estimated means will have a lower variance. We have increased the bias by assuming that the average of the estimates will be a more accurate estimate than a single estimate. Another approach would be to dramatically increase the size of the data sample on which we estimate the population mean, leaning on the law of large numbers. The principles used to reduce the variance for a population statistic can also be used to reduce the variance of a final model. We must add bias. Depending on the specific form of the final model (e.g. tree, weights, etc.) you can get creative with this idea. Below are three approaches that you may want to try. If possible, I recommend designing a test harness to experiment and discover an approach that works best or makes the most sense for your specific data set and machine learning algorithm. Instead of fitting a single final model, you can fit multiple final models. Together, the group of final models may be used as an ensemble. For a given input, each model in the ensemble makes a prediction and the final output prediction is taken as the average of the predictions of the models. A sensitivity analysis can be used to measure the impact of ensemble size on prediction variance. As above, multiple final models can be created instead of a single final model. Instead of calculating the mean of the predictions from the final models, a single final model can be constructed as an ensemble of the parameters of the group of final models. This would only make sense in cases where each model has the same number of parameters, such as neural network weights or regression coefficients. For example, consider a linear regression model with three coefficients [b0, b1, b2]. We could fit a group of linear regression models and calculate a final b0 as the average of b0 parameters in each model, and repeat this process for b1 and b2. Again, a sensitivity analysis can be used to measure the impact of ensemble size on prediction variance. Leaning on the law of large numbers, perhaps the simplest approach to reduce the model variance is to fit the model on more training data. In those cases where more data is not readily available, perhaps data augmentation methods can be used instead. A sensitivity analysis of training dataset size to prediction variance is recommended to find the point of diminishing returns. There are approaches to preparing a final model that aim to get the variance in the final model to work for you rather than against you. The commonality in these approaches is that they seek a single best final model. Two examples include: I would argue that these approaches and others like them are fragile. Perhaps you can gamble and aim for the variance to play-out in your favor. This might be a good approach for machine learning competitions where there is no real downside to losing the gamble. I won¡¯t. I think it¡¯s safer to limit the aim for the best average performance and limit the downside. I think that the trick with navigating the bias-variance tradeoff for a final model is to think in samples, not in terms of single models. To optimize for average model performance. This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered how to think about model variance in a final model and techniques that you can use to reduce the variance in predictions from a final model. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. Hi Jason, In the paragraph: Why not use early stopping? You could check the skill of the model against a holdout set during training and stop training when the skill of the model on the hold set starts to degrade. Do you mean:
¡®¡¦ against a validation set ¡¦when the skill of the model on the validation set starts to degrade.¡¯ ? To the best of my knowledge, the hold out set should be kept untouched for final evaluation. Best,
Elie Yes, one and the same. Love these Lessons and examples.
Wish I could find an ¡°Instructor led Course¡± in the USA (?) I¡¯m sure there are thousands of such courses. Do Bayesian ML models have less variance ? Than what? Than the regular ML models which use point estimates for parameters (weights). A great article again. Thank you so much. Would you like to give us an easy example to explain your whole ideas explicitly? Because we need to make several decisions during training the final model and making predictions in the real world. I want to learn how you make decisions when you do a real project. I strongly agree with your opinion on ¡°a final model is to think in samples, not in terms of single models¡± A single best model is too risky for the real problems. It really depends on the problem. I try to provide frameworks to help you make these decisions on your specific dataset. I read your blog for the first time and I guess I became a fan of you.
In the section ¡°Ensemble Parameters from Final Models¡± when using neural networks, how can I use this approach. Do I need to train multiple neural networks having same size and average their weight values?
Will it improve the performance in terms of generalization? Thanks in advance. Correct. It will improve the stability of the forecast via an increase to the bias. It may or may not positively impact generalization <U+2013> my feeling is that it is orthogonal. Is ¡°I think it¡¯s safer to limit the aim for the best average performance and limit the downside.¡± meant to say  ¡°I think it¡¯s safer to aim for the best average performance and limit the downside.¡± Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning. Read More","Keyword(freq): model(15), prediction(15), parameter(6), approach(4), decision(3), estimate(3), example(3), sample(3), source(3), term(3)"
"4","vidhya",2018-08-16,"Top 7 Sectors where Data Science can Transform India (with Free Datasets)","https://www.analyticsvidhya.com/blog/2018/08/top-7-sectors-where-data-science-can-transform-india-with-free-datasets/","

DATA DAYS DISCOUNT: Get FLAT 33% OFF on All Courses | Use COUPON CODE: DATA33 |
OFFER ENDS: 26th Aug 23:59 hrs IST | 
Valid for first 33 users(daily) | Buy Now 

 Data science is a powerful tool and has the ability to transform the world. We are already seeing massive changes to the way industries work in the Western world and data science is powering that change. But when it comes to India, we are still a long, long way from reaching that stage. As Dr. Avik mentioned in his insightful DataHack Radio podcast, most of the data we collect is highly unstructured and difficult to make sense of. But optimism abounds <U+2013> there is a growing expectation that people are realizing how crucial data is to the economy. In this article, we look at the top sectors in this wonderful nation of ours that are ripe for applying data science. We have also provided resource links for each sector with the hope that our AV community can take up the challenge and make this country a better place, all with the help of data science! The agriculture industry needs the use of data science more than any other right now. 40% of our population is employed in this field but unfortunately, agriculture¡¯s contribution to the nation¡¯s economy is a paltry 16% of the overall GPD. Given how critical this sector is, should that number not be significantly higher? There are a lot of facets in agriculture that can be worked upon <U+2013> predicting monthly/quarterly/annual yield, forecasting demand, analyzing weather patterns to decide when to sow, predicting the prices of vegetables so as to pick which crop to sow, etc. Open datasets on agriculture: There are three datasets on this page <U+2013> two are monthly and one is annual. These deal with the stock of different food grains in a year, the production of these food grains, and the central statistics of food and beverages. Dataset on the crop production in India. It¡¯s a fairly straightforward dataset but excellent for producing visualizations and basic insights. Rainfall in India dataset. Another crucial aspect to agriculture, and one that decides the livelihood of farmers. Predicting rainfall is essential to farming, and with this dataset, you can do just that! It contains monthly rainfall data from 1901-2015. This article, written by Shweta Gupta, is an excellent resource on the state of the agriculture industry in India, challenges that we face right now, and how we can use data to improve it. It should be a mandatory read for all Indians who are into data science, it¡¯s just that important! The average electricity use in India during the 2016-17 FY was a staggering 1,122 kWh per capita. Out of this, the industrial consumption was 40%, followed by residential consumption at 24%. This is all to say that the power demand is surging beyond expectations as the population increases year-on-year. Predicting power supply and demand, understanding the consumption pattern of households, classifying this by region/district/blocks, etc. are just some of the ways we can use data science in this sector. The resources I have mentioned below are enough to get you started and even go beyond that. A collection of open datasets by the Government. Datasets on pattern of electricity consumption, per capita consumption, consumption by sectors, and more are available here. Check it out! This Wikipedia article has up-to-date statistics on the electricity sector in India. This should be a compulsory read for all Indians, from students to working professionals. It is an eye opener to the fact that we are consuming power at a never-before-seen rate. It also contains granular details about rural areas. If you know even the basics of web scraping, this page is a goldmine. Dataset on individual household electric power consumption. While this isn¡¯t strictly Indian data, it sheds light on the kind of data we do need to collect in the first place. I encourage you to download this dataset, play around with it and come up with solutions as to how we, as a community, can utilize and maximize power consumption to our benefit. The most critical resource of all, and one of the most misused in India. It seems we see a drought every summer in quite a lot of rural areas, and the situation does not seem to be improving. The water usage is increasing each year and unless we properly assess the usage, it could end up turning into a crisis very soon. You can predict things like the predicted water level, the usage in certain areas in order to send adequate water supply tanks there in time, etc. You can come up with more ideas as you think about the challenges in this sector. Open datasets from the Central Ground Water Board. This contains granular information about water levels in every district in India. The variables it includes are the district name, latitude and longitude, type of site, the year the data was observed, and the water level during, after and before monsoons. It¡¯s a good place for you to understand the kind of data collected by the Government, and even work on it on your own! Open datasets on water quality in 2014. Plenty of datasets to download and work with here. It is available for different states so pick and choose as per your interest. Did you know that the Indian constitution guarantees free healthcare for all citizens? And that¡¯s the practice Government hospitals follow, at least for those who are below the poverty line. But the truth is that the private healthcare sector takes care of the majority of the healthcare business in the country. With the amount of people populating government hospital, it is not easy to get proper attention there. Which is why people who can afford it tend to turn to the private hospitals. They prefer paying from their own pockets than putting themselves through the rigors of a government hospital. According to Wikipedia, 58% of the hospitals in India are private along with a mind-boggling 81% doctors. The current infrastructure is just not good enough to handle the growing demands and the surging population. This is where data science can step in and ease the burden. Predicting things like how many days will a patient be admitted so as to calculate the proper allotment of beds, child mortality rate, heart issues, diabetes, etc. are some of the points you can work with for starters. The NITI Aayog initiative is already working on quite a lot of these points. Dataset on key indicators of annual health survey. These are survey results for nine Indian states from 2012-13. It is a very comprehensive dataset and contains 1,287 columns. If you are serious about analyzing and working with Indian healthcare data, this is as good a place as any to start. Multiple datasets on the government¡¯s data site. If you wish to analyze the state of healthcare at a more granular level, check out this link. It contains all sorts of information about the various aspects of healthcare, from OPD attendance to the comparison of various health indicators around the country. Datasets curated by the World health Organization. This is a treasure trove of data on healthcare in India, collected by WHO. It contains datasets on infant mortality rate, life expectancy at birth, hospital beds, etc. The state of education in India is appalling, to say the least. While more Indians are enrolled in schools than ever before, they are not really being educated. Outside the cream of the crop private schools, there is no proper structure, focus or attention given to the majority of children in rural areas. Almost 95% children have enrolled in primary school, 69% in secondary and a shockingly meager 25% in post secondary. Where is it all going wrong? Why can¡¯t one of the biggest school systems in the world improve upon this? What is the expected years of schooling education? Using data from national surveys, you can analyze and try to find answers to these pressing questions. As with any data science project, curiosity will help you a lot. This is a field that¡¯s very close to me so any progress, however minor, has the potential to start a ripple effect. Comprehensive district-level dataset. This is a really detailed dataset covering the length and breadth of report card information categorized by district. It contains 439 columns with zero milling values. What a great place to begin! Open datasets from the government. These are not so neat and tidy. You will require a bit of preprocessing and research to work with these properly, but they highlight the true nature of education here, including data on teaching staff and education loans. Ah, one of the most frustrating things we encounter on an almost daily basis. As more and more people flock to metro cities, the state of traffic on the roads is getting worse. Long traffic jams are an accepted part of our lives, but should they be? The NITI Aayog team is working on understanding why this happens, and how to deal with them. Aspects like choke points, narrow or broken roads, lack of traffic personnel, and failure of traffic lights, are just some of the features you can look at when trying to solve this problem. Cities like Kuala Lumpur and Toronto are already being converted into smart cities, with CCTV cameras and sensors everywhere to monitor traffic and imediaetely solve the problem. India is a fair way off that, though we saw earlier this year how the Kolkata police is trying to use Google Maps with the aim of dealing with long jams. Another aspect of road transport is the number of accidents on the road. India records some of the world¡¯s largest road fatalities every year. According to an Economic Times article, more than 150,000 people are killed in these accidents every year! This is a terribly distressing number and I hope data science can be used to analyze patterns and take immediate action on this. Datasets on road accidents. These are quite a few in number and cover features like accidents due to intake of alcohol/drugs, overspeeding, over crowding, over loading of trucks, etc. Accidents in India by month (2001-2014) dataset. You will need to carefully import this data but it¡¯s a good starting point for analyzing and extracting any patterns, if you can. Traffic Data. Unfortunately there¡¯s no single resource that contains the traffic data for India. Thankfully there are simple ways to get it. You can head over to Google Maps and export the data in a jiffy. Check out this article which explains how to do it. Once you download it, you can get to analyzing where the traffic jams regularly occur, at what time that happens, and can come up with ways to mitigate it. The possibilities are vast and making your city a smart one is now in your hands! Anyone with access to news will be aware how bad the air pollution levels are in certain parts on India. It is beyond the ¡°out of hand¡± stage. Despite taking precautions and trying out different measures, the pollution level has not really come down to a manageable state. According to a WHO report from 2016, 11 of the top 12 most polluted cities come from India (Kanpur leads the way). The Environment Performance Index ranked India 141 out of 180 nations. All this is to say that the problem is grave, and we need a permanent solution to this in double-quick time. Variables like crop burning, pollution from vehicles, industry fuel and biomass burning, etc. are major contributors to the alarming rise in air pollution. While there have been recent studies done using data science on the topic, none have so far been able to bring down the numbers. Air Quality Data for India. This contains historical data on India¡¯s air pollution levels and has spawned many a projects. It¡¯s a brilliant starting point for anyone looking to work with this kind of data. Daily Ambient Air Quality Data. Coming from the Government itself, this is a location wise dataset measuring the air quality in 2015. You can also check out their entire catalog on air pollution here if you so wish. It¡¯s a little unstructured so patience is key! Air Quality Info Site. A really cool website displaying the different statistics associated with air quality indices in India. It has forecasts for daily, monthly, and hourly numbers. Bookmark this site! The resources I have mentioned here are enough to get you started in each sector. There are other datasets and resources out there which you can get your hands on to practice more. There are government sites where you can request more data, if required. There is so much scope for improving each of these sectors with the help of data science. I¡¯m looking forward to our community making a huge impact (in a positive way of course!) soon! If there are any other datasets you are aware of and want to share with the community, please feel free to do so in the comments section below. helpful Hey guys. Tried to access data at the following link <U+2013> Dataset on individual household electric power consumption. (https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption). Received the following error <U+2013> You don¡¯t have permission to access /ml/datasets/individual+household+electric+power+consumption on this server. Just thought i¡¯d mention, for it could be Browser related or maybe a genuine error in itself. This is my way of saying, thanks for the great job you all do with the articles and knowledge sharing. It¡¯s very much appreciated. Hi Michael, Thanks for pointing this out. It seems the Machine Learning UCI repository site is down and hence you¡¯re unable to access the link. It should be back up and functioning soon.","Keyword(freq): dataset(14), accident(5), hospital(3), indian(3), jam(3), level(3), pattern(3), point(3), resource(3), sector(3)"
"5","vidhya",2018-08-14,"DataHack Radio Episode #7: Tackling Data Science Challenges in India with NITI Aayog¡¯s Dr. Avik Sarkar (Independence Day Special!)","https://www.analyticsvidhya.com/blog/2018/08/datahack-radio-episode-7-dr-avik-sarkar/","

DATA DAYS DISCOUNT: Get FLAT 33% OFF on All Courses | Use COUPON CODE: DATA33 |
OFFER ENDS: 26th Aug 23:59 hrs IST | 
Valid for first 33 users(daily) | Buy Now 

 Data science is still a very nascent field in India, despite the recent surge in interest. From agriculture to healthcare, there are a plethora of challenges the Government faces on a day-to-day basis, and that was the primary reason for founding a data science department under the NITI Aayog initiative. On this Independence Day, we thought what better way to acquaint our community with these challenges and how our Goverment is using data science to tackle them, than bring NITI Aayog¡¯s Head of Data Science straight to you? It was a thrilling experience to have one of India¡¯s foremost data science leaders, Dr. Avik Sarkar, on our DataHack Radio podcast. He is an eloquent speaker and he talked about various topics, from his love of mathematics to his master¡¯s and Ph.D thesis techniques. He also provided details about the work performed by the data science team under the NITI Aayog initiative, a must-listen for all Indians. In this article, we look at the top key points Dr. Avik made during his conversation with Kunal. Happy listening! You can subscribe to DataHack Radio and listen to this, and all previous episodes, on any of the below platforms: Dr. Avik¡¯s penchant for numbers can be traced back to his childhood. He was interested in mathematics since his school days and that led him to do his bachelor¡¯s in statistics and master¡¯s (from IIT-Bombay) in applied statistics and informatics. He also holds a Ph.D in computer science and statistics. As you can surmise from this, he was the perfect candidate for data science! Before he joined NITI Aayog as the Head of the Data Science cell, Dr. Avik worked in senior roles at companies like Accenture, IBM and Nokia Siemens, among others. A trend emerges when you look at his profile <U+2013> he worked with data well before data science become a buzzword, and thus has a very strong background in this domain. When Dr. Avik was learning and working on Artificial Intelligence, it was a different experience to how we see AI these days. This is what he had to say about how quickly the world of data science, machine learning and AI is advancing: ¡°In this domain, learning new things is something you have to do every year. It¡¯s a rapidly evolving field <U+2013> new technologies, new platforms and new coding languages come every year, so getting acquainted with these is very important.¡± The subject of Dr. Avik¡¯s master¡¯s thesis was around multi-topic text classification. He took this up because it was an important topic due to the hierarchical information arrangement that was prevalent at that time (early 2000s). The main aim of the hierarchy was to arrange whatever text data you had into categories <U+2013> it could be news articles, blogs, etc. The internet was getting democratized as more and more Indians (and global users) started getting online in the late 90s/early 2000s. So, suddenly we went from seeing a few editors putting content online to a plethora of writers gaining access to the internet. The amount of content spiked, nothing close to what we see now, but enough to ensure that one could not manually categorize the articles into a hierarchy. Dr. Avik saw a need for an automatic classification system that would identify these topics and put them into a hierarchy model. The more challenging problem, which he took up, was that some articles might be relevant for multiple topics. His Ph.D was in text mining and statistical modeling on text distribution. If you are interested in NLP, do listen to this section where Dr. Avik explains why and how he took up this topic.<U+00A0>He discusses the nuances of various techniques he used and how they helped him build up his study. It makes for fascinating listening! ¡°We are trying to make sense of the operational data to get a good picture about the state of the economy.¡± The data science team at NITI Aayog, as Dr. Avik put it, is more of a horizontal organization. The type of analytics he and his team perform are vast in nature. Even though he had over fifteen years of experience working with data prior to joining the Government, this was an almost new body of work for him. There is a lot of simulation and scenario modeling that they need to perform. He gave some really intuitive examples of how the team thinks about certain industries (like oil and automobiles), and the variables to consider when forecasting production and manufacturing. This qualifies as long term forecasting. The team also uses analytics for short-term challenges as well, which are operational in nature. For example, malnutrition is a major problem in India (and has been for decades). They extract insights about which districts need more funds to deal with the issue and this has helped the people on the ground. There are other aspects where data science is helping the government tackle long standing challenges. Taking the example of surveys, Dr. Avik explained how there is a lag of 2-3 years between initiating surveys and finally extracting meaningful insights from them. His current team at NITI Aayog is trying to do more of a real-time analysis of these things, especially critical fields like healthcare, education and agriculture. ¡°80% of my day goes in phone calls!¡± Data collection, as Kunal pointed out, would be a major obstacle for Dr. Avik¡¯s team. All of the things they are doing are fairly new from an Indian perspective and nothing so far has been done in a systematic or structured manner. As the above quote summarized, he spends most of his day trying to convince people to share their data. Often there are data quality issues. Since most of the data is operational, people assume it might not be used anywhere and hence it¡¯s stored in a very unfocused manner. A lot of the fields need to be dropped because of the serious gaps in data quality. The hope is that with time, as Dr. Avik continues his work, departments will soon realize the need to properly store this data. A lack of data also inevitably leads to biases in the model you build. Unfortunately this is a problem India faces in almost all sectors. Mitigating these issues has become a big challenge as well and Dr. Avik pointed out this is the biggest obstacle he had to deal with. For energy modeling, a long term initiative (takes up to 1-2 years), ¡®Message Models¡¯ and ¡®Times Markel Model¡¯ are the team¡¯s tools of choice. For generating visualizations and dashboards to be shared with state governments, the team uses popular tools like: Different countries have their unique challenges when it comes to adopting AI. For India, Dr. Avik believes it¡¯s the obstacle of inclusion, or ¡°AI for all¡±. This is what his team is piloting throughout the country. Taking the example of healthcare, he explained how automating certain parts of a nurse and/or doctor¡¯s job will help cut down on the time it takes to make a diagnosis, as well as spread the benefits of healthcare to rural places. Intriguing is the only word I can think of to describe the task Dr. Avik and his team are dealing with.","Keyword(freq): challenge(5), article(3), avik(3), master(3), statistics(3), topic(3), analytics(2), india(2), indian(2), insight(2)"
"6","vidhya",2018-08-13,"Complete tutorial on Text Classification using Conditional Random Fields Model (in Python)","https://www.analyticsvidhya.com/blog/2018/08/nlp-guide-conditional-random-fields-text-classification/","

DATA DAYS DISCOUNT: Get FLAT 33% OFF on All Courses | Use COUPON CODE: DATA33 |
OFFER ENDS: 26th Aug 23:59 hrs IST | 
Valid for first 33 users(daily) | Buy Now 

 The amount of text data being generated in the world is staggering. Google processes more than 40,000 searches EVERY second!<U+00A0> According to a Forbes report, every single minute we send 16 million text messages and post 510,00 comments on Facebook. For a layman, it is difficult to even grasp the sheer magnitude of data out there? News sites and other online media alone generate tons of text content on an hourly basis. Analyzing patterns in that data can become daunting if you don¡¯t have the right tools. Here we will discuss one such approach, using entity recognition, called Conditional Random Fields (CRF). This article explains the concept and python implementation of conditional random fields on a self-annotated dataset. This is a really fun concept and I¡¯m sure you¡¯ll enjoy taking this ride with me! Entity recognition has seen a recent surge in adoption with the interest in Natural Language Processing (NLP). An entity can generally be defined as a part of text that is of interest to the data scientist or the business. Examples of frequently extracted entities are names of people, address, account numbers, locations etc. These are only simple examples and one could come up with one¡¯s own entity for the problem at hand. To take a simple application of entity recognition, if there¡¯s any text with ¡°London¡± in the dataset, the algorithm would automatically categorize or classify that as a location (you must be getting a general idea of where I¡¯m going with this). Let¡¯s take a simple case study to understand our topic in a better way. Suppose that you are part of an analytics team in an insurance company where each day, the claims team receives thousands of emails from customers regarding their claims. The claims operations team goes through each email and updates an online form with the details before acting on them. Source: mugo.ca You are asked to work with the IT team to automate the process of pre-populating the online form. For this task, the analytics team needs to build a custom entity recognition algorithm. To identify entities in text, one must be able to identify the pattern. For example, if we need to identify the claim number, we can look at the words around it such as ¡°my id is¡± or ¡°my number is¡±, etc. Let us examine a few approaches mentioned below for identifying the patterns. The bag of words (BoW) approach works well for multiple text classification problems. This approach assumes that presence or absence of word(s) matter more than the sequence of the words. However, there are problems such as entity recognition, part of speech identification where word sequences matter as much, if not more. Conditional Random Fields (CRF) comes to the rescue here as it uses word sequences as opposed to just words.  Let us now understand how CRF is formulated. Below is the formula for CRF where Y is the hidden state (for example, part of speech) and X is the observed variable (in our example this is the entity or other words around it).  Broadly speaking, there are 2 components to the CRF formula:  Now that you are aware of the CRF model, let us curate the training data. The first step to doing this is annotation.<U+00A0> Annotation is a process of tagging the word(s) with the corresponding tag. For simplicity, let us suppose that we only need 2 entities to populate the online form, namely the claimant name and the claim number. The following is a sample email received as is. Such emails need to be annotated so that the CRF model can be trained. The annotated text needs to be in an XML format. Although you may choose to annotate the documents in your way, I¡¯ll walk you through the use of the GATE architecture to do the same.  Email received: ¡°Hi,  I am writing this email to claim my insurance amount. My id is abc123 and I claimed it on 1st January 2018. I did not receive any acknowledgement. Please help.  Thanks, randomperson¡± Annotated Email: ¡°<document>Hi, I am writing this email to claim my insurance amount. My id is <claim_number>abc123</claim_number> and I claimed on 1st January 2018. I did not receive any acknowledgement. Please help. Thanks, <claimant>randomperson</claimant></document>¡± Let us understand how to use the General Architecture for Text Engineering (GATE). Please follow the below steps to install GATE. Once the installation is complete, you are ready to train and build your own CRF module. Let¡±s do this! Let¡¯s define and build a few functions. Now we will import the annotated training data. Generate features. These are the default features that NER algorithm uses in nltk. One can modify it for customization. Now we¡¯ll build features and create train and test data frames. Let¡¯s test our model. You can inspect any predicted value by selecting the corresponding row number ¡°i¡±. Check the performance of the model. Print out the classification report. Based on the model performance, build better features to improve the performance. By now, you would have understood how to annotate training data, how to use Python to train a CRF model, and finally how to identify entities from new text. Although this algorithm provides some basic set of features, you can come up with your own set of features to improve the accuracy of the model. To summarize, here are the key points that we have covered in this article:","Keyword(freq): feature(6), entity(4), claim(3), analytics(2), email(2), example(2), pattern(2), problem(2), sequence(2), thank(2)"
