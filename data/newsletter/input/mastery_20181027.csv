"site","date","headline","url_address","text"
"mastery",2018-10-26,"How to Grid Search Naive Methods for Univariate Time Series Forecasting","https://machinelearningmastery.com/how-to-grid-search-naive-methods-for-univariate-time-series-forecasting/","Simple forecasting methods include naively using the last observation as the prediction or an average of prior observations. It is important to evaluate the performance of simple forecasting methods on univariate time series forecasting problems before using more sophisticated methods as their performance provides a lower-bound and point of comparison that can be used to determine of a model has skill or not for a given problem. Although simple, methods such as the naive and average forecast strategies can be tuned to a specific problem in terms of the choice of which prior observation to persist or how many prior observations to average. Often, tuning the hyperparameters of these simple strategies can provide a more robust and defensible lower bound on model performance, as well as surprising results that may inform the choice and configuration of more sophisticated methods. In this tutorial, you will discover how to develop a framework from scratch for grid searching simple naive and averaging strategies for time series forecasting with univariate data. After completing this tutorial, you will know: Let¡¯s get started. How to Grid Search Naive Methods for Univariate Time Series ForecastingPhoto by Rob and Stephanie Levy, some rights reserved. This tutorial is divided into six parts; they are: It is important and useful to test simple forecast strategies prior to testing more complex models. Simple forecast strategies are those that assume little or nothing about the nature of the forecast problem and are fast to implement and calculate. The results can be used as a baseline in performance and used as a point of a comparison. If a model can perform better than the performance of a simple forecast strategy, then it can be said to be skillful. There are two main themes to simple forecast strategies; they are: Let¡¯s take a closer look at both of these strategies. A naive forecast involves using the previous observation directly as the forecast without any change. It is often called the persistence forecast as the prior observation is persisted. This simple approach can be adjusted slightly for seasonal data. In this case, the observation at the same time in the previous cycle may be persisted instead. This can be further generalized to testing each possible offset into the historical data that could be used to persist a value for a forecast. For example, given the series: We could persist the last observation (relative index -1) as the value 9 or persist the second last prior observation (relative index -2) as 8, and so on. One step above the naive forecast is the strategy of averaging prior values. All prior observations are collected and averaged, either using the mean or the median, with no other treatment to the data. In some cases, we may want to shorten the history used in the average calculation to the last few observations. We can generalize this to the case of testing each possible set of n-prior observations to be included into the average calculation. For example, given the series: We could average the last one observation (9), the last two observations (8, 9), and so on. In the case of seasonal data, we may want to average the last n-prior observations at the same time in the cycle as the time that is being forecasted. For example, given the series with a 3-step cycle: We could use a window size of 3 and average the last one observation (-3 or 1), the last two observations (-3 or 1, and -(3 * 2) or 1), and so on. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this section, we will develop a framework for grid searching the two simple forecast strategies described in the previous section, namely the naive and average strategies. We can start off by implementing a naive forecast strategy. For a given dataset of historical observations, we can persist any value in that history, that is from the previous observation at index -1 to the first observation in the history at -(len(data)). The naive_forecast() function below implements the naive forecast strategy for a given offset from 1 to the length of the dataset. We can test this function out on a small contrived dataset. Running the example first prints the contrived dataset, then the naive forecast for each offset in the historical dataset. We can now look at developing a function for the average forecast strategy. Averaging the last n observations is straight-forward; for example: We may also want to test out the median in those cases where the distribution of observations is non-Gaussian. The average_forecast() function below implements this taking the historical data and a config array or tuple that specifies the number of prior values to average as an integer, and a string that describe the way to calculate the average (¡®mean¡® or ¡®median¡®). The complete example on a small contrived dataset is listed below. Running the example forecasts the next value in the series as the mean value from contiguous subsets of prior observations from -1 to -10, inclusively. We can update the function to support averaging over seasonal data, respecting the seasonal offset. An offset argument can be added to the function that when not set to 1 will determine the number of prior observations backwards to count before collecting values from which to include in the average. For example, if n=1 and offset=3, then the average is calculated from the single value at n*offset or 1*3 = -3. If n=2 and offset=3, then the average is calculated from the values at 1*3 or -3 and 2*3 or -6. We can also add some protection to raise an exception when a seasonal configuration (n * offset) extends beyond the end of the historical observations. The updated function is listed below. We can test out this function on a small contrived dataset with a seasonal cycle. The complete example is listed below. Running the example calculates the mean values of [10], [10, 10] and [10, 10, 10]. It is possible to combine both the naive and the average forecast strategies together into the same function. There is a little overlap between the methods, specifically the n-offset into the history that is used to either persist values or determine the number of values to average. It is helpful to have both strategies supported by one function so that we can test a suite of configurations for both strategies at once as part of a broader grid search of simple models. The simple_forecast() function below combines both strategies into a single function. Next, we need to build up some functions for fitting and evaluating a model repeatedly via walk-forward validation, including splitting a dataset into train and test sets and evaluating one-step forecasts. We can split a list or NumPy array of data using a slice given a specified size of the split, e.g. the number of time steps to use from the data in the test set. The train_test_split() function below implements this for a provided dataset and a specified number of time steps to use in the test set. After forecasts have been made for each step in the test dataset, they need to be compared to the test set in order to calculate an error score. There are many popular error scores for time series forecasting. In this case, we will use root mean squared error (RMSE), but you can change this to your preferred measure, e.g. MAPE, MAE, etc. The measure_rmse() function below will calculate the RMSE given a list of actual (the test set) and predicted values. We can now implement the walk-forward validation scheme. This is a standard approach to evaluating a time series forecasting model that respects the temporal ordering of observations. First, a provided univariate time series dataset is split into train and test sets using the train_test_split() function. Then the number of observations in the test set are enumerated. For each we fit a model on all of the history and make a one step forecast. The true observation for the time step is then added to the history, and the process is repeated. The simple_forecast() function is called in order to fit a model and make a prediction. Finally, an error score is calculated by comparing all one-step forecasts to the actual test set by calling the measure_rmse() function. The walk_forward_validation() function below implements this, taking a univariate time series, a number of time steps to use in the test set, and an array of model configuration. If you are interested in making multi-step predictions, you can change the call to predict() in the simple_forecast() function and also change the calculation of error in the measure_rmse() function. We can call walk_forward_validation() repeatedly with different lists of model configurations. One possible issue is that some combinations of model configurations may not be called for the model and will throw an exception. We can trap exceptions and ignore warnings during the grid search by wrapping all calls to walk_forward_validation() with a try-except and a block to ignore warnings. We can also add debugging support to disable these protections in the case we want to see what is really going on. Finally, if an error does occur, we can return a None result; otherwise, we can print some information about the skill of each model evaluated. This is helpful when a large number of models are evaluated. The score_model() function below implements this and returns a tuple of (key and result), where the key is a string version of the tested model configuration. Next, we need a loop to test a list of different model configurations. This is the main function that drives the grid search process and will call the score_model() function for each model configuration. We can dramatically speed up the grid search process by evaluating model configurations in parallel. One way to do that is to use the Joblib library. We can define a Parallel object with the number of cores to use and set it to the number of scores detected in your hardware. We can then create a list of tasks to execute in parallel, which will be one call to the score_model() function for each model configuration we have. Finally, we can use the Parallel object to execute the list of tasks in parallel. That¡¯s it. We can also provide a non-parallel version of evaluating all model configurations in case we want to debug something. The result of evaluating a list of configurations will be a list of tuples, each with a name that summarizes a specific model configuration and the error of the model evaluated with that configuration as either the RMSE or None if there was an error. We can filter out all scores set to<U+00A0>None. We can then sort all tuples in the list by the score in ascending order (best are first), then return this list of scores for review. The grid_search() function below implements this behavior given a univariate time series dataset, a list of model configurations (list of lists), and the number of time steps to use in the test set. An optional parallel argument allows the evaluation of models across all cores to be tuned on or off, and is on by default. We¡¯re nearly done. The only thing left to do is to define a list of model configurations to try for a dataset. We can define this generically. The only parameter we may want to specify is the periodicity of the seasonal component in the series (offset), if one exists. By default, we will assume no seasonal component. The simple_configs() function below will create a list of model configurations to evaluate. The function only requires the maximum length of the historical data as an argument and optionally the periodicity of any seasonal component, which is defaulted to 1 (no seasonal component). We now have a framework for grid searching simple model hyperparameters via one-step walk-forward validation. It is generic and will work for any in-memory univariate time series provided as a list or NumPy array. We can make sure all the pieces work together by testing it on a contrived 10-step dataset. The complete example is listed below. Running the example first prints the contrived time series dataset. Next, the model configurations and their errors are reported as they are evaluated. Finally, the configurations and the error for the top three configurations are reported. We can see that the persistence model with a configuration of 1 (e.g. persist the last observation) achieves the best performance of the simple models tested, as would be expected. Now that we have a robust framework for grid searching simple model hyperparameters, let¡¯s test it out on a suite of standard univariate time series datasets. The results demonstrated on each dataset provide a baseline of performance that can be used to compare more sophisticated methods, such as SARIMA, ETS, and even machine learning methods. The ¡®daily female births¡¯ dataset summarizes the daily total female births in California, USA in 1959. The dataset has no obvious trend or seasonal component. Line Plot of the Daily Female Births Dataset You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®daily-total-female-births.csv¡® in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). The dataset has one year, or 365 observations. We will use the first 200 for training and the remaining 165 as the test set. The complete example grid searching the daily female univariate time series forecasting problem is listed below. Running the example prints the model configurations and the RMSE are printed as the models are evaluated. The top three model configurations and their error are reported at the end of the run. We can see that the best result was an RMSE of about 6.93 births with the following configuration: This is surprising given the lack of trend or seasonality, I would have expected either a persistence of -1 or an average of the entire historical dataset to result in the best performance. The ¡®shampoo¡¯ dataset summarizes the monthly sales of shampoo over a three-year period. The dataset contains an obvious trend but no obvious seasonal component. Line Plot of the Monthly Shampoo Sales Dataset You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®shampoo.csv¡® in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). The dataset has three years, or 36 observations. We will use the first 24 for training and the remaining 12 as the test set. The complete example grid searching the shampoo sales univariate time series forecasting problem is listed below. Running the example prints the configurations and the RMSE are printed as the models are evaluated. The top three model configurations and their error are reported at the end of the run. We can see that the best result was an RMSE of about 95.69 sales with the following configuration: This is surprising as the trend structure of the data would suggest that persisting the previous value (-1) would be the best approach, not persisting the second last value. The ¡®monthly mean temperatures¡¯ dataset summarizes the monthly average air temperatures in Nottingham Castle, England from 1920 to 1939 in degrees Fahrenheit. The dataset has an obvious seasonal component and no obvious trend. Line Plot of the Monthly Mean Temperatures Dataset You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®monthly-mean-temp.csv¡® in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). The dataset has 20 years, or 240 observations. We will trim the dataset to the last five years of data (60 observations) in order to speed up the model evaluation process and use the last year or 12 observations for the test set. The period of the seasonal component is about one year, or 12 observations. We will use this as the seasonal period in the call to the simple_configs() function when preparing the model configurations. The complete example grid searching the monthly mean temperature time series forecasting problem is listed below. Running the example prints the model configurations and the RMSE are printed as the models are evaluated. The top three model configurations and their error are reported at the end of the run. We can see that the best result was an RMSE of about 1.501 degrees with the following configuration: This finding is not too surprising. Given the seasonal structure of the data, we would expect a function of the last few observations at prior points in the yearly cycle to be effective. The ¡®monthly car sales¡¯ dataset summarizes the monthly car sales in Quebec, Canada between 1960 and 1968. The dataset has an obvious trend and seasonal component. Line Plot of the Monthly Car Sales Dataset You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®monthly-car-sales.csv¡® in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). The dataset has 9 years, or 108 observations. We will use the last year or 12 observations as the test set. The period of the seasonal component could be six months or 12 months. We will try both as the seasonal period in the call to the simple_configs() function when preparing the model configurations. The complete example grid searching the monthly car sales time series forecasting problem is listed below. Running the example prints the model configurations and the RMSE are printed as the models are evaluated. The top three model configurations and their error are reported at the end of the run. We can see that the best result was an RMSE of about 1841.155 sales with the following configuration: It is not surprising that the chosen model is a function of the last few observations at the same point in prior cycles, although the use of the median instead of the mean may not have been immediately obvious and the results were much better than the mean. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop a framework from scratch for grid searching simple naive and averaging strategies for time series forecasting with univariate data. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. sir It is one of the nice blog about learning.I am also writing blogs related to machine learning and block chain, if you could help me out I don¡¯t know about block chain, sorry. Comment  Name (required)  Email (will not be published) (required)  Website"
"mastery",2018-10-24,"How to Grid Search SARIMA Model Hyperparameters for Time Series Forecasting in Python","https://machinelearningmastery.com/how-to-grid-search-sarima-model-hyperparameters-for-time-series-forecasting-in-python/","The Seasonal Autoregressive Integrated Moving Average, or SARIMA, model is an approach for modeling univariate time series data that may contain trend and seasonal components. It is an effective approach for time series forecasting, although it requires careful analysis and domain expertise in order to configure the seven or more model hyperparameters. An alternative approach to configuring the model that makes use of fast and parallel modern hardware is to grid search a suite of hyperparameter configurations in order to discover what works best. Often, this process can reveal non-intuitive model configurations that result in lower forecast error than those configurations specified through careful analysis. In this tutorial, you will discover how to develop a framework for grid searching all of the SARIMA model hyperparameters for univariate time series forecasting. After completing this tutorial, you will know: Let¡¯s get started. How to Grid Search SARIMA Model Hyperparameters for Time Series Forecasting in PythonPhoto by Thomas, some rights reserved. This tutorial is divided into six parts; they are: Seasonal Autoregressive Integrated Moving Average, SARIMA or Seasonal ARIMA, is an extension of ARIMA that explicitly supports univariate time series data with a seasonal component. It adds three new hyperparameters to specify the autoregression (AR), differencing (I), and moving average (MA) for the seasonal component of the series, as well as an additional parameter for the period of the seasonality. A seasonal ARIMA model is formed by including additional seasonal terms in the ARIMA [¡¦] The seasonal part of the model consists of terms that are very similar to the non-seasonal components of the model, but they involve backshifts of the seasonal period. <U+2014> Page 242, Forecasting: principles and practice, 2013. Configuring a SARIMA requires selecting hyperparameters for both the trend and seasonal elements of the series. There are three trend elements that require configuration. They are the same as the ARIMA model; specifically: There are four seasonal elements that are not part of ARIMA that must be configured; they are: Together, the notation for a SARIMA model is specified as: The SARIMA model can subsume the ARIMA, ARMA, AR, and MA models via model configuration parameters. The trend and seasonal hyperparameters of the model can be configured by analyzing autocorrelation and partial autocorrelation plots, and this can take some expertise. An alternative approach is to grid search a suite of model configurations and discover which configurations work best for a specific univariate time series. Seasonal ARIMA models can potentially have a large number of parameters and combinations of terms. Therefore, it is appropriate to try out a wide range of models when fitting to data and choose a best fitting model using an appropriate criterion ¡¦ <U+2014> Pages 143-144, Introductory Time Series with R, 2009. This approach can be faster on modern computers than an analysis process and can reveal surprising findings that might not be obvious and result in lower forecast error. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this section, we will develop a framework for grid searching SARIMA model hyperparameters for a given univariate time series forecasting problem. We will use the implementation of SARIMA provided by the statsmodels library. This model has hyperparameters that control the nature of the model performed for the series, trend and seasonality, specifically: If you know enough about your problem to specify one or more of these parameters, then you should specify them. If not, you can try grid searching these parameters. We can start-off by defining a function that will fit a model with a given configuration and make a one-step forecast. The sarima_forecast() below implements this behavior. The function takes an array or list of contiguous prior observations and a list of configuration parameters used to configure the model, specifically two tuples and a string for the trend order, seasonal order trend, and parameter. We also try to make the model robust by relaxing constraints, such as that the data must be stationary and that the MA transform be invertible. Next, we need to build up some functions for fitting and evaluating a model repeatedly via walk-forward validation, including splitting a dataset into train and test sets and evaluating one-step forecasts. We can split a list or NumPy array of data using a slice given a specified size of the split, e.g. the number of time steps to use from the data in the test set. The train_test_split() function below implements this for a provided dataset and a specified number of time steps to use in the test set. After forecasts have been made for each step in the test dataset, they need to be compared to the test set in order to calculate an error score. There are many popular error scores for time series forecasting. In this case we will use root mean squared error (RMSE), but you can change this to your preferred measure, e.g. MAPE, MAE, etc. The measure_rmse() function below will calculate the RMSE given a list of actual (the test set) and predicted values. We can now implement the walk-forward validation scheme. This is a standard approach to evaluating a time series forecasting model that respects the temporal ordering of observations. First, a provided univariate time series dataset is split into train and test sets using the train_test_split() function. Then the number of observations in the test set are enumerated. For each we fit a model on all of the history and make a one step forecast. The true observation for the time step is then added to the history and the process is repeated. The sarima_forecast() function is called in order to fit a model and make a prediction. Finally, an error score is calculated by comparing all one-step forecasts to the actual test set by calling the measure_rmse() function. The walk_forward_validation() function below implements this, taking a univariate time series, a number of time steps to use in the test set, and an array of model configuration. If you are interested in making multi-step predictions, you can change the call to predict() in the sarima_forecast() function and also change the calculation of error in the measure_rmse() function. We can call walk_forward_validation() repeatedly with different lists of model configurations. One possible issue is that some combinations of model configurations may not be called for the model and will throw an exception, e.g. specifying some but not all aspects of the seasonal structure in the data. Further, some models may also raise warnings on some data, e.g. from the linear algebra libraries called by the statsmodels library. We can trap exceptions and ignore warnings during the grid search by wrapping all calls to walk_forward_validation() with a try-except and a block to ignore warnings. We can also add debugging support to disable these protections in the case we want to see what is really going on. Finally, if an error does occur, we can return a None result, otherwise we can print some information about the skill of each model evaluated. This is helpful when a large number of models are evaluated. The score_model() function below implements this and returns a tuple of (key and result), where the key is a string version of the tested model configuration. Next, we need a loop to test a list of different model configurations. This is the main function that drives the grid search process and will call the score_model() function for each model configuration. We can dramatically speed up the grid search process by evaluating model configurations in parallel. One way to do that is to use the Joblib library. We can define a Parallel object with the number of cores to use and set it to the number of scores detected in your hardware. We can then can then create a list of tasks to execute in parallel, which will be one call to the score_model() function for each model configuration we have. Finally, we can use the Parallel object to execute the list of tasks in parallel. That¡¯s it. We can also provide a non-parallel version of evaluating all model configurations in case we want to debug something. The result of evaluating a list of configurations will be a list of tuples, each with a name that summarizes a specific model configuration and the error of the model evaluated with that configuration as either the RMSE or None if there was an error. We can filter out all scores with a None. We can then sort all tuples in the list by the score in ascending order (best are first), then return this list of scores for review. The grid_search() function below implements this behavior given a univariate time series dataset, a list of model configurations (list of lists), and the number of time steps to use in the test set. An optional parallel argument allows the evaluation of models across all cores to be tuned on or off, and is on by default. We¡¯re nearly done. The only thing left to do is to define a list of model configurations to try for a dataset. We can define this generically. The only parameter we may want to specify is the periodicity of the seasonal component in the series, if one exists. By default, we will assume no seasonal component. The sarima_configs() function below will create a list of model configurations to evaluate. The configurations assume each of the AR, MA, and I components for trend and seasonality are low order, e.g. off (0) or in [1,2]. You may want to extend these ranges if you believe the order may be higher. An optional list of seasonal periods can be specified, and you could even change the function to specify other elements that you may know about your time series. In theory, there are 1,296 possible model configurations to evaluate, but in practice, many will not be valid and will result in an error that we will trap and ignore. We now have a framework for grid searching SARIMA model hyperparameters via one-step walk-forward validation. It is generic and will work for any in-memory univariate time series provided as a list or NumPy array. We can make sure all the pieces work together by testing it on a contrived 10-step dataset. The complete example is listed below. Running the example first prints the contrived time series dataset. Next, the model configurations and their errors are reported as they are evaluated, truncated below for brevity. Finally, the configurations and the error for the top three configurations are reported. We can see that many models achieve perfect performance on this simple linearly increasing contrived time series problem. Now that we have a robust framework for grid searching SARIMA model hyperparameters, let¡¯s test it out on a suite of standard univariate time series datasets. The datasets were chosen for demonstration purposes; I am not suggesting that a SARIMA model is the best approach for each dataset; perhaps an ETS or something else would be more appropriate in some cases. The ¡®daily female births¡¯ dataset summarizes the daily total female births in California, USA in 1959. The dataset has no obvious trend or seasonal component. Line Plot of the Daily Female Births Dataset You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®daily-total-female-births.csv¡® in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). The dataset has one year, or 365 observations. We will use the first 200 for training and the remaining 165 as the test set. The complete example grid searching the daily female univariate time series forecasting problem is listed below. Running the example may take a few minutes on modern hardware. Model configurations and the RMSE are printed as the models are evaluated The top three model configurations and their error are reported at the end of the run. We can see that the best result was an RMSE of about 6.77 births with the following configuration: It is surprising that a configuration with some seasonal elements resulted in the lowest error. I would not have guessed at this configuration and would have likely stuck with an ARIMA model. The ¡®shampoo¡¯ dataset summarizes the monthly sales of shampoo over a three-year period. The dataset contains an obvious trend but no obvious seasonal component. Line Plot of the Monthly Shampoo Sales Dataset You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®shampoo.csv¡¯ in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). The dataset has three years, or 36 observations. We will use the first 24 for training and the remaining 12 as the test set. The complete example grid searching the shampoo sales univariate time series forecasting problem is listed below. Running the example may take a few minutes on modern hardware. Model configurations and the RMSE are printed as the models are evaluated The top three model configurations and their error are reported at the end of the run. We can see that the best result was an RMSE of about 54.76 sales with the following configuration: The ¡®monthly mean temperatures¡¯ dataset summarizes the monthly average air temperatures in Nottingham Castle, England from 1920 to 1939 in degrees Fahrenheit. The dataset has an obvious seasonal component and no obvious trend. Line Plot of the Monthly Mean Temperatures Dataset You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®monthly-mean-temp.csv¡® in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). The dataset has 20 years, or 240 observations. We will trim the dataset to the last five years of data (60 observations) in order to speed up the model evaluation process and use the last year, or 12 observations, for the test set. The period of the seasonal component is about one year, or 12 observations. We will use this as the seasonal period in the call to the sarima_configs() function when preparing the model configurations. The complete example grid searching the monthly mean temperature time series forecasting problem is listed below. Running the example may take a few minutes on modern hardware. Model configurations and the RMSE are printed as the models are evaluated The top three model configurations and their error are reported at the end of the run. We can see that the best result was an RMSE of about 1.5 degrees with the following configuration: As we would expect, the model has no trend component and a 12-month seasonal ARMA component. The ¡®monthly car sales¡¯ dataset summarizes the monthly car sales in Quebec, Canada between 1960 and 1968. The dataset has an obvious trend and seasonal component. Line Plot of the Monthly Car Sales Dataset You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®monthly-car-sales.csv¡® in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). The dataset has 9 years, or 108 observations. We will use the last year, or 12 observations, as the test set. The period of the seasonal component could be six months or 12 months. We will try both as the seasonal period in the call to the sarima_configs() function when preparing the model configurations. The complete example grid searching the monthly car sales time series forecasting problem is listed below. Running the example may take a few minutes on modern hardware. Model configurations and the RMSE are printed as the models are evaluated The top three model configurations and their error are reported at the end of the run. We can see that the best result was an RMSE of about 1,551 sales with the following configuration: This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop a framework for grid searching all of the SARIMA model hyperparameters for univariate time series forecasting. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Thanks for sharing.can you support a pay way of Zhi fubao or Wechat?Because i am in china,and payment of visa or paypal is kind of complex. Thanks for the suggestion, I answer this question here:https://machinelearningmastery.com/faq/single-faq/can-i-pay-via-wechat-pay-or-alipay Many thanks for the article. It¡¯s really impressive, but eight nested for loops are not pythonic. Probably, there is a smarter way to do it. For instance, use ¡®itertools.product¡¯ I could not agree more! Yes, cartesian product or similar. Thanks for this, I¡¯ve been learning about ARIMA for my applications. I learnt a lot from Forecasting Principles and Practice which has an opentexts online version: https://otexts.org/fpp2/arima.html One of the main issues I have with SARIMA (or maybe it¡¯s just statsmodels) is that it doesn¡¯t handle very large datasets very well. I have timeseries data with records every 5 minutes for ~6 years with 1 day or 1 week season length, ie. monday 9am is likely to correlate with monday 9am next week, which shows up in the differencing and ACF plots. Is there a better way or a better library to do this? You could try exposing/engineering the features and fit a linear regression model directly to the data. Comment  Name (required)  Email (will not be published) (required)  Website"
"mastery",2018-10-22,"How to Grid Search Triple Exponential Smoothing for Time Series Forecasting in Python","https://machinelearningmastery.com/how-to-grid-search-triple-exponential-smoothing-for-time-series-forecasting-in-python/","Exponential smoothing is a time series forecasting method for univariate data that can be extended to support data with a systematic trend or seasonal component. It is common practice to use an optimization process to find the model hyperparameters that result in the exponential smoothing model with the best performance for a given time series dataset. This practice applies only to the coefficients used by the model to describe the exponential structure of the level, trend, and seasonality. It is also possible to automatically optimize other hyperparameters of an exponential smoothing model, such as whether or not to model the trend and seasonal component and if so, whether to model them using an additive or multiplicative method. In this tutorial, you will discover how to develop a framework for grid searching all of the exponential smoothing model hyperparameters for univariate time series forecasting. After completing this tutorial, you will know: Let¡¯s get started. How to Grid Search Triple Exponential Smoothing for Time Series Forecasting in PythonPhoto by john mcsporran, some rights reserved. This tutorial is divided into six parts; they are: Exponential smoothing is a time series forecasting method for univariate data. Time series methods like the Box-Jenkins ARIMA family of methods develop a model where the prediction is a weighted linear sum of recent past observations or lags. Exponential smoothing forecasting methods are similar in that a prediction is a weighted sum of past observations, but the model explicitly uses an exponentially decreasing weight for past observations. Specifically, past observations are weighted with a geometrically decreasing ratio. Forecasts produced using exponential smoothing methods are weighted averages of past observations, with the weights decaying exponentially as the observations get older. In other words, the more recent the observation, the higher the associated weight. <U+2014> Page 171, Forecasting: principles and practice, 2013. Exponential smoothing methods may be considered as peers and an alternative to the popular Box-Jenkins ARIMA class of methods for time series forecasting. Collectively, the methods are sometimes referred to as ETS models, referring to the explicit modeling of Error, Trend, and Seasonality. There are three types of exponential smoothing; they are: A triple exponential smoothing model subsumes single and double exponential smoothing by the configuration of the nature of the trend (additive, multiplicative, or none) and the nature of the seasonality (additive, multiplicative, or none), as well as any dampening of the trend. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this section, we will develop a framework for grid searching exponential smoothing model hyperparameters for a given univariate time series forecasting problem. We will use the implementation of Holt-Winters Exponential Smoothing provided by the statsmodels library. This model has hyperparameters that control the nature of the exponential performed for the series, trend, and seasonality, specifically: All four of these hyperparameters can be specified when defining the model. If they are not specified, the library will automatically tune the model and find the optimal values for these hyperparameters (e.g. optimized=True). There are other hyperparameters that the model will not automatically tune that you may want to specify; they are: If you know enough about your problem to specify one or more of these parameters, then you should specify them. If not, you can try grid searching these parameters. We can start-off by defining a function that will fit a model with a given configuration and make a one-step forecast. The exp_smoothing_forecast() below implements this behavior. The function takes an array or list of contiguous prior observations and a list of configuration parameters used to configure the model. The configuration parameters in order are: the trend type, the dampening type, the seasonality type, the seasonal period, whether or not to use a Box-Cox transform, and whether or not to remove the bias when fitting the model. Next, we need to build up some functions for fitting and evaluating a model repeatedly via walk-forward validation, including splitting a dataset into train and test sets and evaluating one-step forecasts. We can split a list or NumPy array of data using a slice given a specified size of the split, e.g. the number of time steps to use from the data in the test set. The train_test_split() function below implements this for a provided dataset and a specified number of time steps to use in the test set. After forecasts have been made for each step in the test dataset, they need to be compared to the test set in order to calculate an error score. There are many popular errors scores for time series forecasting. In this case, we will use root mean squared error (RMSE), but you can change this to your preferred measure, e.g. MAPE, MAE, etc. The measure_rmse() function below will calculate the RMSE given a list of actual (the test set) and predicted values. We can now implement the walk-forward validation scheme. This is a standard approach to evaluating a time series forecasting model that respects the temporal ordering of observations. First, a provided univariate time series dataset is split into train and test sets using the train_test_split() function. Then the number of observations in the test set are enumerated. For each, we fit a model on all of the history and make a one step forecast. The true observation for the time step is then added to the history, and the process is repeated. The exp_smoothing_forecast() function is called in order to fit a model and make a prediction. Finally, an error score is calculated by comparing all one-step forecasts to the actual test set by calling the measure_rmse() function. The walk_forward_validation() function below implements this, taking a univariate time series, a number of time steps to use in the test set, and an array of model configurations. If you are interested in making multi-step predictions, you can change the call to predict() in the exp_smoothing_forecast() function and also change the calculation of error in the measure_rmse() function. We can call walk_forward_validation() repeatedly with different lists of model configurations. One possible issue is that some combinations of model configurations may not be called for the model and will throw an exception, e.g. specifying some but not all aspects of the seasonal structure in the data. Further, some models may also raise warnings on some data, e.g. from the linear algebra libraries called by the statsmodels library. We can trap exceptions and ignore warnings during the grid search by wrapping all calls to walk_forward_validation() with a try-except and a block to ignore warnings. We can also add debugging support to disable these protections in case we want to see what is really going on. Finally, if an error does occur, we can return a None result; otherwise, we can print some information about the skill of each model evaluated. This is helpful when a large number of models are evaluated. The score_model() function below implements this and returns a tuple of (key and result), where the key is a string version of the tested model configuration. Next, we need a loop to test a list of different model configurations. This is the main function that drives the grid search process and will call the score_model() function for each model configuration. We can dramatically speed up the grid search process by evaluating model configurations in parallel. One way to do that is to use the Joblib library. We can define a Parallel object with the number of cores to use and set it to the number of CPU cores detected in your hardware. We can then create a list of tasks to execute in parallel, which will be one call to the score_model() function for each model configuration we have. Finally, we can use the Parallel object to execute the list of tasks in parallel. That¡¯s it. We can also provide a non-parallel version of evaluating all model configurations in case we want to debug something. The result of evaluating a list of configurations will be a list of tuples, each with a name that summarizes a specific model configuration and the error of the model evaluated with that configuration as either the RMSE or None if there was an error. We can filter out all scores with a None. We can then sort all tuples in the list by the score in ascending order (best are first), then return this list of scores for review. The grid_search() function below implements this behavior given a univariate time series dataset, a list of model configurations (list of lists), and the number of time steps to use in the test set. An optional parallel argument allows the evaluation of models across all cores to be tuned on or off, and is on by default. We¡¯re nearly done. The only thing left to do is to define a list of model configurations to try for a dataset. We can define this generically. The only parameter we may want to specify is the periodicity of the seasonal component in the series, if one exists. By default, we will assume no seasonal component. The exp_smoothing_configs() function below will create a list of model configurations to evaluate. An optional list of seasonal periods can be specified, and you could even change the function to specify other elements that you may know about your time series. In theory, there are 72 possible model configurations to evaluate, but in practice, many will not be valid and will result in an error that we will trap and ignore. We now have a framework for grid searching triple exponential smoothing model hyperparameters via one-step walk-forward validation. It is generic and will work for any in-memory univariate time series provided as a list or NumPy array. We can make sure all the pieces work together by testing it on a contrived 10-step dataset. The complete example is listed below. Running the example first prints the contrived time series dataset. Next, the model configurations and their errors are reported as they are evaluated. Finally, the configurations and the error for the top three configurations are reported. We do not report the model parameters optimized by the model itself. It is assumed that you can achieve the same result again by specifying the broader hyperparameters and allow the library to find the same internal parameters. You can access these internal parameters by refitting a standalone model with the same configuration and printing the contents of the ¡®params¡® attribute on the model fit; for example: Now that we have a robust framework for grid searching ETS model hyperparameters, let¡¯s test it out on a suite of standard univariate time series datasets. The datasets were chosen for demonstration purposes; I am not suggesting that an ETS model is the best approach for each dataset, and perhaps an SARIMA or something else would be more appropriate in some cases. The ¡®daily female births¡¯ dataset summarizes the daily total female births in California, USA in 1959. The dataset has no obvious trend or seasonal component. Line Plot of the Daily Female Births Dataset You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®daily-total-female-births.csv¡® in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). The dataset has one year, or 365 observations. We will use the first 200 for training and the remaining 165 as the test set. The complete example grid searching the daily female univariate time series forecasting problem is listed below. Running the example may take a few minutes as fitting each ETS model can take about a minute on modern hardware. Model configurations and the RMSE are printed as the models are evaluated The top three model configurations and their error are reported at the end of the run. We can see that the best result was an RMSE of about 6.96 births with the following configuration: What is surprising is that a model that assumed an multiplicative trend performed better than one that didn¡¯t. We would not know that this is the case unless we threw out assumptions and grid searched models. The ¡®shampoo¡¯ dataset summarizes the monthly sales of shampoo over a three-year period. The dataset contains an obvious trend but no obvious seasonal component. Line Plot of the Monthly Shampoo Sales Dataset You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®shampoo.csv¡¯ in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). The dataset has three years, or 36 observations. We will use the first 24 for training and the remaining 12 as the test set. The complete example grid searching the shampoo sales univariate time series forecasting problem is listed below. Running the example is fast given there are a small number of observations. Model configurations and the RMSE are printed as the models are evaluated. The top three model configurations and their error are reported at the end of the run. We can see that the best result was an RMSE of about 83.74 sales with the following configuration: The ¡®monthly mean temperatures¡¯ dataset summarizes the monthly average air temperatures in Nottingham Castle, England from 1920 to 1939 in degrees Fahrenheit. The dataset has an obvious seasonal component and no obvious trend. Line Plot of the Monthly Mean Temperatures Dataset You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®monthly-mean-temp.csv¡¯ in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). The dataset has 20 years, or 240 observations. We will trim the dataset to the last five years of data (60 observations) in order to speed up the model evaluation process and use the last year, or 12 observations, for the test set. The period of the seasonal component is about one year, or 12 observations. We will use this as the seasonal period in the call to the exp_smoothing_configs() function when preparing the model configurations. The complete example grid searching the monthly mean temperature time series forecasting problem is listed below. Running the example is relatively slow given the large amount of data. Model configurations and the RMSE are printed as the models are evaluated. The top three model configurations and their error are reported at the end of the run. We can see that the best result was an RMSE of about 1.50 degrees with the following configuration: The ¡®monthly car sales¡¯ dataset summarizes the monthly car sales in Quebec, Canada between 1960 and 1968. The dataset has an obvious trend and seasonal component. Line Plot of the Monthly Car Sales Dataset You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®monthly-car-sales.csv¡¯ in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). The dataset has nine years, or 108 observations. We will use the last year, or 12 observations, as the test set. The period of the seasonal component could be six months or 12 months. We will try both as the seasonal period in the call to the exp_smoothing_configs() function when preparing the model configurations. The complete example grid searching the monthly car sales time series forecasting problem is listed below. Running the example is slow given the large amount of data. Model configurations and the RMSE are printed as the models are evaluated. The top three model configurations and their error are reported at the end of the run. We can see that the best result was an RMSE of about 1,672 sales with the following configuration: This is a little surprising as I would have guessed that a six-month seasonal model would be the preferred approach. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop a framework for grid searching all of the exponential smoothing model hyperparameters for univariate time series forecasting. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. =>sorry,dear.my nam¡¦ Jon. Living now ¡°Bangladesh¡± proffsanally me 15yeasr my life supot my brothers business said, mobilephone service and seal said,so technology said allows real us so good,bat i am not Happy because i no job, only miss us moneys, so one qus???
=>god son for you,any good said us me,and my barind,?give me you my life.
=>one help full job, (i no interst job service moneys/only free service? Your couasc god son any poor man)us me?pleasc dear,
@-joni Sorry, I don¡¯t follow. Thanks a lot , very interesting article !!! Thanks. Hey Jason,
So I tried this above code, but one interesting issue that¡¯s occurring is that it is not performing the ETS on whenever there is multiplicative trend or seasonality. If you see your results as well, you¡¯ll notice this thing. Since you are using try and except, this isn¡¯t giving an error, but in the actual case, it gives error by saying that endog<=0.0 Now with a 0 value in the TS data, this analogy makes sense, but even your dataset cases, there is no 0 value as such. So why is it happening and can you just check if that's what happening in your case as well. Moreover, I converted my time series data into just an array of values and this worked. But the same thing you have done but it still doesn't give any multiplicative results in your code that you've created. Can you take a look. Thanks, I¡¯ll investigate. UPDATE You¡¯re right. I have updated the usage of the ExponentialSmoothing to take a numpy array instead of a list. This results in correct usage of ¡®mul¡¯ for trend and seasonality. I have updated the code in all examples. Thanks! Comment  Name (required)  Email (will not be published) (required)  Website"
