"","site","date","headline","url_address","text","keyword"
"1","kaggle",2018-10-04,"Data Notes: Are Those Honey Bees Healthy?","http://blog.kaggle.com/2018/10/04/data-notes-are-those-honey-bees-healthy/","Honey bees, handwashing, and cancer: Enjoy these new, intriguing, and overlooked datasets and kernels 1. <U+0001F41D> Honey Bee Health Detection with CNN (link) 2.<U+00A0><U+2B50> Predicting Star, Galaxy & Quasar with SVM (link) 3.<U+00A0><U+0001F3A5> Getting Started with a Movie Recommendation System (link) 4.<U+00A0><U+0001F9D9> Analyzing The Lord of the Rings Dataset (link) 5.<U+00A0><U+0001F590> Dr. Semmelweis Handwashing Survey (link) 6.<U+00A0><U+0001F468><U+200D><U+0001F4BB> Deep Learning Frameworks 2018 (link) 7.<U+00A0><U+0001F986> Kaggle Trends 2018 (link) 8.<U+00A0><U+0001F489> Dataset: Skin Cancer MNIST (link) 9.<U+00A0><U+0001F3E5> Dataset: Colorectal Histology MNIST (link) 10.<U+00A0><U+0001F68C> Dataset: US Graduate School Admissions (link) Working with high cardinality features? Try using mean likelihood encodings! Copyright ¨Ï 2018 Kaggle, All rights reserved. You are receiving this email because you are a member of the Kaggle community.  Our mailing address is: Kaggle 1600 Amphitheatre Parkway Mountain View, CA 94043 
  

","Keyword(freq): admission(1), bee(1), dataset(1), encoding(1), feature(1), framework(1), kernel(1), right(1), ring(1), trend(1)"
"2","datacamp",2018-10-01,"Full Stack Data Science (Transcript)","https://www.datacamp.com/community/blog/full-stack-data-science-transcript","Here is a link to the podcast. Hugo:               Hi there, Vicki, and welcome to DataFramed. Vicki:                Thank you so much for having me. Hugo:               It's an absolute pleasure to have you on the show. I'm really excited to talk about your work in Python education, full stack data science, end-to-end data science, what these things actually mean, and your work in consulting. Before we get into all of that, I'd love to know a bit about you. I'm wondering what you're known for in the data community. Vicki:                Probably first and foremost, terrible puns and memes about all sorts of data and programming related things. Secondary is the content. My strategy is a little bit like BuzzFeed, right? Hit them with the memes and then sneak in serious content in between. Vicki:                I've written a lot of blog posts about how to do specific things in Python, how to do specific things in data, and then just talking about like where we are in the data community in general, so very high level articles, and talking about things that break down complicated concepts into easy to understand analogies. Hugo:               Fantastic. I love that secondary is the content and that primary are terrible puns and memes. I don't mean to put you on the spot, but what's one of the worst puns you've said or come up with or heard? Vicki:                They're all so terrible. I have this series of puns where it's basically me pretending to talk to a TV producer to pitch them on possible shows or movies, and so that series is s pretty terrible series of tweets. Hugo:               We'll definitely link to that in the show notes. That's primary. Secondary is the content. I thought I would just mention that you're also, in terms of content, in the process of creating a DataCamp course. Vicki:                Yeah, that's right. I'm working on a course that teaches object-oriented programming for Python, specifically in the context of a data setting. I'll be going throughout how to create objects and do manipulations with CSV files and digging into NumPy and pandas internals, so I'm pretty excited about that. Hugo:               Fantastic. Something you've also mentioned previously is that the educational stuff you do now is that you're essentially being the person you needed when you started out. Vicki:                Yep. Yeah, so the internet is a pretty big place and there's a lot of resources, but if you're just learning to program or you're just getting into data science, the best thing you can do is have an in-person mentor or someone who's ahead of you who you can ask questions. I didn't really have that person when I just started out, so my goal is to be that person for people just getting into the field. Hugo:               Fantastic. Actually, DataCamp itself has a similar origin story in that the CEO, our CEO Jonathan Cornelissen, when he was at grad school he was looking for something like DataCamp and just couldn't find it. He was like, ""Okay, when I finish grad school I'm going to make this thing,"" essentially. Vicki:                Yes. Hugo:               That's one of our several origin stories. That having been said, can you tell us what you do professionally at the moment? Vicki:                Yep, so I am a consultant. I work for CapTech Consulting. We do a bunch of different stuff. Part of our company deals with management consulting, and part of it is deeply technical consulting practice. Right now I do both data science and data engineering consulting depending on the project scope. Hugo:               That sounds very much like this idea of full stack data science, right? Vicki:                Yeah, so the idea is that a lot of companies will start out by not having the infrastructure set up to do data science, because really data science is kind of a mature product offering. We'll come in, we'll build out those pipelines, and then we'll get to the data science aspect, which is creating the models and presenting those results. Hugo:               Great, and we'll get to more of that later. In particular, I'm really interested in thinking about this job of building out the pipelines, doing that, but at the same time needing to demonstrate value as quickly as possible within an organization. This is something ... That's a little teaser for some things we'll chat about later. Hugo:               Before we get to that, though, data science is interesting because so many people have different avenues, all roads lead to data science in some sense. I'm wondering what your journey was. How did you get into data and data science, originally? Vicki:                I think I come from kind of a nontraditional, kind of a traditional background. It's kind of in the middle. I started out as an undergrad who majored in economics, and the reason I picked that was because I didn't want to be an English major and I didn't want to be a math major, and I like that econ kind of combined the two. I like using both sides of my brain a lot. That was my undergrad degree. Vicki:                Then after that, I actually got into economic consulting, which was pretty rare because I don't know a lot of people who focus on their major out of undergrad, so I guess I was lucky, or maybe unlucky in that sense. That's where I got tuned into doing stuff with data. Usually when you start right out of college you start doing stuff with spreadsheets, so I started doing stuff with spreadsheets. Then I heard about this new cool programming language that was free that was called R. I got exposed to that a little bit. I had a couple of roles that were analytics-based. Then my last role was as a data analyst where I learned SQL. Vicki:                Then I got tired of waiting for data to come into the SQL database for me, which is when I started really focusing on learning programing with Python and statistical methods, and then I became a data scientist as my next position. At the same time, I decided that I also wanted to get an MBA because I was interested in technical leadership. I actually don't have a statistics or development background in terms of a Master's program, but I kind of came to it through the job field. Hugo:               That's really interesting. Because a lot of people I speak to when thinking about advice to give to aspiring data scientists is one of the most important skills isn't to be able to build a thousand layer recurrent neural network, but to be able to learn on the job and pick up new skills as you go along, and it sounds like that was an integral part of your journey. Vicki:                Yeah, I think that's been really important for me the entire time in figuring out what to learn, because there's just so much to learn in data science. In consulting, that's one of the primary skills as well because you never know what kind of environment you're going to come into or what the client needs are going to be. Learning and a broad set of skills. Hugo:               Great. I'm just wondering, with your background in economics and your MBA, how do these play into your work as a data scientist in general? Do you find skills and tools you've developed and ways of thinking in economics and your MBA useful in your work in data science? Vicki:                Yeah, so economics and econometrics is actually pretty close to data science, and I think that's probably partly where data science came from. There's a lot of hypothesis testing, for example. There's a lot of statistic and econometrics that goes on. There's a lot of like the social science aspect where you have a hypothesis about how especially large scale systems would work, and that's what a lot of data scientists these days do, right? They test large scale social systems like social networks or platforms to see how things will perform, so that's part of it. Hugo:               Let's talk about your work in consulting. I presume you work across a variety of different industries, but which verticals do you see data science having the most impact on, in your experience? Vicki:                This is going to be a really consulting-y answer, but it really depends, and it's really a broad, broad variety of verticals. The ones that I focus on in my consulting career so far have been telecommunications, banking, and healthcare. Data science has an impact or a place in all of them as long as it's implemented correctly and as long as the business believes in data and sees it as a priority. Hugo:               What type of challenges have you found in demonstrating the value of data science across these industries? Vicki:                A lot of the times ... so we'll probably get to this later, but a lot of the times it's even building out that pipeline to get to the point where you can do data science, but a lot of the times, especially in larger companies, so my company deals a lot primarily with Fortune 500 companies, is getting to the point where you can demonstrate that your hypothesis or whatever it is that you said to do, your call to action, actually results in a change in the business. Hugo:               Great. Are you able to give any specific examples? I don't mean the names of companies or anything like that, but specific examples in telcos, bank, or healthcare, of actual data science projects? Vicki:                A lot of projects ... so this has been true for every industry I've been in. Every company wants to be able to measure churn or why customers are leaving or joining their platform, and especially tracing the fact of why companies are unhappy. For larger companies, this might result in an enormous amount of features, not all of which you can control. For example, the signup process, the billing process, issues they've had with your service or with their service, outside people that have approached them. You can create a model of what potentially causes customers to churn, but that might not necessarily be reflective of the real world. I think that ties back to econometrics, too, because in econometrics you're trying to create a model of the entire economy, but what you really have is a representation because you can't trace all of it. Hugo:               Yeah, great. This is a great example and something I've actually been thinking about a lot recently and talking about this morning, in fact, the churn example in particular, the potential for customers to take their business elsewhere, is the intersection between data science and decision science. Because you can build a model that may tell you or approximate what's happening in the world as to why customers are churning, but that doesn't tell you what to do, right? Vicki:                Right, so ultimately it's for the data scientist, in my opinion, to present a number of options, to present clearly what they think their view of the company is, and then a way for the company to move forward. That's kind of the point where we hand that off to a client. We'll recommend a couple of options, but we obviously won't say, ""Here's what you have to do."" Hugo:               Great. In the churn case, I can imagine several courses of action. The first would be to, if you think a customer's going to churn, reach out to them and make them some sort of offer dependent on how valuable a customer they are to your company. Another would be to try to nip it in the bud well before they're going to to churn. Are these the types of suggestions that you make or are there others as well? Vicki:                Yeah. Usually it's preventive, or you can change it when they're about to churn, or you can create preventive measures so that they can channel their frustration somewhere, for example, new support channels. Hugo:               Great. In your work across all these industries, what are common patterns you've seen in data science across them? Vicki:                One is that, I think we've heard this a lot, but getting the data to a point where you can actually do data science is always 80% of the work. Usually when we come into a company, a lot of the work will be getting the data to a point where we can do data science. The selection of tools and understanding what everybody else in the industry is doing. Kind of this need for understanding best practices. Are we picking the right tool? Is this what other people in the industry are doing? Is this what people in our industry are doing? Or people who are interested in having data science making the case that we need someone to come in and help us do this data science practice, that we actually need data science, that we actually need help making these decisions. Those are probably the big ones. Hugo:               Interesting. There are actually a lot of things that spring to mind there. The first I want to zoom in on is a lot of it's data preparation, getting into a form where you can do analytic or data science work with it. This amount of preparation you have to do, do you see this changing in the next 2, 5, 10 years? Will these types of things become more and more automated and hopefully productized? Vicki:                Some of that, but ultimately I think it's just the feature of data. Because usually unless you're working in manufacturing or some related field, what you have is you have humans generating the data, making sense of the data, defining how it's going to be in a business sense, and that kind of data is always going to be messy. Especially across larger organizations where you might have 5 or 10 or even 20 different data flows. Sometimes you have 2 data flows. They're exactly the same, but just with a little bit of difference. That kind of reconciliation is always going to be existing. Vicki:                What I do see happening more and more lately is a lot of organizations are calling for more data governance. More metadata management is becoming increasingly important in larger organizations. I think over the last 4 years or so, the push was to get stuff into a data lake. It doesn't matter how. It just needs to all be in one place so we can do something with it. Now the idea is we want to be able to manage our assets in a data lake. We need to be able to see them, represent them, and have the business be able to inventory like an S3 bucket or Hadoop cluster or something like that. Hugo:               Great. The other thing you mentioned that I'd like to discuss is you mentioned kind of the movement towards figuring out best practices for the industry, what other people are doing. I wanted to discuss this kind of in the sense that it appears to me that a lot of people ... a lot of data science work is occurring in silos across many different consulting groups, many different organizations, and that a lot of people seem to be reinventing the wheel in parallel in a lot of ways. Is that something you've seen as well? Vicki:                Yeah, I think that can definitely be true. What I've seen in a couple of my projects that were really successful is the organization or the client was dedicated to centralizing all of this stuff. What I've seen come up in larger organizations is something called a Center of Excellence where you have cross functional teams. You have engineers, you have data analysts, you have data scientists, and they all meet together to talk about what they're doing as a team. I've seen that kind of structure come up more and more recently. Hugo:               Is this the type of structure within an organization for data science teams that you think is the most effective? Vicki:                I think so. I'm a big proponent of always having all the stakeholders of any given data science project in the room, if it's practical. For example, if you have maybe 200 people that are going to impact, probably not, but I really always push for developers to sit with data analysts, and more importantly, with business users. Because usually the developers are the first part of the process, and the business users are all the way down there. It can be like a game of telephone where the developers build something, that gets put into some warehouse, that gets put into a dashboard. By the time it's built, the business users don't necessarily always want it and can't act on it. I always like to have all those people all in the same room. Hugo:               What do you think about the future of, I suppose, data literacy for business users? Will we increasingly see people in management, C-level, people using dashboards become more and more knowledgeable about what data is and how it works? Vicki:                I think so. I'm really optimistic about that, and not just because it's job security for me, as people want more and more data. I do believe that the popular press, or at least the tech press, has gotten to a point where ... and I've seen this in business literature like The Harvard Business Review or what have you, it's gotten to the point where a lot of executives now understand the need to be data-driven. Usually when meeting the clients, they say, ""We want to be data-driven."" I think the next two to three years will be ironing out what that means for them specifically. Hugo:               I presume it will mean some sort of computational literacy. I think it will probably mean a bit of statistics as well. Do you think people will need to learn like the basics of math even and linear algebra and logistic regression and these types of things, or is that expecting too much? Vicki:                No. I think the onus there is on the data scientist to present things for different audiences. If you're a data scientist and you're presenting to other data scientists, you can obviously talk about the specifics, the parameters that you have in your logistic regression or what have you. If you are talking to project managers, and especially executives, you should be speaking in a very different way, and you should be talking in a way that makes sense with what they're interested in. An executive is probably not going to be interested in the algorithm you used, but they're going to be interested in what you found and what kinds of actions you think that they should take. I am a firm believer in speaking to people in the language that they understand. Hugo:               I want to shift gears slightly and talk about your approach to building full stack end-to-end data science solutions. Before we do that though, I'm wondering if you could give us the elevator pitch or something analogous on what even full stack end-to-end data science is or means. Vicki:                Full stack to me means basically building out a data science product. You start with some kind of data flow, you transform that data in some environment, and then you output a model and you display that model. That, to me, is end-to-end data science and that's more of a product rather than a project, which I see as iterating on a specific model, for example. Hugo:               Great, so what's your approach to building these solutions then? Vicki:                I don't have a standard approach. It really depends. I usually come into the client's site and just kind of observe for the first week or so. I see what the team norms are, what kinds of tools they're using, where their pain points are. I get really annoying and I ask a ton of questions, and I do a lot of documentation. Then we usually start with looking at where the data flows into that team or that organization and seeing what we can leave behind that will be easy to maintain, reproducible, where you can understand the model that's going into it and where you can easily visualize the output. This is the golden ideal of an end-to-end project. Hugo:               Great. Can you give me an example of one you've worked on recently that you think was particularly valuable? Vicki:                Yep, so I did a project a couple of projects ago that was building predictive modeling capabilities into a Software as a Service platform. This client had a number of, let's say, a number of things that they wanted to predict about their clients. They had the descriptive capability, but they didn't have the predictive capability. My part was taking the data that they were already getting from their clients, putting that data through a model, so I used a Markov chain model that was kind of similar to modeling page views for this particular industry. Then I integrated that back into their existing software platform. Vicki:                Really my role there was, one, ingesting the data that the company was currently collecting in its task platform, analyzing that data, making sense of it because there had been no data analysis done before, figuring out what kind of model best to use to predict, and it turned out to be a Markov because, again, the product was similar to page views where you want to predict kind of the next move of the person or the client. Then wrapping that model around something that you could integrate back into their Software as a Service platform. Hugo:               Once this model is in production, who then is responsible for maintenance of it, and essentially also responsible for checking on model drift? Which, for our listeners out there, model drift is a phenomenon where when you have a productionized machine learning model, for example, it may not work, it may not give the results you're expecting after three to six months, for example. Who's responsible for this type of maintenance then? Vicki:                That depends on the type of project. Usually what we'll do with our company is we'll work with clients to stay on a month or so after and monitor the model, but usually we'll make it so that it can be easy to change on the client side, because ultimately it's theirs. We have to then make sure that it's easy to document and easy to change, which is why it's important to come in and observe it first, like I talked about, to see what toolsets they're comfortable with, what programming languages they use, what the statistical skillset of the people on the team are, so we can pass it back to them and not have it be a black box. Hugo:               Fantastic. That really is setting the expectation to make sure there's someone in house there who even has the capabilities to do this type of maintenance. Hugo:               Another thing that sprung to mind when you elucidated the process of building full stack end-to-end data science solutions was there are so many steps along the way. To be able to do this as one person as opposed to a team of people with different specialties, it seems like you ... one needs to be, and you are, a data science generalist in order to do this. Vicki:                Yeah, I think that's true. In general, I hate to propel the myth of the data science unicorn. I am certainly not a unicorn, but I do think there are generalists and specialists. For consulting particularly, it makes sense if you are a generalist and if you like to be a generalist, because you could be doing a bunch of different things. Vicki:                Recently I've done some prototyping in R. Right now I'm working on a data ingest into AWS. I've done, like I said, the Markov chain modeling before. All of that really is the skillset of understanding what the client needs and being able to figure out how to research and to get to the point where you can offer a solution versus a specialist who might be very, very knowledgeable in, for example, deep learning, for a specific industry. Hugo:               Yeah, and you mentioned R there, implicit in your work, of course, is that you work with SQL. In order to do what you need to do, I'm sure you need to do a bunch of command line stuff and you work in Python as well, so there's this kind of whole array of tools that you use to get the job done, right? Vicki:                Yep. Yeah, I would say my primary tool, when I can use it, is Python just because it's also kind of like the Swiss Army knife of languages. I actually read somewhere recently that Python is the second best language for almost anything, which I agree with. It's my personal favorite language. If you want to do almost anything, you can do it with Python. For my position particularly it works really well. Vicki:                Like I said, I've worked with R, I've worked with Scala, I do a bunch of command line stuff. Recently more and more I've been working with cloud platforms, AWS in particular, which is a whole new skillset, and more and more with engineering things like continuous integration, which is putting your model and making sure that you can keep building it and integrating it into the software. Hugo:               Actually, so I've referred to Python as the Swiss Army knife and I've heard it referred to as a Swiss Army knife for years now. I just had a brain flash, if that's even a term, that maybe we could call it the Dutch Army knife because of Guido. Vicki:                In honor, yes, in honor. Hugo:               Okay, great. I just want to also make clear to all our listeners that although Vicki ... Many guests I have on are data scientist generalists. Definitively not everyone is, and there is not a need to be a generalist, either. Something we may discuss later is that we actually are seeing a lot of specialization emerge within the discipline, right, Vicki? Vicki:                Yep, I totally agree with that. I think there's a place for both. I'm also a big proponent of data science teams as opposed to just one person doing it alone. I always work in teams. Usually it'll be someone who knows a little more statistics, someone who knows a little more engineering, and someone who's more business or business analyst oriented, and someone who's completely business facing. You have a combination of three or four of those kind of people. The best teams that I've been on complement each other in those ways. Hugo:               For people who want to get into this type of work building full stack end-to-end data science products and solutions, what advice with the respect to learning paths would you give them? Vicki:                I would say to just learn one thing that you're interested in. The bast advice I ever got was to just learn one language really well. It doesn't matter what language you're learning, although probably for the generalist Python would make more sense. Learn one language really well, and learn the internals of that language so then you can apply it to other things. Vicki:                Because what generalists do really well is to understand how different things apply to other things. For example, this is how objects work in R, this is how objects work in Python, this is how data flows into AWS, this is how data flows into Hadoop, this is how we would do something in Tableau versus D3. Generalists generally work well with patterns and are able to research different things. Vicki:                What I would suggest is, one, learning one language and then being able to extrapolate from that, and trying building a product or a project end-to-end. I had a tweet about this, which I can link to. Because it can sometimes be really hard to come up with project ideas and daunting, too. The way that I kind of scratch that itch for myself was I built a project called Soviet Art Bot, which tweets out socialist realism art. For that, I had to get that art from a website. I had to put it in AWS, and I had to have an AWS Lambda to create the bot to tweet. That kind of scratched my itch to figure out how all those different parts came together. Like I said, I have a tweet that I can link to that has a couple of different project ideas that you can ... Hugo:               I love that, and we'll definitely link to that in the show notes. Hugo:               Something that's in the cultural consciousness at the moment has been emerging for some time is this trade off in predictive analytics, machine learning and deep learning, between multiple forms, so how well a model is at predicting what it wants to predict, and being interpretable, so trying to figure out why it's making the predictions it does. I'm wondering in your work and your client work, what is the approach to this trade off, generally? Vicki:                My personal approach is to always create models that are a little bit simpler, but always easier to look in under the covers. The reason for that ... and I probably would have a different answer if I were full time at a company, but as a consultant you always need to be able to leave behind work that other people can look at, they can take apart, they can rely on, is easily documented. Especially for dealing with people that are not as technical, it's important to be able to explain those things really well. For me, I always err on the side of simpler is better. Hugo:               Something you spoke to earlier was the fact that more and more data science work is moving to the cloud, and I'd just love to pick your brain about that. This is a relatively large challenge for us as a community to do, and I was just wondering how you approach this in your work. Vicki:                Yeah, so what we've seen recently, while it's been a trend over the past couple years, but I've seen it come up in more and more projects is a lot of clients are starting to realize that they don't want to maintain infrastructure, and they want to take everything to the cloud. Of course, when they're doing this they want consider the fact that there's now things that you have to manage. For example, you have to manage the security of the cloud. Vicki:                Like there's been a lot of stories in the news lately with, for example, S3 buckets just kind of left wide open and all the data leaking out, so that's important to handle off. You need to handle some of the cloud management, and most importantly, you need to understand how all of these parts work together, because it can be harder than just, for example, creating a model in scikit, pickling it, and then putting it on some server. You have to understand how all the parts of the ecosystem work together, so that's becoming more important, too, in data science. I think specifically for data science in the cloud, the toolset is really just emerging at this point. For example, I know there's SageMaker and Google Cloud has some stuff and there's Azure Machine Learning, but all of these, I feel like, are just starting to come into their own, but they'll become more important components as people move in that direction. Hugo:               Also, I think the fact that these are emerging and rapidly developing technologies means that the barrier to entry might be slightly higher, right? Vicki:                It could be. Yeah, it could be in some ways, it's less in others. If you already know how to move in cloud environments, the barrier to entry to the cloud is low, and then the barrier to entry for machine learning is lower, too, because there's already some prototyped components that you can put together. If you don't know how to operate in those environments, in that sense the barrier to entry can be higher. What I've seen recently is a lot of people doing data science are kind of moving a little more towards the engineering path, even. Hugo:               Right. Yeah, I suppose I'm really thinking of the people who are working data scientists or are proficient in machine learning trying to go to the cloud, and it may not even be obvious even documentation wise what to do and how to do it. Vicki:                Right. Yeah, the documentation for a lot of these cloud services leaves a lot to be desired. Hugo:               We'll see that improving, surely. Vicki:                Yeah. In fact, I know AWS and I think also Microsoft have open sourced their documentation on GitHub, which a really positive. Hugo:               That's right, and I actually recently had Paige Bailey on the podcast who's a software developer advocate at Microsoft Azure, and she's instrumental in a lot of this work as well. Hugo:               Great, so we've talked a lot about kind of the data science landscape and your work currently. I'm wondering what the future of data science looks like to you. Vicki:                I think what we'll see is a lot of standardization and kind of like a narrowing out of the industry. The last five years have been about this explosive growth in this new field called data science, which nobody really knew what it was at first, and so we started to define that. There's a lot of now kind of shifting to data science. Everybody almost knows that data scientists are statisticians. Vicki:                What we're seeing now I think is a lot more, to your point, specialization. There's a lot of people specifically deep learning or specifically AI. A lot more movement to software development, like I mentioned. Especially as more stuff goes into the cloud, data scientists will need to know how to work in those environments. As always, I think the future belongs to people who can be flexible, who can write and read good code in whatever language, and who can teach themselves as the environment shifts. Hugo:               Great. Something you spoke to previously is trying to understand what best practices in data science look like. There isn't as of yet ... I mean people talk about certain things, but there isn't kind of solidified system of best practices like there is in front end software engineering, for example, right? Vicki:                Yeah, and I think that's just starting out. Like I've seen both Facebook and Google release guides on machine learning and things to look at. Google's is particularly good because it has things you should look at, and Facebook just released a bunch of videos. I think that will start to become more solidified. The other side of that is you also hear a lot of people talking about ethics in machine learning and data science, and I think there might be some pressure from that perspective as well to define just what data science means. Of course, there's GDPR regulations which will have us define what data we can collect. I think all those three things together will give us a little more fleshed out view of what that is. Hugo:               Yeah, great. I think the GDPR's an interesting example. We'll be seeing more and more of this. That's EU specific in a number of ways, if you have any data going through the EU potentially as well. As we see more and more countries adopting these types of things, I'm wondering if that will impact how we use cloud technologies as well. Vicki:                I'm sure it will to some extent. I think the big thing in cloud will be figuring out security... security and data flows first. Hugo:               Yeah. You mentioned ethics in data science. I'm wondering what you think the biggest concerns are in the ethical landscape. Vicki:                Personally I would say right now probably the biggest issue is data leaks. There's a number of different things, but I want to focus on the practical issue, which is a lot of people are not securing their data. The issue there is potentially collecting too much and then not monitoring it carefully enough. Hugo:               Okay. Yeah, I agree with that. We've talked a lot about different aspects of data science and the data science flow. I'm wondering in particular what's one of your favorite data science-y things to do, I mean techniques or methodologies? Vicki:                Yeah, so the ones that I enjoy doing the most because I get the most return out of them are probably decision trees. The reason I like them so much is because they're very easy to discuss with people who aren't necessarily data scientists. They're very easy to visualize and they give you a clear path to a call of action. If I can utilize them, I do. Hugo:               This really speaks again to something we're discussing earlier of one interpretability, you can actually show someone going down the tree and what decisions it makes at each branching point, but also ease of explicability or just being able to explain something to someone else. Vicki:                Yep, and the ease of porting between multiple platforms as well. Hugo:               In what sense? Vicki:                Implementation details so you can create a decision tree locally in scikit-learn. You can create one in R. You can create one on almost any platform that exists, so I like it with that. Hugo:               That's great. Of course in scikit-learn you can ... it's nice it's compatible with Graphviz, right, so you can visualize it immediately. Vicki:                Yep. Hugo:               What about with respect to data engineering? What really gets ... do you love doing there? Vicki:                I'm really into AWS Lambdas, which are basically... think of them as like virtual environments that exist ephemerally. They spin up, they do something, and then they go away. There's a lot of potential for use with them, and I'm really interested in exploring them a lot more. I've used them in my past two projects and I see them only growing. Hugo:               What's the gain? What's the big win to be made with AWS Lambda environments, do you think? Vicki:                They're kind of like functions that do things very quickly. They can move data. They can tweet. I use Lambda functions in my bot to tweet every certain amount of time. They're very easy to maintain. Once you set them up and have them going, they just kind of keep going. Hugo:               Fantastic. All right, so my last question is do you have a final call to action for our listeners out there? Vicki:                Yeah, so I'm on Twitter. I'm @vboykis. You can find my site there, my tech blog. If you're interested in more about what my company CapTech does, you can go to captechconsulting.com. We're always hiring and we're always taking on new clients. Hugo:               Fantastic. I suppose I do have a follow-up question there. In terms of the hiring process, this is a question I get a lot, do you have any advice or general rules of thumb for people entering an interview process, I mean with you or elsewhere? Vicki:                One, prepare well to understand the company that you're interviewing for. Especially in consulting it's a little bit different because we're looking for people who are good technically, but we're also looking for people who are interested in doing a lot of different things and are good at doing a lot of different things and can be self learners and do a lot of research. Vicki:                The second thing is to be enthusiastic about what you talk about. Tell me about what you're passionate about. Tell me about what kinds of projects you've done, if you've done projects outside of work. Tell me as much as you can about your work projects. Vicki:                Basically when I come into an interview with someone, I'm looking to have ... I'm not looking to trick you. I'm looking to have a conversation with you and to see if I can work with you, and that's it. Hugo:               Vicki, it's been an absolute pleasure having you on the show. Vicki:                Thank you for having me.","Keyword(freq): project(9), scientist(8), client(6), company(6), environment(5), organization(5), skill(5), team(5), customer(4), econometric(4)"
"3","mastery",2018-10-05,"Multi-step Time Series Forecasting with Machine Learning for Household Electricity Consumption","https://machinelearningmastery.com/multi-step-time-series-forecasting-with-machine-learning-models-for-household-electricity-consumption/","Given the rise of smart electricity meters and the wide adoption of electricity generation technology like solar panels, there is a wealth of electricity usage data available. This data represents a multivariate time series of power-related variables that in turn could be used to model and even forecast future electricity consumption. Machine learning algorithms predict a single value and cannot be used directly for multi-step forecasting. Two strategies that can be used to make multi-step forecasts with machine learning algorithms are the recursive and the direct methods. In this tutorial, you will discover how to develop recursive and direct multi-step forecasting models with machine learning algorithms. After completing this tutorial, you will know: Let¡¯s get started. Multi-step Time Series Forecasting with Machine Learning Models for Household Electricity ConsumptionPhoto by Sean McMenemy, some rights reserved. This tutorial is divided into five parts; they are: The ¡®Household Power Consumption¡® dataset is a multivariate time series dataset that describes the electricity consumption for a single household over four years. The data was collected between December 2006 and November 2010 and observations of power consumption within the household were collected every minute. It is a multivariate series comprised of seven variables (besides the date and time); they are: Active and reactive energy refer to the technical details of alternative current. A fourth sub-metering variable can be created by subtracting the sum of three defined sub-metering variables from the total active energy as follows: The dataset can be downloaded from the UCI Machine Learning repository as a single 20 megabyte .zip file: Download the dataset and unzip it into your current working directory. You will now have the file ¡°household_power_consumption.txt¡± that is about 127 megabytes in size and contains all of the observations. We can use the read_csv() function to load the data and combine the first two columns into a single date-time column that we can use as an index. Next, we can mark all missing values indicated with a ¡®?¡® character with a NaN value, which is a float. This will allow us to work with the data as one array of floating point values rather than mixed types (less efficient.) We also need to fill in the missing values now that they have been marked. A very simple approach would be to copy the observation from the same time the day before. We can implement this in a function named fill_missing() that will take the NumPy array of the data and copy values from exactly 24 hours ago. We can apply this function directly to the data within the DataFrame. Now we can create a new column that contains the remainder of the sub-metering, using the calculation from the previous section. We can now save the cleaned-up version of the dataset to a new file; in this case we will just change the file extension to .csv and save the dataset as ¡®household_power_consumption.csv¡®. Tying all of this together, the complete example of loading, cleaning-up, and saving the dataset is listed below. Running the example creates the new file ¡®household_power_consumption.csv¡® that we can use as the starting point for our modeling project. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this section, we will consider how we can develop and evaluate predictive models for the household power dataset. This section is divided into four parts; they are: There are many ways to harness and explore the household power consumption dataset. In this tutorial, we will use the data to explore a very specific question; that is: Given recent power consumption, what is the expected power consumption for the week ahead? This requires that a predictive model forecast the total active power for each day over the next seven days. Technically, this framing of the problem is referred to as a multi-step time series forecasting problem, given the multiple forecast steps. A model that makes use of multiple input variables may be referred to as a multivariate multi-step time series forecasting model. A model of this type could be helpful within the household in planning expenditures. It could also be helpful on the supply side for planning electricity demand for a specific household. This framing of the dataset also suggests that it would be useful to downsample the per-minute observations of power consumption to daily totals. This is not required, but makes sense, given that we are interested in total power per day. We can achieve this easily using the resample() function on the pandas DataFrame. Calling this function with the argument ¡®D¡® allows the loaded data indexed by date-time to be grouped by day (see all offset aliases). We can then calculate the sum of all observations for each day and create a new dataset of daily power consumption data for each of the eight variables. The complete example is listed below. Running the example creates a new daily total power consumption dataset and saves the result into a separate file named ¡®household_power_consumption_days.csv¡®. We can use this as the dataset for fitting and evaluating predictive models for the chosen framing of the problem. A forecast will be comprised of seven values, one for each day of the week ahead. It is common with multi-step forecasting problems to evaluate each forecasted time step separately. This is helpful for a few reasons: The units of the total power are kilowatts and it would be useful to have an error metric that was also in the same units. Both Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) fit this bill, although RMSE is more commonly used and will be adopted in this tutorial. Unlike MAE, RMSE is more punishing of forecast errors. The performance metric for this problem will be the RMSE for each lead time from day 1 to day 7. As a short-cut, it may be useful to summarize the performance of a model using a single score in order to aide in model selection. One possible score that could be used would be the RMSE across all forecast days. The function evaluate_forecasts() below will implement this behavior and return the performance of a model based on multiple seven-day forecasts. Running the function will first return the overall RMSE regardless of day, then an array of RMSE scores for each day. We will use the first three years of data for training predictive models and the final year for evaluating models. The data in a given dataset will be divided into standard weeks. These are weeks that begin on a Sunday and end on a Saturday. This is a realistic and useful way for using the chosen framing of the model, where the power consumption for the week ahead can be predicted. It is also helpful with modeling, where models can be used to predict a specific day (e.g. Wednesday) or the entire sequence. We will split the data into standard weeks, working backwards from the test dataset. The final year of the data is in 2010 and the first Sunday for 2010 was January 3rd. The data ends in mid November 2010 and the closest final Saturday in the data is November 20th. This gives 46 weeks of test data. The first and last rows of daily data for the test dataset are provided below for confirmation. The daily data starts in late 2006. The first Sunday in the dataset is December 17th, which is the second row of data. Organizing the data into standard weeks gives 159 full standard weeks for training a predictive model. The function split_dataset() below splits the daily data into train and test sets and organizes each into standard weeks. Specific row offsets are used to split the data using knowledge of the dataset. The split datasets are then organized into weekly data using the NumPy split() function. We can test this function out by loading the daily dataset and printing the first and last rows of data from both the train and test sets to confirm they match the expectations above. The complete code example is listed below. Running the example shows that indeed the train dataset has 159 weeks of data, whereas the test dataset has 46 weeks. We can see that the total active power for the train and test dataset for the first and last rows match the data for the specific dates that we defined as the bounds on the standard weeks for each set. Models will be evaluated using a scheme called walk-forward validation. This is where a model is required to make a one week prediction, then the actual data for that week is made available to the model so that it can be used as the basis for making a prediction on the subsequent week. This is both realistic for how the model may be used in practice and beneficial to the models, allowing them to make use of the best available data. We can demonstrate this below with separation of input data and output/predicted data. The walk-forward validation approach to evaluating predictive models on this dataset is provided in a function below, named evaluate_model(). A scikit-learn model object is provided as an argument to the function, along with the train and test datasets. An additional argument n_input is provided that is used to define the number of prior observations that the model will use as input in order to make a prediction. The specifics of how a scikit-learn model is fit and makes predictions is covered in later sections. The forecasts made by the model are then evaluated against the test dataset using the previously defined evaluate_forecasts() function. Once we have the evaluation for a model we can summarize the performance. The function below, named summarize_scores(), will display the performance of a model as a single line for easy comparison with other models. We now have all of the elements to begin evaluating predictive models on the dataset. Most predictive modeling algorithms will take some number of observations as input and predict a single output value. As such, they cannot be used directly to make a multi-step time series forecast. This applies to most linear, nonlinear, and ensemble machine learning algorithms. One approach where machine learning algorithms can be used to make a multi-step time series forecast is to use them recursively. This involves making a prediction for one time step, taking the prediction, and feeding it into the model as an input in order to predict the subsequent time step. This process is repeated until the desired number of steps have been forecasted. For example: In this section, we will develop a test harness for fitting and evaluating machine learning algorithms provided in scikit-learn using a recursive model for multi-step forecasting. The first step is to convert the prepared training data in window format into a single univariate series. The to_series() function below will convert a list of weekly multivariate data into a single univariate series of daily total power consumed. Next, the sequence of daily power needs to be transformed into inputs and outputs suitable for fitting a supervised learning problem. The prediction will be some function of the total power consumed on prior days. We can choose the number of prior days to use as inputs, such as one or two weeks. There will always be a single output: the total power consumed on the next day. The model will be fit on the true observations from prior time steps. We need to iterate through the sequence of daily power consumed and split it into inputs and outputs. This is called a sliding window data representation. The to_supervised() function below implements this behavior. It takes a list of weekly data as input as well as the number of prior days to use as inputs for each sample that is created. The first step is to convert the history into a single data series. The series is then enumerated, creating one input and output pair per time step. This framing of the problem will allow a model to learn to predict any day of the week given the observations of prior days. The function returns the inputs (X) and outputs (y) ready for training a model. The scikit-learn library allows a model to be used as part of a pipeline. This allows data transforms to be applied automatically prior to fitting the model. More importantly, the transforms are prepared in the correct way, where they are prepared or fit on the training data and applied on the test data. This prevents data leakage when evaluating models. We can use this capability when in evaluating models by creating a pipeline prior to fitting each model on the training dataset. We will both standardize and normalize the data prior to using the model. The make_pipeline() function below implements this behavior, returning a Pipeline that can be used just like a model, e.g. it can be fit and it can make predictions. The standardization and normalization operations are performed per column. In the to_supervised() function, we have essentially split one column of data (total power) into multiple columns, e.g. seven for seven days of input observations. This means that each of the seven columns in the input data will have a different mean and standard deviation for standardization and a different min and max for normalization. Given that we used a sliding window, almost all values will appear in each column, therefore, this is not likely an issue. But it is important to note that it would be more rigorous to scale the data as a single column prior to splitting it into inputs and outputs. We can tie these elements together into a function called sklearn_predict(), listed below. The function takes a scikit-learn model object, the training data, called history, and a specified number of prior days to use as inputs. It transforms the training data into inputs and outputs, wraps the model in a pipeline, fits it, and uses it to make a prediction. The model will use the last row from the training dataset as input in order to make the prediction. The forecast() function will use the model to make a recursive multi-step forecast. The recursive forecast involves iterating over each of the seven days required of the multi-step forecast. The input data to the model is taken as the last few observations of the input_data list. This list is seeded with all of the observations from the last row of the training data, and as we make predictions with the model, they are added to the end of this list. Therefore, we can take the last n_input observations from this list in order to achieve the effect of providing prior outputs as inputs. The model is used to make a prediction for the prepared input data and the output is added both to the list for the actual output sequence that we will return and the list of input data from which we will draw observations as input for the model on the next iteration. We now have all of the elements to fit and evaluate scikit-learn models using a recursive multi-step forecasting strategy. We can update the evaluate_model() function defined in the previous section to call the sklearn_predict() function. The updated function is listed below. An important final function is the get_models() that defines a dictionary of scikit-learn model objects mapped to a shorthand name we can use for reporting. We will start-off by evaluating a suite of linear algorithms. We would expect that these would perform similar to an autoregression model (e.g. AR(7) if seven days of inputs were used). The get_models() function with ten linear models is defined below. This is a spot check where we are interested in the general performance of a diverse range of algorithms rather than optimizing any given algorithm. Finally, we can tie all of this together. First, the dataset is loaded and split into train and test sets. We can then prepare the dictionary of models and define the number of prior days of observations to use as inputs to the model. The models in the dictionary are then enumerated, evaluating each, summarizing their scores, and adding the results to a line plot. The complete example is listed below. Running the example evaluates the ten linear algorithms and summarizes the results. As each of the algorithms is evaluated and the performance is reported with a one-line summary, including the overall RMSE as well as the per-time step RMSE. We can see that most of the evaluated models performed well, below 400 kilowatts in error over the whole week, with perhaps the Stochastic Gradient Descent (SGD) regressor performing the best with an overall RMSE of about 383. A line plot of the daily RMSE for each of the 10 classifiers is also created. We can see that all but two of the methods cluster together with equally well performing results across the seven day forecasts. Line Plot of Recursive Multi-step Forecasts With Linear Algorithms Better results may be achieved by tuning the hyperparameters of some of the better performing algorithms. Further, it may be interesting to update the example to test a suite of nonlinear and ensemble algorithms. An interesting experiment may be to evaluate the performance of one or a few of the better performing algorithms with more or fewer prior days as input. An alternate to the recursive strategy for multi-step forecasting is to use a different model for each of the days to be forecasted. This is called a direct multi-step forecasting strategy. Because we are interested in forecasting seven days, this would require preparing seven different models, each specialized for forecasting a different day. There are two approaches to training such a model: Predicting a day will be more specific, but will mean that less of the training data can be used for each model. Predicting a lead time makes use of more of the training data, but requires the model to generalize across the different days of the week. We will explore both approaches in this section. First, we must update the to_supervised() function to prepare the data, such as the prior week of observations, used as input and an observation from a specific day in the following week used as the output. The updated to_supervised() function that implements this behavior is listed below. It takes an argument output_ix that defines the day [0,6] in the following week to use as the output. This function can be called seven times, once for each of the seven models required. Next, we can update the sklearn_predict()<U+00A0>function to create a new dataset and a new model for each day in the one-week forecast. The body of the function is mostly unchanged, only it is used within a loop over each day in the output sequence, where the index of the day ¡°i¡± is passed to the call to to_supervised() in order to prepare a specific dataset for training a model to predict that day. The function no longer takes an n_input argument, as we have fixed the input to be the seven days of the prior week. The complete example is listed below. Running the example first summarizes the performance of each model. We can see that the performance is slightly worse than the recursive model on this problem. A line plot of the per-day RMSE scores for each model is also created, showing a similar grouping of models as was seen with the recursive model. Line Plot of Direct Per-Day Multi-step Forecasts With Linear Algorithms The direct lead time approach is the same, except that the to_supervised() makes use of more of the training dataset. The function is the same as it was defined in the recursive model example, except it takes an additional output_ix argument to define the day in the following week to use as the output. The updated to_supervised() function for the direct per-lead time strategy is listed below. Unlike the per-day strategy, this version of the function does support variable sized inputs (not just seven days), allowing you to experiment if you like. The complete example is listed below. Running the example summarizes the overall and per-day RMSE for each of the evaluated linear models. We can see that generally the per-lead time approach resulted in better performance than the per-day version. This is likely because the approach made more of the training data available to the model. A line plot of the per-day RMSE scores was again created. Line Plot of Direct Per-Lead Time Multi-step Forecasts With Linear Algorithms It may be interesting to explore a blending of the per-day and per-time step approaches to modeling the problem. It may also be interesting to see if increasing the number of prior days used as input for the per-lead time improves performance, e.g. using two weeks of data instead of one week. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop recursive and direct multi-step forecasting models with machine learning algorithms. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. How do you handle seasonality in the time series when using machine learning? You can calculate a seasonal difference or let the model learn the relationship. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): model(24), algorithm(18), observation(15), input(12), forecast(7), value(6), result(5), variable(5), output(4), score(4)"
"4","mastery",2018-10-03,"How to Develop an Autoregression Forecast Model for Household Electricity Consumption","https://machinelearningmastery.com/how-to-develop-an-autoregression-forecast-model-for-household-electricity-consumption/","Given the rise of smart electricity meters and the wide adoption of electricity generation technology like solar panels, there is a wealth of electricity usage data available. This data represents a multivariate time series of power-related variables that in turn could be used to model and even forecast future electricity consumption. Autocorrelation models are very simple and can provide a fast and effective way to make skillful one-step and multi-step forecasts for electricity consumption. In this tutorial, you will discover how to develop and evaluate an autoregression model for multi-step forecasting household power consumption. After completing this tutorial, you will know: Let¡¯s get started. How to Develop an Autoregression Forecast Model for Household Electricity ConsumptionPhoto by wongaboo, some rights reserved. This tutorial is divided into five parts; they are: The ¡®Household Power Consumption¡® dataset is a multivariate time series dataset that describes the electricity consumption for a single household over four years. The data was collected between December 2006 and November 2010 and observations of power consumption within the household were collected every minute. It is a multivariate series comprised of seven variables (besides the date and time); they are: Active and reactive energy refer to the technical details of alternative current. A fourth sub-metering variable can be created by subtracting the sum of three defined sub-metering variables from the total active energy as follows: The dataset can be downloaded from the UCI Machine Learning repository as a single 20 megabyte .zip file: Download the dataset and unzip it into your current working directory. You will now have the file ¡°household_power_consumption.txt¡± that is about 127 megabytes in size and contains all of the observations. We can use the read_csv() function to load the data and combine the first two columns into a single date-time column that we can use as an index. Next, we can mark all missing values indicated with a ¡®?¡® character with a NaN value, which is a float. This will allow us to work with the data as one array of floating point values rather than mixed types (less efficient.) We also need to fill in the missing values now that they have been marked. A very simple approach would be to copy the observation from the same time the day before. We can implement this in a function named fill_missing() that will take the NumPy array of the data and copy values from exactly 24 hours ago. We can apply this function directly to the data within the DataFrame. Now we can create a new column that contains the remainder of the sub-metering, using the calculation from the previous section. We can now save the cleaned-up version of the dataset to a new file; in this case we will just change the file extension to .csv and save the dataset as ¡®household_power_consumption.csv¡®. Tying all of this together, the complete example of loading, cleaning-up, and saving the dataset is listed below. Running the example creates the new file ¡®household_power_consumption.csv¡® that we can use as the starting point for our modeling project. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this section, we will consider how we can develop and evaluate predictive models for the household power dataset. This section is divided into four parts; they are: There are many ways to harness and explore the household power consumption dataset. In this tutorial, we will use the data to explore a very specific question; that is: Given recent power consumption, what is the expected power consumption for the week ahead? This requires that a predictive model forecast the total active power for each day over the next seven days. Technically, this framing of the problem is referred to as a multi-step time series forecasting problem, given the multiple forecast steps. A model that makes use of multiple input variables may be referred to as a multivariate multi-step time series forecasting model. A model of this type could be helpful within the household in planning expenditures. It could also be helpful on the supply side for planning electricity demand for a specific household. This framing of the dataset also suggests that it would be useful to downsample the per-minute observations of power consumption to daily totals. This is not required, but makes sense, given that we are interested in total power per day. We can achieve this easily using the resample() function on the pandas DataFrame. Calling this function with the argument ¡®D¡® allows the loaded data indexed by date-time to be grouped by day (see all offset aliases). We can then calculate the sum of all observations for each day and create a new dataset of daily power consumption data for each of the eight variables. The complete example is listed below. Running the example creates a new daily total power consumption dataset and saves the result into a separate file named ¡®household_power_consumption_days.csv¡®. We can use this as the dataset for fitting and evaluating predictive models for the chosen framing of the problem. A forecast will be comprised of seven values, one for each day of the week ahead. It is common with multi-step forecasting problems to evaluate each forecasted time step separately. This is helpful for a few reasons: The units of the total power are kilowatts and it would be useful to have an error metric that was also in the same units. Both Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) fit this bill, although RMSE is more commonly used and will be adopted in this tutorial. Unlike MAE, RMSE is more punishing of forecast errors. The performance metric for this problem will be the RMSE for each lead time from day 1 to day 7. As a short-cut, it may be useful to summarize the performance of a model using a single score in order to aide in model selection. One possible score that could be used would be the RMSE across all forecast days. The function evaluate_forecasts() below will implement this behavior and return the performance of a model based on multiple seven-day forecasts. Running the function will first return the overall RMSE regardless of day, then an array of RMSE scores for each day. We will use the first three years of data for training predictive models and the final year for evaluating models. The data in a given dataset will be divided into standard weeks. These are weeks that begin on a Sunday and end on a Saturday. This is a realistic and useful way for using the chosen framing of the model, where the power consumption for the week ahead can be predicted. It is also helpful with modeling, where models can be used to predict a specific day (e.g. Wednesday) or the entire sequence. We will split the data into standard weeks, working backwards from the test dataset. The final year of the data is in 2010 and the first Sunday for 2010 was January 3rd. The data ends in mid November 2010 and the closest final Saturday in the data is November 20th. This gives 46 weeks of test data. The first and last rows of daily data for the test dataset are provided below for confirmation. The daily data starts in late 2006. The first Sunday in the dataset is December 17th, which is the second row of data. Organizing the data into standard weeks gives 159 full standard weeks for training a predictive model. The function split_dataset() below splits the daily data into train and test sets and organizes each into standard weeks. Specific row offsets are used to split the data using knowledge of the dataset. The split datasets are then organized into weekly data using the NumPy split() function. We can test this function out by loading the daily dataset and printing the first and last rows of data from both the train and test sets to confirm they match the expectations above. The complete code example is listed below. Running the example shows that indeed the train dataset has 159 weeks of data, whereas the test dataset has 46 weeks. We can see that the total active power for the train and test dataset for the first and last rows match the data for the specific dates that we defined as the bounds on the standard weeks for each set. Models will be evaluated using a scheme called walk-forward validation. This is where a model is required to make a one week prediction, then the actual data for that week is made available to the model so that it can be used as the basis for making a prediction on the subsequent week. This is both realistic for how the model may be used in practice and beneficial to the models allowing them to make use of the best available data. We can demonstrate this below with separation of input data and output/predicted data. The walk-forward validation approach to evaluating predictive models on this dataset is implement below, named evaluate_model(). The name of a function is provided for the model as the argument ¡°model_func¡°. This function is responsible for defining the model, fitting the model on the training data, and making a one-week forecast. The forecasts made by the model are then evaluated against the test dataset using the previously defined evaluate_forecasts() function. Once we have the evaluation for a model, we can summarize the performance. The function below named summarize_scores() will display the performance of a model as a single line for easy comparison with other models. We now have all of the elements to begin evaluating predictive models on the dataset. Statistical correlation summarizes the strength of the relationship between two variables. We can assume the distribution of each variable fits a Gaussian (bell curve) distribution. If this is the case, we can use the Pearson¡¯s correlation coefficient to summarize the correlation between the variables. The Pearson¡¯s correlation coefficient is a number between -1 and 1 that describes a negative or positive correlation respectively. A value of zero indicates no correlation. We can calculate the correlation for time series observations with observations with previous time steps, called lags. Because the correlation of the time series observations is calculated with values of the same series at previous times, this is called a serial correlation, or an autocorrelation. A plot of the autocorrelation of a time series by lag is called the AutoCorrelation Function, or the acronym ACF. This plot is sometimes called a correlogram, or an autocorrelation plot. A partial autocorrelation function or PACF is a summary of the relationship between an observation in a time series with observations at prior time steps with the relationships of intervening observations removed. The autocorrelation for an observation and an observation at a prior time step is comprised of both the direct correlation and indirect correlations. These indirect correlations are a linear function of the correlation of the observation, with observations at intervening time steps. It is these indirect correlations that the partial autocorrelation function seeks to remove. Without going into the math, this is the intuition for the partial autocorrelation. We can calculate autocorrelation and partial autocorrelation plots using the plot_acf() and plot_pacf() statsmodels functions respectively. In order to calculate and plot the autocorrelation, we must convert the data into a univariate time series. Specifically, the observed daily total power consumed. The to_series() function below will take the multivariate data divided into weekly windows and will return a single univariate time series. We can call this function for the prepared training dataset. First, the daily power consumption dataset must be loaded. The dataset must then be split into train and test sets with the standard week window structure. A univariate time series of daily power consumption can then be extracted from the training dataset. We can then create a single figure that contains both an ACF and a PACF plot. The number of lag time steps can be specified. We will fix this to be one year of daily observations, or 365 days. The complete example is listed below. We would expect that the power consumed tomorrow and in the coming week will be dependent upon the power consumed in the prior days. As such, we would expect to see a strong autocorrelation signal in the ACF and PACF plots. Running the example creates a single figure with both ACF and PACF plots. The plots are very dense, and hard to read. Nevertheless, we might be able to see a familiar autoregression pattern. We might also see some significant lag observations at one year out. Further investigation may suggest a seasonal autocorrelation component, which would not be a surprising finding. ACF and PACF plots for the univariate series of power consumption We can zoom in the plot and change the number of lag observations from 365 to 50. Re-running the code example with this change results is a zoomed-in version of the plots with much less clutter. We can clearly see a familiar autoregression pattern across the two plots. This pattern is comprised of two elements: The ACF plot indicates that there is a strong autocorrelation component, whereas the PACF plot indicates that this component is distinct for the first approximately seven lag observations. This suggests that a good starting model would be an AR(7); that is an autoregression model with seven lag observations used as input. Zoomed in ACF and PACF plots for the univariate series of power consumption We can develop an autoregression model for univariate series of daily power consumption. The Statsmodels library provides multiple ways of developing an AR model, such as using the AR, ARMA, ARIMA, and SARIMAX classes. We will use the ARIMA implementation as it allows for easy expandability into differencing and moving average. First, the history data comprised of weeks of prior observations must be converted into a univariate time series of daily power consumption. We can use the to_series() function developed in the previous section. Next, an ARIMA model can be defined by passing arguments to the constructor of the ARIMA class. We will specify an AR(7) model, which in ARIMA notation is ARIMA(7,0,0). Next, the model can be fit on the training data. We will use the defaults and disable all debugging information during the fit by setting disp=False. Now that the model has been fit, we can make a prediction. A prediction can be made by calling the predict() function and passing it either an interval of dates or indices relative to the training data. We will use indices starting with the first time step beyond the training data and extending it six more days, giving a total of a seven day forecast period beyond the training dataset. We can wrap all of this up into a function below named arima_forecast() that takes the history and returns a one week forecast. This function can be used directly in the test harness described previously. The complete example is listed below. Running the example first prints the performance of the AR(7) model on the test dataset. We can see that the model achieves the overall RMSE of about 381 kilowatts. This model has skill when compared to naive forecast models, such as a model that forecasts the week ahead using observations from the same time one year ago that achieved an overall RMSE of about 465 kilowatts. A line plot of the forecast is also created, showing the RMSE in kilowatts for each of the seven lead times of the forecast. We can see an interesting pattern. We might expect that earlier lead times are easier to forecast than later lead times, as the error at each successive lead time compounds. Instead, we see that Friday (lead time +6) is the easiest to forecast and Saturday (lead time +7) is the most challenging to forecast. We can also see that the remaining lead times all have a similar error in the mid- to high-300 kilowatt range. Line plot of ARIMA forecast error for each forecasted lead times This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop and evaluate an autoregression model for multi-step forecasting household power consumption. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Great article. Wonder how this compares with time series forecasting with Prophet?
Thank you Thanks. Perhaps develop a comparison? Great read, thanks for this. You¡¯re welcome. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): observation(17), model(12), plot(8), variable(7), time(6), value(6), step(5), forecast(4), kilowatt(4), correlation(3)"
"5","mastery",2018-10-01,"How to Develop and Evaluate Naive Methods for Forecasting Household Electricity Consumption","https://machinelearningmastery.com/naive-methods-for-forecasting-household-electricity-consumption/","Given the rise of smart electricity meters and the wide adoption of electricity generation technology like solar panels, there is a wealth of electricity usage data available. This data represents a multivariate time series of power-related variables that in turn could be used to model and even forecast future electricity consumption. In this tutorial, you will discover how to develop a test harness for the ¡®household power consumption¡¯ dataset and evaluate three naive forecast strategies that provide a baseline for more sophisticated algorithms. After completing this tutorial, you will know: Let¡¯s get started. How to Develop and Evaluate Naive Forecast Methods for Forecasting Household Electricity ConsumptionPhoto by Philippe Put, some rights reserved. This tutorial is divided into four parts; they are: The ¡®Household Power Consumption¡® dataset is a multivariate time series dataset that describes the electricity consumption for a single household over four years. The data was collected between December 2006 and November 2010 and observations of power consumption within the household were collected every minute. It is a multivariate series comprised of seven variables (besides the date and time); they are: Active and reactive energy refer to the technical details of alternative current. A fourth sub-metering variable can be created by subtracting the sum of three defined sub-metering variables from the total active energy as follows: The dataset can be downloaded from the UCI Machine Learning repository as a single 20 megabyte .zip file: Download the dataset and unzip it into your current working directory. You will now have the file ¡°household_power_consumption.txt¡± that is about 127 megabytes in size and contains all of the observations. We can use the read_csv() function to load the data and combine the first two columns into a single date-time column that we can use as an index. Next, we can mark all missing values indicated with a ¡®?¡® character with a NaN value, which is a float. This will allow us to work with the data as one array of floating point values rather than mixed types (less efficient.) We also need to fill in the missing values now that they have been marked. A very simple approach would be to copy the observation from the same time the day before. We can implement this in a function named fill_missing() that will take the NumPy array of the data and copy values from exactly 24 hours ago. We can apply this function directly to the data within the DataFrame. Now we can create a new column that contains the remainder of the sub-metering, using the calculation from the previous section. We can now save the cleaned-up version of the dataset to a new file; in this case we will just change the file extension to .csv and save the dataset as ¡®household_power_consumption.csv¡®. Tying all of this together, the complete example of loading, cleaning-up, and saving the dataset is listed below. Running the example creates the new file ¡®household_power_consumption.csv¡® that we can use as the starting point for our modeling project. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this section, we will consider how we can develop and evaluate predictive models for the household power dataset. This section is divided into four parts; they are: There are many ways to harness and explore the household power consumption dataset. In this tutorial, we will use the data to explore a very specific question; that is: Given recent power consumption, what is the expected power consumption for the week ahead? This requires that a predictive model forecast the total active power for each day over the next seven days. Technically, this framing of the problem is referred to as a multi-step time series forecasting problem, given the multiple forecast steps. A model that makes use of multiple input variables may be referred to as a multivariate multi-step time series forecasting model. A model of this type could be helpful within the household in planning expenditures. It could also be helpful on the supply side for planning electricity demand for a specific household. This framing of the dataset also suggests that it would be useful to downsample the per-minute observations of power consumption to daily totals. This is not required, but makes sense, given that we are interested in total power per day. We can achieve this easily using the resample() function on the pandas DataFrame. Calling this function with the argument ¡®D¡® allows the loaded data indexed by date-time to be grouped by day (see all offset aliases). We can then calculate the sum of all observations for each day and create a new dataset of daily power consumption data for each of the eight variables. The complete example is listed below. Running the example creates a new daily total power consumption dataset and saves the result into a separate file named ¡®household_power_consumption_days.csv¡®. We can use this as the dataset for fitting and evaluating predictive models for the chosen framing of the problem. A forecast will be comprised of seven values, one for each day of the week ahead. It is common with multi-step forecasting problems to evaluate each forecasted time step separately. This is helpful for a few reasons: The units of the total power are kilowatts and it would be useful to have an error metric that was also in the same units. Both Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) fit this bill, although RMSE is more commonly used and will be adopted in this tutorial. Unlike MAE, RMSE is more punishing of forecast errors. The performance metric for this problem will be the RMSE for each lead time from day 1 to day 7. As a short-cut, it may be useful to summarize the performance of a model using a single score in order to aide in model selection. One possible score that could be used would be the RMSE across all forecast days. The function evaluate_forecasts() below will implement this behavior and return the performance of a model based on multiple seven-day forecasts. Running the function will first return the overall RMSE regardless of day, then an array of RMSE scores for each day. We will use the first three years of data for training predictive models and the final year for evaluating models. The data in a given dataset will be divided into standard weeks. These are weeks that begin on a Sunday and end on a Saturday. This is a realistic and useful way for using the chosen framing of the model, where the power consumption for the week ahead can be predicted. It is also helpful with modeling, where models can be used to predict a specific day (e.g. Wednesday) or the entire sequence. We will split the data into standard weeks, working backwards from the test dataset. The final year of the data is in 2010 and the first Sunday for 2010 was January 3rd. The data ends in mid November 2010 and the closest final Saturday in the data is November 20th. This gives 46 weeks of test data. The first and last rows of daily data for the test dataset are provided below for confirmation. The daily data starts in late 2006. The first Sunday in the dataset is December 17th, which is the second row of data. Organizing the data into standard weeks gives 159 full standard weeks for training a predictive model. The function split_dataset() below splits the daily data into train and test sets and organizes each into standard weeks. Specific row offsets are used to split the data using knowledge of the dataset. The split datasets are then organized into weekly data using the NumPy split() function. We can test this function out by loading the daily dataset and printing the first and last rows of data from both the train and test sets to confirm they match the expectations above. The complete code example is listed below. Running the example shows that indeed the train dataset has 159 weeks of data, whereas the test dataset has 46 weeks. We can see that the total active power for the train and test dataset for the first and last rows match the data for the specific dates that we defined as the bounds on the standard weeks for each set. Models will be evaluated using a scheme called walk-forward validation. This is where a model is required to make a one week prediction, then the actual data for that week is made available to the model so that it can be used as the basis for making a prediction on the subsequent week. This is both realistic for how the model may be used in practice and beneficial to the models allowing them to make use of the best available data. We can demonstrate this below with separation of input data and output/predicted data. The walk-forward validation approach to evaluating predictive models on this dataset is implement below, named evaluate_model(). The name of a function is provided for the model as the argument ¡°model_func¡°. This function is responsible for defining the model, fitting the model on the training data, and making a one-week forecast. The forecasts made by the model are then evaluated against the test dataset using the previously defined evaluate_forecasts() function. Once we have the evaluation for a model, we can summarize the performance. The function below named summarize_scores() will display the performance of a model as a single line for easy comparison with other models. We now have all of the elements to begin evaluating predictive models on the dataset. It is important to test naive forecast models on any new prediction problem. The results from naive models provide a quantitative idea of how difficult the forecast problem is and provide a baseline performance by which more sophisticated forecast methods can be evaluated. In this section, we will develop and compare three naive forecast methods for the household power prediction problem; they are: The first naive forecast that we will develop is a daily persistence model. This model takes the active power from the last day prior to the forecast period (e.g. Saturday) and uses it as the value of the power for each day in the forecast period (Sunday to Saturday). The daily_persistence() function below implements the daily persistence forecast strategy. Another good naive forecast when forecasting a standard week is to use the entire prior week as the forecast for the week ahead. It is based on the idea that next week will be very similar to this week. The weekly_persistence() function below implements the weekly persistence forecast strategy. Similar to the idea of using last week to forecast next week is the idea of using the same week last year to predict next week. That is, use the week of observations from 52 weeks ago as the forecast, based on the idea that next week will be similar to the same week one year ago. The week_one_year_ago_persistence() function below implements the week one year ago forecast strategy. We can compare each of the forecast strategies using the test harness developed in the previous section. First, the dataset can be loaded and split into train and test sets. Each of the strategies can be stored in a dictionary against a unique name. This name can be used in printing and in creating a plot of the scores. We can then enumerate each of the strategies, evaluating it using walk-forward validation, printing the scores, and adding the scores to a line plot for visual comparison. Tying all of this together, the complete example evaluating the three naive forecast strategies is listed below. Running the example first prints the total and daily scores for each model. We can see that the weekly strategy performs better than the daily strategy and that the week one year ago (week-oya) performs slightly better again. We can see this in both the overall RMSE scores for each model and in the daily scores for each forecast day. One exception is the forecast error for the first day (Sunday) where it appears that the daily persistence model performs better than the two weekly strategies. We can use the week-oya strategy with an overall RMSE of 465.294 kilowatts as the baseline in performance for more sophisticated models to be considered skillful on this specific framing of the problem. A line plot of the daily forecast error is also created. We can see the same observed pattern of the weekly strategies performing better than the daily strategy in general, except in the case of the first day. It is surprising (to me) that the week one-year-ago performs better than using the prior week. I would have expected that the power consumption from last week to be more relevant. Reviewing all strategies on the same plot suggests possible combinations of the strategies that may result in even better performance. Line Plot Comparing Naive Forecast Strategies for Household Power Forecasting This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop a test harness for the household power consumption dataset and evaluate three naive forecast strategies that provide a baseline for more sophisticated algorithms. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. hello Jason,
your article is best for us to learn ML AND DL.
do you have any article about reinforcement learning ,such as Sarsa¡¢Q learning¡¢Monte-carlo learning¡¢Deep-Q-Network and so on? thanks a lot. Not at this stage, I don¡¯t see how they can be useful on anything other than toy problems (e.g. I don¡¯t see how developers can use the methods ¡°at work¡±.) Hi Jason!
I am currently working on a NMT project for translation of my native language to English and vice versa. As told my project mentor I generated 20 epochs with the help of the readme file that comes by default with openNMT package. Now I am asked to generate 80 more epochs. He told that it can be achieved by start_index and end_index. I searched a lot how to do that and finally found thishttps://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/ Please direct me how to do this.!
Thank you Sorry, I don¡¯t have material on openNMT, I cannot give you good off the cuff advice. Okay then!
Thanks for your response thanks for your reply.
have a good day. No problem. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): model(13), strategy(11), score(7), observation(5), value(5), variable(5), method(4), implement(3), row(3), set(3)"
"6","vidhya",2018-10-04,"5 Amazing Machine Learning GitHub Repositories & Reddit Threads from September 2018","https://www.analyticsvidhya.com/blog/2018/10/best-machine-learning-github-repositories-reddit-threads-september-2018/","

FLASH SALE: Course On Introduction To Data Science | Use COUPON CODE: FLASH25 for 25% Discount| Work on 4 real-life projects |
Buy Now 

 Welcome to the September edition of our popular GitHub repositories and Reddit discussions series! GitHub repositories continue to change the way teams code and collaborate on projects. They¡¯re a great source of knowledge for anyone willing to tap into their infinite potential. As more and more professionals are vying to break into the machine learning field, everyone needs to keep themselves updated with the latest breakthroughs and frameworks. GitHub serves as a gateway to learn from the best in the business. And as always, Analytics Vidhya is at the forefront of bringing the best of the bunch straight to you. This month¡¯s GitHub collection is awe-inspiring. Ever wanted to convert a research paper into code? We have you covered. How about implementing the top object detection algorithms using a framework of your choosing? Sure, we have that as well. And the fun doesn¡¯t stop there! Scroll down to check out this, and other top repositories, launched in September. On the Reddit front, I have included the most thought-provoking discussions in this field. My suggestion is to not only read through these threads but also actively participate in them to enhance and supplement your existing knowledge. You can check out the top GitHub repositories and top Reddit discussions (from April onwards) we have covered each month below: How many times have you come across research papers and wondered how to implement them on your own? I have personally struggled on multiple occasions to convert a paper into code. Well, the painstaking process of scouring the internet for specific pieces of code is over! Hundreds of machine learning and deep learning research papers and their respective codes are included here. This repository is truly stunning in its scope and is a treasure trove of knowledge for a data scientist. New links are added weekly and the NIPS 2018 conference papers have been added as well! If there¡¯s one GitHub repository you bookmark, make sure it¡¯s this one. Object detection is quickly becoming commonplace in the deep learning universe. And why wouldn¡¯t it? It¡¯s a fascinating concept with tons of real-life applications, ranging from games to surveillance. So how about a one-stop shop where you can find all the top object detection algorithms designed since 2014? Yes, you landed in the right place. This repository, much like the one above, contains links to the full research papers and the accompanying object detection code to implement the approach mentioned in them. And the best part? The code is available for multiple frameworks! So whether you¡¯re a TensorFlow, Keras, PyTorch, or Caffe user, this repository has something for everyone. At the time of publishing this article, 43 different papers were listed here. Yes, you really can train a model on the ImageNet dataset in under 18 minutes. The great Jeremy Howard and his team of students designed an algorithm that outperformed even Google, according to the popular DAWNBench benchmark.<U+00A0>This benchmark measures the training time, cost and other aspects of deep learning models. And now you can reproduce their results on your own machine! You need to have Python 3.6 (or higher) to get started. Go ahead and dive right in. Source: North Concepts Data engineering is a critical function in any machine learning project. Most aspiring data scientists these days tend to skip over this part, preferring to focus on the model building side of things. Not a great idea! You need to be aware (and even familiar) with how data pipelines work, what role Hadoop, Spark and Dask have to play, etc. Sounds daunting? Then check out this repository. Pypeline is a simple yet very effective Python library for creating concurrent data pipelines. The aim of this library is to solve low to medium data tasks (that involve concurrency or parallelism) where the use of Spark might feel unnecessary. This repository contains codes, benchmarks, documentation and other resources to help you become a data pipeline expert! This one is a personal favourite. I covered the release of the research paper back in August and have continued to be in awe of this technique. It is capable of transferring motion between human objects in different videos. I high recommend checking out the video available in the above link, it will blow your mind! This repository contains a PyTorch implementation of this approach. The sheer amount of details this algorithm can pick up and replicate are staggering. I can¡¯t wait to try this on my own machine! This thread continues our theme of implementing research papers. It¡¯s an ideal spot for beginners in AI looking for a new challenge. There¡¯a two fold advantage of checking out this thread: Don¡¯t you love the open source community? A keen-eyed Redditor recently found a flaw in one of the CVPR (Computer Vision and Pattern Recognition) 2018 research papers. This is quite a big thing since the paper had already been accepted by the conference committee and successfully presented to the community. The original author of the paper took time out to respond to this mistake. It led to a very civil and thought-provoking discussion between the top ML folks on what should be done when a mistake like this is unearthed. Should the paper be retracted or amended with the corrections? There are over 100 comments in this thread and render this a must-read for everyone. We all get stuck at some point while going through a research paper. The math can often be difficult to understand, and the approach used can bamboozle the best of us. So why not reach out to the community and ask for help? That¡¯s exactly what this thread aims to do. Make sure you follow the format mentioned in the original post and your queries will be answered. There are plenty of Q&As already there so you can browse through them to get a feel for how the process works. A pertinent question. A lot of people I speak to are interested in getting into the research side of ML, without having a clue of what to expect. Is a background in mathematics and statistics enough? Or should you be capable enough of cracking open all research papers and making sense of them on your own? The answer lies more towards the latter. Research is a field where the experts can guide you, but no one really knows the right answer until someone figures it out. There¡¯s no single book or course that can prepare you for such a role. Needless to say, this thread is an enlightening one with different takes on what the prerequisites are. This is a controversial topic, but one I feel everyone should be aware of. Researchers release the paper and mention that the code will follow soon in order to make the reviewers happy. But sometimes this just doesn¡¯t happen. The paper gets accepted to the conference, but the code is never released to the community. It¡¯s a question of ethics than anything else. Why not mention that the data is private and can only be shared with a select few? If your code cannot be validated, what¡¯s the point of presenting it to the audience? This is a microcosm of the questions asked in this thread. Curating this list and writing about each repository and discussion thread was quite a thrill. It filled with me a sense of wonder and purpose <U+2013> there is so much knowledge out there and most of it is open-source. It would be highly negligent of us to not learn from it and put it to good use. If there are any other links you feel the community should know about, feel free to let us know in the comments section below. I need recommendations about this . Hi Praveen,","Keyword(freq): repository(4), discussion(3), link(3), algorithm(2), code(2), comment(2), framework(2), pipeline(2), project(2), analytics(1)"
"7","vidhya",2018-10-03,"DataHack Radio #11: Decision Intelligence with Google Cloud¡¯s Chief Decision Scientist, Cassie Kozyrkov","https://www.analyticsvidhya.com/blog/2018/10/datahack-radio-decision-intelligence-google-cloud-cassie-kozyrkov/","

FLASH SALE: Course On Introduction To Data Science | Use COUPON CODE: FLASH25 for 25% Discount| Work on 4 real-life projects |
Buy Now 

 What is decision intelligence? How does it tie into the world of data science? And what does Google have to do with it all? Click on the above SoundCloud link and find out! Welcome to episode #11 of DataHack Radio, where we were joined by Google Cloud¡¯s Chief Decision Scientist, Cassie Kozyrkov! Cassie is a well-known speaker in the data science sphere, and often pens down her thoughts in articulate fashion in this field. She takes us on a journey into her life at Google and how she went from being a Statistician at Google to her current role. This is a short summary of Cassie in conversation with Kunal. Give the podcast a listen and find out how a top Google scientist thinks, works, and structures her thoughts! As a bonus, there are some brilliant quotable quotes in this episode, which you will find yourself chuckling and nodding to. Subscribe to DataHack Radio NOW and listen to this, as well as all previous episodes, on any of the below platforms: The story behind Cassie¡¯s shift from using MATLAB to R is a fascinating one. While working with MATLAB, she wanted to make a particular kind of chart. After spending hours trying to figure it out, she decided to try her hand at R. It took her half an hour to design the plot she wanted! And the rest, as they say, is history. She¡¯s a big R fan, and turns to Python if absolutely necessary. Cassie joined Google in 2014 as a Statistician and one of her early projects was getting rid of duplicate entries in Google Maps. It was a far more challenging task than one might think. How do you actually define duplicate entries? You need to define good processes for measuring and verifying each duplicate entry. On a global scale, this is not a straightforward task. There were a lot of statistical techniques involved in this process, like hypothesis testing. But she wasn¡¯t the only statistician on board this project, which meant coordinating and collaborating with others. Just getting people to agree on one definition of a duplicate entry was a long winding process (anyone who has worked on a project staffed with over 50 employees will be able to relate to this!). ¡°It¡¯s quite risky for data scientists to join teams that don¡¯t quite know what they¡¯re doing.¡± Decision Intelligence (DI) augments data science with theory from social science, decision theory, and managerial science, among other applications. DI provides a framework for best practices in organizational decision-making and processes for applying machine learning and AI at scale. Cassie finds herself these days working on multiple projects, especially in the initial stages. This way she can assign the task of particular things to the correct people, instead of letting projects get bogged down due to teams not being aware of what the next step should be. If you just rely on data science in a project, the whole thing just might flop. You need to add some extra muscle, which decision intelligence supplies. This is quite a fascinating concept and worth listening to in the podcast. She also aims to help the outside world (outside Google, that is) do some of this stuff, in a more organised and better manner. Check out some of her talks in various global forums to get an idea of what she means by that. ¡°When it comes to AI, I think the whole world is making a mistake of talking about it as some form of holy water, when it¡¯s just water.¡± What Cassie means by the above quote is that when you start thinking about it as holy water, it instantly means it¡¯s accessible only to a select few. This is absolutely not true for AI and there¡¯s no special magic attached to it. ¡°Think of it as a different way of communicating your wishes to a computer.¡± You can use it to power your business and improve your results, without needing to rely on intuition and good old luck. Cassie encourages everyone to explore programming and machine learning, at least at a basic level. It¡¯s such a wonderful gateway to a whole new world where you have the power to change results, so why shouldn¡¯t you leverage that? Coming to the future of this field, Cassie is most excited about the applied side of machine learning and AI. She uses her popular analogy of a microwave and other kitchen appliances to explain this <U+2013> a truly innovative way of thinking about this domain! I personally had only vaguely heard about decision intelligence before listening to this podcast, so it was quite an eye-opener. It¡¯s a intriguing discipline and one I feel anyone in the data science field should explore. This is one those podcasts you just don¡¯t want to end, it has so much knowledge packed into an hour! I hope you enjoy listening to it as much as I did.","Keyword(freq): project(4), entry(2), process(2), result(2), team(2), thought(2), appliance(1), application(1), augment(1), cassy(1)"
"8","vidhya",2018-10-01,"Text Mining 101: A Stepwise Introduction to Topic Modeling using Latent Semantic Analysis (using Python)","https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/","

FLASH SALE: Course On Introduction To Data Science | Use COUPON CODE: FLASH25 for 25% Discount| Work on 4 real-life projects |
 Buy Now 

 Have you ever been inside a well-maintained library? I¡¯m always incredibly impressed with the way the librarians keep everything organized, by name, content, and other topics. But if you gave these librarians thousands of books and asked them to arrange each book on the basis of their genre, they will struggle to accomplish this task in a day, let alone an hour! However, this won¡¯t happen to you if these books came in a digital format, right? All the arrangement seems to happen in a matter of seconds, without requiring any manual effort. All hail Natural Language Processing (NLP). Source: confessionsofabookgeek.com Have a look at the below text snippet: As you might gather from the highlighted text, there are three topics (or concepts) <U+2013> Topic 1, Topic 2, and Topic 3. A good topic model will identify similar words and put them under one group or topic. The most dominant topic in the above example is Topic 2, which indicates that this piece of text is primarily about fake videos. Intrigued, yet? Good! In this article, we will learn about a text mining approach called Topic Modeling. It is an extremely useful technique for extracting topics, and one you will work with a lot when faced with NLP challenges. Note: I highly recommend going through this article<U+00A0>to understand terms like SVD and UMAP. They are leveraged in this article so having a basic understanding of them will help solidify these concepts. A Topic Model can be defined as an unsupervised technique to discover topics across various text documents. These topics are abstract in nature, i.e., words which are related to each other form a topic. Similarly, there can be multiple topics in an individual document. For the time being, let¡¯s understand a topic model as a black box, as illustrated in the below figure: This black box (topic model) forms clusters of similar and related words which are called topics. These topics have a certain distribution in a document, and every topic is defined by the proportion of different words it contains. Recall the example we saw earlier of arranging similar books together. Now suppose you have to perform a similar task with a few digital text documents. You would be able to manually accomplish this, as long as the number of documents is manageable (aka not too many of them). But what happens when there¡¯s an impossible number of these digital text documents? That¡¯s where NLP techniques come to the fore. And for this particular task, topic modeling is the technique we will turn to. Source: topix.io/tutorial/tutorial.html Topic modeling helps in exploring large amounts of text data, finding clusters of words, similarity between documents, and discovering abstract topics. As if these reasons weren¡¯t compelling enough, topic modeling is also used in search engines wherein the search string is matched with the results. Getting interesting, isn¡¯t it? Well, read on then! All languages have their own intricacies and nuances which are quite difficult for a machine to capture (sometimes they¡¯re even misunderstood by us humans!). This can include different words that mean the same thing, and also the words which have the same spelling but different meanings. For example, consider the following two sentences: In the first sentence, the word ¡®novel¡¯ refers to a book, and in the second sentence it means new or fresh. We can easily distinguish between these words because we are able to understand the context behind these words. However, a machine would not be able to capture this concept as it cannot understand the context in which the words have been used. This is where Latent Semantic Analysis (LSA) comes into play as it attempts to leverage the context around the words to capture the hidden concepts, also known as topics.  So, simply mapping words to documents won¡¯t really help. What we really need is to figure out the hidden concepts or topics behind the words. LSA is one such technique that can find these hidden topics. Let¡¯s now deep dive into the inner workings of LSA. Let¡¯s say we have m number of text documents with n number of total unique terms (words). We wish to extract k topics from all the text data in the documents. The number of topics, k, has to be specified by the user. It¡¯s time to power up Python and understand how to implement LSA in a topic modeling problem. Once your Python environment is open, follow the steps I have mentioned below. Let¡¯s load the required libraries before proceeding with anything else. In this article, we will use the ¡¯20 Newsgroup¡¯ dataset from sklearn. You can download the dataset here, and follow along with the code. Output: 11,314 The dataset has 11,314 text documents distributed across 20 different newsgroups. To start with, we will try to clean our text data as much as possible. The idea is to remove the punctuations, numbers, and special characters all in one step using the regex replace(¡°[^a-zA-Z#]¡±, ¡± ¡°), which will replace everything, except alphabets with space. Then we will remove shorter words because they usually don¡¯t contain useful information. Finally, we will make all the text lowercase to nullify case sensitivity. It¡¯s good practice to remove the stop-words from the text data as they are mostly clutter and hardly carry any information. Stop-words are terms like ¡®it¡¯, ¡®they¡¯, ¡®am¡¯, ¡®been¡¯, ¡®about¡¯, ¡®because¡¯, ¡®while¡¯, etc. To remove stop-words from the documents, we will have to tokenize the text, i.e., split the string of text into individual tokens or words. We will stitch the tokens back together once we have removed the stop-words. This is the first step towards topic modeling. We will use sklearn¡¯s TfidfVectorizer to create a document-term matrix with 1,000 terms. We could have used all the terms to create this matrix but that would need quite a lot of computation time and resources. Hence, we have restricted the number of features to 1,000. If you have the computational power, I suggest trying out all the terms. The next step is to represent each and every term and document as a vector. We will use the document-term matrix and decompose it into multiple matrices. We will use sklearn¡¯s TruncatedSVD to perform the task of matrix decomposition. Since the data comes from 20 different newsgroups, let¡¯s try to have 20 topics for our text data. The number of topics can be specified by using the<U+00A0>n_components<U+00A0>parameter. The components of svd_model are our topics, and we can access them using svd_model.components_. Finally, let¡¯s print a few most important words in each of the 20 topics and see how our model has done. To find out how distinct our topics are, we should visualize them. Of course, we cannot visualize more than 3 dimensions, but there are techniques like PCA and t-SNE which can help us visualize high dimensional data into lower dimensions. Here we will use a relatively new technique called UMAP (Uniform Manifold Approximation and Projection). As you can see above, the result is quite beautiful. Each dot represents a document and the colours represent the 20 newsgroups. Our LSA model seems to have done a good job. Feel free to play around with the parameters of UMAP to see how the plot changes its shape. The entire code for this article can be found in this GitHub repository. Latent Semantic Analysis can be very useful as we saw above, but it does have its limitations. It¡¯s important to understand both the sides of LSA so you have an idea of when to leverage it and when to try something else. Pros: Cons: Apart from LSA, there are other advanced and efficient topic modeling techniques such as<U+00A0>Latent Dirichlet Allocation (LDA) and<U+00A0>lda2Vec. We have a wonderful article on LDA which you can check out here. lda2vec is a much more advanced topic modeling which is based on word2vec word embeddings. If you want to find out more about it, let me know in the comments section below and I¡¯ll be happy to answer your questions/.","Keyword(freq): topic(19), document(10), term(6), concept(4), book(3), newsgroup(3), technique(3), cluster(2), dimension(2), librarian(2)"
"9","vidhya",2018-10-01," Text Mining 101: A Stepwise Introduction to Topic Modeling using Latent Semantic Analysis (using Python)","https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/","

FLASH SALE: Course On Introduction To Data Science | Use COUPON CODE: FLASH25 for 25% Discount| Work on 4 real-life projects |
 Buy Now 

 Have you ever been inside a well-maintained library? I¡¯m always incredibly impressed with the way the librarians keep everything organized, by name, content, and other topics. But if you gave these librarians thousands of books and asked them to arrange each book on the basis of their genre, they will struggle to accomplish this task in a day, let alone an hour! However, this won¡¯t happen to you if these books came in a digital format, right? All the arrangement seems to happen in a matter of seconds, without requiring any manual effort. All hail Natural Language Processing (NLP). Source: confessionsofabookgeek.com Have a look at the below text snippet: As you might gather from the highlighted text, there are three topics (or concepts) <U+2013> Topic 1, Topic 2, and Topic 3. A good topic model will identify similar words and put them under one group or topic. The most dominant topic in the above example is Topic 2, which indicates that this piece of text is primarily about fake videos. Intrigued, yet? Good! In this article, we will learn about a text mining approach called Topic Modeling. It is an extremely useful technique for extracting topics, and one you will work with a lot when faced with NLP challenges. Note: I highly recommend going through this article<U+00A0>to understand terms like SVD and UMAP. They are leveraged in this article so having a basic understanding of them will help solidify these concepts. A Topic Model can be defined as an unsupervised technique to discover topics across various text documents. These topics are abstract in nature, i.e., words which are related to each other form a topic. Similarly, there can be multiple topics in an individual document. For the time being, let¡¯s understand a topic model as a black box, as illustrated in the below figure: This black box (topic model) forms clusters of similar and related words which are called topics. These topics have a certain distribution in a document, and every topic is defined by the proportion of different words it contains. Recall the example we saw earlier of arranging similar books together. Now suppose you have to perform a similar task with a few digital text documents. You would be able to manually accomplish this, as long as the number of documents is manageable (aka not too many of them). But what happens when there¡¯s an impossible number of these digital text documents? That¡¯s where NLP techniques come to the fore. And for this particular task, topic modeling is the technique we will turn to. Source: topix.io/tutorial/tutorial.html Topic modeling helps in exploring large amounts of text data, finding clusters of words, similarity between documents, and discovering abstract topics. As if these reasons weren¡¯t compelling enough, topic modeling is also used in search engines wherein the search string is matched with the results. Getting interesting, isn¡¯t it? Well, read on then! All languages have their own intricacies and nuances which are quite difficult for a machine to capture (sometimes they¡¯re even misunderstood by us humans!). This can include different words that mean the same thing, and also the words which have the same spelling but different meanings. For example, consider the following two sentences: In the first sentence, the word ¡®novel¡¯ refers to a book, and in the second sentence it means new or fresh. We can easily distinguish between these words because we are able to understand the context behind these words. However, a machine would not be able to capture this concept as it cannot understand the context in which the words have been used. This is where Latent Semantic Analysis (LSA) comes into play as it attempts to leverage the context around the words to capture the hidden concepts, also known as topics.  So, simply mapping words to documents won¡¯t really help. What we really need is to figure out the hidden concepts or topics behind the words. LSA is one such technique that can find these hidden topics. Let¡¯s now deep dive into the inner workings of LSA. Let¡¯s say we have m number of text documents with n number of total unique terms (words). We wish to extract k topics from all the text data in the documents. The number of topics, k, has to be specified by the user. It¡¯s time to power up Python and understand how to implement LSA in a topic modeling problem. Once your Python environment is open, follow the steps I have mentioned below. Let¡¯s load the required libraries before proceeding with anything else. In this article, we will use the ¡¯20 Newsgroup¡¯ dataset from sklearn. You can download the dataset here, and follow along with the code. Output: 11,314 The dataset has 11,314 text documents distributed across 20 different newsgroups. To start with, we will try to clean our text data as much as possible. The idea is to remove the punctuations, numbers, and special characters all in one step using the regex replace(¡°[^a-zA-Z#]¡±, ¡± ¡°), which will replace everything, except alphabets with space. Then we will remove shorter words because they usually don¡¯t contain useful information. Finally, we will make all the text lowercase to nullify case sensitivity. It¡¯s good practice to remove the stop-words from the text data as they are mostly clutter and hardly carry any information. Stop-words are terms like ¡®it¡¯, ¡®they¡¯, ¡®am¡¯, ¡®been¡¯, ¡®about¡¯, ¡®because¡¯, ¡®while¡¯, etc. To remove stop-words from the documents, we will have to tokenize the text, i.e., split the string of text into individual tokens or words. We will stitch the tokens back together once we have removed the stop-words. This is the first step towards topic modeling. We will use sklearn¡¯s TfidfVectorizer to create a document-term matrix with 1,000 terms. We could have used all the terms to create this matrix but that would need quite a lot of computation time and resources. Hence, we have restricted the number of features to 1,000. If you have the computational power, I suggest trying out all the terms. The next step is to represent each and every term and document as a vector. We will use the document-term matrix and decompose it into multiple matrices. We will use sklearn¡¯s TruncatedSVD to perform the task of matrix decomposition. Since the data comes from 20 different newsgroups, let¡¯s try to have 20 topics for our text data. The number of topics can be specified by using the<U+00A0>n_components<U+00A0>parameter. The components of svd_model are our topics, and we can access them using svd_model.components_. Finally, let¡¯s print a few most important words in each of the 20 topics and see how our model has done. To find out how distinct our topics are, we should visualize them. Of course, we cannot visualize more than 3 dimensions, but there are techniques like PCA and t-SNE which can help us visualize high dimensional data into lower dimensions. Here we will use a relatively new technique called UMAP (Uniform Manifold Approximation and Projection). As you can see above, the result is quite beautiful. Each dot represents a document and the colours represent the 20 newsgroups. Our LSA model seems to have done a good job. Feel free to play around with the parameters of UMAP to see how the plot changes its shape. The entire code for this article can be found in this GitHub repository. Latent Semantic Analysis can be very useful as we saw above, but it does have its limitations. It¡¯s important to understand both the sides of LSA so you have an idea of when to leverage it and when to try something else. Pros: Cons: Apart from LSA, there are other advanced and efficient topic modeling techniques such as<U+00A0>Latent Dirichlet Allocation (LDA) and<U+00A0>lda2Vec. We have a wonderful article on LDA which you can check out here. lda2vec is a much more advanced topic modeling which is based on word2vec word embeddings. If you want to find out more about it, let me know in the comments section below and I¡¯ll be happy to answer your questions/.","Keyword(freq): topic(19), document(10), term(6), concept(4), book(3), newsgroup(3), technique(3), cluster(2), dimension(2), librarian(2)"
