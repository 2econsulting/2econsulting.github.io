"site","date","headline","url_address","text"
"mastery",2018-11-02,"LSTM Model Architecture for Rare Event Time Series Forecasting","https://machinelearningmastery.com/lstm-model-architecture-for-rare-event-time-series-forecasting/","Time series forecasting with LSTMs directly has shown little success. This is surprising as neural networks are known to be able to learn complex non-linear relationships and the LSTM is perhaps the most successful type of recurrent neural network that is capable of directly supporting multivariate sequence prediction problems. A recent study performed at Uber AI Labs demonstrates how both the automatic feature learning capabilities of LSTMs and their ability to handle input sequences can be harnessed in an end-to-end model that can be used for drive demand forecasting for rare events like public holidays. In this post, you will discover an approach to developing a scalable end-to-end LSTM model for time series forecasting. After reading this post, you will know: Let¡¯s get started. In this post, we will review the 2017 paper titled ¡°Time-series Extreme Event Forecasting with Neural Networks at Uber¡± by Nikolay Laptev, et al. presented at the Time Series Workshop, ICML 2017. This post is divided into four sections; they are: The goal of the work was to develop an end-to-end forecast model for multi-step time series forecasting that can handle multivariate inputs (e.g. multiple input time series). The intent of the model was to forecast driver demand at Uber for ride sharing, specifically to forecast demand on challenging days such as holidays where the uncertainty for classical models was high. Generally, this type of demand forecasting for holidays belongs to an area of study called extreme event prediction. Extreme event prediction has become a popular topic for estimating peak electricity demand, traffic jam severity and surge pricing for ride sharing and other applications. In fact there is a branch of statistics known as extreme value theory (EVT) that deals directly with this challenge. <U+2014> Time-series Extreme Event Forecasting with Neural Networks at Uber, 2017. Two existing approaches were described: The difficulty of these existing models motivated the desire for a single end-to-end model. Further, a model was required that could generalize across locales, specifically across data collected for each city. This means a model trained on some or all cities with data available and used to make forecasts across some or all cities. We can summarize this as the general need for a model that supports multivariate inputs, makes multi-step forecasts, and generalizes across multiple sites, in this case cities. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course The model was fit in a propitiatory Uber dataset comprised of five years of anonymized ride sharing data across top cities in the US. A five year daily history of completed trips across top US cities in terms of population was used to provide forecasts across all major US holidays. <U+2014> Time-series Extreme Event Forecasting with Neural Networks at Uber, 2017. The input to each forecast consisted of both the information about each ride, as well as weather, city, and holiday variables. To circumvent the lack of data we use additional features including weather information (e.g., precipitation, wind speed, temperature) and city level information (e.g., current trips, current users, local holidays). <U+2014> Time-series Extreme Event Forecasting with Neural Networks at Uber, 2017. The figure below taken from the paper provides a sample of six variables for one year. Scaled Multivariate Input for ModelTaken from ¡°Time-series Extreme Event Forecasting with Neural Networks at Uber¡±. A training dataset was created by splitting the historical data into sliding windows of input and output variables. The specific size of the look-back and forecast horizon used in the experiments were not specified in the paper. Sliding Window Approach to Modeling Time SeriesTaken from ¡°Time-series Extreme Event Forecasting with Neural Networks at Uber¡±. Time series data was scaled by normalizing observations per batch of samples and each input series was de-trended, but not deseasonalized. Neural networks are sensitive to unscaled data, therefore we normalize every minibatch. Furthermore, we found that de-trending the data, as opposed to de-seasoning, produces better results. <U+2014> Time-series Extreme Event Forecasting with Neural Networks at Uber, 2017. LSTMs, e.g. Vanilla LSTMs, were evaluated on the problem and show relatively poor performance. This is not surprising as it mirrors findings elsewhere. Our initial LSTM implementation did not show superior performance relative to the state of the art approach. <U+2014> Time-series Extreme Event Forecasting with Neural Networks at Uber, 2017. A more elaborate architecture was used, comprised of two LSTM models: An LSTM autoencoder model was developed for use as the feature extraction model and a Stacked LSTM was used as the forecast model. We found that the vanilla LSTM model¡¯s performance is worse than our baseline. Thus, we propose a new architecture, that leverages an autoencoder for feature extraction, achieving superior performance compared to our baseline. <U+2014> Time-series Extreme Event Forecasting with Neural Networks at Uber, 2017. When making a forecast, time series data is first provided to the autoencoders, which is compressed to multiple feature vectors that are averaged and concatenated. The feature vectors are then provided as input to the forecast model in order to make a prediction. ¡¦ the model first primes the network by auto feature extraction, which is critical to capture complex time-series dynamics during special events at scale. [¡¦] Features vectors are then aggregated via an ensemble technique (e.g., averaging or other methods). The final vector is then concatenated with the new input and fed to LSTM forecaster for prediction. <U+2014> Time-series Extreme Event Forecasting with Neural Networks at Uber, 2017. It is not clear what exactly is provided to the autoencoder when making a prediction, although we may guess that it is a multivariate time series for the city being forecasted with observations prior to the interval being forecasted. A multivariate time series as input to the autoencoder will result in multiple encoded vectors (one for each series) that could be concatenated. It is not clear what role averaging may take at this point, although we may guess that it is an averaging of multiple models performing the autoencoding process. Overview of Feature Extraction Model and Forecast ModelTaken from ¡°Time-series Extreme Event Forecasting with Neural Networks at Uber.¡± The authors comment that it would be possible to make the autoencoder a part of the forecast model, and that this was evaluated, but the separate model resulted in better performance. Having a separate auto-encoder module, however, produced better results in our experience. <U+2014> Time-series Extreme Event Forecasting with Neural Networks at Uber, 2017. More details of the developed model were made available in the slides used when presenting the paper. The input for the autoencoder was 512 LSTM units and the bottleneck in the autoencoder used to create the encoded feature vectors as 32 or 64 LSTM units. Details of LSTM Autoencoder for Feature ExtractionTaken from ¡°Time-series Extreme Event Forecasting with Neural Networks at Uber.¡± The encoded feature vectors are provided to the forecast model with ¡®new input¡®, although it is not specified what this new input is; we could guess that it is a time series, perhaps a multivariate time series of the city being forecasted with observations prior to the forecast interval. Or, features extracted from this series as the blog post on the paper suggests (although I¡¯m skeptical as the paper and slides contradict this). The model was trained on a lot of data, which is a general requirement of stacked LSTMs or perhaps LSTMs in general. The described production Neural Network Model was trained on thousands of time-series with thousands of data points each. <U+2014> Time-series Extreme Event Forecasting with Neural Networks at Uber, 2017. The model is not retrained when making new forecasts. An interesting approach to estimating forecast uncertainty was also implemented that used the bootstrap. It involved estimating model uncertainty and forecast uncertainty separately, using the autoencoder and the forecast model respectively. Inputs were provided to a given model and dropout of the activations (as commented in the slides) was used. This process was repeated 100 times, and the model and forecast error terms were used in an estimate of the forecast uncertainty. Overview of Forecast Uncertainty EstimationTaken from ¡°Time-series Extreme Event Forecasting with Neural Networks at Uber.¡± This approach to forecast uncertainty may be better described in the 2017 paper ¡°Deep and Confident Prediction for Time Series at Uber.¡± The model was evaluated with a special focus on demand forecasting for U.S. holidays by U.S. city. The specifics of the model evaluation were not specified. The new generalized LSTM forecast model was found to outperform the existing model used at Uber, which may be impressive if we assume that the existing model was well tuned. The results presented show a 2%-18% forecast accuracy improvement compared to the current proprietary method comprising a univariate timeseries and machine learned model. <U+2014> Time-series Extreme Event Forecasting with Neural Networks at Uber, 2017. The model trained on the Uber dataset was then applied directly to a subset of the M3-Competition dataset comprised of about 1,500 monthly univariate time series forecasting datasets. This is a type of transfer learning, a highly-desirable goal that allows the reuse of deep learning models across problem domains. Surprisingly, the model performed well, not great compared to the top performing methods, but better than many sophisticated models. The result is suggests that perhaps with fine tuning (e.g. as is done in other transfer learning case studies) the model could be reused and be skillful. Performance of LSTM Model Trained on Uber Data and Evaluated on the M3 DatasetsTaken from ¡°Time-series Extreme Event Forecasting with Neural Networks at Uber.¡± Importantly, the authors suggest that perhaps the most beneficial application of deep LSTM models to time series forecasting are situations where: From our experience there are three criteria for picking a neural network model for time-series: (a) number of timeseries (b) length of time-series and (c) correlation among the time-series. If (a), (b) and (c) are high then the neural network might be the right choice, otherwise classical timeseries approach may work best. <U+2014> Time-series Extreme Event Forecasting with Neural Networks at Uber, 2017. This is summarized well by a slide used in the presentation of the paper. Lessons Learned Applying LSTMs for Time Series ForecastingTaken from ¡°Time-series Extreme Event Forecasting with Neural Networks at Uber¡± Slides. This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered a scalable end-to-end LSTM model for time series forecasting. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hi, Is there a way to identify and remove outliers from data sets without affecting rare events?
Or how not to mistakenly have outliers as rare events ?
Thanks Vali You must carefully define what you mean by ¡°outlier¡± and ¡°rare event¡± so that the methods that detect the former don¡¯t detect the latter. Outliers usually are anomalies which are abnormal ie. outside a normal distribution. Something like mean+/-2*std. in a time series outliers are sparks, with much higher freq than the normal signal even with rare events.
For instance a Black Friday is rare event but fits in the normal frequency whereas an outlier is much higher frequency.
So how can I bring the frequency part in the equation?
Thanks Comment  Name (required)  Email (will not be published) (required)  Website"
"mastery",2018-10-31,"Results From Comparing Classical and Machine Learning Methods for Time Series Forecasting","https://machinelearningmastery.com/findings-comparing-classical-and-machine-learning-methods-for-time-series-forecasting/","Machine learning and deep learning methods are often reported to be the key solution to all predictive modeling problems. An important recent study evaluated and compared the performance of many classical and modern machine learning and deep learning methods on a large and diverse set of more than 1,000 univariate time series forecasting problems. The results of this study suggest that simple classical methods, such as linear methods and exponential smoothing, outperform complex and sophisticated methods, such as decision trees, Multilayer Perceptrons (MLP), and Long Short-Term Memory (LSTM) network models. These findings highlight the requirement to both evaluate classical methods and use their results as a baseline when evaluating any machine learning and deep learning methods for time series forecasting in order demonstrate that their added complexity is adding skill to the forecast. In this post, you will discover the important findings of this recent study evaluating and comparing the performance of a classical and modern machine learning methods on a large and diverse set of time series forecasting datasets. After reading this post, you will know: Let¡¯s get started. Findings Comparing Classical and Machine Learning Methods for Time Series ForecastingPhoto by Lyndon Hatherall, some rights reserved. Spyros Makridakis, et al. published a study in 2018 titled ¡°Statistical and Machine Learning forecasting methods: Concerns and ways forward.¡± In this post, we will take a close look at the study by Makridakis, et al. that carefully evaluated and compared classical time series forecasting methods to the performance of modern machine learning methods. This post is divided into seven sections; they are: The goal of the study was to clearly demonstrate the capability of a suite of different machine learning methods as compared to classical time series forecasting methods on a very large and diverse collection of univariate time series forecasting problems. The study was a response to the increasing number of papers and claims that machine learning and deep learning methods offer superior results for time series forecasting with little objective evidence. Literally hundreds of papers propose new ML algorithms, suggesting methodological advances and accuracy improvements. Yet, limited objective evidence is available regarding their relative performance as a standard forecasting tool. <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. The authors clearly lay out three issues with the flood of claims; they are: As a response, the study includes eight classical methods and 10 machine learning methods evaluated using one-step and multiple-step forecasts across a collection of 1,045 monthly time series. Although not definitive, the results are intended to be objective and robust. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course The time series datasets used in the study were drawn from the time series datasets used in the M3-Competition. The M3-Competition was the third in a series of competitions that sought to discover exactly what algorithms perform well in practice on real time series forecasting problems. The results of the competition were published in the 2000 paper titled ¡°The M3-Competition: Results, Conclusions and Implications.¡± The datasets used in the competition were drawn from a wide range of industries and had a range of different time intervals, from hourly to annual. The 3003 series of the M3-Competition were selected on a quota basis to include various types of time series data (micro, industry, macro, etc.) and different time intervals between successive observations (yearly, quarterly, etc.). The table below, taken from the paper, provides a summary of the 3,003 datasets used in the competition. Table of Datasets, Industry and Time Interval Used in the M3-CompetitionTaken from ¡°The M3-Competition: Results, Conclusions and Implications.¡± The finding of the competition was that simpler time series forecasting methods outperform more sophisticated methods, including neural network models. This study, the previous two M-Competitions and many other empirical studies have proven, beyond the slightest doubt, that elaborate theoretical constructs or more sophisticated methods do not necessarily improve post-sample forecasting accuracy, over simple methods, although they can better fit a statistical model to the available historical data. <U+2014> The M3-Competition: Results, Conclusions and Implications, 2000. The more recent study that we are reviewing in this post that evaluate machine learning methods selected a subset of 1,045 time series with a monthly interval from those used in the M3 competition. ¡¦ evaluate such performance across multiple forecasting horizons using a large subset of 1045 monthly time series used in the M3 Competition. <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. The study evaluates the performance of eight classical (or simpler) methods and 10 machine learning methods. ¡¦ of eight traditional statistical methods and eight popular ML ones, [¡¦], plus two more that have become popular during recent years. <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. The eight classical methods evaluated were as follows: A total of eight machine learning methods were used in an effort to reproduce and compare to results presented in the 2010 paper ¡°An Empirical Comparison of Machine Learning Models for Time Series Forecasting.¡± They were: An additional two ¡®modern¡® neural network algorithms were also added to the list given the recent rise in their adoption; they were: A careful data preparation methodology was used, again, based on the methodology described in the 2010 paper ¡°An Empirical Comparison of Machine Learning Models for Time Series Forecasting.¡± In that paper, each time series was adjusted using a power transform, deseasonalized and detrended. [¡¦] before computing the 18 forecasts, they preprocessed the series in order to achieve stationarity in their mean and variance. This was done using the log transformation, then deseasonalization and finally scaling, while first differences were also considered for removing the component of trend. <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. Inspired by these operations, variations of five different data transforms were applied for an MLP for one-step forecasting and their results were compared. The five transforms were: Generally, it was found that the best approach was to apply a power transform and deseasonalize the data, and perhaps detrend the series as well. The best combination according to sMAPE is number 7 (Box-Cox transformation, deseasonalization) while the best one according to MASE is number 10 (Box-Cox transformation, deseasonalization and detrending) <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. All models were evaluated using one-step time series forecasting. Specifically, the last 18 time steps were used as a test set, and models were fit on all remaining observations. A separate one-step forecast was made for each of the 18 observations in the test set, presumably using a walk-forward validation method where true observations were used as input in order to make each forecast. The forecasting model was developed using the first n <U+2013> 18 observations, where n is the length of the series. Then, 18 forecasts were produced and their accuracy was evaluated compared to the actual values not used in developing the forecasting model. <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. Reviewing the results, the MLP and BNN were found to achieve the best performance from all of the machine learning methods. The results [¡¦] show that MLP and BNN outperform the remaining ML methods. <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. A surprising result was that RNNs and LSTMs were found to perform poorly. It should be noted that RNN is among the less accurate ML methods, demonstrating that research progress does not necessarily guarantee improvements in forecasting performance. This conclusion also applies in the performance of LSTM, another popular and more advanced ML method, which does not enhance forecasting accuracy too. <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. Comparing the performance of all methods, it was found that the machine learning methods were all out-performed by simple classical methods, where ETS and ARIMA models performed the best overall. This finding confirms the results from previous similar studies and competitions. Bar Chart Comparing Model Performance (sMAPE) for One-Step ForecastsTaken from ¡°Statistical and Machine Learning forecasting methods: Concerns and ways forward.¡± Multi-step forecasting involves predicting multiple steps ahead of the last known observation. Three approaches to multi-step forecasting were evaluated for the machine learning methods; they were: The classical methods were found to outperform the machine learning methods again. In this case, methods such as Theta, ARIMA, and a combination of exponential smoothing (Comb) were found to achieve the best performance. In brief, statistical models seem to generally outperform ML methods across all forecasting horizons, with Theta, Comb and ARIMA being the dominant ones among the competitors according to both error metrics examined. <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. The study provides important supporting evidence that classical methods may dominate univariate time series forecasting, at least on the types of forecasting problems evaluated. The study demonstrates the worse performance and the increase in computational cost of machine learning and deep learning methods for univariate time series forecasting for both one-step and multi-step forecasts. These findings strongly encourage the use of classical methods, such as ETS, ARIMA, and others as a first step before more elaborate methods are explored, and requires that the results from these simpler methods be used as a baseline in performance that more elaborate methods must clear in order to justify their usage. It also highlights the need to not just consider the careful use of data preparation methods, but to actively test multiple different combinations of data preparation schemes for a given problem in order to discover what works best, even in the case of classical methods. Machine learning and deep learning methods may still achieve better performance on specific univariate time series problems and should be evaluated. The study does not look at more complex time series problems, such as those datasets with: The study concludes with an honest puzzlement at why machine learning methods perform so poorly in practice, given their impressive performance in other areas of artificial intelligence. The most interesting question and greatest challenge is to find the reasons for their poor performance with the objective of improving their accuracy and exploiting their huge potential. AI learning algorithms have revolutionized a wide range of applications in diverse fields and there is no reason that the same cannot be achieved with the ML methods in forecasting. Thus, we must find how to be applied to improve their ability to forecast more accurately. <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. Comments are made by the authors regarding LSTMs and RNNs, that are generally believed to be the deep learning approach for sequence prediction problems in general, and in this case their clearly poor performance in practice. [¡¦] one would expect RNN and LSTM, which are more advanced types of NNs, to be far more accurate than the ARIMA and the rest of the statistical methods utilized. <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. They comment that LSTMs appear to be more suited at fitting or overfitting the training dataset rather than forecasting it. Another interesting example could be the case of LSTM that compared to simpler NNs like RNN and MLP, report better model fitting but worse forecasting accuracy <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. There is work to do and machine learning methods and deep learning methods hold the promise of better learning time series data than classical statistical methods, and even doing so directly on the raw observations via automatic feature learning. Given their ability to learn, ML methods should do better than simple benchmarks, like exponential smoothing. Accepting the problem is the first step in devising workable solutions and we hope that those in the field of AI and ML will accept the empirical findings and work to improve the forecasting accuracy of their methods. <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered the important findings of a recent study evaluating and comparing the performance of classical and modern machine learning methods on a large and diverse set of time series forecasting datasets. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Thanks for posting this. I recently started going through a course on Time Series forecasting and this just confirms my need to understand the more basic and standard statistical methods before I look at deep-learning techniques. You can get started with the basics here:https://machinelearningmastery.com/start-here/#timeseries Cool, like this write up. One question, it seems like it was all just univariate models am I right? Would be really curious to see something similar but using multivariate data and models. Yes, just univariate data/models.  I agree, I similar study for multivariate datasets would be fascinating! Very informatinve thanks Jason. Confirmed what I¡¯ve always assumed that more variables are probably required to give deep learning an upper hand in TS prediction.
I¡¯m surprised that you don¡¯t know of any comparative studies involving the latter? Agreed! Such studies may exist, but I have not come across them, at least not at this level of robustness. Appreciate the insightful article! Maybe this was stated in the readings, but it seems like for each time series, no work was done on feature selection / feature engineering, i.e. the inputs to both the traditional models and the ML models consisted solely of the time series¡¯ own historical values?  Could you confirm if this is true? All of the data was univariate, only one feature to select. Thanks for this. Juicy! Have you a ¡®go to¡¯ reference for survey of issues and methods for (quoting you) Complex irregular temporal structures.
Missing observations
Heavy noise.
Complex interrelationships between multiple variates. Thank you again ¡¦would gladly repay your effort with a coffee or drink any time you¡¯re in the DMV. Thanks! Not at this stage. The benchmarking has been done for various models with just time series as one of the input component and typical output as predictive component is that correct, if so whats the value plotted on the right side, and why is it mentioned as the basic statistic methods to be compartively better, can you throw some light on it ¡°Right side¡±?  I don¡¯t follow, can you elaborate please? Comment  Name (required)  Email (will not be published) (required)  Website"
"mastery",2018-10-29,"How to Develop Deep Learning Models for Univariate Time Series Forecasting","https://machinelearningmastery.com/how-to-develop-deep-learning-models-for-univariate-time-series-forecasting/","Deep learning neural networks are capable of automatically learning and extracting features from raw data. This feature of neural networks can be used for time series forecasting problems, where models can be developed directly on the raw observations without the direct need to scale the data using normalization and standardization or to make the data stationary by differencing. Impressively, simple deep learning neural network models are capable of making skillful forecasts as compared to naive models and tuned SARIMA models on univariate time series forecasting problems that have both trend and seasonal components with no pre-processing. In this tutorial, you will discover how to develop a suite of deep learning models for univariate time series forecasting. After completing this tutorial, you will know: Let¡¯s get started. How to Develop Deep Learning Models for Univariate Time Series ForecastingPhoto by Nathaniel McQueen, some rights reserved. This tutorial is divided into five parts; they are: The ¡®monthly car sales¡® dataset summarizes the monthly car sales in Quebec, Canada between 1960 and 1968. You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®monthly-car-sales.csv¡® in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). Once loaded, we can summarize the shape of the dataset in order to determine the number of observations. We can then create a line plot of the series to get an idea of the structure of the series. We can tie all of this together; the complete example is listed below. Running the example first prints the shape of the dataset. The dataset is monthly and has nine years, or 108 observations. In our testing, will use the last year, or 12 observations, as the test set. A line plot is created. The dataset has an obvious trend and seasonal component. The period of the seasonal component could be six months or 12 months. Line Plot of Monthly Car Sales From prior experiments, we know that a naive model can achieve a root mean squared error, or RMSE, of 1841.155 by taking the median of the observations at the three prior years for the month being predicted; for example: Where the negative indexes refer to observations in the series relative to the end of the historical data for the month being predicted. From prior experiments, we know that a SARIMA model can achieve an RMSE of 1551.842 with the configuration of SARIMA(0, 0, 0),(1, 1, 0),12 where no elements are specified for the trend and a seasonal difference with a period of 12 is calculated and an AR model of one season is used. The performance of the naive model provides a lower bound on a model that is considered skillful. Any model that achieves a predictive performance of lower than 1841.155 on the last 12 months has skill. The performance of the SARIMA model provides a measure of a good model on the problem. Any model that achieves a predictive performance lower than 1551.842 on the last 12 months should be adopted over a SARIMA model. Now that we have defined our problem and expectations of model skill, we can look at defining the test harness. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this section, we will develop a test harness for developing and evaluating different types of neural network models for univariate time series forecasting. This section is divided into the following parts: The first step is to split the loaded series into train and test sets. We will use the first eight years (96 observations) for training and the last 12 for the test set. The train_test_split() function below will split the series taking the raw observations and the number of observations to use in the test set as arguments. Next, we need to be able to frame the univariate series of observations as a supervised learning problem so that we can train neural network models. A supervised learning framing of a series means that the data needs to be split into multiple examples that the model learn from and generalize across. Each sample must have both an input component and an output component. The input component will be some number of prior observations, such as three years or 36 time steps. The output component will be the total sales in the next month because we are interested in developing a model to make one-step forecasts. We can implement this using the shift() function on the pandas DataFrame. It allows us to shift a column down (forward in time) or back (backward in time). We can take the series as a column of data, then create multiple copies of the column, shifted forward or backward in time in order to create the samples with the input and output elements we require. When a series is shifted down, NaN values are introduced because we don¡¯t have values beyond the start of the series. For example, the series defined as a column: Can be shifted and inserted as a column beforehand: We can see that on the second row, the value 1 is provided as input as an observation at the prior time step, and 2 is the next value in the series that can be predicted, or learned by the model to be predicted when 1 is presented as input. Rows with NaN values can be removed. The series_to_supervised() function below implements this behavior, allowing you to specify the number of lag observations to use in the input and the number to use in the output for each sample. It will also remove rows that have NaN values as they cannot be used to train or test a model. Time series forecasting models can be evaluated on a test set using walk-forward validation. Walk-forward validation is an approach where the model makes a forecast for each observation in the test dataset one at a time. After each forecast is made for a time step in the test dataset, the true observation for the forecast is added to the test dataset and made available to the model. Simpler models can be refit with the observation prior to making the subsequent prediction. More complex models, such as neural networks, are not refit given the much greater computational cost. Nevertheless, the true observation for the time step can then be used as part of the input for making the prediction on the next time step. First, the dataset is split into train and test sets. We will call the train_test_split() function to perform this split and pass in the pre-specified number of observations to use as the test data. A model will be fit once on the training dataset for a given configuration. We will define a generic model_fit() function to perform this operation that can be filled in for the given type of neural network that we may be interested in later. The function takes the training dataset and the model configuration and returns the fit model ready for making predictions. Each time step of the test dataset is enumerated. A prediction is made using the fit model. Again, we will define a generic function named model_predict() that takes the fit model, the history, and the model configuration and makes a single one-step prediction. The prediction is added to a list of predictions and the true observation from the test set is added to a list of observations that was seeded with all observations from the training dataset. This list is built up during each step in the walk-forward validation, allowing the model to make a one-step prediction using the most recent history. All of the predictions can then be compared to the true values in the test set and an error measure calculated. We will calculate the root mean squared error, or RMSE, between predictions and the true values. RMSE is calculated as the square root of the average of the squared differences between the forecasts and the actual values. The measure_rmse() implements this below using the mean_squared_error() scikit-learn function to first calculate the mean squared error, or MSE, before calculating the square root. The complete walk_forward_validation()<U+00A0>function that ties all of this together is listed below. It takes the dataset, the number of observations to use as the test set, and the configuration for the model, and returns the RMSE for the model performance on the test set. Neural network models are stochastic. This means that, given the same model configuration and the same training dataset, a different internal set of weights will result each time the model is trained that will in turn have a different performance. This is a benefit, allowing the model to be adaptive and find high performing configurations to complex problems. It is also a problem when evaluating the performance of a model and in choosing a final model to use to make predictions. To address model evaluation, we will evaluate a model configuration multiple times via walk-forward validation and report the error as the average error across each evaluation. This is not always possible for large neural networks and may only make sense for small networks that can be fit in minutes or hours. The repeat_evaluate() function below implements this and allows the number of repeats to be specified as an optional parameter that defaults to 30 and returns a list of model performance scores: in this case, RMSE values. Finally, we need to summarize the performance of a model from the multiple repeats. We will summarize the performance first using summary statistics, specifically the mean and the standard deviation. We will also plot the distribution of model performance scores using a box and whisker plot to help get an idea of the spread of performance. The summarize_scores() function below implements this, taking the name of the model that was evaluated and the list of scores from each repeated evaluation, printing the summary and showing a plot. Now that we have defined the elements of the test harness, we can tie them all together and define a simple persistence model. Specifically, we will calculate the median of a subset of prior observations relative to the time to be forecasted. We do not need to fit a model so the model_fit() function will be implemented to simply return None. We will use the config to define a list of index offsets in the prior observations relative to the time to be forecasted that will be used as the prediction. For example, 12 will use the observation 12 months ago (-12) relative to the time to be forecasted. The model_predict() function can be implemented to use this configuration to collect the observations, then return the median of those observations. The complete example of using the framework with a simple persistence model is listed below. Running the example prints the RMSE of the model evaluated using walk-forward validation on the final 12 months of data. The model is evaluated 30 times, although, because the model has no stochastic element, the score is the same each time. We can see that the RMSE of the model is 1841, providing a lower-bound of performance by which we can evaluate whether a model is skillful or not on the problem. Box and Whisker Plot of Persistence RMSE Forecasting Car Sales Now that we have a robust test harness, we can use it to evaluate a suite of neural network models. The first network that we will evaluate is a multilayer Perceptron, or MLP for short. This is a simple feed-forward neural network model that should be evaluated before more elaborate models are considered. MLPs can be used for time series forecasting by taking multiple observations at prior time steps, called lag observations, and using them as input features and predicting one or more time steps from those observations. This is exactly the framing of the problem provided by the series_to_supervised() function in the previous section. The training dataset is therefore a list of samples, where each sample has some number of observations from months prior to the time being forecasted, and the forecast is the next month in the sequence. For example: The model will attempt to generalize over these samples, such that when a new sample is provided beyond what is known by the model, it can predict something useful; for example: We will implement a simple MLP using the Keras deep learning library. The model will have an input layer with some number of prior observations. This can be specified using the input_dim argument when we define the first hidden layer. The model will have a single hidden layer with some number of nodes, then a single output layer. We will use the rectified linear activation function on the hidden layer as it performs well. We will use a linear activation function (the default) on the output layer because we are predicting a continuous value. The loss function for the network will be the mean squared error loss, or MSE, and we will use the efficient Adam flavor of stochastic gradient descent to train the network. The model will be fit for some number of training epochs (exposures to the training data) and batch size can be specified to define how often the weights are updated within each epoch. The model_fit() function for fitting an MLP model on the training dataset is listed below. The function expects the config to be a list with the following configuration hyperparameters: Making a prediction with a fit MLP model is as straightforward as calling the predict() function and passing in one sample worth of input values required to make the prediction. In order to make a prediction beyond the limit of known data, this requires that the last n known observations are taken as an array and used as input. The predict() function expects one or more samples of inputs when making a prediction, so providing a single sample requires the array to have the shape [1, n_input], where n_input is the number of time steps that the model expects as input. Similarly, the predict() function returns an array of predictions, one for each sample provided as input. In the case of one prediction, there will be an array with one value. The model_predict() function below implements this behavior, taking the model, the prior observations, and model configuration as arguments, formulating an input sample and making a one-step prediction that is then returned. We now have everything we need to evaluate an MLP model on the monthly car sales dataset. A simple grid search of model hyperparameters was performed and the configuration below was chosen. This may not be an optimal configuration, but is the best that was found. This configuration can be defined as a list: Note that when the training data is framed as a supervised learning problem, there are only 72 samples that can be used to train the model. Using a batch size of 72 or more means that the model is being trained using batch gradient descent instead of mini-batch gradient descent. This is often used for small datasets and means that weight updates and gradient calculations are performed at the end of each epoch, instead of multiple times within each epoch. The complete code example is listed below. Running the example prints the RMSE for each of the 30 repeated evaluations of the model. At the end of the run, the average and standard deviation RMSE are reported of about 1,526 sales. We can see that, on average, the chosen configuration has better performance than both the naive model (1841.155) and the SARIMA model (1551.842). This is impressive given that the model operated on the raw data directly without scaling or the data being made stationary. A box and whisker plot of the RMSE scores is created to summarize the spread of the performance for the model. This helps to understand the spread of the scores. We can see that although on average the performance of the model is impressive, the spread is large. The standard deviation is a little more than 134 sales, meaning a worse case model run that is 2 or 3 standard deviations in error from the mean error may be worse than the naive model. A challenge in using the MLP model is in harnessing the higher skill and minimizing the variance of the model across multiple runs. This problem applies generally for neural networks. There are many strategies that you could use, but perhaps the simplest is simply to train multiple final models on all of the available data and use them in an ensemble when making predictions, e.g. the prediction is the average of 10-to-30 models. Box and Whisker Plot of Multilayer Perceptron RMSE Forecasting Car Sales Convolutional Neural Networks, or CNNs, are a type of neural network developed for two-dimensional image data, although they can be used for one-dimensional data such as sequences of text and time series. When operating on one-dimensional data, the CNN reads across a sequence of lag observations and learns to extract features that are relevant for making a prediction. We will define a CNN with two convolutional layers for extracting features from the input sequences. Each will have a configurable number of filters and kernel size and will use the rectified linear activation function. The number of filters determines the number of parallel fields on which the weighted inputs are read and projected. The kernel size defines the number of time steps read within each snapshot as the network reads along the input sequence. A max pooling layer is used after convolutional layers to distill the weighted input features into those that are most salient, reducing the input size by 1/4. The pooled inputs are flattened to one long vector before being interpreted and used to make a one-step prediction. The CNN model expects input data to be in the form of multiple samples, where each sample has multiple input time steps, the same as the MLP in the previous section. One difference is that the CNN can support multiple features or types of observations at each time step, which are interpreted as channels of an image. We only have a single feature at each time step, therefore the required three-dimensional shape of the input data will be [n_samples, n_input, 1]. The model_fit() function for fitting the CNN model on the training dataset is listed below. The model takes the following five configuration parameters as a list: Making a prediction with the fit CNN model is very much like making a prediction with the fit MLP model in the previous section. The one difference is in the requirement that we specify the number of features observed at each time step, which in this case is 1. Therefore, when making a single one-step prediction, the shape of the input array must be: The model_predict() function below implements this behavior. A simple grid search of model hyperparameters was performed and the configuration below was chosen. This is not an optimal configuration, but is the best that was found. The chosen configuration is as follows: This can be specified as a list as follows: Tying all of this together, the complete example is listed below. Running the example first prints the RMSE for each repeated evaluation of the model. At the end of the run, we can see that indeed the model is skillful, achieving an average RMSE of 1,524.067, which is better than the naive model, the SARIMA model, and even the MLP model in the previous section. This is impressive given that the model operated on the raw data directly without scaling or the data being made stationary. The standard deviation of the score is large, at about 57 sales, but is 1/3 the size of the variance observed with the MLP model in the previous section. We have some confidence that in a bad-case scenario (3 standard deviations), the model RMSE will remain below (better than) the performance of the naive model. A box and whisker plot of the scores is created to help understand the spread of error across the runs. We can see that the spread does seem to be biased towards larger error values, as we would expect, although the upper whisker of the plot (in this case, the largest error that are not outliers) is still limited at an RMSE of 1,650 sales. Box and Whisker Plot of Convolutional Neural Network RMSE Forecasting Car Sales Recurrent neural networks, or RNNs, are those types of neural networks that use an output of the network from a prior step as an input in attempt to automatically learn across sequence data. The Long Short-Term Memory, or LSTM, network is a type of RNN whose implementation addresses the general difficulties in training RNNs on sequence data that results in a stable model. It achieves this by learning the weights for internal gates that control the recurrent connections within each node. Although developed for sequence data, LSTMs have not proven effective on time series forecasting problems where the output is a function of recent observations, e.g. an autoregressive type forecasting problem, such as the car sales dataset. Nevertheless, we can develop LSTM models for autoregressive problems and use them as a point of comparison with other neural network models. In this section, we will explore three variations on the LSTM model for univariate time series forecasting; they are: The LSTM neural network can be used for univariate time series forecasting. As an RNN, it will read each time step of an input sequence one step at a time. The LSTM has an internal memory allowing it to accumulate internal state as it reads across the steps of a given input sequence. At the end of the sequence, each node in a layer of hidden LSTM units will output a single value. This vector of values summarizes what the LSTM learned or extracted from the input sequence. This can be interpreted by a fully connected layer before a final prediction is made. Like the CNN, the LSTM can support multiple variables or features at each time step. As the car sales dataset only has one value at each time step, we can fix this at 1, both when defining the input to the network in the input_shape argument [n_input, 1], and in defining the shape of the input samples. Unlike the MLP and CNN that do not read the sequence data one-step at a time, the LSTM does perform better if the data is stationary. This means that difference operations are performed to remove the trend and seasonal structure. In the case of the car sales dataset, we can make the data stationery by performing a seasonal adjustment, that is subtracting the value from one year ago from each observation. This can be performed systematically for the entire training dataset. It also means that the first year of observations must be discarded as we have no prior year of data to difference them with. The difference() function below will difference a provided dataset with a provided offset, called the difference order, e.g. 12 for one year of months prior. We can make the difference order a hyperparameter to the model and only perform the operation if a value other than zero is provided. The model_fit() function for fitting an LSTM model is provided below. The model expects a list of five model hyperparameters; they are: Making a prediction with the LSTM model is the same as making a prediction with a CNN model. A single input must have the three-dimensional structure of samples, timesteps, and features, which in this case we only have 1 sample and 1 feature: [1, n_input, 1]. If the difference operation was performed, we must add back the value that was subtracted after the model has made a forecast. We must also difference the historical data prior to formulating the single input used to make a prediction. The model_predict() function below implements this behavior. A simple grid search of model hyperparameters was performed and the configuration below was chosen. This is not an optimal configuration, but is the best that was found. The chosen configuration is as follows: This can be specified as a list: Tying all of this together, the complete example is listed below. Running the example, we can see the RMSE for each repeated evaluation of the model. At the end of the run, we can see that the average RMSE is about 2,109, which is worse than the naive model. This suggests that the chosen model is not skillful, and it was the best that could be found given the same resources used to find model configurations in the previous sections. This provides further evidence (although weak evidence) that LSTMs, at least alone, are perhaps a bad fit for autoregressive-type sequence prediction problems. A box and whisker plot is also created summarizing the distribution of RMSE scores. Even the base case for the model did not achieve the performance of a naive model. Box and Whisker Plot of Long Short-Term Memory Neural Network RMSE Forecasting Car Sales We have seen that the CNN model is capable of automatically learning and extracting features from the raw sequence data without scaling or differencing. We can combine this capability with the LSTM where a CNN model is applied to sub-sequences of input data, the results of which together form a time series of extracted features that can be interpreted by an LSTM model. This combination of a CNN model used to read multiple subsequences over time by an LSTM is called a CNN-LSTM model. The model requires that each input sequence, e.g. 36 months, is divided into multiple subsequences, each read by the CNN model, e.g. 3 subsequence of 12 time steps. It may make sense to divide the sub-sequences by years, but this is just a hypothesis, and other splits could be used, such as six subsequences of six time steps. Therefore, this splitting is parameterized with the n_seq and n_steps for the number of subsequences and number of steps per subsequence parameters. The number of lag observations per sample is simply (n_seq * n_steps). This is a 4-dimensional input array now with the dimensions: The same CNN model must be applied to each input subsequence. We can achieve this by wrapping the entire CNN model in a TimeDistributed layer wrapper. The output of one application of the CNN submodel will be a vector. The output of the submodel to each input subsequence will be a time series of interpretations that can be interpreted by an LSTM model. This can be followed by a fully connected layer to interpret the outcomes of the LSTM and finally an output layer for making one-step predictions. The complete model_fit() function is listed below. The model expects a list of seven hyperparameters; they are: Making a prediction with the fit model is much the same as the LSTM or CNN, although with the addition of splitting each sample into subsequences with a given number of time steps. The updated model_predict() function is listed below. A simple grid search of model hyperparameters was performed and the configuration below was chosen. This may not be an optimal configuration, but it is the best that was found. We can define the configuration as a list; for example: The complete example of evaluating the CNN-LSTM model for forecasting the univariate monthly car sales is listed below. Running the example prints the RMSE for each repeated evaluation of the model. The final averaged RMSE is reported at the end of about 1,626, which is lower than the naive model, but still higher than a SARIMA model. The standard deviation of this score is also very large, suggesting that the chosen configuration may not be as stable as the standalone CNN model. A box and whisker plot is also created summarizing the distribution of RMSE scores. The plot shows one single outlier of very poor performance just below 3,000 sales. Box and Whisker Plot of CNN-LSTM RMSE Forecasting Car Sales It is possible to perform a convolutional operation as part of the read of the input sequence within each LSTM unit. This means, rather than reading a sequence one step at a time, the LSTM would read a block or subsequence of observations at a time using a convolutional process, like a CNN. This is different to first reading an extracting features with an LSTM and interpreting the result with an LSTM; this is performing the CNN operation at each time step as part of the LSTM. This type of model is called a Convolutional LSTM, or ConvLSTM for short. It is provided in Keras as a layer called ConvLSTM2D for 2D data. We can configure it for use with 1D sequence data by assuming that we have one row with multiple columns. As with the CNN-LSTM, the input data is split into subsequences where each subsequence has a fixed number of time steps, although we must also specify the number of rows in each subsequence, which in this case is fixed at 1. The shape is five-dimensional, with the dimensions: Like the CNN, the ConvLSTM layer allows us to specify the number of filter maps and the size of the kernel used when reading the input sequences. The output of the layer is a sequence of filter maps that must first be flattened before it can be interpreted and followed by an output layer. The model expects a list of seven hyperparameters, the same as the CNN-LSTM; they are: The model_fit() function that implements all of this is listed below. A prediction is made with the fit model in the same way as the CNN-LSTM, although with the additional rows dimension that we fix to 1. The model_predict() function for making a single one-step prediction is listed below. A simple grid search of model hyperparameters was performed and the configuration below was chosen. This may not be an optimal configuration, but is the best that was found. We can define the configuration as a list; for example: We can tie all of this together. The complete code listing for the ConvLSTM model evaluated for one-step forecasting of the monthly car sales dataset is listed below. Running the example prints the RMSE for each repeated evaluation of the model. The final averaged RMSE is reported at the end of about 1,660, which is lower than the naive model, but still higher than a SARIMA model. It is a result that is perhaps on par with the CNN-LSTM model. The standard deviation of this score is also very large, suggesting that the chosen configuration may not be as stable as the standalone CNN model. A box and whisker plot is also created, summarizing the distribution of RMSE scores. Box and Whisker Plot of ConvLSTM RMSE Forecasting Car Sales This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop a suite of deep learning models for univariate time series forecasting. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hello . Andy. Ukraine. Kiev. In your example  CNN LSTM (whole code model) I have error.   File ¡°C:\Users\User\Dropbox\DeepLearning3\realmodproj\project\educationNew\sub2\step_005.py¡±, line 89, in model_fit
    model.add(TimeDistributed(Conv1D(filters=n_filters, kernel_size=n_kernel, activation=¡¯relu¡¯, input_shape=(None,n_steps,1))))
  File ¡°C:\Users\User\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\models.py¡±, line 454, in add
    raise ValueError(¡®The first layer in a ¡®
ValueError: The first layer in a Sequential model must get an input_shape or batch_input_shape argument.  Coud you explame me what the reason ? Thank you. I believe you need to update your version of Keras to 2.2.4 or higher. Using TensorFlow backend.
1.8.0 I recommend using tensorflow 1.11.0 or higher. Andy , Kiev May be i should use ?
model.add(TimeDistributed(   Conv1D(filters=n_filters, kernel_size=n_kernel, activation=¡¯relu¡¯,  input_shape=(None,n_steps,1) ) , input_shape=(None,n_steps,1) )  ) Sure, that was required for older versions of Keras. So, in a nutshell, RNN doesn¡¯t work very well when dealing with time series forecasting? Correct, they will be outperformed by linear methods on univariate data in most cases. Also see this:https://machinelearningmastery.com/findings-comparing-classical-and-machine-learning-methods-for-time-series-forecasting/ Thanks for the article
I have a univariate time series classification problem. Data comes from the accelerometer sensor (vibration analysis). only X-axis. and the sampling rate is 2.5 KHz and I need a real-time classification.
what¡¯s your suggestion for classifying this time series? I want to differentiate between multiple labeled classes (failures).
which of your articles can help me? I read all of them but many of them are not univariate and this article which is univariate is about forecasting and not classification.
thanks indeed. Sounds like time series classification. You could look at the HAR articles, for example:https://machinelearningmastery.com/how-to-develop-rnn-models-for-human-activity-recognition-time-series-classification/ Hi Jason and thanks for all of your amazing turorials on ML!
I am new to time-series forecasting, but I have been learning a lot from your articles.  I have a issue were am trying to forecast the amount of tickets sold per day, but I would like to the forecast 60 days before the actual date. I have 3 years of historical data on the tickets sold per day.  Testing the above models on my data, the persistence model scored the best (I think) with a 4.041 RMSE, the next best was the MLP with 7.501 (+/- 0.589)  RMSE.  Do you have any recommendation on how to proceed doing the forecast for my data?  Thanks again for your great work! Nice work. Yes, what is the problem that you are having exactly? Hey, Say I have show, and 60 days before the show I want to forecast how many tickets sold during those 60days.(t+60) If I look at the the 60days as one time period, the forecast would not consider seasonal effects, which have a big effect on the tickets sold. Therfore, I think I need to use each day during those 60 days as a period for a more accurate forecast.  So, I want to forecast the tickest sold each day during a 60 day period, but I want to know the forecast before that 60 day period starts.  And the output of the model is a list of 60 elements representing the expected ticket sale for each day  [0,1,7,5,3¡¦,3] My problem is choosing the correct approach for such forecast. Do you have any tips? That sounds like a very challenging problem. We cannot know the best approach. You must discover the best approach through experimentation with a suite of different methods. This might help:https://machinelearningmastery.com/how-to-develop-a-skilful-time-series-forecasting-model/ Comment  Name (required)  Email (will not be published) (required)  Website"
