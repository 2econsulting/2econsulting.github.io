"site","date","headline","url_address","text"
"vidhya",2018-10-11,"A Step-by-Step Introduction to the Basic Object Detection Algorithms (Part 1)","https://www.analyticsvidhya.com/blog/2018/10/a-step-by-step-introduction-to-the-basic-object-detection-algorithms-part-1/","

FLASH SALE: Course On Introduction To Data Science | Use COUPON CODE: FLASH25 for 25% Discount| Work on 4 real-life projects |
 Buy Now 

 How much time have you spent looking for lost room keys in an untidy and messy house? It happens to the best of us and till date remains an incredibly frustrating experience. But what if a simple computer algorithm could locate your keys in a matter of milliseconds? That is the power of object detection algorithms. While this was a simple example, the applications of object detection span multiple and diverse industries, from round-the-clock surveillance to real-time vehicle detection in smart cities. In short, these are powerful deep learning algorithms. In this article specifically, we will dive deeper and look at various algorithms that can be used for object detection. We will start with the algorithms belonging to RCNN family, i.e. RCNN, Fast RCNN and Faster RCNN. In the upcoming article of this series, we will cover more advanced algorithms like YOLO, SSD, etc. I encourage you to go through this<U+00A0>previous article on object detection, where we cover the basics of this wonderful technique and show you an implementation in Python using the ImageAI library. Let¡¯s get started! The below image is a popular example of illustrating how an object detection algorithm works. Each object in the image, from a person to a kite, have been located and identified with a certain level of precision. Let¡¯s start with the simplest deep learning approach, and a widely used one, for detecting objects in images <U+2013> Convolutional Neural Networks or CNNs. If your understanding of CNNs is a little rusty, I recommend going through<U+00A0>this article<U+00A0>first. But I¡¯ll briefly summarize the inner workings of<U+00A0> a CNN for you. Take a look at the below image: We pass an image to the network, and it is then sent through various convolutions and pooling layers. Finally, we get the output in the form of the object¡¯s class. Fairly straightforward, isn¡¯t it? For each input image, we get a corresponding class as an output. Can we use this technique to detect various objects in an image? Yes, we can! Let¡¯s look at how we can solve a general object detection problem using a CNN. 1. First, we take an image as input: 2. Then we divide the image into various regions: 3. We will then consider each region as a separate image.
4. Pass all these regions (images) to the CNN and classify them into various classes.
5. Once we have divided each region into its corresponding class, we can combine all these regions to get the original image with the detected objects: The problem with using this approach is that the objects in the image can have different aspect ratios and spatial locations. For instance, in some cases the object might be covering most of the image, while in others the object might only be covering a small percentage of the image. The shapes of the objects might also be different (happens a lot in real-life use cases). As a result of these factors, we would require a very large number of regions resulting in a huge amount of computational time. So to solve this problem and reduce the number of regions, we can use region-based CNN, which selects the regions using a proposal method. Let¡¯s understand what this region-based CNN can do for us. Instead of working on a massive number of regions, the RCNN<U+00A0>algorithm proposes a bunch of boxes in the image and checks if any of these boxes contain any object. RCNN uses selective search to extract these boxes from an image (these boxes are called regions). Let¡¯s first understand what selective search is and how it identifies the different regions. There are basically four regions that form an object: varying scales, colors, textures, and enclosure. Selective search identifies these patterns in the image and based on that, proposes various regions. Here is a brief overview of how selective search works: Below is a succint summary of the steps followed in RCNN to detect objects: You might get a better idea of the above steps with a visual example (Images for the example shown below are taken from this paper) . So let¡¯s take one! And this, in a nutshell, is how an RCNN helps us to detect objects. So far, we¡¯ve seen how RCNN can be helpful<U+00A0>for object detection. But this technique comes with its own limitations. Training an RCNN model is expensive and slow thanks to the below steps: All these processes combine to make RCNN very slow. It takes around 40-50 seconds to make predictions for each new image, which essentially makes the model cumbersome and practically impossible to build when faced with a gigantic dataset. Here¡¯s the good news <U+2013> we have another object detection technique which fixes most of the limitations we saw in RCNN. What else can we do to reduce the computation time a RCNN algorithm typically takes? Instead of running a CNN 2,000 times per image, we can run it just once per image and get all the regions of interest (regions containing some object). Ross Girshick, the author of<U+00A0>RCNN, came up with this idea of running the CNN just once per image and then finding a way to share that computation across the 2,000 regions. In Fast RCNN, we feed the input image to the CNN, which in turn generates the convolutional feature maps. Using these maps, the regions of proposals are extracted. We then use a RoI pooling layer to reshape all the proposed regions into a fixed size, so that it can be fed into a fully connected network. Let¡¯s break this down into steps to simplify the concept: So, instead of using three different models (like in<U+00A0>RCNN), Fast RCNN uses a single model which extracts features from the regions, divides them into different classes, and returns the boundary boxes for the identified classes simultaneously. To break this down even further, I¡¯ll visualize each step to add a practical angle to the explanation. This is how Fast RCNN resolves two major issues of RCNN, i.e., passing one instead of 2,000 regions per image to the ConvNet, and using one instead of three different models for extracting features, classification and generating bounding boxes. But even Fast RCNN<U+00A0>has certain problem areas. It also uses selective search as a proposal method to find the Regions of Interest, which is a slow and time consuming<U+00A0>process. It takes around 2 seconds per image to detect objects, which is much better compared to RCNN. But when we consider large real-life datasets, then even a Fast RCNN doesn¡¯t look so fast anymore. But there¡¯s yet another object detection algorithm that trump Fast RCNN. And something tells me you won¡¯t be surprised by it¡¯s name. Faster RCNN is the modified version of Fast RCNN. The major difference between them is that Fast RCNN uses selective search for generating Regions of Interest, while Faster RCNN uses ¡°Region Proposal Network¡±, aka RPN. RPN takes image feature maps as an input and generates a set of object proposals, each with an objectness score as output. The<U+00A0> below steps are typically followed in a Faster RCNN<U+00A0>approach: Let me briefly explain how this Region Proposal Network (RPN) actually works. To begin with, Faster RCNN takes the feature maps from CNN and passes them on to the Region Proposal Network. RPN uses a sliding window over these feature maps, and at each window, it generates k Anchor boxes of different shapes and sizes: Anchor boxes are fixed sized boundary boxes that are placed throughout the image and have different shapes and sizes. For each anchor, RPN predicts two things: We now have bounding boxes of different shapes and sizes which are passed on to the RoI pooling layer. Now it might be possible that after the RPN step, there are proposals with no classes assigned to them. We can take each proposal and crop it so that each proposal contains an object. This is what the RoI pooling layer does. It extracts fixed sized feature maps for each anchor: Then these feature maps are passed to a fully connected layer which has a softmax and a linear regression layer. It finally classifies the object and predicts the bounding boxes for the identified objects. All of the object detection algorithms we have discussed so far use regions to identify the objects. The network does not look at the complete image in one go, but focuses on parts of the image sequentially. This creates two complications: The below table is a nice summary of all the algorithms we have covered in this article. I suggest keeping this handy next time you¡¯re working on an object detection challenge! Object detection is a fascinating field, and is rightly seeing a ton of traction in commercial, as well as research applications. Thanks to advances in modern hardware and computational resources, breakthroughs in this space have been quick and ground-breaking. This article is just the beginning of our object detection journey. In the next article of this series, we will encounter modern object detection algorithms such as YOLO and RetinaNet. So stay tuned! I always appreciate any feedback or suggestions on my articles, so please feel free to connect with me in the comments section below. Very interesting paper. Good job!!! Thank You Jacques! Excellent Article..Really its helpfull for beginers..Thanks a lot..!!"
"vidhya",2018-10-08,"An Introduction to Random Forest using the fastai Library (Machine Learning for Programmers <U+2013> Part 1)","https://www.analyticsvidhya.com/blog/2018/10/comprehensive-overview-machine-learning-part-1/","

FLASH SALE: Course On Introduction To Data Science | Use COUPON CODE: FLASH25 for 25% Discount| Work on 4 real-life projects |
Buy Now 

 Programming is a crucial prerequisite for anyone wanting to learn machine learning. Sure quite a few autoML tools are out there, but most are still at a very nascent stage and well beyond an individual¡¯s budget. The sweet spot for a data scientist lies in combining programming with machine learning algorithms. Fast.ai is led by the amazing partnership of Jeremy Howard and Rachel Thomas. So when they recently released their machine learning course, I couldn¡¯t wait to get started. What I personally liked about this course is the top-down approach to teaching. You first learn how to code an algorithm in Python, and then move to the theory aspect. While not a unique approach, it certainly has it¡¯s advantages. While going these videos, I decided to curate my learning in the form of a series of articles for our awesome community! So in this first post, I have provided a comprehensive summary (including the code) of the first two videos where Jeremy Howard teaches us how to build a random forest model using the fastai library, and how tuning the different hyperparameters can significantly alter our model¡¯s accuracy. You need to have a bit of experience in Python to follow along with the code.<U+00A0>So if you¡¯re a beginner in machine learning and have not used Python and Jupyter Notebooks before, I recommend checking out the below two resources first: The video lectures are available on YouTube and the course has been divided into twelve lectures as per the below structure: This course assumes that you have Jupyter Notebook installed on your machine. In case you don¡¯t (and don¡¯t prefer installing it either), you can choose any of the following (these have a nominal fee attached to them): All the Notebooks associated with each lecture are available on fast.ai¡¯s GitHub repository. You can clone or download the entire repository in one go. You can locate the full installation steps under the to-install section. Ready to get started? Then check out the<U+00A0>Jupyter Notebook and the below video for the first lesson. In this lecture, we will learn how to build a random forest model in Python. Since a top-down approach is followed in this course, we will go ahead and code first while simultaneously understanding how the code work. We¡¯ll then look into the inner workings of the random forest algorithm. Let¡¯s deep dive into what this lecture covers. The above two commands will automatically modify the notebook when the source code is updated. Thus, using ext_autoreload<U+00A0>will automatically and dynamically make the changes in your notebook. Using %matplotlib inline, we can visualize the plots inside the notebook. Using import* will import everything in the fastai library. Other necessary libraries have also been imported for reading the dataframe summary, creating random forest models and metrics for calculating the RMSE (evaluation metric). The dataset we¡¯ll be using is the ¡®Blue Book for Bulldozers¡¯. The problem statement for this challenge is described below: The goal is to predict the sale price of a particular piece of heavy equipment at an auction, based on its usage, equipment type, and configuration. <U+00A0>The data is sourced from auction result postings and includes information on usage and equipment configurations. Fast Iron is creating a ¡°blue book for bulldozers¡±, for customers to value what their heavy equipment fleet is worth at an auction. The evaluation metric is RMSLE (root mean squared log error). Don¡¯t worry if you haven¡¯t heard of it before, we¡¯ll understand and deal with it during the code walk-through. Assuming you have successfully downloaded the dataset, let¡¯s move on to coding! This command is used to set the location of our dataset. We currently have the downloaded dataset stored in a folder named bulldozers within the data folder. To check what are the files inside the PATH, you can type: Or, The dataset provided is in a .csv format. This is a structured dataset, with columns representing a range of things, such as ID, Date, state, product group, etc. For dealing with structured data, pandas is the most important library. We already imported pandas as pd when we used the import* command earlier. We will now use the read_csv function of pandas to read the data : Let us look at the first few rows of the data: Since the dataset is large, this command does not show us the complete column-wise data. Instead, we will see some dots for the data that isn¡¯t being displayed (as shown in the screenshot): To fix this, we will define the following function, where we set max.rows and max.columns to 1000. We can now print the head of the dataset using this newly minted function. We have taken the transpose to make it visually appealing (we see column names as the index). Remember the evaluation metric is RMSLE <U+2013> which is basically the RMSE between the log values of the result. So we will transform the target variable by taking it¡¯s log values. This is where the popular library<U+00A0>numpy comes to the rescue. The concept of how a Random Forest model works from scratch will be discussed in detail in the later sections of the course, but here is a brief introduction in Jeremy Howard¡¯s words: Sounds like a smashing technique, right? RandomForestRegressor and RandomForestClassifier<U+00A0>functions are used in Python for regression and classification problems respectively. Since we¡¯re dealing with a regression challenge, we will stick to the RandomForestRegressor. The m.fit function takes two inputs: The target variable here is<U+00A0>df_raw.SalePrice. The independent variables are all the variables except SalePrice. Here, we are using<U+00A0>df_raw.drop to drop the SalePrice column (axis = 1 represents column). This would throw up an error like the one below: This suggests that the model could not deal with the value ¡®Conventional¡¯. Most machine learning models (including random forest) cannot directly use categorical columns. We need to convert these columns into numbers first. So naturally the next step is to convert all the categorical columns into continuous variables. Let¡¯s take each categorical column individually. First, consider the saledate column which is of datetime format. From the date column, we can extract numerical values such as <U+2013> year, month, day of month, day of the week, holiday or not, weekend or weekday, was it raining?, etc. We¡¯ll leverage the<U+00A0>add_datepart<U+00A0>function from the fastai library to create these features for us. The function creates the following features: Let¡¯s run the function and check the columns: The next step is to convert the categorical variables into numbers. We can use the train_cats function from fastai for this: While converting categorical to numeric columns, we have to take the following two issues into consideration: Although this won¡¯t make much of a difference in our current case since random forest works on splitting the dataset (we will understand how random forest works in detail in the shortly), it¡¯s still good to know this for other algorithms. The next step is to look at the number of missing values in the dataset and understand how to deal with them. This is a pretty widespread challenge in both machine learning competitions and real-life industry problems. We use .isnull().sum() to get the total number of missing values. This is divided by the length of the dataset to determine the ratio of missing values. The dataset is now ready to be used for creating a model. Data cleaning is always a tedious and time consuming process. Hence, ensure to save the transformed dataset so that the next time we load the data, we will not have to perform the above tasks again. We will save it in a feather format, as this let¡¯s us access the data efficiently: We have to impute the missing values and store the data as dependent and independent part. This is done by using the fastai function proc_df. The function performs the following tasks: We have dealt with the categorical columns and the date values. We have also taken care of the missing values. Now we can finally power up and build the random forest model we have been inching towards. The n_jobs is set to -1 to use all the available cores on the machine. This gives us a score (r^2) of 0.98, which is excellent. The caveat here is that we have trained the model on the training set, and checked the result on the same. There¡¯s a high chance that this model might not perform as well on unseen data (test set, in our case). The only way to find out is to create a validation set and check the performance of the model on it. So let¡¯s create a validation set that contains 12,000 data points (and the train set will contain the rest). Here, we will train the model on our new set (which is a sample of the original set) and check the performance across both <U+2013> train and validation sets. In order to compare the score against the train and test sets, the below function returns the RMSE value and score for both datasets. The result of the above code is shown below. The train set has a score of 0.98, while the validation set has a score of 0.88. A bit of a drop-off, but the model still performed well overall. Now that you know how to code a random forest model in Python, it¡¯s equally important to understand how it actually works underneath all that code. Random forest is often cited as a black box model, and it¡¯s time to put that misconception to bed. We observed in the first lesson that the model performs extremely well on the training data (the points it has seen before) but dips when tested on the validation set (the data points model was not trained on). Let us first understand how we created the validation set and why it¡¯s so crucial. Creating a good validation set that closely resembles the test set is one of the most important tasks in machine learning. The validation score is representative of how our model performs on real-world data, or on the test data. Keep in mind that if there¡¯s a<U+00A0>time component involved, then the most recent rows should be included in the validation set. So, our validation set will be of the same size as the test set (last 12,000 rows from the training data). The data points from 0 to (length <U+2013> 12000) are stored as the train set (x_train, y_train). A model is built using the train set and its performance is measured on both the train and validation sets as before. From the above code, we get the results: It¡¯s clear that the model is overfitting on the training set. Also, it takes a smidge over 1 minute to train. Can we reduce the training time? Yes, we can! To do this, we will further take a subset of the original dataset: A subset of 30,000 samples has been created from which we take 20,000 for training the Random Forest model. Random Forest is a group of trees which are called estimators. The number of trees in a random forest model is defined by the parameter n_estimator. We will first look at a single tree (set n_estimator = 1) with a maximum depth of 3. Plotting the tree: The tree is a set of binary decisions. Looking at the first box, the first split is on coupler-system value: less than/equal to 0.5 or greater than 0.5. After the split, we get 3,185 rows with coupler_system>0.5 and remaining 16,815 with <0.5. Similarly, next split is on enclosure and Year_made. For the first box, a model is created using only the average value (10.189). This means that all the rows have a predicted value of 10.189 and the MSE (Mean Squared Error) for these predictions is 0.459. Instead, if we make a split and separate the rows based on coupler_system <0.5, the MSE is reduced to 0.414 for samples satisfying the condition (true) and 0.109 for the remaining samples. So how do we decide which variable to split on? The idea is to split the data into two groups which are as different from each other as possible. This can be done by checking each possible split point for each variable, and then figuring out which one gives the lower MSE. To do this, we can take a weighted average of the two MSE values after the split. The splitting stops when it either reaches the pre-specified<U+00A0>max_depth<U+00A0>value or when each leaf node has only one value. We have a basic model <U+2013> a single tree, but this is not a very good model. We need something a bit more complex that builds upon this structure. For creating a forest, we will use a statistical technique called bagging. In the bagging technique, we create multiple models, each giving predictions which are not correlated to the other. Then we average the predictions from these models. Random Forest is a bagging technique. If all the trees created are similar to each other and give similar predictions, then averaging these predictions will not improve the model performance. Instead, we can create multiple trees on a different subset of data, so that even if these trees overfit, they will do so on a different set of points. These samples are taken with replacement. In simple words, we create multiple poor performing models and average them to create one good model. The individual models must be as predictive as possible, but together should be uncorrelated. We will now increase the number of estimators in our random forest and see the results. If we do not give a value to the n_estimator parameter, it is taken as 10 by default. We will get predictions from each of the 10 trees. Further, np.stack will be used to concatenate the predictions one over the other. The dimensions of the predictions is (10, 12000) . This means we have 10 predictions for each row in the validation set. Now for comparing our model¡¯s results against the validation set, here is the row of predictions, the mean of the predictions and the actual value from validation set. The actual value is 9.17 but none of our predictions comes close to this value. On taking the average of all our predictions we get 9.07, which is a better prediction than any of the individual trees. It¡¯s always a good idea to visualize your model as much as possible. Here is a plot that shows the variation in r^2 value as the number of trees increases. As expected, the r^2 becomes better as the number of trees increases. You can experiment with the n_estimator value and see how the r^2 value changes with each iteration. You¡¯ll notice that after a certain number of trees, the r^2 value plateaus. Creating a separate validation set for a small dataset can potentially be a problem since it will result in an even smaller training set. In such cases, we can use the data points (or samples) which the tree was not trained on. For this, we set the parameter oob_score =True. The oob_score is 0.84 which is close to that of the validation set. Let us look at some other interesting techniques by which we can improve our model. Earlier, we created a subset of 30,000 rows and the train set was randomly chosen from this subset. As an alternative, we can create a different subset each time so that the model is trained on a larger part of the data. We use set_rf_sample to specify the sample size. Let us check if the performance of the model has improved or not. We get a validation score of 0.876. So far, we have worked on a subset of one sample. We can fit this model on the entire dataset (but it will take a long time to run depending on how good your computational resources are!). This can be treated as the stopping criteria for the tree. The tree stops growing (or splitting) when the number of samples in the leaf node is less than specified. Here we have specified the min_sample_leaf<U+00A0>as 3. This means that the minimum number of samples in the node should be 3 for each split. We see that the r^2 has improved for the validation set and reduced on the test set, concluding that the model does not overfit on the training data. Another important parameter in random forest is max_features. We have discussed previously that the individual trees must be as uncorrelated as possible. For the same, random forest uses a subset of rows to train each tree. Additionally, we can also use a subset of columns (features) instead of using all the features. This is achieved by tweaking the<U+00A0>max_features<U+00A0>parameter. Setting max_features has slightly improved the validation score. Here the max_features is set to 0.5 which means using 50% of the features for each split. Keep in mind that this parameter can also take values like log2 or sqrt. Jeremy Howard mentioned a few tips and tricks for navigating Jupyter Notebooks which newcomers will find quite useful. Below are some of the highlights: The curse of dimensionality is the idea that the more dimensions we have, the more points sit on the edge of that space. So if the number of columns is more, it creates more and more empty space. What that means, in theory, is that the distance between points is much less meaningful. This should not be true because the points still are different distances away from each other. Even though they are on the edges, we can still determine how far away they are from each other. The evaluation metric of our dataset is RMSLE. The formula for this is We first take the mean of squared differences of log values. We take a square root of the result obtained. This is equivalent to calculating the root mean squared error (rmse) of log of the values. Here is the mathematical formula for R-square: The value of R-square can be anything less than 1. If the r square is negative, it means that your model is worse than predicting mean. In scikit-learn, we have another algorithm ExtraTreeClassifier which is extremely randomized tree model. Unlike Random forest, instead of trying each split point for every variable, it randomly tries a few split points for a few variables. This article was a pretty comprehensive summary of the first two videos from fast.ai¡¯s machine learning course. During the first lesson we learnt to code a simple random forest model on the bulldozer dataset. Random forest (and most ml algorithms) do not work with categorical variables. We faced a similar problem during the random forest implementation and we saw how can we use the date column and other categorical columns in the dataset for creating a model. In the second video, the concept of creating a validation set was introduced. We then used this validation set to check the performance of the model and tuned some basic hyper-parameters to improve the model. My favorite part from this video was plotting and visualizing the tree we built. I am sure you would have learnt a lot through these videos. I will shortly post another article covering the next two videos from the course. Good Article Aishwarya, nice explanation. Thank you <U+0001F642> Hi..Thanks for putting this into words..really helpful.
I am facing issue with df.to_feather(¡®tmp/bulldozers-raw¡¯).
Not able to install feather library. Any suggestions/pointers?"
"vidhya",2018-10-08,"Simplifying Data Preparation and Machine Learning Tasks using RapidMiner","https://www.analyticsvidhya.com/blog/2018/10/rapidminer-data-preparation-machine-learning/","

FLASH SALE: Course On Introduction To Data Science | Use COUPON CODE: FLASH25 for 25% Discount| Work on 4 real-life projects |
Buy Now 

 It¡¯s a well-known fact that we spend too much time on data preparation and not as much time as we want on building cool machine learning models. In fact, a Harvard Business Review publication confirmed what we always knew:<U+00A0>analytics teams spend 80%<U+00A0>of their time preparing data. And they are typically slowed down by clunky data preparation tools coupled with a scarcity of data science experts. But not for much longer, folks! RapidMiner recently released a really nice functionality for data preparation, RapidMiner Turbo Prep. You will soon know why we picked this name <U+0001F642>, but the basic idea is that Turbo Prep provides a new data preparation experience that is fast and fun to use with a drag and drop interface. Let¡¯s walk through some of the possibilities of this feature, as well as demonstrate how it integrates with<U+00A0>RapidMiner Auto Model, our automated machine learning product. These two features truly make data prep and machine learning fast, fun, and simple. If you would like to follow along, make sure you have<U+00A0>RapidMiner Studio 9.0 downloaded.<U+00A0>All free users have access to Auto Model and Turbo Prep for 30 days. First, we¡¯re going to start by loading some data. Data can be added from all repository-based sources or be imported from your local machine. RapidMiner Turbo Prep start screen In this example, we¡¯re using a dataset of all the domestic flights leaving from New England in 2007, roughly 230,000 rows. You can find this dataset inside Studio¡¯s pre-installed repository. Click ¡®Load Data¡¯ / ¡®Community Samples¡¯ / ¡®Community Data Sets¡¯ / ¡®Transportation¡¯. Loading sample data sets Once you load the data it can be seen immediately in a data-centric view, along with some data quality indicators. At the top of the columns, the distributions and quality measurements of the data are displayed. These indicate whether the columns will be helpful for machine learning and modeling. Say, for example, the majority of the data in a column is missing, this could confuse a machine learning model, so it is often better to remove it all together. If the column acts as an ID, that means practically all of the values only occur once in the data set, so this not useful for identifying patterns, and also should be removed. Data centric view of RapidMiner Turbo Prep As a first step, in order to look at the data in aggregate, we are going to create a pivot table. To generate this pivot table, first, we will look at the airport codes, indicated by ¡®Origin¡¯, with the airport name ¡®OriginName¡¯, and calculate the average delay at these locations. We can see the result immediately by dragging ¡®DepDelay¡¯ into the ¡®Aggregates¡¯ area, which calculates the average. In this case, the biggest delays are happening at the Nantucket airport, which is a very small airport; there is a high average delay of more than 51 minutes. In order to take the number of flights into account, we will also add in ¡®Origin count¡¯ and sort to show the largest airport by flight. In this case, Boston Logan Airport is the largest with almost 130,000 flights. Pivot table in RapidMiner Turbo Prep This pivot table helped us quickly determine that we should focus on Boston Logan, so we will exit out of this view and go back to the original data we started with. Now, to only show ¡®BOS¡¯ flight data: select the ¡®Origin¡¯ column, right click, and select ¡®Transformations¡¯, then ¡®Filter¡¯. Immediately, there will be a preview of the data, so you know whether the results are correct. All the statistics and quality measurements are updated as well. Applying a filter Next, we¡¯re going to bring in some additional data about the weather in New England for the same year. This data set can be found in the same ¡®Transportation¡¯ folder as the flight data. We know from personal experience, that weather can create delays, so we want to add this data in to see if the model picks up on it. In a longer scenario, we might take a look at the flight data alone at first and discover that the model is 60% accurate. Then add in the weather information and see how the accuracy of the model improves. But for this demonstration, we will go straight to adding in the weather. In this data, there is a single ¡®Date¡¯ column but in our flight data there were two columns, one for the day and one for the month, so we¡¯ll need to transform the weather data to match. Single ¡®Date¡¯ Column in weather data Start the transformation by copying the ¡®Date¡¯ column so there are two duplicate columns next to each other. Then rename the columns to ¡®W_Day¡¯ and ¡®W_Month¡¯ for consistency. Copied and renamed ¡®Date¡¯ columns in weather data Then we need to split the data from these columns. To do so, click on the ¡®W_Day¡¯ column and select ¡®Change Type¡¯ which will display ¡®Change to number¡¯ with the option to ¡®Extract¡¯. In this case we need to extract the day relative to the month and click ¡®Apply¡¯. In the case of the ¡®W_Month¡¯ column, we need to follow the same steps, except we need to extract the month relative to the year and click ¡®Apply¡¯. Once the results look good, we commit the transformation. Extracting the day from the month Extracting the month from the year Now,<U+00A0>we need to merge the two data sets together.<U+00A0>Turbo Prep uses<U+00A0>smart algorithms<U+00A0>to intelligently identify data matches.<U+00A0>Two data sets are a good match if they have two columns that match with each other.<U+00A0>And two columns match well with each other if they contain similar values.<U+00A0>In this example, we see a pretty high match of 94%. % match of the two data sets Now<U+00A0>if we would like to join on the airport code, we select merge type ¡®Inner Join¡¯ and ¡®AirportCode¡¯ from the dropdown,<U+00A0>and it ranks the best matching columns. The best match is the<U+202F>¡®Origin¡¯<U+202F>code column in the other data set, which is a good<U+00A0>sign. Next, we pick the month and the month pops up to the top showing it¡¯s the best match.<U+202F>Last, we select day and ¡®DayofMonth¡¯,<U+00A0>which<U+00A0>is at the top of the list as the best match.<U+00A0>This is helpful to make sure that the joins and merges deliver the correct results.<U+00A0>Clicking ¡®Update preview¡¯ will show us<U+00A0>the three<U+00A0>joined keys<U+00A0>in purple, all of the weather information<U+00A0>in blue, and all of the original flight information for Boston Logan<U+00A0>in green. Merged data view Next, we<U+00A0>will<U+00A0>generate a new column<U+00A0>based<U+00A0>on<U+00A0>existing information<U+00A0>in our data sets. The data in the ¡®DepDelay¡¯ column<U+00A0>indicates the number of minutes the flight was delayed.<U+00A0>If a flight is a minute late, we would not (for this purpose)<U+00A0>actually<U+00A0>consider that to be delayed so this column, as is,<U+00A0>isn¡¯t all that important to us.<U+00A0>What we<U+00A0>want<U+00A0>to do is<U+00A0>use this column to<U+00A0>define<U+00A0>what a delay is.<U+00A0>For this example, we will consider any flights more than 15 minutes<U+00A0>late<U+00A0>as a delay.<U+00A0>To<U+00A0>generate<U+00A0>this<U+00A0>new column,<U+00A0>we<U+00A0>will click ¡®Generate¡¯ and<U+00A0>will start by<U+00A0>creating a<U+00A0>new column called ¡®Delay Class¡¯. Next, we can<U+00A0>either<U+00A0>drag in<U+00A0>or type out,<U+00A0>the formula of ¡®if()¡¯. The drag in, or type out ¡®DepDelay¡¯ where<U+00A0>a delay<U+00A0>greater<U+00A0>than fifteen minutes<U+00A0>is<U+00A0>true,<U+00A0>and<U+00A0>the rest<U+00A0>is<U+00A0>false. Ultimately, the formula will read, ¡®if([DepDelay]>15,TRUE,FALSE)¡¯. Then,<U+00A0>we want to update the preview to see the amount of false versus<U+00A0>true. In our case, the formula seems to work, so we commit the Generate and the new column is added. Generating a ¡®Delay Class¡¯ column The last step before Modeling,<U+00A0>here,<U+00A0>is cleansing our data. When we first saw our data, we could see a couple of data quality issues indicated, for example,<U+00A0>in the ¡®Cancellation¡¯ column, so we know that needs to be addressed. We could go through the data column by column,<U+00A0>or we could use the ¡®Auto Cleansing¡¯ feature.<U+00A0>Clicking on that feature will pop up a dialogue box, prompting us to identify what we would like to predict. RapidMiner Turbo Prep auto cleansing option Defining a target in auto cleanse Based on that<U+00A0>selection, RapidMiner suggests and selects the columns that should be removed. These are suggested because there is too much data missing or because the data is too stable, for example.<U+00A0>By simply clicking ¡®Next¡¯ those columns are removed. There are more ways to improve the data quality, but that step is the only one we will use for this example,<U+00A0>leaving<U+00A0>the rest to the default settings. Then,<U+00A0>we click ¡®Commit Cleanse¡¯. Removing columns with quality issues in auto cleanse We made quite a few changes and we can review them all by clicking on ¡®History¡¯, which shows all of the individual steps we took.<U+00A0>If we want to, we can<U+00A0>click on one of the steps and decide to roll back the changes before that step or create a copy of the rollback step. History view Possibly the most exciting aspect of Turbo Prep is that we<U+00A0>can<U+00A0>see the full data preparation process by clicking ¡®Process¡¯, leading to a fully annotated view in<U+00A0>RapidMiner<U+00A0>Studio.<U+00A0>There are no black boxes in RapidMiner.<U+00A0>We<U+00A0>can see every step and can make edits and changes as necessary.<U+00A0>Whenever we see a model that we like, we can click on it and can open the process. The process is generated with annotations, so we get a full explanation of what happens.<U+00A0>We can save this, we can apply it on new data sets, say the flight data from 2008,<U+00A0>and<U+00A0>we can share this process with our colleagues, or we can deploy it and run it<U+00A0>frequently. Process view in RapidMiner Studio The results can also be exported into the RapidMiner repositories,<U+00A0>into<U+00A0>various file formats, or<U+00A0>it can be handed over to RapidMiner Auto Model for immediate model creation.<U+00A0>In this case, we are going to explore building<U+00A0>some quick<U+00A0>models<U+00A0>using<U+00A0>RapidMiner Auto Model<U+00A0>by<U+00A0>simply<U+00A0>clicking on the<U+00A0>¡®Auto Model¡¯<U+00A0>tab. RapidMiner Auto Model From here, we<U+00A0>are<U+00A0>able to<U+00A0>build some clustering, segmentation, or outlier predictions. In this case, we want to predict<U+00A0>the ¡®Delay Class¡¯<U+00A0>column.<U+00A0>To do that, we just click on the ¡®Delay Class¡¯ and<U+00A0>¡®Predict¡¯ is already selected so we continue on and click ¡®Next¡¯. Predicting the ¡®Delay Class¡¯ In the ¡®Prepare Target¡¯ view, we can choose to map values or change the class of highest<U+00A0>interest,<U+00A0>but we are most interested in predicting delays, so we will keep the<U+00A0>default<U+00A0>settings here.<U+00A0> Prepare target view in RapidMiner Auto Model On the next<U+00A0>screen, we see<U+00A0>those quality measurements<U+00A0>are visible again, and<U+00A0>we see that<U+00A0>there are no red columns in this overview.<U+00A0>That¡¯s<U+00A0>because we did the auto cleansing already in Turbo Prep.<U+00A0>But we do still have a couple of suspicious columns marked in yellow. It is<U+00A0>important<U+00A0>that Auto Model is pointing out the ¡®DepDelay¡¯ as a suspicious column because this is the column that we used to create our predictions. If you recall,<U+00A0>when the<U+00A0>¡®DepDelay¡¯ is greater than 15 minutes<U+00A0>late<U+00A0>then this is<U+00A0>a delay, otherwise, it is not.<U+00A0>If we kept<U+00A0>this in,<U+00A0>all of<U+00A0>the models would focus on that one column and that is not what we want to base our predictions on, so we have to remove the column.<U+00A0>In this case, we are also going to remove the other two suspicious columns<U+00A0>by clicking ¡®Deselect Yellow¡¯<U+00A0>but those could stay in. This<U+00A0>is an important feature of Turbo Prep and Auto Model,<U+00A0>while we automate as much as we can, we still give the option to overwrite the recommendations. Removing suspicious columns in yellow With all three suspicious columns deselected, we click ¡®Next¡¯ and move on to the ¡®Model Types¡¯ view.<U+00A0>In this view, we see a couple of models selected already<U+00A0>(suggested by Auto Model), Naive Bayes and GLM<U+00A0>and<U+00A0>we can choose to see Logistic Regression as well here. Selecting the model types In a few seconds, we see the<U+00A0>Naive<U+00A0>Bayes model and can start inspecting it<U+00A0>by clicking on ¡®Model¡¯ underneath ¡®Naive Bayes¡¯ in the Results window.<U+00A0>Here we have a visual way to inspect the model, so, for example, the<U+00A0>¡®ActualLapsedTime¡¯ attribute<U+00A0>isn¡¯t super helpful, but<U+00A0>we can dropdown<U+00A0>and select ¡®Min Humidity¡¯ instead and start to see that the two classes differ a bit. Actual Lapsed Time Min Humidity There¡¯s another way to see this information<U+00A0>as well,<U+00A0>through<U+00A0>Auto Model, by clicking on ¡®Simulator¡¯ underneath ¡®Model¡¯ in the Results window. Here<U+00A0>we can experiment with the model a bit.<U+00A0>Right off the bat, we see that for the average inputs for our model, it¡¯s more likely<U+00A0>that the flight will be delayed. And<U+00A0>then<U+00A0>we can make some changes. Visibility seems to be pretty important, indicated by the length of the gray bar beneath the class name,<U+00A0>so<U+00A0>let¡¯s<U+00A0>change the visibility a little bit<U+00A0>by<U+00A0>reducing<U+00A0>it, which<U+00A0>makes it even more likely that the flight is delayed. Naive Bayes simulator with average inputs Naive Bayes simulator with decreased visibility In ¡®Overview¡¯ we can see how well the different models performed, here we see that GLM and Logistic Regression performed better than Naive Bayes.<U+00A0>We could also<U+00A0>look at the ROC Comparison, or the individual Model Performance and Lift Chart. Auto Model results overview Finally, you can see the data itself, under ¡®General¡¯ and the most important influence factors<U+00A0>by clicking on<U+00A0>¡®Weights¡¯.<U+00A0>Here the most influential factor is if the incoming aircraft is delayed, which makes sense. We may want<U+00A0>to consider taking that out because it might not be something that we can influence but we will keep it in for now. Important influence factors And just<U+00A0>like Turbo Prep, Auto Model processes<U+00A0>can be opened<U+00A0>in RapidMiner Studio,<U+00A0>showing<U+00A0>the<U+00A0>full<U+00A0>process<U+00A0>with annotations.<U+00A0>With Auto Model, every step is explained with its importance and why certain predictions are made during model creation. We can see exactly how the full model was created; there are no black boxes! Auto Model process Through this demonstration,<U+00A0>we¡¯ve shown<U+00A0>that<U+00A0>Turbo Prep<U+00A0>is an incredibly exciting and useful new capability, radically simplifying<U+00A0>and accelerating<U+00A0>the time-consuming data preparation task.<U+00A0>We demonstrated that it makes it easy to<U+00A0>quickly extract, join, filter, group, pivot, transform and cleanse data.<U+00A0>You can also connect to<U+00A0>a variety of sources like relational databases, spreadsheets, applications, social media, and more.<U+00A0> You can also create repeatable data prep steps, making it faster to reuse processes. Data can also be saved as Excel or CSV or sent to data visualization products like Qlik.<U+00A0> We also<U+00A0>demonstrated that once we¡¯re<U+00A0>ready to build predictive models with<U+00A0>the<U+00A0>newly<U+00A0>transformed data<U+00A0>it¡¯s simple to<U+00A0>jump into<U+00A0>Auto Model with<U+00A0>just<U+00A0>one click.<U+00A0>RapidMiner<U+00A0>Auto Model, unlike any other<U+00A0>tool<U+00A0>available in the market,<U+00A0>automates machine learning<U+00A0>by<U+00A0>leveraging a decade of data science<U+00A0>wisdom<U+00A0>so you<U+00A0>can focus on<U+00A0>quickly<U+00A0>unlocking valuable insights from your data.<U+00A0>And best of all,<U+202F>there are no black boxes, we can always see exactly what happened in the background and we can replicate it.<U+00A0><U+00A0> If you haven¡¯t tried<U+00A0>these two features yet,<U+00A0>we¡¯re offering a 30-day trial of Studio Large to all free users<U+00A0>so<U+00A0>download it now.<U+00A0><U+00A0><U+00A0> RapidMiner brings artificial intelligence to the enterprise through an open and extensible data science platform. Built for analytics teams, RapidMiner unifies the entire data science lifecycle from data prep to machine learning to predictive model deployment. 400,000 analytics professionals use RapidMiner products to drive revenue, reduce costs, and avoid risks. For more information visit www.rapidminer.com."
