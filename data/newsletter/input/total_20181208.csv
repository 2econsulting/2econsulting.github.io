"","site","date","headline","url_address","text","keyword"
"1","kaggle",2018-12-05,"Announcing Kaggle integration with Google Data Studio","http://blog.kaggle.com/2018/12/05/announcing-kaggle-integration-with-google-data-studio/","As of today, Kaggle is now officially integrated with Data Studio, Google¡¯s serverless business intelligence and data visualization platform. You¡¯ll be able to connect to and visualize Kaggle datasets directly from Data Studio using the Kaggle Community Connector. Kaggle is the world's largest online community of data scientists. Its two million users come to the platform to explore, create, and share data projects using Kernels, a free hosted notebook IDE. Over 10,000 public datasets can be analyzed using Kernels, accessed via the web or Kaggle's public API. This new integration, users can analyze these datasets in Kaggle; and then visualize findings and publish their data stories using Data Studio. Here¡¯s an example dashboard in Data Studio using a Kaggle dataset: As a free-to-use reporting solution, Data Studio makes it easier for users to understand their data, derive key insights, and effectively communicate findings using compelling interactive dashboards. Data Studio is creating an innovative landscape where users can spend less time building their data pipeline and more time on creating data stories and sharing them with the right audience. With this integration, users can browse to a dataset in Kaggle, pick a file, and use the one-click integration button to launch Data Studio with the selected data. From there, users can create and publish their own interactive dashboard, which can be embedded in websites and blogs. Since there is no cost to use Data Studio and the infrastructure is handled by Google, users don't have to worry about scalability, even if millions of people view the dashboard. Here¡¯s a quick clip showing how easy it is to use this integration to build dashboards in Data Studio: See Connecting Kaggle Datasets to Data Studio to learn more. Data Studio helps data professionals bring the power of visual analytics to their data. The hassle-free publishing process means everyone can tell engaging stories, open up dashboards for others to interact with, and make better-informed decisions. We're also releasing the connector code for this integration in the Data Studio Open Source Repository. This should help both Data Studio developers and Kaggle users to build newer and better solutions. To get started, try out a Kaggle maintained dataset and launch the Kaggle connector for Data Studio. Let¡¯s analyze more data and build awesome dashboards! Minhaz Kazi (@_mkazi_), Developer Advocate, Data StudioMegan Risdal (@meganrisdal), Product Lead, Kaggle Datasets","Keyword(freq): user(8), dataset(5), dashboard(4), story(3), finding(2), kernel(2), analytics(1), blog(1), decision(1), developer(1)"
"2","mastery",2018-12-07,"A Gentle Introduction to Early Stopping to Avoid Overtraining Deep Learning Neural Network Models","https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/","A major challenge in training neural networks is how long to train them. Too little training will mean that the model will underfit the train and the test sets. Too much training will mean that the model will overfit the training dataset and have poor performance on the test set. A compromise is to train on the training dataset but to stop training at the point when performance on a validation dataset starts to degrade. This simple, effective, and widely used approach to training neural networks is called early stopping. In this post, you will discover that stopping the training of a neural network early before it has overfit the training dataset can reduce overfitting and improve the generalization of deep neural networks. After reading this post, you will know: Let¡¯s get started. A Gentle Introduction to Early Stopping for Avoiding Overtraining Neural Network ModelsPhoto by Benson Kua, some rights reserved. This tutorial is divided into five parts; they are: Training neural networks is challenging. When training a large network, there will be a point during training when the model will stop generalizing and start learning the statistical noise in the training dataset. This overfitting of the training dataset will result in an increase in generalization error, making the model less useful at making predictions on new data. The challenge is to train the network long enough that it is capable of learning the mapping from inputs to outputs, but not training the model so long that it overfits the training data. However, all standard neural network architectures such as the fully connected multi-layer perceptron are prone to overfitting [10]: While the network seems to get better and better, i.e., the error on the training set decreases, at some point during training it actually begins to get worse again, i.e., the error on unseen examples increases. <U+2014> Early Stopping <U+2013> But When?, 2002. One approach to solving this problem is to treat the number of training epochs as a hyperparameter and train the model multiple times with different values, then select the number of epochs that result in the best performance on the train or a holdout test dataset. The downside of this approach is that it requires multiple models to be trained and discarded. This can be computationally inefficient and time-consuming, especially for large models trained on large datasets over days or weeks. An alternative approach is to train the model once for a large number of training epochs. During training, the model is evaluated on a holdout validation dataset after each epoch. If the performance of the model on the validation dataset starts to degrade (e.g. loss begins to increase or accuracy begins to decrease), then the training process is stopped. ¡¦ the error measured with respect to independent data, generally called a validation set, often shows a decrease at first, followed by an increase as the network starts to over-fit. Training can therefore be stopped at the point of smallest error with respect to the validation data set <U+2014> Page 259, Pattern Recognition and Machine Learning, 2006. The model at the time that training is stopped is then used and is known to have good generalization performance. This procedure is called ¡°early stopping¡± and is perhaps one of the oldest and most widely used forms of neural network regularization. This strategy is known as early stopping. It is probably the most commonly used form of regularization in deep learning. Its popularity is due both to its effectiveness and its simplicity. <U+2014> Page 247, Deep Learning, 2016. If regularization methods like weight decay that update the loss function to encourage less complex models are considered ¡°explicit¡± regularization, then early stopping may be thought of as a type of ¡°implicit¡± regularization, much like using a smaller network that has less capacity. Regularization may also be implicit as is the case with early stopping. <U+2014> Understanding deep learning requires rethinking generalization, 2017. Early stopping requires that you configure your network to be under constrained, meaning that it has more capacity than is required for the problem. When training the network, a larger number of training epochs is used than may normally be required, to give the network plenty of opportunity to fit, then begin to overfit the training dataset. There are three elements to using early stopping; they are: The performance of the model must be monitored during training. This requires the choice of a dataset that is used to evaluate the model and a metric used to evaluate the model. It is common to split the training dataset and use a subset, such as 30%, as a validation dataset used to monitor performance of the model during training. This validation set is not used to train the model. It is also common to use the loss on a validation dataset as the metric to monitor, although you may also use prediction error in the case of regression, or accuracy in the case of classification. The loss of the model on the training dataset will also be available as part of the training procedure, and additional metrics may also be calculated and monitored on the training dataset. Performance of the model is evaluated on the validation set at the end of each epoch, which adds an additional computational cost during training. This can be reduced by evaluating the model less frequently, such as every 2, 5, or 10 training epochs. Once a scheme for evaluating the model is selected, a trigger for stopping the training process must be chosen. The trigger will use a monitored performance metric to decide when to stop training. This is often the performance of the model on the holdout dataset, such as the loss. In the simplest case, training is stopped as soon as the performance on the validation dataset decreases as compared to the performance on the validation dataset at the prior training epoch (e.g. an increase in loss). More elaborate triggers may be required in practice. This is because the training of a neural network is stochastic and can be noisy. Plotted on a graph, the performance of a model on a validation dataset may go up and down many times. This means that the first sign of overfitting may not be a good place to stop training. ¡¦ the validation error can still go further down after it has begun to increase [¡¦] Real validation error curves almost always have more than one local minimum. <U+2014> Early Stopping <U+2013> But When?, 2002. Some more elaborate triggers may include: Some delay or ¡°patience¡± in stopping is almost always a good idea. ¡¦ results indicate that ¡°slower¡± criteria, which stop later than others, on the average lead to improved generalization compared to ¡°faster¡± ones. However, the training time that has to be expended for such improvements is rather large on average and also varies dramatically when slow criteria are used. <U+2014> Early Stopping <U+2013> But When?, 2002. At the time that training is halted, the model is known to have slightly worse generalization error than a model at a prior epoch. As such, some consideration may need to be given as to exactly which model is saved. Specifically, the training epoch from which weights in the model that are saved to file. This will depend on the trigger chosen to stop the training process. For example, if the trigger is a simple decrease in performance from one epoch to the next, then the weights for the model at the prior epoch will be preferred. If the trigger is required to observe a decrease in performance over a fixed number of epochs, then the model at the beginning of the trigger period will be preferred. Perhaps a simple approach is to always save the model weights if the performance of the model on a holdout dataset is better than at the previous epoch. That way, you will always have the model with the best performance on the holdout set. Every time the error on the validation set improves, we store a copy of the model parameters. When the training algorithm terminates, we return these parameters, rather than the latest parameters. <U+2014> Page 246, Deep Learning, 2016. This section summarizes some examples where early stopping has been used. Yoon Kim in his seminal application of convolutional neural networks to sentiment analysis in the 2014 paper titled ¡°Convolutional Neural Networks for Sentence Classification¡± used early stopping with 10% of the training dataset used as the validation hold outset. We do not otherwise perform any dataset-specific tuning other than early stopping on dev sets. For datasets without a standard dev set we randomly select 10% of the training data as the dev set. Chiyuan Zhang, et al. from MIT, Berkeley, and Google in their 2017 paper titled ¡°Understanding deep learning requires rethinking generalization¡± highlight that on very deep convolutional neural networks for photo classification where there is an abundant dataset that early stopping may not always offer benefit, as the model is less likely to overfit such large datasets. [regarding] the training and testing accuracy on ImageNet [results suggest] a reference of potential performance gain for early stopping. However, on the CIFAR10 dataset, we do not observe any potential benefit of early stopping. Yarin Gal and Zoubin Ghahramani from Cambridge in their 2015 paper titled ¡°A Theoretically Grounded Application of Dropout in Recurrent Neural Networks¡± use early stopping as an ¡°unregularized baseline¡± for LSTM models on a suite of language modeling problems. Lack of regularisation in RNN models makes it difficult to handle small data, and to avoid overfitting researchers often use early stopping, or small and under-specified models Alex Graves, et al., in their famous 2013 paper titled ¡°Speech recognition with deep recurrent neural networks¡± achieved state-of-the-art results with LSTMs for speech recognition, while making use of early stopping. Regularisation is vital for good performance with RNNs, as their flexibility makes them prone to overfitting. Two regularisers were used in this paper: early stopping and weight noise ¡¦ This section provides some tips for using early stopping regularization with your neural network. Early stopping is so easy to use, e.g. with the simplest trigger, that there is little reason to not use it when training neural networks. Use of early stopping may be a staple of the modern training of deep neural networks. Early stopping should be used almost universally. <U+2014> Page 425, Deep Learning, 2016. Before using early stopping, it may be interesting to fit an under constrained model and monitor the performance of the model on a train and validation dataset. Plotting the performance of the model in real-time or at the end of a long run will show how noisy the training process is with your specific model and dataset. This may help in the choice of a trigger for early stopping. Loss is an easy metric to monitor during training and to trigger early stopping. The problem is that loss does not always capture what is most important about the model to you and your project. It may be better to choose a performance metric to monitor that best defines the performance of the model in terms of the way you intend to use it. This may be the metric that you intend to use to report the performance of the model. A problem with early stopping is that the model does not make use of all available training data. It may be desirable to avoid overfitting and to train on all possible data, especially on problems where the amount of training data is very limited. A recommended approach would be to treat the number of training epochs as a hyperparameter and to grid search a range of different values, perhaps using k-fold cross-validation. This will allow you to fix the number of training epochs and fit a final model on all available data. Early stopping could be used instead. The early stopping procedure could be repeated a number of times. The epoch number at which training was stopped could be recorded. Then, the average of the epoch number across all repeats of early stopping could be used when fitting a final model on all available training data. This process could be performed using a different split of the training set into train and validation steps each time early stopping is run. An alternative might be to use early stopping with a validation dataset, then update the final model with further training on the held out validation set. Early stopping could be used with k-fold cross-validation, although it is not recommended. The k-fold cross-validation procedure is designed to estimate the generalization error of a model by repeatedly refitting and evaluating it on different subsets of a dataset. Early stopping is designed to monitor the generalization error of one model and stop training when generalization error begins to degrade. They are at odds because cross-validation assumes you don¡¯t know the generalization error and early stopping is trying to give you the best model based on knowledge of generalization error. It may be desirable to use cross-validation to estimate the performance of models with different hyperparameter values, such as learning rate or network structure, whilst also using early stopping. In this case, if you have the resources to repeatedly evaluate the performance of the model, then perhaps the number of training epochs may also be treated as a hyperparameter to be optimized, instead of using early stopping. Instead of using cross-validation with early stopping, early stopping may be used directly without repeated evaluation when evaluating different hyperparameter values for the model (e.g. different learning rates). One possible point of confusion is that early stopping is sometimes referred to as ¡°cross-validated training.¡± Further, research into early stopping that compares triggers may use cross-validation to compare the impact of different triggers. Repeating the early stopping procedure many times may result in the model overfitting the validation dataset. This can happen just as easily as overfitting the training dataset. One approach is to only use early stopping once all other hyperparameters of the model have been chosen. Another strategy may be to use a different split of the training dataset into train and validation sets each time early stopping is used. This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered that stopping the training of neural network early before it has overfit the training dataset can reduce overfitting and improve the generalization of deep neural networks. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. Great. thanks a lot. Thanks. Thank you for the advice here, I think the
¡°stop training early because I feel like stopping¡±
needs to be stopped.
and a more controlled way needs to be taken.
Thank you for the tools for that. Thanks, I¡¯m happy it helped. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): network(12), epoch(9), model(7), time(4), trigger(4), value(4), dataset(3), parameter(3), result(3), set(3)"
"3","mastery",2018-12-05,"How to Reduce Overfitting With Dropout Regularization in Keras","https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/","Dropout regularization is a computationally cheap way to regularize a deep neural network. Dropout works by probabilistically removing, or ¡°dropping out,¡± inputs to a layer, which may be input variables in the data sample or activations from a previous layer. It has the effect of simulating a large number of networks with very different network structure and, in turn, making nodes in the network generally more robust to the inputs. In this tutorial, you will discover the Keras API for adding dropout regularization to deep learning neural network models. After completing this tutorial, you will know: Let¡¯s get started. How to Reduce Overfitting With Dropout Regularization in KerasPhoto by PROJorge Lascar, some rights reserved. This tutorial is divided into three parts; they are: Keras supports dropout regularization. The simplest form of dropout in Keras is provided by a Dropout core layer. When created, the dropout rate can be specified to the layer as the probability of setting each input to the layer to zero. This is different from the definition of dropout rate from the papers, in which the rate refers to the probability of retaining an input. Therefore, when a dropout rate of 0.8 is suggested in a paper (retain 80%), this will, in fact, will be a dropout rate of 0.2 (set 20% of inputs to zero). Below is an example of creating a dropout layer with a 50% chance of setting inputs to zero. The Dropout layer is added to a model between existing layers and applies to outputs of the prior layer that are fed to the subsequent layer. For example, given two dense layers: We can insert a dropout layer between them, in which case the outputs or activations of the first layer have dropout applied to them, which are then taken as input to the next layer. It is this second layer now which has dropout applied. Dropout can also be applied to the visible layer, e.g. the inputs to the network. This requires that you define the network with the Dropout layer as the first layer and add the input_shape argument to the layer to specify the expected shape of the input samples. Let¡¯s take a look at how dropout regularization can be used with some common network types. The example below adds dropout between two dense fully connected layers. Dropout can be used after convolutional layers (e.g. Conv2D) and after pooling layers (e.g. MaxPooling2D). Often, dropout is only used after the pooling layers, but this is just a rough heuristic. In this case, dropout is applied to each element or cell within the feature maps. An alternative way to use dropout with convolutional neural networks is to dropout entire feature maps from the convolutional layer which are then not used during pooling. This is called spatial dropout (or ¡°SpatialDropout¡°). Instead we formulate a new dropout method which we call SpatialDropout. For a given convolution feature tensor [¡¦] [we] extend the dropout value across the entire feature map. <U+2014> Efficient Object Localization Using Convolutional Networks, 2015. Spatial Dropout is provided in Keras via the SpatialDropout2D layer (as well as 1D and 3D versions). The example below adds dropout between two layers: an LSTM recurrent layer and a dense fully connected layers. This example applies dropout to, in this case, 32 outputs from the LSTM layer provided as input to the Dense layer. Alternately, the inputs to the LSTM may be subjected to dropout. In this case, a different dropout mask is applied to each time step within each sample presented to the LSTM. There is an alternative way to use dropout with recurrent layers like the LSTM. The same dropout mask may be used by the LSTM for all inputs within a sample. The same approach may be used for recurrent input connections across the time steps of the sample. This approach to dropout with recurrent models is called a Variational RNN. The proposed technique (Variational RNN [¡¦]) uses the same dropout mask at each time step, including the recurrent layers. [¡¦] Implementing our approximate inference is identical to implementing dropout in RNNs with the same network units dropped at each time step, randomly dropping inputs, outputs, and recurrent connections. This is in contrast to existing techniques, where different network units would be dropped at different time steps, and no dropout would be applied to the recurrent connections <U+2014> A Theoretically Grounded Application of Dropout in Recurrent Neural Networks, 2016. Keras supports Variational RNNs (i.e. consistent dropout across the time steps of a sample for inputs and recurrent inputs) via two arguments on the recurrent layers, namely ¡°dropout¡± for inputs and ¡°recurrent_dropout¡± for recurrent inputs. In this section, we will demonstrate how to use dropout regularization to reduce overfitting of an MLP on a simple binary classification problem. This example provides a template for applying dropout regularization to your own neural network for classification and regression problems. We will use a standard binary classification problem that defines two two-dimensional concentric circles of observations, one circle for each class. Each observation has two input variables with the same scale and a class output value of either 0 or 1. This dataset is called the ¡°circles¡± dataset because of the shape of the observations in each class when plotted. We can use the make_circles() function to generate observations from this problem. We will add noise to the data and seed the random number generator so that the same samples are generated each time the code is run. We can plot the dataset where the two variables are taken as x and y coordinates on a graph and the class value is taken as the color of the observation. The complete example of generating the dataset and plotting it is listed below. Running the example creates a scatter plot showing the concentric circles shape of the observations in each class. We can see the noise in the dispersal of the points making the circles less obvious. Scatter Plot of Circles Dataset with Color Showing the Class Value of Each Sample This is a good test problem because the classes cannot be separated by a line, e.g. are not linearly separable, requiring a nonlinear method such as a neural network to address. We have only generated 100 samples, which is small for a neural network, providing the opportunity to overfit the training dataset and have higher error on the test dataset: a good case for using regularization. Further, the samples have noise, giving the model an opportunity to learn aspects of the samples that don¡¯t generalize. We can develop an MLP model to address this binary classification problem. The model will have one hidden layer with more nodes than may be required to solve this problem, providing an opportunity to overfit. We will also train the model for longer than is required to ensure the model overfits. Before we define the model, we will split the dataset into train and test sets, using 30 examples to train the model and 70 to evaluate the fit model¡¯s performance. Next, we can define the model. The hidden layer uses 500 nodes in the hidden layer and the rectified linear activation function. A sigmoid activation function is used in the output layer in order to predict class values of 0 or 1. The model is optimized using the binary cross entropy loss function, suitable for binary classification problems and the efficient Adam version of gradient descent. The defined model is then fit on the training data for 4,000 epochs and the default batch size of 32. We will also use the test dataset as a validation dataset. We can evaluate the performance of the model on the test dataset and report the result. Finally, we will plot the performance of the model on both the train and test set each epoch. If the model does indeed overfit the training dataset, we would expect the line plot of accuracy on the training set to continue to increase and the test set to rise and then fall again as the model learns statistical noise in the training dataset. We can tie all of these pieces together; the complete example is listed below. Running the example reports the model performance on the train and test datasets. We can see that the model has better performance on the training dataset than the test dataset, one possible sign of overfitting. Your specific results may vary given the stochastic nature of the neural network and the training algorithm. Because the model is severely overfit, we generally would not expect much, if any, variance in the accuracy across repeated runs of the model on the same dataset. A figure is created showing line plots of the model accuracy on the train and test sets. We can see that expected shape of an overfit model where test accuracy increases to a point and then begins to decrease again. Line Plots of Accuracy on Train and Test Datasets While Training Showing an Overfit We can update the example to use dropout regularization. We can do this by simply inserting a new Dropout layer between the hidden layer and the output layer. In this case, we will specify a dropout rate (probability of setting outputs from the hidden layer to zero) to 40% or 0.4. The complete updated example with the addition of dropout after the hidden layer is listed below: Running the example reports the model performance on the train and test datasets. Your results will likely vary. In this case, the resulting model has a high variance. In this specific case, we can see that dropout resulted in a slight drop in accuracy on the training dataset, down from 100% to 96%, and a lift in accuracy on the test set, up from 75% to 81%. Reviewing the line plot of train and test accuracy during training, we can see that it no longer appears that the model has overfit the training dataset. Model accuracy on both the train and test sets continues to increase to a plateau, albeit with a lot of noise given the use of dropout during training. Line Plots of Accuracy on Train and Test Datasets While Training With Dropout Regularization This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered the Keras API for adding dropout regularization to deep learning neural network models. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): input(12), layer(11), circle(5), output(5), sample(5), dataset(4), kera(4), model(4), network(4), observation(4)"
"4","mastery",2018-12-03,"A Gentle Introduction to Dropout for Regularizing Deep Neural Networks","https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/","Deep learning neural networks are likely to quickly overfit a training dataset with few examples. Ensembles of neural networks with different model configurations are known to reduce overfitting, but require the additional computational expense of training and maintaining multiple models. A single model can be used to simulate having a large number of different network architectures by randomly dropping out nodes during training. This is called dropout and offers a very computationally cheap and remarkably effective regularization method to reduce overfitting and generalization error in deep neural networks of all kinds. In this post, you will discover the use of dropout regularization for reducing overfitting and improving the generalization of deep neural networks. After reading this post, you will know: Let¡¯s get started. A Gentle Introduction to Dropout for Regularizing Deep Neural NetworksPhoto by Jocelyn Kinghorn, some rights reserved. This tutorial is divided into five parts; they are: Large neural nets trained on relatively small datasets can overfit the training data. This has the effect of the model learning the statistical noise in the training data, which results in poor performance when the model is evaluated on new data, e.g. a test dataset. Generalization error increases due to overfitting. One approach to reduce overfitting is to fit all possible different neural networks on the same dataset and to average the predictions from each model. This is not feasible in practice, and can be approximated using a small collection of different models, called an ensemble. With unlimited computation, the best way to ¡°regularize¡± a fixed-sized model is to average the predictions of all possible settings of the parameters, weighting each setting by its posterior probability given the training data. <U+2014> Dropout: A Simple Way to Prevent Neural Networks from Overfitting, 2014. A problem even with the ensemble approximation is that it requires multiple models to be fit and stored, which can be a challenge if the models are large, requiring days or weeks to train and tune. Dropout is a regularization method that approximates training a large number of neural networks with different architectures in parallel. During training, some number of layer outputs are randomly ignored or ¡°dropped out.¡± This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. In effect, each update to a layer during training is performed with a different ¡°view¡± of the configured layer. By dropping a unit out, we mean temporarily removing it from the network, along with all its incoming and outgoing connections <U+2014> Dropout: A Simple Way to Prevent Neural Networks from Overfitting, 2014. Dropout has the effect of making the training process noisy, forcing nodes within a layer to probabilistically take on more or less responsibility for the inputs. This conceptualization suggests that perhaps dropout breaks-up situations where network layers co-adapt to correct mistakes from prior layers, in turn making the model more robust. ¡¦ units may change in a way that they fix up the mistakes of the other units. This may lead to complex co-adaptations. This in turn leads to overfitting because these co-adaptations do not generalize to unseen data. [¡¦] <U+2014> Dropout: A Simple Way to Prevent Neural Networks from Overfitting, 2014. Dropout simulates a sparse activation from a given layer, which interestingly, in turn, encourages the network to actually learn a sparse representation as a side-effect. As such, it may be used as an alternative to activity regularization for encouraging sparse representations in autoencoder models. We found that as a side-effect of doing dropout, the activations of the hidden units become sparse, even when no sparsity inducing regularizers are present. <U+2014> Dropout: A Simple Way to Prevent Neural Networks from Overfitting, 2014. Because the outputs of a layer under dropout are randomly subsampled, it has the effect of reducing the capacity or thinning the network during training. As such, a wider network, e.g. more nodes, may be required when using dropout. Dropout is implemented per-layer in a neural network. It can be used with most types of layers, such as dense fully connected layers, convolutional layers, and recurrent layers such as the long short-term memory network layer. Dropout may be implemented on any or all hidden layers in the network as well as the visible or input layer. It is not used on the output layer. The term ¡°dropout¡± refers to dropping out units (hidden and visible) in a neural network. <U+2014> Dropout: A Simple Way to Prevent Neural Networks from Overfitting, 2014. A new hyperparameter is introduced that specifies the probability at which outputs of the layer are dropped out, or inversely, the probability at which outputs of the layer are retained. The interpretation is an implementation detail that can differ from paper to code library. A common value is a probability of 0.5 for retaining the output of each node in a hidden layer and a value close to 1.0, such as 0.8, for retaining inputs from the visible layer. In the simplest case, each unit is retained with a fixed probability p independent of other units, where p can be chosen using a validation set or can simply be set at 0.5, which seems to be close to optimal for a wide range of networks and tasks. For the input units, however, the optimal probability of retention is usually closer to 1 than to 0.5. <U+2014> Dropout: A Simple Way to Prevent Neural Networks from Overfitting, 2014. Dropout is not used after training when making a prediction with the fit network. The weights of the network will be larger than normal because of dropout. Therefore, before finalizing the network, the weights are first scaled by the chosen dropout rate. The network can then be used as per normal to make predictions. If a unit is retained with probability p during training, the outgoing weights of that unit are multiplied by p at test time <U+2014> Dropout: A Simple Way to Prevent Neural Networks from Overfitting, 2014. The rescaling of the weights can be performed at training time instead, after each weight update at the end of the mini-batch. This is sometimes called ¡°inverse dropout¡± and does not require any modification of weights during training. Both the Keras and PyTorch deep learning libraries implement dropout in this way. At test time, we scale down the output by the dropout rate. [¡¦] Note that this process can be implemented by doing both operations at training time and leaving the output unchanged at test time, which is often the way it¡¯s implemented in practice <U+2014> Page 109, Deep Learning With Python, 2017. Dropout works well in practice, perhaps replacing the need for weight regularization (e.g. weight decay) and activation regularization (e.g. representation sparsity). ¡¦ dropout is more effective than other standard computationally inexpensive regularizers, such as weight decay, filter norm constraints and sparse activity regularization. Dropout may also be combined with other forms of regularization to yield a further improvement. <U+2014> Page 265, Deep Learning, 2016. This section summarizes some examples where dropout was used in recent research papers to provide a suggestion for how and where it may be used. Geoffrey Hinton, et al. in their 2012 paper that first introduced dropout titled ¡°Improving neural networks by preventing co-adaptation of feature detectors¡± applied used the method with a range of different neural networks on different problem types achieving improved results, including handwritten digit recognition (MNIST), photo classification (CIFAR-10), and speech recognition (TIMIT). ¡¦ we use the same dropout rates <U+2013> 50% dropout for all hidden units and 20% dropout for visible units Nitish Srivastava, et al. in their 2014 journal paper introducing dropout titled ¡°Dropout: A Simple Way to Prevent Neural Networks from Overfitting¡± used dropout on a wide range of computer vision, speech recognition, and text classification tasks and found that it consistently improved performance on each problem. We trained dropout neural networks for classification problems on data sets in different domains. We found that dropout improved generalization performance on all data sets compared to neural networks that did not use dropout. On the computer vision problems, different dropout rates were used down through the layers of the network in conjunction with a max-norm weight constraint. Dropout was applied to all the layers of the network with the probability of retaining the unit being p = (0.9, 0.75, 0.75, 0.5, 0.5, 0.5) for the different layers of the network (going from input to convolutional layers to fully connected layers). In addition, the max-norm constraint with c = 4 was used for all the weights. [¡¦] A simpler configuration was used for the text classification task. We used probability of retention p = 0.8 in the input layers and 0.5 in the hidden layers. Max-norm constraint with c = 4 was used in all the layers. Alex Krizhevsky, et al. in their famous 2012 paper titled ¡°ImageNet Classification with Deep Convolutional Neural Networks¡± achieved (at the time) state-of-the-art results for photo classification on the ImageNet dataset with deep convolutional neural networks and dropout regularization. We use dropout in the first two fully-connected layers [of the model]. Without dropout, our network exhibits substantial overfitting. Dropout roughly doubles the number of iterations required to converge. George Dahl, et al. in their 2013 paper titled ¡°Improving deep neural networks for LVCSR using rectified linear units and dropout¡± used a deep neural network with rectified linear activation functions and dropout to achieve (at the time) state-of-the-art results on a standard speech recognition task. They used a bayesian optimization procedure to configure the choice of activation function and the amount of dropout. ¡¦ the Bayesian optimization procedure learned that dropout wasn¡¯t helpful for sigmoid nets of the sizes we trained. In general, ReLUs and dropout seem to work quite well together. This section provides some tips for using dropout regularization with your neural network. Dropout regularization is a generic approach. It can be used with most, perhaps all, types of neural network models, not least the most common network types of Multilayer Perceptrons, Convolutional Neural Networks, and Long Short-Term Memory Recurrent Neural Networks. In the case of LSTMs, it may be desirable to use different dropout rates for the input and recurrent connections. The default interpretation of the dropout hyperparameter is the probability of training a given node in a layer, where 1.0 means no dropout, and 0.0 means no outputs from the layer. A good value for dropout in a hidden layer is between 0.5 and 0.8. Input layers use a larger dropout rate, such as of 0.8. It is common for larger networks (more layers or more nodes) to more easily overfit the training data. When using dropout regularization, it is possible to use larger networks with less risk of overfitting. In fact, a large network (more nodes per layer) may be required as dropout will probabilistically reduce the capacity of the network. A good rule of thumb is to divide the number of nodes in the layer before dropout by the proposed dropout rate and use that as the number of nodes in the new network that uses dropout. For example, a network with 100 nodes and a proposed dropout rate of 0.5 will require 200 nodes (100 / 0.5) when using dropout. If n is the number of hidden units in any layer and p is the probability of retaining a unit [¡¦] a good dropout net should have at least n/p units <U+2014> Dropout: A Simple Way to Prevent Neural Networks from Overfitting, 2014. Rather than guess at a suitable dropout rate for your network, test different rates systematically. For example, test values between 1.0 and 0.1 in increments of 0.1. This will both help you discover what works best for your specific model and dataset, as well as how sensitive the model is to the dropout rate. A more sensitive model may be unstable and could benefit from an increase in size. Network weights will increase in size in response to the probabilistic removal of layer activations. Large weight size can be a sign of an unstable network. To counter this effect a weight constraint can be imposed to force the norm (magnitude) of all weights in a layer to be below a specified value. For example, the maximum norm constraint is recommended with a value between 3-4. [¡¦] we can use max-norm regularization. This constrains the norm of the vector of incoming weights at each hidden unit to be bound by a constant c. Typical values of c range from 3 to 4. <U+2014> Dropout: A Simple Way to Prevent Neural Networks from Overfitting, 2014. This does introduce an additional hyperparameter that may require tuning for the model. Like other regularization methods, dropout is more effective on those problems where there is a limited amount of training data and the model is likely to overfit the training data. Problems where there is a large amount of training data may see less benefit from using dropout. For very large datasets, regularization confers little reduction in generalization error. In these cases, the computational cost of using dropout and larger models may outweigh the benefit of regularization. <U+2014> Page 265, Deep Learning, 2016. This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered the use of dropout regularization for reducing overfitting and improving the generalization of deep neural networks. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. This is off-topic.
Why do you write most blogs on deep learning methods instead of other methods more suitable for time series data? Good question, generally because I get 100:1 more questions and interest in deep learning, and specifically deep learning with python open source libraries. So if you are working on a personal project, will you use deep learning or the method that gives best results? I use the method that gives the best results and the lowest complexity for a project. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): network(29), layer(18), unit(11), node(10), weight(9), model(7), output(5), result(5), problem(4), rate(4)"
"5","vidhya",2018-12-06,"A Practical Guide to Object Detection using the Popular YOLO Framework <U+2013> Part III (with Python codes)","https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/","How easy would our life be if we simply took an already designed framework, executed it, and got the desired result? Minimum effort, maximum reward. Isn¡¯t that what we strive for in any profession? I feel incredibly lucky to be part of our machine learning community where even the top tech behemoths embrace open source technology. Of course it¡¯s important to understand and grasp concepts before implementing them, but it¡¯s always helpful when the ground work has been laid for you by top industry data scientists and researchers. This is especially true for deep learning domains like computer vision. Not everyone has the computational resources to build a DL model from scratch. That¡¯s where predefined frameworks and pretained models come in handy. And in this article, we will look at one such framework for object detection <U+2013> YOLO. It¡¯s a supremely fast and accurate framework, as we¡¯ll see soon. So far in our series of posts detailing object detection (links below), we¡¯ve seen the various algorithms that are used, and how we can detect objects in an image and predict bounding boxes using algorithms of the R-CNN family. We have also looked at the implementation of Faster-RCNN in Python. In part 3 here, we will learn what makes YOLO tick, why you should use it over other object detection algorithms, and the different techniques used by YOLO. Once we have understood the concept thoroughly, we will then implement it it in Python. It¡¯s the ideal guide to gain invaluable knowledge and then apply it in a practical hands-on manner. I highly recommend going through the first two parts before diving into this guide: The R-CNN family of techniques we saw in Part 1 primarily use regions to localize the objects within the image. The network does not look at the entire image, only at the parts of the images which have a higher chance of containing an object. The YOLO framework (You Only Look Once) on the other hand, deals with object detection in a different way. It takes the entire image in a single instance and predicts the bounding box coordinates and class probabilities for these boxes. The biggest advantage of using YOLO is its superb speed <U+2013> it¡¯s incredibly fast and can process 45 frames per second. YOLO also understands generalized object representation. This is one of the best algorithms for object detection and has shown a comparatively similar performance to the R-CNN algorithms. In the upcoming sections, we will learn about different techniques used in YOLO algorithm. The following explanations are inspired by Andrew NG¡¯s course on Object Detection which helped me a lot in understanding the working of YOLO. Now that we have grasp on why YOLO is such a useful framework, let¡¯s jump into how it actually works. In this section, I have mentioned the steps followed by YOLO for detecting objects in a given image. Pretty straightforward, isn¡¯t it? Let¡¯s break down each step to get a more granular understanding of what we just learned. We need to pass the labelled data to the model in order to train it. Suppose we have divided the image into a grid of size 3 X 3 and there are a total of 3 classes which we want the objects to be classified into. Let¡¯s say the classes are Pedestrian, Car, and Motorcycle respectively. So, for each grid cell, the label y will be an eight dimensional vector: Here, Let¡¯s say we select the first grid from the above example: Since there is no object in this grid, pc will be zero and the y label for this grid will be: Here, ¡®?¡¯ means that it doesn¡¯t matter what bx, by, bh, bw, c1, c2, and c3<U+00A0>contain as there is no object in the grid. Let¡¯s take another grid in which we have a car (c2 = 1): Before we write the y label for this grid, it¡¯s important to first understand how YOLO decides whether there actually is an object in the grid. In the above image, there are two objects (two cars), so YOLO will take the mid-point of these two objects and these objects will be assigned to the grid which contains the mid-point of these objects. The y label for the centre left grid with the car will be: Since there is an object in this grid, pc<U+00A0>will be equal to 1. bx, by, bh, bw will be calculated relative to the particular grid cell we are dealing with. Since car is the second class, c2 = 1 and c1 and c3 = 0. So, for each of the 9 grids, we will have an eight dimensional output vector. This output will have a shape of 3 X 3 X 8. So now we have an input image and it¡¯s corresponding target vector. Using the above example (input image <U+2013> 100 X 100 X 3, output <U+2013> 3 X 3 X 8), our model will be trained as follows: We will run both forward and backward propagation to train our model. During the testing phase, we pass an image to the model and run forward propagation until we get an output y. In order to keep things simple, I have explained this using a 3 X 3 grid here, but generally in real-world scenarios we take larger grids (perhaps 19 X 19). Even if an object spans out to more than one grid, it will only be assigned to a single grid in which its mid-point is located. We can reduce the chances of multiple objects appearing in the same grid cell by increasing the more number of grids (19 X 19, for example). As I mentioned earlier, bx, by, bh, and bw are calculated relative to the grid cell we are dealing with. Let¡¯s understand this concept with an example. Consider the center-right grid which contains a car: So, bx, by, bh, and bw will be calculated relative to this grid only. The y label for this grid will be: pc = 1 since there is an object in this grid and since it is a car, c2 = 1. Now, let¡¯s see how to decide bx, by, bh, and bw. In YOLO, the coordinates assigned to all the grids are: bx, by are the x and y coordinates of the midpoint of the object with respect to this grid. In this case, it will be (around) bx = 0.4 and by = 0.3: bh<U+00A0>is the ratio of the height of the bounding box (red box in the above example) to the height of the corresponding grid cell, which in our case is around 0.9. So, <U+00A0>bh = 0.9. bw is the ratio of the width of the bounding box to the width of the grid cell. So, bw = 0.5 (approximately). The y label for this grid will be: Notice here that bx and by will always range between 0 and 1 as the midpoint will always lie within the grid. Whereas bh and bw can be more than 1 in case the dimensions of the bounding box are more than the dimension of the grid. In the next section, we will look at more ideas that can potentially help us in making this algorithm¡¯s performance even better. Here¡¯s some food for thought <U+2013> how can we decide whether the predicted bounding box is giving us a good outcome (or a bad one)? This is where Intersection over Union comes into the picture. It calculates the intersection over union of the actual bounding box and the predicted bonding box. Consider the actual and predicted bounding boxes for a car as shown below: Here, the red box is the actual bounding box and the blue box is the predicted one. How can we decide whether it is a good prediction or not? IoU, or Intersection over Union, will calculate the area of the intersection over union of these two boxes. That area will be: IoU = Area of the intersection / Area of the union, i.e. IoU = Area of yellow box / Area of green box If IoU is greater than 0.5, we can say that the prediction is good enough. 0.5 is an arbitrary threshold we have taken here, but it can be changed according to your specific problem. Intuitively, the more you increase the threshold, the better the predictions become. There is one more technique that can improve the output of YOLO significantly <U+2013> Non-Max Suppression. One of the most common problems with object detection algorithms is that rather than detecting an object just once, they might detect it multiple times. Consider the below image: Here, the cars are identified more than once. The Non-Max Suppression technique cleans up this up so that we get only a single detection per object. Let¡¯s see how this approach works. 1. It first looks at the probabilities associated with each detection and takes the largest one. In the above image, 0.9 is the highest probability, so the box with 0.9 probability will be selected first: 2. Now, it looks at all the other boxes in the image. The boxes which have high IoU with the current box are suppressed. So, the boxes with 0.6 and 0.7 probabilities will be suppressed in our example: 3. After the boxes have been suppressed, it selects the next box from all the boxes with the highest probability, which is 0.8 in our case: 4. Again it will look at the IoU of this box with the remaining boxes and compress the boxes with a high IoU: 5. We repeat these steps until all the boxes have either been selected or compressed and we get the final bounding boxes: This is what Non-Max Suppression is all about. We are taking the boxes with maximum probability and suppressing the close-by boxes with non-max probabilities. Let¡¯s quickly summarize the points which we¡¯ve seen in this section about the Non-Max suppression algorithm: There is another method we can use to improve the perform of a YOLO algorithm <U+2013> let¡¯s check it out! We have seen that each grid can only identify one object. But what if there are multiple objects in a single grid? That can so often be the case in reality. And that leads us to the concept of anchor boxes. Consider the following image, divided into a 3 X 3 grid: Remember how we assigned an object to a grid? We took the midpoint of the object and based on its location, assigned the object to the corresponding grid. In the above example, the midpoint of both the objects lies in the same grid. This is how the actual bounding boxes for the objects will be: We will only be getting one of the two boxes, either for the car or for the person. But if we use anchor boxes, we might be able to output both boxes! How do we go about doing this? First, we pre-define two different shapes called anchor boxes or anchor box shapes. Now, for each grid, instead of having one output, we will have two outputs. We can always increase the number of anchor boxes as well. I have taken two here to make the concept easy to understand: This is how the y label for YOLO without anchor boxes looks like: What do you think the y label will be if we have 2 anchor boxes? I want you to take a moment to ponder this before reading further. Got it? The y label will be: The first 8 rows belong to anchor box 1 and the remaining 8 belongs to anchor box 2. The objects are assigned to the anchor boxes based on the similarity of the bounding boxes and the anchor box shape. Since the shape of anchor box 1 is similar to the bounding box for the person, the latter will be assigned to anchor box 1 and the car will be assigned to anchor box 2. The output in this case, instead of 3 X 3 X 8 (using a 3 X 3 grid and 3 classes), will be 3 X 3 X 16 (since we are using 2 anchors). So, for each grid, we can detect two or more objects based on the number of anchors. Let¡¯s combine all the ideas we have covered so far and integrate them into the YOLO framework. In this section, we will first see how a YOLO model is trained and then how the predictions can be made for a new and previously unseen image. Training The input for training our model will obviously be images and their corresponding y labels. Let¡¯s see an image and make its y label: Consider the scenario where we are using a 3 X 3 grid with two anchors per grid, and there are 3 different object classes. So the corresponding y labels will have a shape of 3 X 3 X 16. Now, suppose if we use 5 anchor boxes per grid and the number of classes has been increased to 5. So the target will be 3 X 3 X 5 X 5 = 3 X 3 X 25. This is how the training process is done <U+2013> taking an image of a particular shape and mapping it with a 3 X 3 X 16 target (this may change as per the grid size, number of anchor boxes and the number of classes). Testing The new image will be divided into the same number of grids which we have chosen during the training period. For each grid, the model will predict an output of shape 3 X 3 X 16 (assuming this is the shape of the target during training time). The 16 values in this prediction will be in the same format as that of the training label. The first 8 values will correspond to anchor box 1, where the first value will be the probability of an object in that grid. Values 2-5 will be the bounding box coordinates for that object, and the last three values will tell us which class the object belongs to. The next 8 values will be for anchor box 2 and in the same format, i.e., first the probability, then the bounding box coordinates, and finally the classes. Finally, the Non-Max Suppression technique will be applied on the predicted boxes to obtain a single prediction per object. That brings us to the end of the theoretical aspect of understanding how the YOLO algorithm works, starting from training the model and then generating prediction boxes for the objects. Below are the exact dimensions and steps that the YOLO algorithm follows: Time to fire up our Jupyter notebooks (or your preferred IDE) and finally implement our learning in the form of code! This is what we have been building up to so far, so let¡¯s get the ball rolling. The code we¡¯ll see in this section for implementing YOLO has been taken from Andrew NG¡¯s GitHub repository on Deep Learning. You will also need to download this zip file which contains the pretrained weights required to run this code. Let¡¯s first define the functions that will help us choose the boxes above a certain threshold, find the IoU, and apply Non-Max Suppression on them. Before everything else however, we¡¯ll first import the required libraries: Now, let¡¯s create a function for filtering the boxes based on their probabilities and threshold: Next, we will define a function to calculate the IoU between two boxes: Let¡¯s define a function for Non-Max Suppression: We now have the functions that will calculate the IoU and perform Non-Max Suppression. We get the output from the CNN of shape (19,19,5,85). So, we will create a random volume of shape (19,19,5,85) and then predict the bounding boxes: Finally, we will define a function which will take the outputs of a CNN as input and return the suppressed boxes: Let¡¯s see how we can use the yolo_eval function to make predictions for a random volume which we created above: How does the outlook look? ¡®scores¡¯ represents how likely the object will be present in the volume. ¡®boxes¡¯ returns the (x1, y1, x2, y2) coordinates for the detected objects. ¡®classes¡¯ is the class of the identified object. Now, let¡¯s use a pretrained YOLO algorithm on new images and see how it works: After loading the classes and the pretrained model, let¡¯s use the functions defined above to get the yolo_outputs. Now, we will define a function to predict the bounding boxes and save the images with these bounding boxes included: Next, we will read an image and make predictions using the predict function: Finally, let¡¯s plot the predictions: Not bad! I especially like that the model correctly picked up the person in the mini-van as well. Here¡¯s a brief summary of what we covered and implemented in this guide: YOLO is one of my all-time favorite frameworks and I¡¯m sure you¡¯ll see why once you implement the code on your own machine. It¡¯s a great way of getting your hands dirty with a popular computer vision algorithm. If you have any questions or feedback regarding this guide, connect with me in the comments section below. Thanks for article.
How does YOLO compare with Faster-RCNN for detection of very small objects like scratches on metal surface? My observation was <U+2013> RCNN lacks an elegant way to compute anchor sizes based on dataset¡¦ also I attempted changing scales,strides,box size results are bad.
How do we custom train for YOLO? Hi, YOLO is faster in comparison to Faster-RCNN. Their accuracies are comparatively similar. YOLO does not work pretty well for small objects.
In order to improve its performance on smaller objects, you can try the following things: This may give better results. Is this Yolo implemented on GPU based?
How to training Yolo for customize object?
Is are sany yolo code or tutorial using Tensorflow GPU base is available ?
I only want to did detect vehicles.
Please guide me.","Keyword(freq): box(38), object(19), algorithm(7), coordinate(5), grid(5), iou(5), prediction(5), probability(5), value(5), image(4)"
"6","vidhya",2018-12-03,"5 Best Machine Learning GitHub Repositories & Reddit Discussions (November 2018)","https://www.analyticsvidhya.com/blog/2018/12/best-machine-learning-github-repositories-reddit-threads-november-2018/","Coding is among one of the best things about being a data scientist. There are often days when I find myself immersed in programming something from scratch. That exhilarating feeling you get when you see your hard work culminate in a successful model? Exhilarating and unparalleled! But as a data scientist (or a programmer), its equally important to create checkpoints of your code at various intervals. It¡¯s incredibly helpful to know where you started off from last time so if you have to rollback your code or simply branch out to a different path, there¡¯s always a fallback option. And that¡¯s why GitHub is such an excellent platform. The previous posts in this monthly series have expounded on why every data scientist should have an active GitHub account. Whether it¡¯s for collaboration, resume/portfolio, or educational purposes, it¡¯s simply the best place to enhance your coding skills and knowledge. And now let¡¯s get to the core of our article <U+2013> machine learning code! I have picked out some really interesting repositories which I feel every data scientist should try out on their own. Apart from coding, there are tons of aspects associated with being a data scientist. We need to be aware of all the latest developments in the community, what other machine learning professionals and thought leaders are talking about, what are the moral implications of working on a controversial project, etc. That is what I aim to bring out in the Reddit discussion threads I showcase every month. To make things easier for you, here¡¯s the entire collection so far of the top GitHub repositories and Reddit discussions (from April onwards) we have covered each month: Keeping our run going of including reinforcement learning resources in this series, here¡¯s one of the best so far <U+2013> OpenAI¡¯s Spinning Up! This is an educational resource open sourced with the aim of making it easier to learn deep RL. Given how complex it can appear to most folks, this is quite a welcome repository. The repo contains a few handy resources: This one is for all the audio/speech processing people out there. WaveGlow is a flow-based generative network for speech synthesis. In other words, it¡¯s a network (yes, a single network!) that can generate impressive high quality speech from<U+00A0>mel-spectrograms. This repo contains the PyTorch implementation of WaveGlow and a pre-trained model to get you started. It¡¯s a really cool framework, and you can check out the below links as well if you wish to delve deeper: We covered the PyTorch implmentation of BERT in last month¡¯s article, and here¡¯s a different take on it. For those who are new to BERT, it stands for<U+00A0>Bidirectional<U+00A0>Encoder<U+00A0>Representations from<U+00A0>Transformers. It¡¯s basically a method for pre-training language representations. BERT has set the NLP world ablaze with it¡¯s results, and the folks at Google have been kind enough to release quite a few pre-trained models to get you on your way. This repository ¡°uses BERT as the sentence encoder and hosts it as a service via ZeroMQ, allowing you to map sentences into fixed-length representations in just two lines of code¡±. It¡¯s easy to use, extremely quick, and scales smoothly. Try it out! Quick, Draw is a popular online game developed by Google where a neural network tries to guess what you¡¯re drawing. The neural network learns from each drawing, hence increasing it¡¯s already impressive ability to correctly guess the doodle. The developers have built up a HUGE dataset from the amount of drawings users have made previously. It¡¯s an open-source dataset which you can check out here. And now you can build your own Quick, Draw game in Python with this repository. There is a step-by-step explanation of how to do this. Using this code, you can run an app to either draw in front of the computer¡¯s webcam, or on a canvas. GAN Dissection, pioneered by researchers at MIT¡¯s<U+00A0>Computer Science & Artificial Intelligence Laboratory, is a unique way of visualizing and understanding the neurons of Generative Adversarial Networks (GANs). But it isn¡¯t just limited to that <U+2013> the researchers have also created GANPaint<U+00A0>to showcase how GAN Dissection works. This helps you explore what a particular GAN model has learned by inspecting and manipulating it¡¯s internal neurons. Check out the research paper here and the below video demo, and then head straight to the GitHub repository to dive straight into the code! Has this question ever crossed your mind while learning basic machine learning concepts? This is one of the fundamental algorithms we come across in our initial learning days and has proven to be quite effective in ML competitions as well. But once you start going through this thread, prepare to seriously question what you¡¯ve studied previously. What started off as a straight forward question turned into a full-blown discussion among the top minds on Reddit. I thoroughly enjoyed browsing through the comments and I¡¯m sure anyone with interest in this field (and mathematical rigour) will find it useful. What do you do when the developer of a complex and massive neural network vanishes without leaving behind the documentation needed to understand it? This isn¡¯t a fictional plot, but a rather common situation the original poster of the thread found himself in. It¡¯s a situation that happens regularly with developers but takes on a whole new level of intrigue when it comes to deep learning. This thread explores the different ways a data scientist can go about examining how a deep neural network model was initially designed. The responses range from practical to absurd, but each adds a layer of perspective which could help you one day if you ever face this predicament. My attention to this thread was drawn by the sheer number of comments (110 at the time of writing) <U+2013> what in the world could be so controversial about this topic? But when you started scrolling down, the sheer difference in opinions among the debators is mind boggling. Apart from TensorFlow being derided for being ¡°not the best framework¡±, there¡¯s a lot of love being shown to PyTorch (which isn¡¯t all that surprising if you¡¯ve used PyTorch). It all started when Francois Chollet posted his thoughts on GitHub and lit a (metaphorical) fire under the machine learning community. Another OpenAI entry in this post <U+2013> and yet another huge breakthrough by them. The title might not leap out of the page as anything special but it¡¯s important to understand what the OpenAI team have conjured up here. As one of the Redditors pointed out, this takes us one step closer to machines mimicking human behavior. It took around a year of total experience to beat the Montezuma¡¯s Revenge game at a super human level <U+2013> pretty impressive! This one is for all the aspiring data scientists reading the article. The author of the thread expounds on how he landed the coveted job, his background, where he studied data science from, etc. After answering these standard questions, he has actually written a very nice post on what others in a similar position can do to further their ambitions. There are some helpful comments as well if you scroll down a little bit. And of course, you can post your own question(s) to the author there. Quite a collection this month. I found the GAN Dissection repository quite absorbing. I¡¯m currently in the process of trying to replicate it on my own machine <U+2013> should be quite the ride. I¡¯m also keeping an eye on the ¡®Reverse Engineering a Massive Neural Network¡¯ thread as the ideas spawning there could be really helpful in case I ever find myself in that situation. Which GitHub repository and Reddit thread stood out for you? Which one will you tackle first? Let me know in the comments section below!","Keyword(freq): comment(4), developer(2), folk(2), neuron(2), repository(2), representation(2), researcher(2), resource(2), algorithm(1), ambition(1)"
"7","vidhya",2018-12-03,"Building a Random Forest from Scratch & Understanding Real-World Data Products (ML for Programmers <U+2013> Part 3)","https://www.analyticsvidhya.com/blog/2018/12/building-a-random-forest-from-scratch-understanding-real-world-data-products-ml-for-programmers-part-3/","As data scientists and machine learning practitioners, we come across and learn a plethora of algorithms. Have you ever wondered where each algorithm¡¯s true usefulness lies? Are most machine learning techniques learned with the primary aim of scaling a hackathon¡¯s leaderboard? Not necessarily. It¡¯s important to examine and understand where and how machine learning is used in real-world industry scenarios. That¡¯s where most of us are working (or will eventually work). And that¡¯s what I aim to show in this part 3 of our popular series covering the fast.ai introduction to machine learning course! We covered a fairly comprehensive introduction to random forests in part 1 using the fastai library, and followed that up with a very interesting look at how to interpret a random forest model. The latter part is especially quite relevant and important to grasp in today¡¯s world. In this article, we will first take a step back and analyze machine learning from a business standpoint. Then we¡¯ll jump straight back to where we left off part 2 <U+2013> building a random forest model from scratch. I encourage you to hop back to the previous posts in case you need to refresh any concept, and carry that learning with you as we move forward. Having learned the basic underlying concept of a random forest model and the techniques used to interpret the results, the obvious follow-up question to ask is <U+2013> where are these models and interpretation techniques used in real life? It¡¯s all well and good knowing the technique, but if we don¡¯t know where and when to apply it, it feels like a wasted effort. Jeremy Howard answers this question in Lesson #6, where he explains how a random forest model can be used to interpret and understand the data. This lesson is also a walkthrough covering all the techniques we have learned in the previous two articles. In this section, we will look at various sectors where machine learning has already made it¡¯s presence felt and is being implemented successfully. The business market, as explained by Jeremy, can broadly be divided into two groups <U+2013> horizontal and vertical. We will look at these individually, but first, let¡¯s understand the most important steps involved in designing a machine learning model. There are broadly four steps we follow for doing this. These collectively form the ¡°drivetrain approach¡±, as explained by Jeremy in his paper:<U+00A0>Designing Great Data Products. Step 1: Define the Objective Before diving into the challenge and building a machine learning model, one must have a clear, well-defined objective or an end goal in mind. This may vary depending on what the organization is trying to achieve. A couple of examples are given below: Step 2: Levers Levers are the inputs that can be controlled, or some changes the organization can make, to drive the objective defined in step 1. For instance, to ensure that the customers are satisfied: A machine learning model cannot be a lever, but it can help the organization identify the levers. It¡¯s important to understand this distinction clearly. Step 3: Data  The next step is to find out what data can be helpful in identifying and setting the lever that the organization may have (or can collect). This can be different from the data already provided or collected by the organization earlier. Step 4: Predictive models Once we have the required data that can be helpful in achieving the above defined goal, the last step is to build a simulation model on this data. Note that a simulation model can have multiple predictive models. For example, building one model identifying what items should be recommended to a user, and another model predicting the probability that a user buys a particular product on a recommendation. The idea is to create an optimization model, rather than a predictive model. You can read the paper I linked above to understand these steps in more detail. We¡¯ll move on to understand the applications of machine learning from the industry and business point-of-view. As we alluded to earlier, we can divide the business market broadly into two groups <U+2013> horizontal and vertical. I will elaborate on each in this section to give you an industry-level perspective on things. Horizontal markets are usually defined by demography (can be common across different kinds of business), which is broadly everything involving marketing. Here is a group of marketing applications where machine learning can (and is) be used. Taking the example of ¡®Churn¡¯, the goal is to determine who is going to leave or attrite. Suppose an organization has a churn model that predicts which employee is going to leave and what can be changed so that the number of employees leaving is reduced. Once the end goal is defined, we can make a list of things that can be changed in order to decrease the number of people who are leaving the organization and collect whatever data we need to build a model. Then we can create a random forest model and use the interpretation techniques we learned previously. For instance, the feature importance aspect from the random forest model can help us understand which features matter the most. Or the pdp plot visualization can be useful in determining how a particular change will affect the target variable (aka probability of an employee attriting). Vertical market refers to a group of businesses sharing the same industry, such as education, healthcare, aerospace, finance, etc. Below are a couple of examples where machine learning is being used in such cases: It¡¯s a good exercise to discuss the applications of machine learning in various domains and answer the following questions for each: Our discussion so far would have given you a fair idea about the plethora of machine learning applications in the industry. We¡¯ll do a quick review of the random forest interpretation techniques and then continue building the random forest model from scratch after that. We¡¯ll quickly recap these techniques since we have covered them in part 2. For a more detailed explanation, you can take a look at this article: Standard deviation We calculated the standard deviation of the predictions (for each level in Enclosure and ProductSize) to figure out which categories are being wrongly predicted by the model and why. We found that for categories with a low value count, the model is giving a high standard deviation. So there are higher chances that the predictions for categories with a larger value count are more accurate (since the model is trained well for these categories). Makes sense, right? Feature Importance Feature importance is basically determining how important a feature is in predicting the target variable. The top 30 variables for the random forest model are the following: As it is evident from the above plot, YearMade is the most important variable. This makes sense because the older the vehicle, the lesser the saleprice. The model performance improved when the less important features were removed from the training set. This, as you can imagine, can be really helpful in understanding the data and variables. Additionally, we can use one-hot encoding to create columns for each level and then calculate the feature importance: Partial Dependence Plot (PDP) Partial dependence is used to understand the dependence of features on the target variable. This is done by predicting the target for each row, keeping a variable constant. For instance, predicting saleprice for each row when YearMade is 1960, then for YearMade 1961, and so on. The result would be a plot like this: Tree Interpreter The tree interpreter is used to evaluate the predictions for each row using all the trees in the random forest model. This also helps us understand how much each variable contributed to the final prediction. Before we understand how the contribution for multiple trees is calculated, let¡¯s take a look at a single tree: The value of Coupler_System<=5 is 10.189, for Enclosure<=2 is 2.0, and model_id comes out to be 9.955 (considering only the top most path for now). The value for Enclosure <=2 is not only because of the feature Enclosure, but a combination of Coupler_System and Enclosure. In other words, we can say that Coupler_System interacted with Enclosure with a contribution of 0.156. Similarly, we can determine the interaction importance between features. Now we can use the average of all the trees in order to calculate the overall contribution by each feature. For the first row in the validation set, below are the contributions by each variable: Just a reminder that the calculation behind the values generated has been covered in the previous post. Extrapolation For this particular topic, Jeremy performs live coding during the lecture by creating a synthetic dataset using linespace. We have set the start and end points as 0 and 1. The next step is to create a dependent variable. For simplicity, we assume a linear relationship between x and y. We can use the following code to generate our target variable and plot the same: We¡¯ll convert our 1D array into a 2D array which will be used as an input to the random forest model. Out of the 50 data points, we¡¯ll take 40 for training our random forest model and keep the remaining 10 to be used as the validation set. We can now fit a random forest model and compare the predictions against actual values. The results are pretty good, but do you think we¡¯ll get similar results on the validation set? We have trained our model on the first 40 data points, the scale of which is actually very different from that of the validation set. So any new point that the random forest model tries to predict, it inevitably identifies that these points are closer to the highest of the given 40 points. Lets have a look at the plot: This confirms our hunch that random forest cannot extrapolate to a type of data that it has never seen before. It¡¯ll basically give you the average of the data it has previously seen. So how should one deal with this type of data? We can potentially use neural nets which have proved to work better with such cases. Another obvious solution is to use time series techniques (which i have personnaly worked on and can confirm that they show far better results). So to conclude lesson #6, we covered the necessary steps involved in building a machine learning model and briefly looked at all the interpretation techniques we learned in the previous article. If you have any questions on this section, please let me know in the comments below the article. We started learning how to build a random forest model from scratch in the previous article. We¡¯ll take it up from where we left off in this section (lesson #7). By the end of this lesson, you¡¯ll be able to build an end-to-end random forest model from the ground up on your own. Sounds pretty exciting so let¡¯s continue! We have discussed the random forest algorithm in detail <U+2013> from understanding how it works to how the split points are selected, and how the predictions are calculated. We are now going to put our understanding into code form, one step at a time, i.e., creating a model that works with few features, smaller number of trees, and a subset of the data. Note: Steps 1 to 6 have been covered in the previous article. Step 1:<U+00A0>Importing the basic libraries. Step 2:<U+00A0>Read the data and split into train and validation sets. Step 3:<U+00A0>Take a subset of data to start with. As I previously mentioned, we¡¯ll take smaller steps, so here we pick only two features and work with them. If that works well, we can complete the model by taking all the features. Step 4:<U+00A0>Define the set of inputs: Step 5:<U+00A0>Define a function that uses a sample of data (with replacement) and creates a decision tree over the same. Step 6:<U+00A0>Create a predict function. The mean of the predicted value from each tree for a particular row is returned as the final prediction. Combining all the above functions, we can create a class TreeEnsemble. Step 7:<U+00A0>Create a class DecisionTree. We call DecisionTree in the function create_tree, so let¡¯s define it here. A decision tree would have a set of independent variables, a target variable, and the index values. For now, we have created only one decision tree (we can make it recursive later). Step 8:<U+00A0>Determine the best split point. For every column, we use the function find_better_split<U+00A0>to identify a split point and then return the column name, value, and score for the split. Step 9:<U+00A0>Build our first model with 10 trees, a sample size of 1,000 and minimum leaf as 3. For the first tree, the results are: Let¡¯s now fill the block that we left above in Step 8 <U+2013> find_better_split. This is by far the most complicated part of the code to understand, but Jeremy has explained using a simple example in excel. I will explain it in an intuitive manner here. For each variable, we split the points to the left and right node, and check the score for every value. The idea is to find a split point where we are able to separate more similar points together. Consider the following example: we have two columns <U+2013> an independent variable which we try to split on, and a binary target variable. We will split at each value of the first column and calculate a standard deviation to identify how well we were able to classify the target. Let¡¯s suppose the first split point is >=3, and then calculate the standard deviation. We can take a weighted average of this value. Similarly, we can calculate for a split at 4, 6, 1 and so on. Let¡¯s put this into code: If we try to print the results of the function for both the columns individually, we get the below result: Looks like YearMade 1974 is a better split point. Step 10:<U+00A0>Compare with the scikit-learn random forest. But keep in mind that there¡¯s a tricky aspect here. While comparing the two models, both of them should have the same input. So let¡¯s store the input that we have used in the random forest we just built. And now we build a model on this subset: We see that the split here is on the column YearMade at year 1974.5, very similar to the results of our model. Not bad! Step 11:<U+00A0>There¡¯s a problem with the code we have seen so far <U+2013> can you recognize what it is? We need to optimise it!<U+00A0>in the current format, we are checking the score for the split at each row, aka we are checking for a single value multiple times. Have a look at the example we used earlier: The function will check the split points 4 and 1 twice because it actually works row-wise. It¡¯s a good idea to optimise our code so as to reduce the computation time (not everyone has a top machine!). The idea is to sort column-wise and then check the score after the split for unique values only. Also, to calculate the standard deviation, we define the following function: We will need to keep a track of the count of data points on each side of the split along with the sum of square of the values. So we initialize the variables rhs_cnt, lhs_cnt, rjs_sum2 and lhs_sum2.  Adding all this up, the code looks like this: Ideally, this function should give the same results. Let¡¯s check: Note that we created a new function (slightly changed the name from find_better_split to find_better_split_foo), and we need to use this in our DecisionTree class. The following command does it for us: Step 12:<U+00A0>Build our tree with more than one split. In step 10, we compared the first level of our model with the scikit-learn random forest model. We will now create a full tree (which splits on both our features) and compare it again. Right now our find_varsplit function looks like this: where we have defined find_better_split separately. We will update this function, so that it automatically checks for the leaf node and stores a list of indices in lhs and rhs after the split. We will again compare both the models. Previously, the max_depth was restricted to 1, and we will make it 2 here (we have only two features for now): And now look at our results: According to the image above, lhs should have 159 samples and a value of 9.66, while rhs should have 841 samples and a value of 10.15. Everything looks perfect so far! Going one level deeper into the tree, the left side of lhs should consist of 150 samples: Great! We are able to build our own tree. Let¡¯s create a function to calculate the predictions and then we¡¯ll compare the r-square values. Step 13:<U+00A0>Calculate the final predictions. We have called a predict function in TreeEnsemble<U+00A0>that returns the prediction for each row: With this, we have completed building our own random forest model. Let¡¯s plot the predictions on the validation set: Checking the performance and r-square against the scikit-learn model: Step 14:<U+00A0>Putting it all together! And there you go! That was quite a learning experience and we have now officially built a machine learning technique right from scratch. Something to be truly proud of! Let¡¯s quickly recap what we covered in part 3. We started with Lesson 6 which broadly covers the applications of machine learning in various business domains and a revision of the interpretation techniques we saw in part 2. The second half of this article covered<U+00A0>Lesson 7 and was a bit code heavy. We built a complete random forest model and compared it¡¯s performance against the scikit-learn¡¯s model. It is a good practice to understand how the model actually works, instead of simply implementing the model. With this, we come to the end of understanding, interpreting and building a random forest model. In the next part, we will shift our focus to neural networks. We¡¯ll be working on the very popular MNIST dataset so that should be quite fun! Nice article, Thanks Aishwarya !","Keyword(freq): point(10), technique(10), feature(9), result(9), prediction(8), step(6), value(6), application(5), model(5), tree(5)"
"8","vidhya",2018-12-06,"A Practical Guide to Object Detection using the Popular YOLO Framework <U+2013> Part III (with Python codes) ","https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/","How easy would our life be if we simply took an already designed framework, executed it, and got the desired result? Minimum effort, maximum reward. Isn¡¯t that what we strive for in any profession? I feel incredibly lucky to be part of our machine learning community where even the top tech behemoths embrace open source technology. Of course it¡¯s important to understand and grasp concepts before implementing them, but it¡¯s always helpful when the ground work has been laid for you by top industry data scientists and researchers. This is especially true for deep learning domains like computer vision. Not everyone has the computational resources to build a DL model from scratch. That¡¯s where predefined frameworks and pretained models come in handy. And in this article, we will look at one such framework for object detection <U+2013> YOLO. It¡¯s a supremely fast and accurate framework, as we¡¯ll see soon. So far in our series of posts detailing object detection (links below), we¡¯ve seen the various algorithms that are used, and how we can detect objects in an image and predict bounding boxes using algorithms of the R-CNN family. We have also looked at the implementation of Faster-RCNN in Python. In part 3 here, we will learn what makes YOLO tick, why you should use it over other object detection algorithms, and the different techniques used by YOLO. Once we have understood the concept thoroughly, we will then implement it it in Python. It¡¯s the ideal guide to gain invaluable knowledge and then apply it in a practical hands-on manner. I highly recommend going through the first two parts before diving into this guide: The R-CNN family of techniques we saw in Part 1 primarily use regions to localize the objects within the image. The network does not look at the entire image, only at the parts of the images which have a higher chance of containing an object. The YOLO framework (You Only Look Once) on the other hand, deals with object detection in a different way. It takes the entire image in a single instance and predicts the bounding box coordinates and class probabilities for these boxes. The biggest advantage of using YOLO is its superb speed <U+2013> it¡¯s incredibly fast and can process 45 frames per second. YOLO also understands generalized object representation. This is one of the best algorithms for object detection and has shown a comparatively similar performance to the R-CNN algorithms. In the upcoming sections, we will learn about different techniques used in YOLO algorithm. The following explanations are inspired by Andrew NG¡¯s course on Object Detection which helped me a lot in understanding the working of YOLO. Now that we have grasp on why YOLO is such a useful framework, let¡¯s jump into how it actually works. In this section, I have mentioned the steps followed by YOLO for detecting objects in a given image. Pretty straightforward, isn¡¯t it? Let¡¯s break down each step to get a more granular understanding of what we just learned. We need to pass the labelled data to the model in order to train it. Suppose we have divided the image into a grid of size 3 X 3 and there are a total of 3 classes which we want the objects to be classified into. Let¡¯s say the classes are Pedestrian, Car, and Motorcycle respectively. So, for each grid cell, the label y will be an eight dimensional vector: Here, Let¡¯s say we select the first grid from the above example: Since there is no object in this grid, pc will be zero and the y label for this grid will be: Here, ¡®?¡¯ means that it doesn¡¯t matter what bx, by, bh, bw, c1, c2, and c3<U+00A0>contain as there is no object in the grid. Let¡¯s take another grid in which we have a car (c2 = 1): Before we write the y label for this grid, it¡¯s important to first understand how YOLO decides whether there actually is an object in the grid. In the above image, there are two objects (two cars), so YOLO will take the mid-point of these two objects and these objects will be assigned to the grid which contains the mid-point of these objects. The y label for the centre left grid with the car will be: Since there is an object in this grid, pc<U+00A0>will be equal to 1. bx, by, bh, bw will be calculated relative to the particular grid cell we are dealing with. Since car is the second class, c2 = 1 and c1 and c3 = 0. So, for each of the 9 grids, we will have an eight dimensional output vector. This output will have a shape of 3 X 3 X 8. So now we have an input image and it¡¯s corresponding target vector. Using the above example (input image <U+2013> 100 X 100 X 3, output <U+2013> 3 X 3 X 8), our model will be trained as follows: We will run both forward and backward propagation to train our model. During the testing phase, we pass an image to the model and run forward propagation until we get an output y. In order to keep things simple, I have explained this using a 3 X 3 grid here, but generally in real-world scenarios we take larger grids (perhaps 19 X 19). Even if an object spans out to more than one grid, it will only be assigned to a single grid in which its mid-point is located. We can reduce the chances of multiple objects appearing in the same grid cell by increasing the more number of grids (19 X 19, for example). As I mentioned earlier, bx, by, bh, and bw are calculated relative to the grid cell we are dealing with. Let¡¯s understand this concept with an example. Consider the center-right grid which contains a car: So, bx, by, bh, and bw will be calculated relative to this grid only. The y label for this grid will be: pc = 1 since there is an object in this grid and since it is a car, c2 = 1. Now, let¡¯s see how to decide bx, by, bh, and bw. In YOLO, the coordinates assigned to all the grids are: bx, by are the x and y coordinates of the midpoint of the object with respect to this grid. In this case, it will be (around) bx = 0.4 and by = 0.3: bh<U+00A0>is the ratio of the height of the bounding box (red box in the above example) to the height of the corresponding grid cell, which in our case is around 0.9. So, <U+00A0>bh = 0.9. bw is the ratio of the width of the bounding box to the width of the grid cell. So, bw = 0.5 (approximately). The y label for this grid will be: Notice here that bx and by will always range between 0 and 1 as the midpoint will always lie within the grid. Whereas bh and bw can be more than 1 in case the dimensions of the bounding box are more than the dimension of the grid. In the next section, we will look at more ideas that can potentially help us in making this algorithm¡¯s performance even better. Here¡¯s some food for thought <U+2013> how can we decide whether the predicted bounding box is giving us a good outcome (or a bad one)? This is where Intersection over Union comes into the picture. It calculates the intersection over union of the actual bounding box and the predicted bonding box. Consider the actual and predicted bounding boxes for a car as shown below: Here, the red box is the actual bounding box and the blue box is the predicted one. How can we decide whether it is a good prediction or not? IoU, or Intersection over Union, will calculate the area of the intersection over union of these two boxes. That area will be: IoU = Area of the intersection / Area of the union, i.e. IoU = Area of yellow box / Area of green box If IoU is greater than 0.5, we can say that the prediction is good enough. 0.5 is an arbitrary threshold we have taken here, but it can be changed according to your specific problem. Intuitively, the more you increase the threshold, the better the predictions become. There is one more technique that can improve the output of YOLO significantly <U+2013> Non-Max Suppression. One of the most common problems with object detection algorithms is that rather than detecting an object just once, they might detect it multiple times. Consider the below image: Here, the cars are identified more than once. The Non-Max Suppression technique cleans up this up so that we get only a single detection per object. Let¡¯s see how this approach works. 1. It first looks at the probabilities associated with each detection and takes the largest one. In the above image, 0.9 is the highest probability, so the box with 0.9 probability will be selected first: 2. Now, it looks at all the other boxes in the image. The boxes which have high IoU with the current box are suppressed. So, the boxes with 0.6 and 0.7 probabilities will be suppressed in our example: 3. After the boxes have been suppressed, it selects the next box from all the boxes with the highest probability, which is 0.8 in our case: 4. Again it will look at the IoU of this box with the remaining boxes and compress the boxes with a high IoU: 5. We repeat these steps until all the boxes have either been selected or compressed and we get the final bounding boxes: This is what Non-Max Suppression is all about. We are taking the boxes with maximum probability and suppressing the close-by boxes with non-max probabilities. Let¡¯s quickly summarize the points which we¡¯ve seen in this section about the Non-Max suppression algorithm: There is another method we can use to improve the perform of a YOLO algorithm <U+2013> let¡¯s check it out! We have seen that each grid can only identify one object. But what if there are multiple objects in a single grid? That can so often be the case in reality. And that leads us to the concept of anchor boxes. Consider the following image, divided into a 3 X 3 grid: Remember how we assigned an object to a grid? We took the midpoint of the object and based on its location, assigned the object to the corresponding grid. In the above example, the midpoint of both the objects lies in the same grid. This is how the actual bounding boxes for the objects will be: We will only be getting one of the two boxes, either for the car or for the person. But if we use anchor boxes, we might be able to output both boxes! How do we go about doing this? First, we pre-define two different shapes called anchor boxes or anchor box shapes. Now, for each grid, instead of having one output, we will have two outputs. We can always increase the number of anchor boxes as well. I have taken two here to make the concept easy to understand: This is how the y label for YOLO without anchor boxes looks like: What do you think the y label will be if we have 2 anchor boxes? I want you to take a moment to ponder this before reading further. Got it? The y label will be: The first 8 rows belong to anchor box 1 and the remaining 8 belongs to anchor box 2. The objects are assigned to the anchor boxes based on the similarity of the bounding boxes and the anchor box shape. Since the shape of anchor box 1 is similar to the bounding box for the person, the latter will be assigned to anchor box 1 and the car will be assigned to anchor box 2. The output in this case, instead of 3 X 3 X 8 (using a 3 X 3 grid and 3 classes), will be 3 X 3 X 16 (since we are using 2 anchors). So, for each grid, we can detect two or more objects based on the number of anchors. Let¡¯s combine all the ideas we have covered so far and integrate them into the YOLO framework. In this section, we will first see how a YOLO model is trained and then how the predictions can be made for a new and previously unseen image. Training The input for training our model will obviously be images and their corresponding y labels. Let¡¯s see an image and make its y label: Consider the scenario where we are using a 3 X 3 grid with two anchors per grid, and there are 3 different object classes. So the corresponding y labels will have a shape of 3 X 3 X 16. Now, suppose if we use 5 anchor boxes per grid and the number of classes has been increased to 5. So the target will be 3 X 3 X 5 X 5 = 3 X 3 X 25. This is how the training process is done <U+2013> taking an image of a particular shape and mapping it with a 3 X 3 X 16 target (this may change as per the grid size, number of anchor boxes and the number of classes). Testing The new image will be divided into the same number of grids which we have chosen during the training period. For each grid, the model will predict an output of shape 3 X 3 X 16 (assuming this is the shape of the target during training time). The 16 values in this prediction will be in the same format as that of the training label. The first 8 values will correspond to anchor box 1, where the first value will be the probability of an object in that grid. Values 2-5 will be the bounding box coordinates for that object, and the last three values will tell us which class the object belongs to. The next 8 values will be for anchor box 2 and in the same format, i.e., first the probability, then the bounding box coordinates, and finally the classes. Finally, the Non-Max Suppression technique will be applied on the predicted boxes to obtain a single prediction per object. That brings us to the end of the theoretical aspect of understanding how the YOLO algorithm works, starting from training the model and then generating prediction boxes for the objects. Below are the exact dimensions and steps that the YOLO algorithm follows: Time to fire up our Jupyter notebooks (or your preferred IDE) and finally implement our learning in the form of code! This is what we have been building up to so far, so let¡¯s get the ball rolling. The code we¡¯ll see in this section for implementing YOLO has been taken from Andrew NG¡¯s GitHub repository on Deep Learning. You will also need to download this zip file which contains the pretrained weights required to run this code. Let¡¯s first define the functions that will help us choose the boxes above a certain threshold, find the IoU, and apply Non-Max Suppression on them. Before everything else however, we¡¯ll first import the required libraries: Now, let¡¯s create a function for filtering the boxes based on their probabilities and threshold: Next, we will define a function to calculate the IoU between two boxes: Let¡¯s define a function for Non-Max Suppression: We now have the functions that will calculate the IoU and perform Non-Max Suppression. We get the output from the CNN of shape (19,19,5,85). So, we will create a random volume of shape (19,19,5,85) and then predict the bounding boxes: Finally, we will define a function which will take the outputs of a CNN as input and return the suppressed boxes: Let¡¯s see how we can use the yolo_eval function to make predictions for a random volume which we created above: How does the outlook look? ¡®scores¡¯ represents how likely the object will be present in the volume. ¡®boxes¡¯ returns the (x1, y1, x2, y2) coordinates for the detected objects. ¡®classes¡¯ is the class of the identified object. Now, let¡¯s use a pretrained YOLO algorithm on new images and see how it works: After loading the classes and the pretrained model, let¡¯s use the functions defined above to get the yolo_outputs. Now, we will define a function to predict the bounding boxes and save the images with these bounding boxes included: Next, we will read an image and make predictions using the predict function: Finally, let¡¯s plot the predictions: Not bad! I especially like that the model correctly picked up the person in the mini-van as well. Here¡¯s a brief summary of what we covered and implemented in this guide: YOLO is one of my all-time favorite frameworks and I¡¯m sure you¡¯ll see why once you implement the code on your own machine. It¡¯s a great way of getting your hands dirty with a popular computer vision algorithm. If you have any questions or feedback regarding this guide, connect with me in the comments section below. Thanks for article.
How does YOLO compare with Faster-RCNN for detection of very small objects like scratches on metal surface? My observation was <U+2013> RCNN lacks an elegant way to compute anchor sizes based on dataset¡¦ also I attempted changing scales,strides,box size results are bad.
How do we custom train for YOLO? Hi, YOLO is faster in comparison to Faster-RCNN. Their accuracies are comparatively similar. YOLO does not work pretty well for small objects.
In order to improve its performance on smaller objects, you can try the following things: This may give better results. Is this Yolo implemented on GPU based?
How to training Yolo for customize object?
Is are sany yolo code or tutorial using Tensorflow GPU base is available ?
I only want to did detect vehicles.
Please guide me.","Keyword(freq): box(38), object(19), algorithm(7), coordinate(5), grid(5), iou(5), prediction(5), probability(5), value(5), image(4)"
