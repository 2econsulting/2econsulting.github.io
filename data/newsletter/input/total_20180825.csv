"","site","date","headline","url_address","text","keyword"
"1","kaggle",2018-08-23,"Data Notes: Drought and the War in Syria","http://blog.kaggle.com/2018/08/23/data-notes-drought-and-the-war-in-syria/","ARIMA, Syria, and mapping The Cure: Enjoy these new, intriguing, and overlooked datasets and kernels. 1.<U+00A0><U+0001F4A6> Did Drought Cause the War in Syria? (link) 2.<U+0001F4C8> Time Series for Beginners with ARIMA (link) 3.<U+0001F914> Understand ARIMA and Tune P, D, Q (link) 4.<U+0001F4B5> A Hitchhiker's Guide to Lending Club Loan Data (link) 5.<U+0001F981> Yellowbrick <U+2014> Regression Visualizer Examples (link) 6.<U+0001F489> (Bio)statistics in R: Part #3 (link) 7.<U+0001F918> EDA - The Cure Discography (link) 8.<U+0001F9E0> Dataset: Example Brain Mapping Data (link) 9.<U+0001F924> Dataset: Face Dataset with Age, Emotion, Ethnicity (link) 10.<U+26BD> Dataset: FIFA World Cup 2018 Tweets (link) Interested in exploring machine learning packages? Try H2O or fastAI!  Copyright ¨Ï 2018 Kaggle, All rights reserved. 
  

 These newsletters are really helpful Paul! Nice work!","Keyword(freq): beginner(1), dataset(1), example(1), kernel(1), newsletter(1), package(1), right(1), statistics(1), tweet(1), NA(NA)"
"2","kaggle",2018-08-22,"Getting Started with Competitions - A Peer to Peer Guide","http://blog.kaggle.com/2018/08/22/machine-learning-kaggle-competition-part-one-getting-started/","Originally published: Towards Data Science<U+00A0>by<U+00A0>William Koehrsen. In the field of data science, there are almost too many resources available: from Datacamp to Udacity to KDnuggets, there are thousands of places online to learn about data science. However, if you are someone who likes to jump in and learn by doing, Kaggle might be the single best location for expanding your skills through hands-on data science projects. While it originally was known as a place for<U+00A0>machine learning competitions,<U+00A0>Kaggle<U+200A><U+2014><U+200A>which bills itself as ¡°Your Home for Data Science¡±<U+200A><U+2014><U+200A>now offers an array of<U+00A0>data science resources. Although this series of articles will focus on a competition, it¡¯s worth pointing out the main aspects of Kaggle: Overall, Kaggle is a great place to learn, whether that¡¯s through the more traditional learning tracks or by competing in competitions. When I want to find out about the latest machine learning method, I could go read a book, or, I could go on Kaggle, find a competition, and see how people use it in practice. Personally, I find this much more enjoyable and a more effective teaching method. Moreover, the community is extremely supportive and always willing to answer questions or provide feedback on a project. In this article, we¡¯ll focus on getting started with a Kaggle machine learning competition: the<U+00A0>Home Credit Default Risk problem. This is a fairly straightforward competition with a reasonable sized dataset (which can¡¯t be<U+00A0>said for all of the competitions) which means we can compete entirely using Kaggle¡¯s kernels. This significantly lowers the barrier to entry because you don¡¯t have to worry about any software on your computer and you don¡¯t even have to download the data! As long as you have a Kaggle account and an Internet connection, you can connect to a kernel and run the code. I plan to do the entire competition on Kaggle and the kernel (a Python Jupyter Notebook) for this post is<U+00A0>available here. To get the most from this article, copy the kernel by creating a Kaggle account, then hitting the blue Fork Notebook button. This will open up the notebook for editing and running in the kernel environment. The<U+00A0>Home Credit Default Risk competition<U+00A0>is a standard supervised machine learning task where the goal is to use historical loan application data to predict whether or not an applicant will repay a loan. During training, we provide our model with the features<U+200A><U+2014><U+200A>the variables describing a loan application<U+200A><U+2014><U+200A>and the label<U+200A><U+2014><U+200A>a binary 0 if the loan was repaid and a 1 if the loan was not repaid<U+200A><U+2014><U+200A>and the model learns a mapping from the features to the label. Then, during testing, we feed the model the features for a new batch of applications and ask it to predict the label. All the data for this competition is structured meaning it exists in neat rows and columns<U+200A><U+2014><U+200A>think a spreadsheet. This means we won¡¯t need to use any<U+00A0>convolutional neural networks<U+00A0>(which excel at processing image data ) and it will give us great practice on a real-world dataset. Home Credit, the host of the competition, is a finance provider that focuses on serving the unbanked population. Predicting whether or not an application will repay a loan is a vital business need, and Home Credit has developed this competition in the hopes that the Kaggle community can develop an effective algorithm for this task. This competition follows the general idea of most Kaggle competitions: a company has data and a problem to solve, and rather than (or in addition to) hiring internal data scientists to build a model, they put up a modest prize to entice the entire world to contribute solutions. A community of thousands of skilled data scientists (Kagglers) then work on the problem for basically no charge to come up with the best solution. As far as cost effective business plans go, this seems like a brilliant idea! When you go to the competition homepage, you¡¯ll see this: Here¡¯s a quick run through of the tabs Although they are called competitions, Kaggle machine learning events should really be termed ¡°collaborative projects¡± because the main goal is not necessarily to win but to practice and learn from fellow data scientists. Once you realize that it¡¯s not so much about beating others but about expanding your own skills, you will get a lot more out of the competitions. When you sign up for Kaggle, you not only get all the resources<U+00A0>, you also get to be part of a community of data scientists with thousands of years of collective experience. Take advantage of all that experience by trying to be an active part of the community! That means anything from sharing a kernel to asking a question in a discussion forum. While it can be intimidating to make your work public, we learn best by making mistakes, receiving feedback, and improving so we don¡¯t make the same mistake again. Everyone starts out a beginner, and the community is very supportive of data scientists of all skill levels. In that mindset, I want to emphasize that discussion with others and building on others¡¯ code is not only acceptable, but encouraged! In school, working with others is called cheating and gets you a zero, but in the real world, it¡¯s called collaboration and an extremely important skill. A great method for throwing yourself into a competition is to find a kernel someone has shared with a good leaderboard score, fork the kernel, edit it to try and improve the score, and then run it to see the results. Then, make the kernel public so others can use your work.<U+00A0>Data scientists stand not on the shoulders of giants, but on the backs of thousands of individuals who have made their work public for the benefit of all. (Sorry for getting philosophical, but this is why I love data science!) Once you have a basic understanding of how Kaggle works and the philosophy of how to get the most out of a competition, it¡¯s time to get started. Here, I¡¯ll briefly outline a<U+00A0>Python Jupyter Notebook I put together in a kernelfor the Home Credit Default Risk problem, but to get the full benefit, you¡¯ll want to fork the notebook on Kaggle and run it yourself (you don¡¯t have to download or set-up anything so I¡¯d highly encourage checking it out). When you open the notebook in a kernel, you¡¯ll see this environment: Think of this as a standard Jupyter Notebook with slightly different aesthetics. You can write Python code and text (using Markdown syntax) just like in Jupyter and run the code completely in the cloud on Kaggle¡¯s servers. However, Kaggle kernels have some unique features not available in Jupyter Notebook. Hit the leftward facing arrow in the upper right to expand the kernel control panel which brings up three tabs (if the notebook is not in fullscreen, then these three tabs may already be visible next to the code). In the data tab, we can view the datasets to which our Kernel is connected. In this case, we have the entire competition data, but we can also connect to any other dataset on Kaggle or upload our own data and access it in the kernel. Data files are available in the<U+00A0>../input/<U+00A0>directory from within the code: The Settings tab lets us control different technical aspects of the kernel. Here we can add a GPU to our session, change the visibility, and install any Python package which is not already in the environment. Finally, the Versions tab lets us see any previous committed runs of the code. We can view changes to the code, look at log files of a run, see the notebook generated by a run, and download the files that are output from a run. To run the entire notebook and record a new Version, hit the blue Commit & Run button in the upper right of the kernel. This executes all the code, shows us the completed notebook (or any errors if there are mistakes), and saves any files that are created during the run. When we commit the notebook, we can then access any predictions our models made and submit them for scoring. The<U+00A0>first notebook<U+00A0>is meant to get you familiar with the problem. We start off much the same way as any data science problem: understanding the data and the task. For this problem, there is 1 main training data file (with the labels included), 1 main testing data file, and 6 additional data files. In this first notebook, we use only the main data, which will get us a decent score, but later work will have to incorporate all the data in order to be competitive. To understand the data, it¡¯s best to take a couple minutes away from the keyboard and read through the problem documentation, such as the<U+00A0>column descriptions of each data file. Because there are multiple files, we need to know how they are all linked together, although for this first notebook we only use the main file to keep things simple. Reading through other kernels can also help us get familiar with the data and which variables are important. Once we understand the data and the problem, we can start structuring it for a machine learning task This means dealing with categorical variables (through one-hot encoding), filling in the missing values (imputation), and scaling the variables to a range. We can do exploratory data analysis, such as finding correlations with the label, and graphing these relationships. We can use these relationships later on for modeling decisions, such as including which variables to use. (See the notebook for implementation). Of course, no exploratory data analysis is complete without my favorite plot, the<U+00A0>Pairs Plot. After thoroughly exploring the data and making sure it¡¯s acceptable for machine learning, we move on to creating baseline models. However, before we quite get to the modeling stage, it¡¯s critical we understand the performance metric for the competition. In a Kaggle competition, it all comes down to a single number, the metric on the test data. While it might make intuitive sense to use accuracy for a binary classification task,<U+00A0>that is a poor choice<U+00A0>because we are dealing with an imbalanced class problem. Instead of accuracy, submissions are judged in terms of ROC AUC or<U+00A0>Receiver Operating Characteristic curve Area Under the Curve. I¡¯ll let you do the<U+00A0>research on this one, or read the explanation in the notebook. Just know that higher is better, with a random model scoring 0.5 and a perfect model scoring 1.0. To calculate a ROC AUC, we need to make predictions in terms of probabilities rather than a binary 0 or 1. The ROC<U+00A0>then shows the True Positive Rate versus the False Positive Rate<U+00A0>as a function of the threshold according to which we classify an instance as positive. Usually we like to make a naive baseline prediction, but in this case, we already know that random guessing on the task would get an ROC AUC of 0.5. Therefore, for our baseline model, we will use a slightly more sophisticated method,<U+00A0>Logistic Regression. This is a popular simple algorithm for binary classification problems and it will set a low bar for future models to surpass. After implementing the logistic regression, we can save the results to a csv file for submission. When the notebook is committed, any results we write will show up in the Output sub-tab on the Versions tab: From this tab, we can download the submissions to our computer and then upload them to the competition. In this notebook, we make four different models with scores as follows: These scores don¡¯t get us anywhere close to the top of the leaderboard, but they leave room for plenty of future improvement! We also get an idea of the performance we can expect using only a single source of data. (Not surprisingly, the extraordinary<U+00A0>Gradient Boosting Machine<U+00A0>(using the<U+00A0>LightGBM library) performs the best. This<U+00A0>model wins nearly every structured Kaggle competition<U+00A0>(where the data is in table format) and we will likely need to use some form of this model if we want to seriously compete!) This article and introductory kernel demonstrated a basic start to a Kaggle competition. It¡¯s not meant to win, but rather to show you the basics of how to approach a machine learning competition and also a few models to get you off the ground (although the LightGBM model is like jumping off the deep end). Furthermore, I laid out my philosophy for machine learning competitions, which is to learn as much as possible by taking part in discussions, building on other¡¯s code, and sharing your own work. It¡¯s enjoyable to best your past scores, but I view doing well not as the main focus but as a positive side effect of learning new data science techniques. While these are known as competitions, they are really collaborative projects where everyone is welcome to participate and hone their abilities. There remains a ton of work to be done, but thankfully we don¡¯t have to do it alone. In later articles and notebooks we¡¯ll see how to build on the work of others to make even better models. I hope this article (and the<U+00A0>notebook kernel) has given you the confidence to start competing on Kaggle or taking on any data science project. As always, I welcome constructive criticism and discussion and can be reached on Twitter<U+00A0>@koehrsen_will. What a great article.. Inspired me to try. Thanks a lot.. A lot of labor has gone into writing this highly systemic article. Thank you for your patience. Google will down grade your website accordingly. The burglar instinctively got up and started to
run but couldn't see because of the tearing wartrol caused.
Trying vehicles video online strategy? https://Www.argo-shop.com.ua/gotourl.php?url=http://win88.today/download-play8oy-android-ios/ For newest information you have to go to see internet and on the web I found this web
page as a finest web page for most recent updates.","Keyword(freq): competition(8), other(7), file(6), model(6), scientist(6), variable(5), thousand(4), feature(3), kernel(3), project(3)"
"3","mastery",2018-08-24,"How to Model Volatility with ARCH and GARCH for Time Series Forecasting in Python","https://machinelearningmastery.com/develop-arch-and-garch-models-for-time-series-forecasting-in-python/","The ARCH or<U+00A0>Autoregressive Conditional Heteroskedasticity method provides a way to model a change in variance in a time series that is time dependent, such as increasing or decreasing volatility. An extension of this approach named GARCH or Generalized Autoregressive Conditional Heteroskedasticity allows the method to support changes in the time dependent volatility, such as increasing and decreasing volatility in the same series. In this tutorial, you will discover the ARCH and GARCH models for predicting the variance of a time series. After completing this tutorial, you will know: Let¡¯s get started. How to Develop ARCH and GARCH Models for Time Series Forecasting in PythonPhoto by Murray Foubister, some rights reserved. This tutorial is divided into five parts; they are: Autoregressive models can be developed for univariate time series data that is stationary (AR), has a trend (ARIMA), and has a seasonal component (SARIMA). One aspect of a univariate time series that these autoregressive models do not model is a change in the variance over time. Classically, a time series with modest changes in variance can sometimes be adjusted using a power transform, such as by taking the Log or using a Box-Cox transform. There are some time series where the variance changes consistently over time. In the context of a time series in the financial domain, this would be called increasing and decreasing volatility. In time series where the variance is increasing in a systematic way, such as an increasing trend, this property of the series is called heteroskedasticity. It¡¯s a fancy word from statistics that means changing or unequal variance across the series. If the change in variance can be correlated over time, then it can be modeled using an autoregressive process, such as ARCH. Autoregressive Conditional Heteroskedasticity, or ARCH, is a method that explicitly models the change in variance over time in a time series. Specifically, an ARCH method models the variance at a time step as a function of the residual errors from a mean process (e.g. a zero mean). The ARCH process introduced by Engle (1982) explicitly recognizes the difference between the unconditional and the conditional variance allowing the latter to change over time as a function of past errors. <U+2014> Generalized autoregressive conditional heteroskedasticity, 1986. A lag parameter must be specified to define the number of prior residual errors to include in the model. Using the notation of the GARCH model (discussed later), we can refer to this parameter as ¡°q¡°. Originally, this parameter was called ¡°p¡°, and is also called ¡°p¡± in the arch Python package used later in this tutorial. A generally accepted notation for an ARCH model is to specify the ARCH() function with the q parameter ARCH(q); for example, ARCH(1) would be a first order ARCH model. The approach expects the series is stationary, other than the change in variance, meaning it does not have a trend or seasonal component. An ARCH model is used to predict the variance at future time steps. [ARCH] are mean zero, serially uncorrelated processes with nonconstant variances conditional on the past, but constant unconditional variances. For such processes, the recent past gives information about the one-period forecast variance. <U+2013> Autoregressive Conditional Heteroscedasticity with Estimates of the Variance of United Kingdom Inflation, 1982. In practice, this can be used to model the expected variance on the residuals after another autoregressive model has been used, such as an ARMA or similar. The model should only be applied to a prewhitened residual series {e_t} that is uncorrelated and contains no trends or seasonal changes, such as might be obtained after fitting a satisfactory SARIMA model. <U+2014> Page 148, Introductory Time Series with R, 2009. Generalized Autoregressive Conditional Heteroskedasticity, or GARCH, is an extension of the ARCH model that incorporates a moving average component together with the autoregressive component. Specifically, the model includes lag variance terms (e.g. the observations if modeling the white noise residual errors of another process), together with lag residual errors from a mean process. The introduction of a moving average component allows the model to both model the conditional change in variance over time as well as changes in the time-dependent variance. Examples include conditional increases and decreases in variance. As such, the model introduces a new parameter ¡°p¡± that describes the number of lag variance terms: A generally accepted notation for a GARCH model is to specify the GARCH() function with the p and q parameters GARCH(p, q); for example GARCH(1, 1) would be a first order GARCH model. A GARCH model subsumes ARCH models, where a GARCH(0, q) is equivalent to an ARCH(q) model. For p = 0 the process reduces to the ARCH(q) process, and for p = q = 0 E(t) is simply white noise. In the ARCH(q) process the conditional variance is specified as a linear function of past sample variances only, whereas the GARCH(p, q) process allows lagged conditional variances to enter as well. This corresponds to some sort of adaptive learning mechanism. <U+2014> Generalized autoregressive conditional heteroskedasticity, 1986. As with ARCH, GARCH predicts the future variance and expects that the series is stationary, other than the change in variance, meaning it does not have a trend or seasonal component. The configuration for an ARCH model is best understood in the context of ACF and PACF plots of the variance of the time series. This can be achieved by subtracting the mean from each observation in the series and squaring the result, or just squaring the observation if you¡¯re already working with white noise residuals from another model. If a correlogram appears to be white noise [¡¦], then volatility ca be detected by looking at the correlogram of the squared values since the squared values are equivalent to the variance (provided the series is adjusted to have a mean of zero). <U+2014> Pages 146-147, Introductory Time Series with R, 2009. The ACF and PACF plots can then be interpreted to estimate values for p and q, in a similar way as is done for the ARMA model. For more information on how to do this, see the post: In this section, we will look at how we can develop ARCH and GARCH models in Python using the arch library. First, let¡¯s prepare a dataset we can use for these examples. We can create a dataset with a controlled model of variance. The simplest case would be a series of random noise where the mean is zero and the variance starts at 0.0 and steadily increases. We can achieve this in Python using the gauss() function that generates a Gaussian random number with the specified mean and standard deviation. We can plot the dataset to get an idea of how the linear change in variance looks. The complete example is listed below. Running the example creates and plots the dataset. We can see the clear change in variance over the course of the series. Line Plot of Dataset with Increasing Variance We know there is an autocorrelation in the variance of the contrived dataset. Nevertheless, we can look at an autocorrelation plot to confirm this expectation. The complete example is listed below. Running the example creates an autocorrelation plot of the squared observations. We see significant positive correlation in variance out to perhaps 15 lag time steps. This might make a reasonable value for the parameter in the ARCH model. Autocorrelation Plot of Data with Increasing Variance Developing an ARCH model involves three steps: Before fitting and forecasting, we can split the dataset into a train and test set so that we can fit the model on the train and evaluate its performance on the test set. A model can be defined by calling the arch_model() function. We can specify a model for the mean of the series: in this case mean=¡¯Zero¡¯ is an appropriate model. We can then specify the model for the variance: in this case vol=¡¯ARCH¡¯. We can also specify the lag parameter for the ARCH model: in this case p=15. Note, in the arch library, the names of p and q parameters for ARCH/GARCH have been reversed. The model can be fit on the data by calling the fit() function. There are many options on this function, although the defaults are good enough for getting started. This will return a fit model. Finally, we can make a prediction by calling the forecast() function on the fit model. We can specify the horizon for the forecast. In this case, we will predict the variance for the last 10 time steps of the dataset, and withhold them from the training of the model. We can tie all of this together; the complete example is listed below. Running the example defines and fits the model then predicts the variance for the last 10 time steps of the dataset. A line plot is created comparing the series of expected variance to the predicted variance. Although the model was not tuned, the predicted variance looks reasonable. Line Plot of Expected Variance to Predicted Variance using ARCH We can fit a GARCH model just as easily using the arch library. The arch_model() function can specify a GARCH instead of ARCH model vol=¡¯GARCH¡¯ as well as the lag parameters for both. The dataset may not be a good fit for a GARCH model given the linearly increasing variance, nevertheless, the complete example is listed below. A plot of the expected and predicted variance is listed below. Line Plot of Expected Variance to Predicted Variance using GARCH This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered the ARCH and GARCH models for predicting the variance of a time series. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ...with just a few lines of python code Discover how in my new Ebook:Introduction to Time Series Forecasting With Python It covers self-study tutorials and end-to-end projects on topics like:Loading data, visualization, modeling, algorithm tuning, and much more... Skip the Academics. Just Results. Click to learn more. Hi Jason, You mentioned the need for PACF but you haven¡¯t plotted it, isn¡¯t PACF needed to determine q? Best,
Elie K Yes, you can use ACF and PACF, learn more here:https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/ Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning. Read More","Keyword(freq): model(9), change(5), error(5), step(5), variance(4), parameter(3), plot(3), value(3), example(2), increase(2)"
"4","mastery",2018-08-22,"4 Common Machine Learning Data Transforms for Time Series Forecasting","https://machinelearningmastery.com/machine-learning-data-transforms-for-time-series-forecasting/","Time series data often requires some preparation prior to being modeled with machine learning algorithms. For example, differencing operations can be used to remove trend and seasonal structure from the sequence in order to simplify the prediction problem. Some algorithms, such as neural networks, prefer data to be standardized and/or normalized prior to modeling. Any transform operations applied to the series also require a similar inverse transform to be applied on the predictions. This is required so that the resulting calculated performance measures are in the same scale as the output variable and can be compared to classical forecasting methods. In this post, you will discover how to perform and invert four common data transforms for time series data in machine learning. After reading this post, you will know: Let¡¯s get started. 4 Common Machine Learning Data Transforms for Time Series ForecastingPhoto by Wolfgang Staudt, some rights reserved. This tutorial is divided into three parts; they are: Given a univariate time series dataset, there are four transforms that are popular when using machine learning methods to model and make predictions. They are: Let¡¯s take a quick look at each in turn and how to perform these transforms in Python. We will also review how to reverse the transform operation as this is required when we want to evaluate the predictions in their original scale so that performance measures can be compared directly. Are there other transforms you like to use on your time series data for modeling with machine learning methods?
Let me know in the comments below. A power transform removes a shift from a data distribution to make the distribution more-normal (Gaussian). On a time series dataset, this can have the effect of removing a change in variance over time. Popular examples are the log transform (positive values) or generalized versions such as the Box-Cox transform (positive values) or the Yeo-Johnson transform (positive and negative values). For example, we can implement the Box-Cox transform in Python using the boxcox() function from the SciPy library. By default, the method will numerically optimize the lambda value for the transform and return the optimal value. The transform can be inverted but requires a custom function listed below named invert_boxcox() that takes a transformed value and the lambda value that was used to perform the transform. A complete example of applying the power transform to a dataset and reversing the transform is listed below. Running the example prints the original dataset, the results of the power transform, and the original values (or close to it) after the transform is inverted. A difference transform is a simple way for removing a systematic structure from the time series. For example, a trend can be removed by subtracting the previous value from each value in the series. This is called first order differencing. The process can be repeated (e.g. difference the differenced series) to remove second order trends, and so on. A seasonal structure can be removed in a similar way by subtracting the observation from the prior season, e.g. 12 time steps ago for monthly data with a yearly seasonal structure. A single differenced value in a series can be calculated with a custom function named difference() listed below. The function takes the time series and the interval for the difference calculation, e.g. 1 for a trend difference or 12 for a seasonal difference. Again, this operation can be inverted with a custom function that adds the original value back to the differenced value named invert_difference() that takes the original series and the interval. We can demonstrate this function below. Running the example prints the original dataset, the results of the difference transform, and the original values after the transform is inverted. Note, the first ¡°interval¡± values will be lost from the sequence after the transform. This is because they do not have a value at ¡°interval¡± prior time steps, therefore cannot be differenced. Standardization is a transform for data with a Gaussian distribution. It subtracts the mean and divides the result by the standard deviation of the data sample. This has the effect of transforming the data to have mean of zero, or centered, with a standard deviation of 1. This resulting distribution is called a standard Gaussian distribution, or a standard normal, hence the name of the transform. We can perform standardization using the StandardScaler object in Python from the scikit-learn library. This class allows the transform to be fit on a training dataset by calling fit(), applied to one or more datasets (e.g. train and test) by calling transform() and also provides a function to reverse the transform by calling inverse_transform(). A complete example is applied below. Running the example prints the original dataset, the results of the standardize transform, and the original values after the transform is inverted. Note the expectation that data is provided as a column with multiple rows. Normalization is a rescaling of data from the original range to a new range between 0 and 1. As with standardization, this can be implemented using a transform object from the scikit-learn library, specifically the MinMaxScaler class. In addition to normalization, this class can be used to rescale data to any range you wish by specifying the preferred range in the constructor of the object. It can be used in the same way to fit, transform, and inverse the transform. A complete example is listed below. Running the example prints the original dataset, the results of the normalize transform, and the original values after the transform is inverted. We have mentioned the importance of being able to invert a transform on the predictions of a model in order to calculate a model performance statistic that is directly comparable to other methods. Additionally, another concern is the problem of data leakage. Three of the above data transforms estimate coefficients from a provided dataset that are then used to transform the data. Specifically: These coefficients must be estimated on the training dataset only. Once estimated, the transform can be applied using the coefficients to the training and the test dataset before evaluating your model. If the coefficients are estimated using the entire dataset prior to splitting into train and test sets, then there is a small leakage of information from the test set to the training dataset. This can result in estimates of model skill that are optimistically biased. As such, you may want to enhance the estimates of the coefficients with domain knowledge, such as expected min/max values for all time in the future. Generally, differencing does not suffer the same problems. In most cases, such as one-step forecasting, the lag observations are available to perform the difference calculation. If not, the lag predictions can be used wherever needed as a proxy for the true observations in difference calculations. You may want to experiment with applying multiple data transforms to a time series prior to modeling. This is quite common, e.g. to apply a power transform to remove an increasing variance, to apply seasonal differencing to remove seasonality, and to apply one-step differencing to remove a trend. The order that the transform operations are applied is important. Intuitively, we can think through how the transforms may interact. As such, a suggested ordering for data transforms is as follows: Obviously, you would only use the transforms required for your specific dataset. Importantly, when the transform operations are inverted, the order of the inverse transform operations must be reversed. Specifically, the inverse operations must be performed in the following order: This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered how to perform and invert four common data transforms for time series data in machine learning. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning. Read More","Keyword(freq): transform(11), value(9), operation(6), coefficient(5), prediction(5), method(4), result(4), algorithm(2), comment(2), estimate(2)"
"5","mastery",2018-08-20,"A Gentle Introduction to Exponential Smoothing for Time Series Forecasting in Python","https://machinelearningmastery.com/exponential-smoothing-for-time-series-forecasting-in-python/","It is a powerful forecasting method that may be used as an alternative to the popular Box-Jenkins ARIMA family of methods. In this tutorial, you will discover the exponential smoothing method for univariate time series forecasting. After completing this tutorial, you will know: Let¡¯s get started. A Gentle Introduction to Exponential Smoothing for Time Series Forecasting in PythonPhoto by Wolfgang Staudt, some rights reserved. This tutorial is divided into 4 parts; they are: Exponential smoothing is a time series forecasting method for univariate data. Time series methods like the Box-Jenkins ARIMA family of methods develop a model where the prediction is a weighted linear sum of recent past observations or lags. Exponential smoothing forecasting methods are similar in that a prediction is a weighted sum of past observations, but the model explicitly uses an exponentially decreasing weight for past observations. Specifically, past observations are weighted with a geometrically decreasing ratio. Forecasts produced using exponential smoothing methods are weighted averages of past observations, with the weights decaying exponentially as the observations get older. In other words, the more recent the observation the higher the associated weight. <U+2014> Page 171, Forecasting: principles and practice, 2013. Exponential smoothing methods may be considered as peers and an alternative to the popular Box-Jenkins ARIMA class of methods for time series forecasting. Collectively, the methods are sometimes referred to as ETS models, referring to the explicit modeling of Error, Trend and Seasonality. There are three main types of exponential smoothing time series forecasting methods. A simple method that assumes no systematic structure, an extension that explicitly handles trends, and the most advanced approach that add support for seasonality. Single Exponential Smoothing, SES for short, also called Simple Exponential Smoothing, is a time series forecasting method for univariate data without a trend or seasonality. It requires a single parameter, called alpha (a), also called the smoothing factor or smoothing coefficient. This parameter controls the rate at which the influence of the observations at prior time steps decay exponentially. Alpha is often set to a value between 0 and 1. Large values mean that the model pays attention mainly to the most recent past observations, whereas smaller values mean more of the history is taken into account when making a prediction. A value close to 1 indicates fast learning (that is, only the most recent values influence the forecasts), whereas a value close to 0 indicates slow learning (past observations have a large influence on forecasts). <U+2014> Page 89, Practical Time Series Forecasting with R, 2016. Hyperparameters: Double Exponential Smoothing is an extension to Exponential Smoothing that explicitly adds support for trends in the univariate time series. In addition to the alpha parameter for controlling smoothing factor for the level, an additional smoothing factor is added to control the decay of the influence of the change in trend called beta (b). The method supports trends that change in different ways: an additive and a multiplicative, depending on whether the trend is linear or exponential respectively. Double Exponential Smoothing with an additive trend is classically referred to as Holt¡¯s linear trend model, named for the developer of the method Charles Holt. For longer range (multi-step) forecasts, the trend may continue on unrealistically. As such, it can be useful to dampen the trend over time. Dampening means reducing the size of the trend over future time steps down to a straight line (no trend). The forecasts generated by Holt¡¯s linear method display a constant trend (increasing or decreasing) indecently into the future. Even more extreme are the forecasts generated by the exponential trend method [¡¦] Motivated by this observation [¡¦] introduced a parameter that ¡°dampens¡± the trend to a flat line some time in the future. <U+2014> Page 183, Forecasting: principles and practice, 2013. As with modeling the trend itself, we can use the same principles in dampening the trend, specifically additively or multiplicatively for a linear or exponential dampening effect. A damping coefficient Phi (p) is used to control the rate of dampening. Hyperparameters: Triple Exponential Smoothing is an extension of Exponential Smoothing that explicitly adds support for seasonality to the univariate time series. This method is sometimes called Holt-Winters Exponential Smoothing, named for two contributors to the method: Charles Holt and Peter Winters. In addition to the alpha and beta smoothing factors, a new parameter is added called gamma (g) that controls the influence on the seasonal component. As with the trend, the seasonality may be modeled as either an additive or multiplicative process for a linear or exponential change in the seasonality. Triple exponential smoothing is the most advanced variation of exponential smoothing and through configuration, it can also develop double and single exponential smoothing models. Being an adaptive method, Holt-Winter¡¯s exponential smoothing allows the level, trend and seasonality patterns to change over time. <U+2014> Page 95, Practical Time Series Forecasting with R, 2016. Additionally, to ensure that the seasonality is modeled correctly, the number of time steps in a seasonal period (Period) must be specified. For example, if the series was monthly data and the seasonal period repeated each year, then the Period=12. Hyperparameters: All of the model hyperparameters can be specified explicitly. This can be challenging for experts and beginners alike. Instead, it is common to use numerical optimization to search for and fund the smoothing coefficients (alpha, beta, gamma, and phi) for the model that result in the lowest error. [¡¦] a more robust and objective way to obtain values for the unknown parameters included in any exponential smoothing method is to estimate them from the observed data. [¡¦] the unknown parameters and the initial values for any exponential smoothing method can be estimated by minimizing the SSE [sum of the squared errors]. <U+2014> Page 177, Forecasting: principles and practice, 2013. The parameters that specify the type of change in the trend and seasonality, such as weather they are additive or multiplicative and whether they should be dampened, must be specified explicitly. This section looks at how to implement exponential smoothing in Python. The implementations of Exponential Smoothing in Python are provided in the Statsmodels Python library. The implementations are based on the description of the method in Rob Hyndman and George Athana¡©sopou¡©los¡¯ excellent book ¡°Forecasting: Principles and Practice,¡± 2013 and their R implementations in their ¡°forecast¡± package. Single Exponential Smoothing or simple smoothing can be implemented in Python via the SimpleExpSmoothing Statsmodels class. First, an instance of the SimpleExpSmoothing class must be instantiated and passed the training data. The fit() function is then called providing the fit configuration, specifically the alpha value called smoothing_level. If this is not provided or set to None, the model will automatically optimize the value. This fit() function returns an instance of the HoltWintersResults class that contains the learned coefficients. The forecast() or the predict() function on the result object can be called to make a forecast. For example: Single, Double and Triple Exponential Smoothing can be implemented in Python using the ExponentialSmoothing Statsmodels class. First, an instance of the ExponentialSmoothing class must be instantiated, specifying both the training data and some configuration for the model. Specifically, you must specify the following configuration parameters: The model can then be fit on the training data by calling the fit() function. This function allows you to either specify the smoothing coefficients of the exponential smoothing model or have them optimized. By default, they are optimized (e.g. optimized=True). These coefficients include: Additionally, the fit function can perform basic data preparation prior to modeling; specifically: The fit() function will return an instance of the HoltWintersResults class that contains the learned coefficients. The forecast() or the predict() function on the result object can be called to make a forecast. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered the exponential smoothing method for univariate time series forecasting. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ...with just a few lines of python code Discover how in my new Ebook:Introduction to Time Series Forecasting With Python It covers self-study tutorials and end-to-end projects on topics like:Loading data, visualization, modeling, algorithm tuning, and much more... Skip the Academics. Just Results. Click to learn more. Hi Jason,  Thank you very much for your post. This is very helpful resources. I would like to know how to install ¡°statsmodels.tsa.holtwinters¡± as I see that it is throwing error when I ran the command :
from statsmodels.tsa.holtwinters import ExponentialSmoothing It seems that statsmodels package do not have that command.
Could you please help me in working that command? ThanK you,
Sandeep It really depends on your platform, for example:  Alternately, try this tutorial:https://machinelearningmastery.com/setup-python-environment-machine-learning-deep-learning-anaconda/ Hello Jason, I am working on a forecasting project with a big dataset which includes 15 columns and around 9000 rows. The problem is I have to forecast the result for the next two years base on 14 columns of independent data, and the result should be binary(0,1).
I saw many forecasting problems online, but most of them forecast base on just one column of independent data with no binary result.
Is there any way to guide me or refer me any references to solve the problem? Thank you in advance,
Ehsan Yes, a neural network can easily forecast multiple variables, perhaps start with an MLP. Hi Jason, Hyndman has published a new edition of ¡®Forecasting, principles and practice¡¯. It is available free of charge at: https://otexts.org/fpp2/ . Best,
Elie Thanks. Thanks for this <U+2013> clear, and gentle, with nice follow up resources! You¡¯re welcome! Thanks for really nice and helpful matter on exponential smoothing. Thanks! Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning. Read More","Keyword(freq): method(9), observation(9), forecast(6), principle(6), statsmodel(6), coefficient(5), value(5), parameter(4), thank(4), hyperparameter(3)"
"6","vidhya",2018-08-24,"The Ultimate Data Science and Machine Learning Blogathon <U+2013> More than $2500 up for grabs!","https://www.analyticsvidhya.com/blog/2018/08/data-science-machine-learning-blogathon-lucrative-prizes-bonus/","

DATA DAYS DISCOUNT: Get FLAT 33% OFF on All Courses | Use COUPON CODE: DATA33 |
OFFER ENDS: 26th Aug 23:59 hrs IST | 
Valid for first 33 users(daily) | Buy Now 

 ¡°If you want to change the world, pick up a pen and write.¡±<U+200A><U+2014><U+200A>Martin Luther We are delighted to announce the launch of<U+00A0>Analytics Vidhya¡¯s Blogathon, the ultimate competition which combines your writing prowess with your machine learning skills! Analytics Vidhya has always been at the forefront of knowledge sharing, and we want to continue this trend among our community members. We will not only publish the best articles on Analytics Vidhya¡¯s Medium page, but also provide feedback to every writer on their article. Along with recognition in front of a broad audience, there are lucrative prizes to be won! The focus of the articles can be on any data related field<U+200A><U+2014><U+200A>data science, machine learning, deep learning, Artificial Intelligence, Business Analytics, etc. The blogathon starts today, 25th August, and will conclude on 23rd September. Beyond this, you will be given an extra week to work on any feedback we have provided on an already submitted article. We will announce the winners in each category on 30th September. To enter the competition, you need to<U+00A0>fill in your details here. We will then add you as a writer to our Medium publication and you can start sending your drafts to us. It really is that simple! Every article which meets Analytics Vidhya¡¯s standards will get published on our Medium page.<U+00A0>There are $2500 + bonuses to be won!<U+00A0>Prizes will be given in 3 categories: Most popular article:<U+00A0>This will be decided by the number of unique fans the article gets. Most number of articles with more than 10 unique fans:<U+00A0>As the name suggests, the more articles you submit and the more fans you gather, the better your chance of grabbing this prize! Editor¡¯s Prize:<U+00A0>This category will be judged by AV¡¯s in-house editing team and the winner will receive $500! There¡¯s more.. The top 25 bloggers will each receive free access for 6 months to AV¡¯s ¡®Computer Vision using Deep Learning¡¯ course!<U+00A0>It is a one-of-a-kind course that will introduce you to the world of CV and ensure you come out a master of the field. The top 50 participants will get access to DataHack 2017<U+00A0>day 1<U+00A0>and<U+00A0>day 2<U+00A0>talks<U+2014> a collection of all the sessions that happened in India¡¯s premier analytics conference last year. Hear from business leaders, domain experts, senior data scientists and other eminent personalities in the analytics domain. An unmissable opportunity! For those articles which do not meet our standards, we will ensure you are provided with feedback on how to make those articles better in the future.","Keyword(freq): analytics(6), article(6), fan(3), prize(2), standard(2), blogger(1), bonus(1), category(1), detail(1), draft(1)"
"7","vidhya",2018-08-22,"A Hands-On Guide to Automated Feature Engineering using Featuretools in Python","https://www.analyticsvidhya.com/blog/2018/08/guide-automated-feature-engineering-featuretools-python/","

DATA DAYS DISCOUNT: Get FLAT 33% OFF on All Courses | Use COUPON CODE: DATA33 |
OFFER ENDS: 26th Aug 23:59 hrs IST | 
Valid for first 33 users(daily) | Buy Now 

 Anyone who has participated in machine learning hackathons and competitions can attest to how crucial feature engineering can be. It is often the difference between getting into the top 10 of the leaderboard and finishing outside the top 50! I have been a huge advocate of feature engineering ever since I realized it¡¯s immense potential. But it can be a slow and arduous process when done manually. I have to spend time brainstorming over what features to come up, and analyze their usability them from different angles. Now, this entire FE process can be automated and I¡¯m going to show you how in this article. <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> Source: VentureBeat We will be using the Python feature engineering library called Featuretools to do this. But before we get into that, we will first look at the basic building blocks of FE, understand them with intuitive examples, and then finally dive into the awesome world of automated feature engineering using the BigMart Sales dataset. In the context of machine learning, a feature can be described as a characteristic, or a set of characteristics, that explains the occurrence of a phenomenon. When these characteristics are converted into some measurable form, they are called features. For example, assume you have a list of students. This list contains the name of each student, number of hours they studied, their IQ, and their total marks in the previous examinations. Now you are given information about a new student<U+2014> the number of hours he/she studied and his IQ, but his/her marks are missing. You have to estimate his/her probable marks. Here, you¡¯d use IQ and study_hours to build a predictive model to estimate these missing marks. So, IQ and study_hours are called the features for this model. Feature Engineering can simply be defined as the process of creating new features from the existing features in a dataset. Let¡¯s consider a sample data that has details about a few items, such as their weight and price. Now, to create a new feature we can use Item_Weight and Item_Price. So, let¡¯s create a feature called Price_per_Weight. It is nothing but the price of the item divided by the weight of the item. This process is called feature engineering. This was just a simple example to create a new feature from existing ones, but in practice, when we have quite a lot of features, feature engineering can become quite complex and cumbersome. Let¡¯s take another example. In the popular Titanic dataset, there is a passenger name feature and below are some of the names in the dataset: These names can actually be broken down<U+00A0>into additional meaningful features. For example, we can extract and group similar titles into single categories. Let¡¯s have a look at the unique number of titles in the passenger names. It turns out that titles like<U+00A0>¡®Dona¡¯, ¡®Lady¡¯, ¡®the Countess¡¯, ¡®Capt¡¯, ¡®Col¡¯, ¡®Don¡¯, ¡®Dr¡¯, ¡®Major¡¯, ¡®Rev¡¯, ¡®Sir¡¯, and ¡®Jonkheer¡¯ are quite rare and can be put under a single label. Let¡¯s call it rare_title. Apart from this, the titles ¡®Mlle¡¯ and ¡®Ms¡¯ can be placed under ¡®Miss¡¯, and ¡®Mme¡¯ can be replaced with ¡®Mrs¡¯. Hence, the new title feature would have only 5 unique values as shown below: So, this is how we can extract useful information with the help of feature engineering, even from features like passenger names which initially seemed fairly pointless. The performance of a predictive model is heavily dependent on the quality of the features in the dataset used to train that model. If you are able to create new features which help in providing more information to the model about the target variable, it¡¯s performance will go up. Hence, when we don¡¯t have enough quality features in our dataset, we have to lean on feature engineering. In one of the most popular Kaggle competitions, Bike Sharing Demand Prediction, the participants are asked to forecast the rental demand in Washington, D.C based on historical usage patterns in relation with weather, time and other data. As explained in this article, smart feature engineering was instrumental in securing a place in the top 5 percentile of the leaderboard. Some of the features created are given below: Creating such features is no cakewalk <U+2013> it takes a great deal of brainstorming and extensive data exploration. Not everyone is good at feature engineering because it is not something that you can learn by reading books or watching videos. This is why feature engineering is also called an art. If you are good at it, then you have a major edge over the competition. Quite like Roger Federer, the master of feature engineering when it comes to Tennis shots. Analyze the two images shown above. The left one shows a car being assembled by a group of men during early 20th century, and the right picture shows robots doing the same job in today¡¯s world. Automating any process has the potential to make it much more efficient and cost-effective. For similar reasons, feature engineering can, and has been, automated in machine learning. Building machine learning models can often be a painstaking and tedious process. It involves many steps so if we are able to automate a certain percentage of feature engineering tasks, then the data scientists or the domain experts can focus on other aspects of the model. Sounds too good to be true, right? Now that we have understood that automating feature engineering is the need of the hour, the next question to ask is <U+2013> how is it going to happen? Well, we have a great tool to address this issue and it¡¯s called Featuretools. Featuretools is an open source library for performing automated feature engineering. It is a great tool designed to fast-forward the feature generation process, thereby giving more time to focus on other aspects of machine learning model building. In other words, it makes your data ¡°machine learning ready¡±. Before taking Featuretools for a spin, there are three major components of the package that we should be aware of: a) An Entity<U+00A0>can be considered as a representation of a Pandas DataFrame. A collection of multiple entities is called an Entityset. b) Deep Feature Synthesis<U+00A0>(DFS) has got nothing to do with deep learning. Don¡¯t worry. DFS is actually a Feature Engineering method and is the backbone of Featuretools. It enables the creation of new features from single, as well as multiple dataframes. c) DFS create features by applying Feature primitives to the Entity-relationships in an EntitySet. These primitives are the often-used methods to generate features manually. For example, the primitive ¡°mean¡± would find the mean of a variable at an aggregated level. The best way to understand and become comfortable with Featuretools is by applying it on a dataset. So, we will use the dataset from our BigMart Sales practice problem<U+00A0>in the next section to solidify our concepts. The objective of the BigMart Sales challenge is to build a predictive model to estimate the sales of each product at a particular store. This would help the decision makers at BigMart to find out the properties of any product or store, which play a key role in increasing the overall sales. Note that there are 1559 products across 10 stores in the given dataset. The below table shows the features provided in our data: You can download the data from here. Featuretools is available for Python 2.7, 3.5, and 3.6. You can easily install Featuretools using pip. To start off, we¡¯ll just store the target Item_Outlet_Sales in a variable called sales and id variables in test_Item_Identifier and<U+00A0>test_Outlet_Identifier. Then we will combine the train and test set as it saves us the trouble of performing the same step(s) twice. Let¡¯s check the missing values in the dataset. Quite a lot of missing values in the Item_Weight and Outlet_size variables. Let¡¯s quickly deal with them: I will not do an extensive preprocessing operation since the objective of this article is to get you started with Featuretools. It seems Item_Fat_Content contains only two categories, i.e., ¡°Low Fat¡± and ¡°Regular¡± <U+2013> the rest of them we will consider redundant. So, let¡¯s convert it into a binary variable. Now we can start using Featuretools to perform automated feature engineering! It is necessary to have a unique identifier feature in the dataset (our dataset doesn¡¯t have any right now). So, we will create one unique ID for our combined dataset. If you notice, we have two IDs in our data<U+2014>one for the item and another for the outlet. So, simply concatenating both will give us a unique ID. Please note that I have dropped the feature Item_Identifier as it is no longer required. However, I have retained the feature<U+00A0>Outlet_Identifier because I plan to use it later. Now before proceeding, we will have to create an EntitySet. An EntitySet is a structure that contains multiple dataframes and relationships between them. So, let¡¯s create an EntitySet and add the dataframe combination to it. Our data contains information at two levels<U+2014>item level and outlet level. Featuretools offers a functionality to split a dataset into multiple tables. We have created a new table ¡®outlet¡¯ from the BigMart table based on the outlet ID Outlet_Identifier. Let¡¯s check the summary of our EntitySet. As you can see above, it contains two entities <U+2013> bigmart and outlet. There is also a relationship formed between the two tables, connected by Outlet_Identifier. This relationship will play a key role in the generation of new features. Now we will use Deep Feature Synthesis to create new features automatically. Recall that DFS uses Feature Primitives to create features using multiple tables present in the EntitySet. target_entity is nothing but the entity ID for which we wish to create new features (in this case, it is the entity ¡®bigmart¡¯). The parameter max_depth<U+00A0>controls the complexity of the features being generated by stacking the primitives. The parameter n_jobs helps in parallel feature computation by using multiple cores. That¡¯s all you have to do with Featuretools. It has generated a bunch of new features on its own. Let¡¯s have a look at these newly created features. DFS has created 29 new features in such a quick time. It is phenomenal as it would have taken much longer to do it manually. If you have datasets with multiple interrelated tables, Featuretools would still work. In that case, you wouldn¡¯t have to normalize a table as multiple tables will already be available. Let¡¯s print the first few rows of feature_matrix. There is one issue with this dataframe <U+2013> it<U+00A0>is not sorted properly. We will have to sort it based on the id variable from the combi dataframe. Now the dataframe feature_matrix is in proper order. It is time to check how useful these generated features actually are. We will use them to build a model and predict Item_Outlet_Sales. Since our final data (feature_matrix) has many categorical features, I decided to use the CatBoost algorithm. It can use categorical features directly and is scalable in nature. You can refer to this article to read more about CatBoost. CatBoost requires all the categorical variables to be in the string format. So, we will convert the categorical variables in our data to string first: Let¡¯s split feature_matrix back into train and test sets. Split the train data into training and validation set to check the model¡¯s performance locally. Finally, we can now train our model. The evaluation metric we will use is RMSE (Root Mean Squared Error). 1091.244 The RMSE score on the validation set is ~1092.24. The same model got a score of 1155.12 on the public leaderboard. Without any feature engineering, the scores were ~1103 and ~1183 on the validation set and the public leaderboard, respectively. Hence, the features created by Featuretools<U+00A0>are not just random features, they are valuable and useful. Most importantly, the amount of time it saves in feature engineering is incredible. Making our data science solutions interpretable is a very important aspect of performing machine learning. Features generated by Featuretools can be easily explained even to a non-technical person because they are based on the primitives, which are easy to understand. For example, the features<U+00A0>outlet.SUM(bigmart.Item_Weight)<U+00A0>and outlet.STD(bigmart.Item_MRP)<U+00A0>mean outlet-level sum of weight of the items and standard deviation of the cost of the items, respectively.","Keyword(freq): feature(31), featuretool(14), sale(6), primitive(5), table(5), mark(4), title(4), variable(4), item(3), value(3)"
"8","vidhya",2018-08-22,"A Practical Introduction to K-Nearest Neighbors Algorithm for Regression (with Python code)","https://www.analyticsvidhya.com/blog/2018/08/k-nearest-neighbor-introduction-regression-python/","

DATA DAYS DISCOUNT: Get FLAT 33% OFF on All Courses | Use COUPON CODE: DATA33 |
OFFER ENDS: 26th Aug 23:59 hrs IST | 
Valid for first 33 users(daily) | Buy Now 

 Out of all the machine learning algorithms I have come across, KNN has easily been the simplest to pick up. Despite it¡¯s simplicity, it has proven to be incredibly effective at certain tasks (as you will see in this article). And even better? It can be used for both classification and regression problems! It¡¯s far more popularly used for classification problems, however. I have seldom seen KNN being implemented on any regression task. My aim here is to illustrate and emphasize how KNN can be equally effective when the target variable is continuous in nature. In this article, we will first understand the intuition behind KNN algorithms, look at the different ways to calculate distances between points, and then finally implement the algorithm in Python on the Big Mart Sales dataset. Let¡¯s go! Let us start with a simple example. Consider the following table <U+2013> it consists of the height, age and weight (target) value for 10 people. As you can see, the weight value of ID11 is missing. We need to predict the weight of this person based on their height and age. Note: The data in this table does not represent actual values. It is merely used as an example to explain this concept. For a clearer understanding of this, below is the plot of height versus age from the above table: In the above graph, the y-axis represents the height of a person (in feet) and the x-axis represents the age (in years). The points are numbered according to the ID values. The yellow point (ID 11) is our test point. If I ask you to identify the weight of ID11 based on the plot, what would be your answer? You would likely say that since ID11 is closer to<U+00A0>points 5 and 1, so it must<U+00A0> have a weight similar to these IDs, probably between 72-77 kgs (weights of ID1 and ID5 from the table). That actually makes sense, but how do you think the algorithm predicts the values? We will find that out in this article. As we saw above, KNN can be used for both classification and regression problems. The algorithm uses ¡®feature similarity¡¯ to predict values of any new data points. This means that the new point is assigned a value based on how closely it resembles the points in the training set. From our example, we know that ID11 has height and age similar to ID1 and ID5, so the weight would also approximately be the same. Had it been a classification problem, we would have taken the mode as the final prediction. In this case, we have two values of weight <U+2013> 72 and 77. Any guesses how the final value will be calculated? The average of the values is taken to be the final prediction. Below is a stepwise explanation of the algorithm: 2. The closest k data points are selected (based on the distance). In this example, points 1, 5, 6 will be selected if value of k is 3. We will further explore the method to select the right value of k later in this article. 3. The average of these data points is the final prediction for the new point. Here, we have weight of ID11 = (77+72+60)/3 = 69.66 kg. In the next few sections we will discuss each of these three steps in detail. The first step is to calculate the distance between the new point and each training point. There are various methods for calculating this distance, of which the most commonly known methods are <U+2013> Euclidian, Manhattan (for continuous) and Hamming distance (for categorical). Once the distance of a new observation from the points in our training set has been measured, the next step is to pick the closest points. The number of points to be considered is defined by the value of k. The second step is to select the k value. This determines the number of neighbors we look at when we assign a value to any new observation. In our example, for a value k = 3, the closest points are ID1, ID5 and ID6. The prediction of weight for ID11 will be: For the value of k=5, the closest point will be ID1, ID4, ID5, ID6, ID10. The prediction for ID11 will be : We notice that based on the k value, the final result tends to change. Then how can we figure out the optimum value of k? Let us decide it based on the error calculation for our train and validation set (after all, minimizing the error is our final goal!). Have a look at the below graphs for training error and validation error for different values of k. For a very low value of k (suppose k=1), the model overfits on the training data, which leads to a high error rate on the validation set. On the other hand, for a high value of k, the model performs poorly on both train and validation set. If you observe closely, the validation error curve reaches a minima at a value of k = 9. This value of k is the optimum value of the model (it will vary for different datasets). This curve is known as an ¡®elbow curve¡® (because it has a shape like an elbow) and is usually used to determine the k value. You can also use the grid search technique to find the best k value. We will implement this in the next section. By now you must have a clear understanding of the algorithm. If you have any questions regarding the same, please use the comments section below and I will be happy to answer them. We will now go ahead and implement the algorithm on a dataset. I have used the Big Mart sales dataset to show the implementation and you can download it from this link. 1. Read the file 2. Impute missing values 3. Deal with categorical variables and drop the id columns 4. Create train and test set 5. Preprocessing <U+2013> Scaling the features 6. Let us have a look at the error rate for different k values Output : As we discussed, when we take k=1, we get a very high RMSE value. The RMSE value decreases as we increase the k value. At k= 7, the RMSE is approximately 1219.06, and shoots up on further increasing the k value. We can safely say that k=7 will give us the best result in this case. These are the predictions using our training dataset. Let us now predict the values for test dataset and make a submission. 7. Predictions on the test dataset On submitting this file, I get an RMSE of 1279.5159651297. 8. Implementing GridsearchCV<U+00A0> For deciding the value of k, plotting the elbow curve every time is be a cumbersome and tedious process. You can simply use gridsearch to find the best value. Output : In this article, we covered the workings of the KNN algorithm and its implementation in Python. It¡¯s one of the most basic, yet effective machine learning techniques. For KNN implementation in R, you can go through this article : kNN Algorithm using R. In this article, we used the KNN model directly from the sklearn library. You can also implement KNN from scratch (I recommend this!), which is covered in the this article: KNN simplified. If you think you know KNN well and have a solid grasp on the technique, test your skills in this MCQ quiz:<U+00A0>30 questions on kNN Algorithm. Good luck! Hi Aishwarya, your explanation on KNN is really helpful. I have a doubt though. KNN suffers from the dimensionality curse i.e. Euclidean distance is not helpful when subjected to high dimensions as it is equidistant for different vectors.
What was your viewpoint while using the KNN despite this fact ? Curious to know. Thank you. Hi,  KNN works well for dataset with less number of features and fails to perform well has the number of inputs increase. Certainly other algorithms would show a better performance in that case. With this article I have tried to introduce the algorithm and explain how it actually works (instead of simply using it as a black box). Hi. I have been following you now for a while. Love your post. I wish you cold provide a pdf format also, because it is hard to archive and read web posts when you are offline. Hi Osman, Really glad you liked the post. Although certain articles and cheat sheets are converted and shared as pdf, but not all articles are available in the format. Will certainly look into it and see if we can have an alternate. Hi thanks for the explanations. Can you explain the intuition behind categorical variable calculations. For example, assume data set has height,age,gender as independent variable and weight as dependent variable.
Now if an unseen data comes with male as gender, then how it works? how it predicts the weight? Hi, Excellent question!  Suppose we have gender as a feature, we would use hamming distance to find the closest point (We need to find the distance with each training point as discussed in the article). Let us take the first training point, if it has the gender male and my test point also has the gender male, then the distance D will be 0. Now we take the second training point, it it has gender female and the test point has gender male, the value of D will be 1. Simply put, we will consider the test point closer to first training point and predict the target value accordingly. cannot find the data on https://datahack.analyticsvidhya.com/contest/practice-problem-big-mart-sales-iii/ Hi Steffen,","Keyword(freq): point(11), value(10), algorithm(3), problem(3), article(2), feature(2), method(2), prediction(2), question(2), sale(2)"
"9","vidhya",2018-08-20,"Launching Analytics Vidhya¡¯s Medium Publication and AV Editor¡¯s club!","https://www.analyticsvidhya.com/blog/2018/08/analytics-vidhyas-medium-publication-av-editors-club/","

DATA DAYS DISCOUNT: Get FLAT 33% OFF on All Courses | Use COUPON CODE: DATA33 |
OFFER ENDS: 26th Aug 23:59 hrs IST | 
Valid for first 33 users(daily) | Buy Now 

 Analytics Vidhya started as a blog in 2013 and has grown into a thriving community of data science professionals. We have evolved the platform based on our community needs and feedback. Today Analytics Vidhya offers Meetups, Webinars,<U+00A0>DataHack platform,<U+00A0>job portal,<U+00A0>training portal<U+00A0>and our flagship event DataHack Summit. Across these engagement points offered by Analytics Vidhya <U+2013> the most popular continues to be our blog. Our blog continues to play an anchor in the way we have built this community. Thousands of people have benefitted from the resource provided on our blog. We are planning to step up our efforts in this direction today. We are extremely excited to announce the launch of Now you might ask, why start a Medium publication? Simply because we believe that we can grow our community and its impact multi-fold by creating a channel where people can contribute in the same way they learnt from the platform. Medium offers seamless manner for a community run publication and we intend to run it that way. We want community members to come and share their knowledge so that everyone in the community benefits further. Here are a few things you can expect from our Medium publication: Excited? Here is what you need to do next As we launch this publication, we are thrilled to open the floor for budding and experienced writers who want to showcase their work to our broad and thriving community. Our aim has always been to spread data science knowledge and make it accessible to as many people as possible, around the globe. We have played an active role in this regard in the last 5 years, and continue to do so proudly. You bring the creativity and your passion for data science, and we now provide you with a platform to display it to the world. Some of the key benefits you will receive as a writer: Interested? Go ahead and fill this form for us.<U+00A0>You can read more about what benefits we offer, and what we expect from you, in this detailed article. Wait, there¡¯s more.. Yes, that¡¯s right <U+2013> there is an opportunity to work with us in the capacity of an Editor! We are looking for Editors and Content creators who can drive our blog and content platform to the next level. Are you passionate about data science journalism? Do you have an itch to try and experiment with the latest tools, techniques and frameworks as soon as they come out? Can you take complex data ideas and explain them in simple easy to understand content? Do you love to share your knowledge and skill with the rest of the universe?<U+00A0>Do you think your writing skills can enable and inspire the next generation of data scientists across the globe? If the answer to the above questions was yes, then you are<U+00A0>welcome to become the Founding member of AV¡¯s Editor Club.","Keyword(freq): benefit(3), analytics(2), creator(1), editor(1), effort(1), framework(1), idea(1), meetup(1), point(1), professional(1)"
"10","vidhya",2018-08-19,"DataHack Radio Episode #8: How Self-Driving Cars Work with Drive.ai¡¯s Brody Huval","https://www.analyticsvidhya.com/blog/2018/08/datahack-radio-episode-8-how-self-driving-cars-work-with-drive-ais-brody-huval/","

DATA DAYS DISCOUNT: Get FLAT 33% OFF on All Courses | Use COUPON CODE: DATA33 |
OFFER ENDS: 26th Aug 23:59 hrs IST | 
Valid for first 33 users(daily) | Buy Now 

 Self-driving cars are expected to rule the streets in the next few years. In fact, countries like the USA, China and Japan have already started using them in real-world situations! One of the leaders in this space is Andrew Ng backed Drive.ai, a self-driving car start-up based in California. So how do these autonomous cars work? How difficult is it making one from scratch? What kind of machine learning techniques are used? In this podcast, Drive.ai¡¯s co-founder Brody Huval sheds light on these questions put forward by Kunal, along with other really intriguing facets of autonomous vehicles. It¡¯s a podcast you better not miss! Check out all the key takeaways from this really cool podcast below. You can check our the research paper Brody has co-authored on highway driving and deep learning (explained in the podcast) here. Happy listening! You can subscribe to DataHack Radio and listen to this, and all previous episodes, on any of the below platforms: Brody completed his graduation in mechanical engineering from the University of Florida in 2010. Back then, AI had just started to become popular and enter the mainstream industry space. He became interested in the field and started applying to computer science programs in the United States. His application was accepted by Stanford in 2011 for their mechanical engineering program but Brody almost immediately switched to the computer science stream to work with Andrew Ng. His first couple of projects there were in deep learning and natural language processing (NLP). In total, he spent 4 years at Stanford doing various projects and research in machine learning and deep learning. Post that, in 2015, he co-founded Drive.ai along with fellow students from Andrew Ng¡¯s Stanford AI lab. Brody¡¯s interest in self-driving cars dates back almost 6 years ago to 2012. He and his fellow researchers worked on an autonomous driving project where they tried to replicate Google¡¯s work in this space. Google had used 16,000 CPU cores to create neural networks that watched YouTube videos all day in order to learn everything about autonomous cars. Brody¡¯s group decided to use 12 GPUs instead to replicate the power of those 16,000 CPUs. At the end of this project, the team was left with a cluster of approximately 64 machines (each with 4 GPUs). Brody and his team knew at this point that they wanted to work with tons of data, and eventually they landed on the idea of self-driving cars. They would use a camera-only approach, along with other hardware equipment to automatically annotate the data. In short, they were looking to solve a meaningful problem with tons of data, and that¡¯s how Drive.ai was born. Unsurprisingly, it took a significant amount of time to label the data! For lanes, they had a special GPS unit using which they could map a path of where the car was driving. Based on this route, the team could understand how far away the lanes were from the car. For dynamic obstacles like pedestrians, Brody set up a mechanical pipeline and that¡¯s when he faced the difficulties of annotating data manually. This process involved a lot of logistics and other overhead, something only folks who have set up a ML project from scratch will truly understand. The initial testing produced mixed results. Because they had collected a ton of highway driving data, the testing on highways went pretty well. But when it came to urban areas, the system did not perform as well because of a lack of proper data. One of the biggest weak points during their testing process was the huge number of false positives regarding gauging the side of the road. After trying out various camera based approaches, the Drive.ai team started exploring LIDAR and RADAR based solutions. They tested out different sensors with the aim of getting a much better and improved precision and recall for their system. Brody mentioned that he believes camera based approaches will definitely get better in urban areas with time. You need a lot of data to understand all the nuances of the various images the cameras collect, and thus it comes down to how much computational power you have. This section was a really insightful explanation of LIDAR, RADAR and camera sensors. If you¡¯re interested in self-driving cars, make sure you listen to this part especially carefully. Right now, Drive.ai has 7 fully functional self-driving cars on the streets in Texas which can service up to 10,000 people. They have been set up to try and solve the ¡®micro-transit problem¡¯, which means distances that are too far for walking but too close for driving. This is especially useful in Texas where it can get blisteringly hot during the summer months. Some of the challenges Brody and his team currently face are with perception and performance. When he mentions perception, it refers to understanding where certain objects are placed, and how the system goes around these dynamic objects given the uncertainty in predictions. One of the ways of dealing with constantly changing scenarios (like re-painting roads, or avoiding construction areas) is to be in touch with the local government and understanding well in advance what changes are expected to happen. Another option Drive.ai have explored is tele-operators, who operate through networks to guide the car using different routes. Rain is also a major problem for LIDAR units, while cameras are not at their best during the night. These are acknowledged weaknesses that AI has not been able to fully solve so far. Currently Drive.ai¡¯s cars operate during daylight and if there¡¯s inclement weather, the company pauses the service until it clears up. Simulators are also becoming a major part of any self-driving car setup. They help the team understand where certain things are going awry, or how long a certain scenario takes to run, etc. These are all machine learning problems. There are certain aspects where classic machine learning or deep learning techniques fit. For example, deep learning algorithms work really well in the perception facet (like geospatial data) of these cars. The motion planning system, on the other hand, is a combination of classic ML and learned ML. Brody made a great point about how reinforcement learning, as powerful as it is, hasn¡¯t made as many advancements as deep learning. It¡¯s getting better and the Drive.ai team do experiment with it, but to use it in a real-world use case is not a feasible option right now.","Keyword(freq): car(9), approach(2), camera(2), lane(2), network(2), object(2), project(2), sensor(2), street(2), technique(2)"
