"","site","date","headline","url_address","text","keyword"
"1","kaggle",2018-09-06,"Data Notes: The Secret to Getting to a Second Date","http://blog.kaggle.com/2018/09/06/data-notes-the-secret-to-getting-to-a-second-date/","Statistical Analyses, Mixed Models, and Second Dates: Enjoy these new, intriguing, and overlooked datasets and kernels. 1.<U+00A0><U+0001F491> The Secret to Getting the Second Date (link) 2.<U+00A0><U+0001F4CA> Statistical Analysis || A Frequentist Approach (link) 3.<U+00A0><U+0001F440> Deep Clustering for Unsupervised Learning Of Visual Features (link) 4.<U+00A0><U+0001F680> Steganography: Hiding Star Wars Scripts in Images (link) 5.<U+00A0><U+0001F629> What's the Relation Between Twitter and Mood? (link) 6.<U+00A0><U+26F9><U+0001F3FE> NBA Player of the Week¡¦ He¡¯s on Fire!!! (link) 7.<U+00A0><U+0001F939><U+0001F3FC> Mixed Models and Bounce Rates (link) 8.<U+00A0><U+0001F4A1> Dataset: Google Patents Public Data (link) 9.<U+00A0><U+0001F637> Dataset: USPTO Cancer Moonshot Patent Data (link) 10.<U+00A0><U+2697> Dataset: ChEMBL EBI Small Molecules Patent Database (link) Want to better understand an ML model¡¯s predictions?  Try using Locally Interpretable Model-Agnostic Explanations (LIME)! Copyright ¨Ï 2018 Kaggle, All rights reserved.","Keyword(freq): model(3), analysis(1), dataset(1), date(1), explanation(1), feature(1), image(1), kernel(1), molecule(1), patent(1)"
"2","datacamp",2018-09-03,"Data Products, Dashboards, and Rapid Prototyping (Transcript)","https://www.datacamp.com/community/blog/data-products-dashboards-rapid-prototyping","Here is a link to the podcast. Hugo:               Hi there, Tanya and welcome to Data Framed. Tanya:              Thank you, Hugo. Glad to be here. Hugo:               It's great to have you on the show and I'm really excited to talk about your work in data consulting, data products, and especially this idea of rapid prototyping that I know you're a huge proponent of. But before all of that, I'd like to find out a bit about you. So maybe you could start by telling us what you're known for in the data community? Tanya:              Sure. What I'm known for in the data community. It's definitely a loaded question and it's one of those things where ... Have you ever done, what do people think you do and what do you actually do at work? Hugo:               Exactly. Tanya:              I think it's one of those. So I think some people know I was a pretty early R user in the community. I got a little bit lucky in that I was an undergrad student at Northeastern and I worked on a co-op which is when you work six months and then you go to school the other six months. And the PI at the lab I was working at said, ""You're gonna learn this thing called R."" And this was back in 2005 or '6. And I had no clue what it was and there was no Rstudio, there was no DataCamp which makes it incredibly easy now to learn it. But I printed out the entire CRAN documentation of just like, I don't know 250 pages at the time or something and brought it home over my summer break and just read it and thought, ""I'm in deep trouble."" Tanya:              But sure enough, it worked out. I had some really cool mentors that were just super smart and I got to publish a paper on some of the work I did learning R, just basically diving in headfirst, blind, and it turned out to be a good thing to learn. Hugo:               Great. And what do you do now? Tanya:              Yeah so the other thing is that I started a consulting firm back in 2015 after working at, mostly startups. I worked at one large company, Biogen. And I think both were very different and fun, wild, experiences but I always wanted to do my own thing and so after seeing so many different use cases and vendor pitches and just what can be done out there and how much opportunity there is, it just became a no-brainer as I was consulting on the side it became too much to have a full-time job and consult so I just made the leap and here we are three years later. Hugo:               Fantastic. And so you've given us a bit of insight in there of how you actually got into data science but maybe you can tell us a bit more about your history. Tanya:              Sure. I always knew I liked computers and technology. I was a huge sports nerd. I basically played every sport under the sun. So between basketball and video games, computer games and somehow stumbling into Q basic in sixth grade, I was definitely on my way to knowing I wanted to pursue doing something with technology, something with computers. Tanya:              So ended up at Northeastern University and dual majored in computer science and I got interested in biology actually which is why I ended up in bioinformatics and biotech ultimately when I graduated. But bioinformatics was really not even well known back in 2005. There were no degree programs like there are now for it, just like there weren't no data science programs and now there are. Hugo:               Exactly. And I was gonna say, the greater Boston area is a really great place for biotech as well, right? Tanya:              Oh, the best. Probably the best in the world. I mean between all of the universities, the hospitals, there's teaching hospitals. Yeah I got really fortunate to be able to work at children's hospital, for the Harvard-MIT division, for Dana Farber Cancer Institute. It's one of the best in the world and that was really great. Tanya:              So networking early on and meeting a lot of just incredibly talented people was definitely a key factor to my decision to go to Boston for school. Hugo:               Great. And now you really think about business questions outside the realm of healthcare and bioinformatics and that type of stuff, right? Tanya:              Yeah. So we still have a large focus in healthcare. It was kind of funny. I sort of full circle came back to healthcare after exploring some other industries. But we do, yeah. We work in healthcare, life sciences, but also sports, some retail and consumer package goods, telecom. So really every industry now is generating data so it ... For me it expands multiple verticals. Hugo:               And which verticals or industries do you see data science having the most serious impact on currently? And through the lens of your experience as a consultant. Tanya:              Yeah. I mean any of the industries that are generating data as a result of just day-to-day business operations are always good ones and just ripe for opportunity. Because not only can you optimize their internal benchmarks, things like how can we save time on this operation we're doing? Or how can we optimize what we're selling based on targeting audiences? And I think any time you have transactional data which is obviously very large in quantity and longitudinal and typically pretty structured, those are also really great opportunities. Tanya:              But healthcare in general just has so much different data across the spectrum to improve outcomes for so many people and patients across the world that that's where my passion lies in terms of which vertical I enjoy the most. Hugo:               And what type of questions are you interested in answering, or are most relevant or is data science kind of most well equipped to answer in life sciences and healthcare and that type of stuff? Tanya:              Yeah. I think it really always come back to the data. Is it ... I started out analyzing gene expression data and genetic data. And back when I was doing it, the data was still noisy. There was a lot of noise to signal where we want to obviously have more signal to noise. So we actually started also looking at electronic medical record data and claims data which is cleaner. I mean it's still messy but it was much easier to have an impact sooner. So when you go on the research side with the genetics and genomics, there's just so much that has to get done even if you make a finding, you publish, ""Hey we have a potential new biomarker."" But then what? You have to go through clinical trial and we know that those take up to 12 years and hundreds of millions of dollars. Tanya:              Whereas, if we can say, ""With these patients, we know that if you intervene this way, a phone call versus a text. This person's gonna respond to a text and take their medication when they should."" And that is something that is not expensive and is something we can do immediately to have a positive effect on patient outcomes. So it's two very different worlds and two very different sort of questions that we're trying to answer, at least in healthcare. Hugo:               Great. And I'm sure you get data in a whole variety of different forms. So is this a challenge thinking about the amount and heterogeneity of the data and how to even convey insight from that to non-technical stakeholders?
Tanya:              Yeah. I think people still don't understand the messiness of data and how much time gets spent on cleaning it and standardizing it and joining it and what can go wrong in that process. You make one wrong join and you've completely inflated the number of sales or you accidentally find and replace the wrong pattern. It's just there's so many different things that can go wrong in that process that I think conveying that is still very tricky and it's ... The statistic is that it's 80% or so of any data analysis project is just getting the data ready for analysis. So I think the more we can get people hands on and dirty with the data themselves, the more they'll start to understand. Tanya:              And I think everyone at any level, C-level, entry level, should be looking and diving into data the same way that you were expected to start using email 20 years ago. Hugo:               Absolutely. And I think one example that I've heard from several people who work in analytics in health is ... The great example is, if you have a patient record or something like that, having a doctor's scribbles in the margins, something hand-written. And we all know how horrible doctor's handwriting can be as well so ... Tanya:              Yes. Yeah, I mean the other problem is, we've created all these forms that are supposed to go in and fill out. You know like Epic has a massive software solution that doctors can go in and check the boxes, if they're a smoker or not and check how many packs. But typically because they're trying to save time and they're seeing a lot of patients, they go to the bottom comment box and just write it all in free text. And they type it at least so we don't have to read their handwriting but now you've got doctors saying things in different ways and it turned into natural language processing which becomes significantly harder than obviously just having that structured form of data. Hugo:               So you mentioned in passing there are other sectors like retail, consumer goods, telecommunications, that maybe aren't quite as mature in how they think about data science but there are steps being made there. So maybe you can tell us a bit about that. Tanya:              Yeah. A couple interesting problems that we worked on was actually ... This was a while ago. One of these I'm interested in it because it was I think early on for this type of work. They were conducting surveys. They were a famous gum manufacturer. And they had user surveys that queried people about the gum. How do you like the taste? How long does it last? The flavor. And we were using that data to try to predict new product launch successes. So using old product surveys that were really successful at launch, we then tried to predict. They gave, I think thousands, a couple thousand people different types of gum to try that they were gonna then bring to the market. That was an interesting one. I don't think we ever got to see whether our predictions panned out or not which is one of the Holy Grail problems in some of these fields. But that was one. Tanya:              And another more recent one was with a massive consumer goods supplier who sells products online and they just have every single thing documented on what's happening on their website. If a user clicks a page, if they click a link, if they add something to their cart which most eCommerce sites are now collecting that data and storing it. But they're all ... So many of them are at this place where they don't know what to do with it next. So we came in to help them try to optimize product placement on their site. Figure out when people were leaving and abandoning their cart and try to make changes to resolve those issues, essentially and sell more products. Hugo:               Yeah. And of course that's one of the reasons what has now become modern data science, a lot of the techniques did emerging tech, right? Because we were able to ... Once you have the data foundation set up as a tech company, to actually get all the data and start to work with it. Tanya:              Yeah, I think companies have been collecting data for a while now and they've caught on. Some are just starting now but we're definitely at a tipping point where now they need to figure out what to do with it. Hugo:               Yeah, absolutely. Hugo:               I want to just pop back to this gum example. I find it very intriguing because I'm a gum guy as well and I've got a lot of questions around gum which we probably shouldn't delve into too much. I was wondering, you said you may not have found out whether the predictions had panned out, but were there actionables from the work that you'd done that the business could take from the insight that you got out of the data? Tanya:              Yeah. I believe they took several. We ranked them essentially and I think they took the top three or so and did a more focused study on those. So you're always trying to figure out where do we spend our money? Where do we spend our marketing budget? And so on. And so they took those and conducted a more focused study on those products and then decided from there which ones they were gonna try to mass produce. Tanya:              So definitely action items. They took it and ran with it and who knows? I may have tried the gum when it came out. Hugo:               So I do want to say two things about gum. Firstly, I feel like gum flavor has decreased in duration as I've gotten older but it may actually just be my olfactory system being less sensitive. Hugo:               Secondly, I was in Schiphol airport. I've got no idea how to pronounce it. The airport in Amsterdam a while ago. Tried to buy gum. They don't sell gum in the airport at all. And I asked, ""Why?"" And they said, ""It's because people will put it under chairs and on the ground."" So they just stopped selling it. And I said, ""But people can bring it in."" And they said, ""Yeah, but we're minimizing it in a way we can."" And I was like, ""Okay, deli dude. That's fine."" Tanya:              I mean I kind of respect that. Hugo:               Yeah, no. It's incredible. And he was so upfront about it as well. It's hyper rational and logical. We don't only want to go from data to insight all the time. A lot of the time we want to make a decision based around that insight, right? Tanya:              Yes. And that's what you always want to be doing. I mean there's some cases where insight may be valuable depending on what it is. But ultimately, I want to drive people to make decisions that are measurable so we can determine, yes there was a success. Or no, there wasn't. Hugo:               And is the role of the data scientist, and this maybe a provocative or ill-formed question. Is the role of the data scientist to make that decision or provide insight into making that decision, or to just provide results from the data? Tanya:              It'll depend on the seniority level of the person. How much they know about the domain or the company but I think they're in a position to, sure. Because they probably know the data better than anyone in the company. What management does with it or if you're a corporate-founder or chief data officer, you can probably make those decisions yourself. But I think ultimately we're building ... We're not replacing humans and our natural instincts but we're building decision recommendation engines in a way. So we're trying to guide and optimize the decision making process rather than put your finger in the air and just take a guess. Hugo:               Awesome, I love it. And something you mentioned earlier was that people who aren't necessarily data scientists will more and more hopefully be able to get into the weeds with data whether it be data exploration, or basic statistical modeling, or cleaning data and understanding how messy data can be. And how do you see that with people you do consulting work for? I mean people at C-level or whatever it may be. Would you like to see these types of people become more data literate in the next five to ten years? Tanya:              Yeah, for sure. It does everything from alleviate whether it's unreasonable expectations and how long something actually takes to get done. It helps them really position the value prop from the analysis better, in a smarter way. It helps them realize what's actually possible. So oftentimes, as we know, there can be empty promises made. Maybe by a salesperson and you absolutely cannot do what they said because either you don't have the data or the data is just too ... There's not enough of it or it's too messy. So absolutely, I mean it will help in a number of ways. Hugo:               I'd be right in saying you're a huge fan of dashboards, right? Tanya:              Yes. Yeah. Hugo:               So maybe you can give us your take on dashboards and data products in general. Tell us what they are to you and then let us know, kind of what their role is in your work and data science for business in general. Tanya:              Sure. So a dashboard is really just a great way for people to consume and interact with data quickly. And an ideal dashboard in my opinion, and I think also maybe Edward Tufte's, is that you should be able to look at something, a visualization or a dashboard and take away insight or an actionable item within 10 seconds or less. And if you didn't accomplish that, you need to go back to the drawing board, redesign it, make it simpler. Because otherwise, you're basically better off just looking at a big Excel sheet of numbers and try to weed through it and figure out what your answers are. So dashboards are great for that, for distilling a bunch of different data down into something consumable. Tanya:              The reason I like them for data products ... And what is a data product? It's either something that your company sells, it could be your entire product. I've worked at companies that were just data driven product companies. So as an example, I worked for a Telecom company that had this proprietary data on customers switching. So I don't know what your providers are in Australia but here we have T-Mobile, Sprint, Verizon. You could imagine that if you switched from T-Mobile to Verizon, which people do because nowadays it's a poaching game, right? Because everyone has a cell phone. We had all those switches in the country. The date it happened, the phone number, and they were about 50 thousand of those happening a day. And obviously you can imagine, a CMO at Sprint would love to know where customers that are almost out of contract, when contracts existed, high value customers that are coming off contract for maybe Verizon, are densely located in Southern California. So now Sprint says, ""We're gonna run a marketing campaign there."" Tanya:              So we built a product around that. It was put in the hands of these providers as a competitive insights tool. And that was essentially our entire business model. But the data was our product. It had to be correct. It had to be clean. It had to scale. The product needed to be user friendly and completely usable. And I think that's a huge use case at a lot of companies now, especially startups that are building essentially their profits from data itself. Hugo:               For sure. And in that case, and maybe in general, these data products tend to abstract over the data in order to show the insights as immediately as possible? Tanya:              Yeah, exactly. Hugo:               And what's the role of interactivity in data products, do you think? Tanya:              Yeah. Especially in rapid prototyping which I think we'll talk about in a little bit, it's really a way to make something real and tangible to someone. So you come up with an idea, you go through a whole bunch of requirements planning, and then you get the data and you build something and you realize, ""Oh, crap. I didn't even think of this."" Because of the complexity of data and how many unexpected things can come up, you're never just gonna be able to document and know ahead of time what's gonna be in there. So we build something quickly to allow the user to interact with it and get an idea for how this might work, put it in the hands of a customer, get their feedback before spending a ton of time and money on building a fully fledged product. Hugo:               This is a really nice segue into this idea of rapid prototyping. And I'd love to hear more about your approach to the trade-off between rapid prototyping of data products and dashboards and building fully mature data products. Tanya:              Sure. Yeah. So there's the old sort of waterfall technique they call it where you spend six months building out your first phase of the product. I always liked the analogy that you could end up building a Ford truck when your client really wanted just a Prius. Hugo:               I love it. Tanya:              So you have to essentially not over complicate things. You want to keep it simple. You want to ship quickly and iterate and get that feedback from the customer. Because I've been in situations where a client or a company has over engineered something and made it so complicated the user was overwhelmed. It just sat on the shelf and collected dust and was never used because we never tested the market. That's what you're doing with data product. You need to test your market before you go get funding and build a company. You should be doing that for data products as well. Hugo:               Yeah and you want to demonstrate at least potential value ASAP so that everyone's in. Or at least some key stakeholders are in, right? Tanya:              Yeah. And it seems like common sense but you'd be amazed. I still run into this where it's just over-engineered and not what the customer actually wanted. A lot of people also like to think that maybe they know their domain and their clients better than they do, but at the end of the day, sometimes they just don't. And there could be something unforeseen and sometimes a client doesn't even know what they want. So let's put something in front of them, get them to react to it, and then start to define the requirements and iterate that way. And that's an agile prototyping method that I swear by. And it's been successful. Tanya:              There are drawbacks and one of them being, let's say you put something in front of a client quickly, you don't always have time to fully QA it. You don't always have time to catch every single edge case. And people sometimes misconstrue that as, ""Well this is completely wrong. It's a broken product and we don't trust this data anymore."" But there needs to be, I think, a shift in thinking. Tanya:              You're always building. There's never final versions. Everything is a draft because otherwise, you spend two years developing something and it either never gets out the door or you built the wrong thing. Hugo:               For sure. And it sounds like essentially what we need is this type of rapid prototyping but hand in hand with some serious management of expectations around it as well. Tanya:              For sure. Managing expectations and also getting the client highly involved in the QA process if possible. So I always try to ask for a helping hand whether they have a junior analyst or anyone that can come in and get some extra eyes on what we're building just because sometimes we don't have a dedicated QA team on stuff. Sometimes we need the extra help and they know their data better than us. So we try to bake that into our projects now, is we want you guys to look at it. Make sure it doesn't look crazy because you know your data better than we do at this point. Hugo:               Absolutely. Do any illustrative examples spring to mind? Tanya:              Of catching wrong data? Hugo:               Yeah, or no just of the power of rapid prototyping before building out fully mature products? Tanya:              Yeah, for sure. So the telecom example that I talked about. We had an idea and it was called Voice of the Customer. So we knew that people were switching from, say Verizon to T-Mobile in vast quantities. But we didn't know the why. So we wanted to build some sort of social media monitoring tool, start monitoring of events that were happening like iPhone releases. And it was just me. Well it was a 12 person company but it was just me in charge of this product. And I had just hired, I think two fresh grads out of college who were just learning R and everything about data. And so we got together and we literally just went to Twitter and started searching to see if people were talking about switching. Turns out they were. People love to go to Twitter to complain about things. Tanya:              So you'd see a lot of things like, ""I'm switching to T-Mobile because Verizon service sucks."" And from that tweet, there's so much information there, right? You'd have, they're talking about the reason they're leaving Verizon. They're talking about who they're switching to. And so we started to quantify that using just basic language processing. And built a prototype. The first one was just in Ruby on Rails at the time. And then we had actually built some R Shiny apps as well. So we were doing a bunch of different things in parallel. Testing out different designs and ideas. And when we realized, ""Hey, there's a viable product here."" Because we put it in front of a client. They were very interested. Tanya:              And then we decided, ""Well let's go to ..."" it was called Gnip at the time. They were the Twitter Fire Hose provider. I believe they got acquired by Twitter. And we decided to purchase data in bulk. So initially we were just scraping whatever tweets we could get which is a very small percentage of the Twitter Fire Hose. But there's no reason to go and buy the Twitter Fire Hose before, it just wouldn't make sense. You need to make sure that the data is there and that it's viable and then you cast your net wider. Hugo:               Yeah and it's what they want as well, whatever the management is that you're dealing with and working with. Tanya:              Yeah, yeah. Exactly. Hugo:               Incredible. So I love that you've mentioned R and Shiny. Now we're not gonna get too much into the programming language R. When I say R, because of my accent, it doesn't sound like an R. So it sounds like what the doctor - Tanya:              It sounds like you're at the doctor, yeah. Hugo:               Yeah, exactly. And Shiny which, for our listeners out there, Shiny is a wonderful technology for rapidly prototyping dashboards. We've got great courses on Shiny at DataCamp. Rstudio has a lot of great resources. But my question for you, Tanya, is these technologies have and are evolving so quickly. So I'm wondering how your ability to actually do the work you do has evolved as a function of the tech and open source software development since 2005 when you first learned R by reading the entire CRAN documentation. Tanya:              Ah, no way. I didn't read the whole thing. I'm sure. But yeah, I was crying myself to sleep, no. It's so crazy how it's changed. Tanya:              But yeah, I constantly feel like I can't keep up, right? And that goes back to probably good old imposter syndrome. But if you don't have a little bit of imposter syndrome I think that you're probably doing something wrong because if you're not just aware of how much there is out there to know. Because I know what I don't know. And there's a lot that I don't know. But I try to ... What's nice is having clients kind of dictate what I need to know and learn. Tanya:              So if I'm gonna need a columnar data base because I know this client's gonna have a ton of data, it needs to be HIPAA compliant maybe for healthcare. Then I brush up on my AWS and redshift. Tanya:              If my client is gonna be doing large batch processing jobs, maybe we look at Spark or Hadoop. Spark is definitely outpacing Hadoop now but luckily, R has built all ... There's been all of these packages that really awesome people have made to make my life easier, and so I always thought for example, I wanted to learn D3. Because I saw those really sexy visualizations in The New York Times. And I thought, ""Crap. Now I have to learn D3 and JavaScript and all these other things."" But then Ramnath came along and built rCharts and I could basically build D3 charts just knowing R. And similarly with Shiny. I can build websites now with web applications just using R. Tanya:              So I've been fortunate and I think the R community is a big reason why I'm able to be successful in my current consultancy. Hugo:               Great. So there's literally stuff that you do now that's part of your daily bread and butter that you wouldn't have been able to do pre these tech being developed? Tanya:              Absolutely, yeah. Hugo:               So I also know that something you're very interested in and passionate about is kind of thinking about the data scientists hiring process. Tanya:              Yes. Hugo:               And I hesitate to use the word opinionated but I think you have very good, strong opinions on it. So maybe you could tell us a bit about how you feel about all this. Tanya:              I do. It's a pretty broken process. I've been through some interviews myself that I thought were just horrendous experiences and I've been through solid, pretty good interviews. And I mean the interview process is the first experience that someone has with your company. You want it to be a good experience. And so, I hate white boarding. I call it the ... It's the waterboarding, essentially, of interviewing. Hugo:               Never heard that before. That's incredible. Tanya:              It puts people in this just weird state of trying to almost dehumanize them and some kind of situation that would never happen. It's like you'd be hacking in the movie Swordfish with a gun to your head or something. Tanya:              But yeah. I gave a talk on this at Strata and I have formed a lot of ideas around it and honestly though, the reason I started getting interested in it was because I was working at startups where I was in charge of hiring a lot of people and interviewing a lot of people. And it's just such a time sink for everybody involved. Tanya:              So I wanted to just streamline it for myself, honestly, and for the candidates as well. So I came up with a small test and people have opinions about this too. Like you shouldn't expect them to spend their time doing something, however this is maybe to a couple hours, two to four hours. You should never expect more than four hours from anyone. And they take it on their own time. We give them some hints in what they should ... Some questions they should answer. There's really no right answer, it's just you evaluate their thinking process and how they document their code and everything. And they come in and present. Tanya:              And it might be an hour to your stakeholders. You put them in front of maybe your business stakeholders and see how they convey technical concepts or if you want them to be very technical-focused people and client facing or facing the engineers or statisticians in your business and have them present to them. Tanya:              So it's still better than an all day gauntlet which I like to call it. The eight hours of just going through and meeting everyone in the company. But those are kind of the big no-nos is I think the white boarding and just having someone spend an entire day at your office. Hugo:               Absolutely. And we'll put the link to your Strata talk which is online in the show notes as well. Tanya:              Oh cool. Okay. Hugo:               And something I think definitely coupled with this line of questioning is ... You know something you're also speaking to is you don't want someone who can do everything necessarily. We're not looking for the data science unicorn. And I'm wondering what your thoughts are on the data science skills gap, essentially, and how as educators we can approach arming future data scientists with the skills that they'll need. Tanya:              Sure, yeah. There's a huge skills gap but it's not as unattainable as some people think. There's a lot of hype around machine learning and now AI and advanced statistical modeling but at least in my experience, that's really only about maybe 20% of the projects I've worked on. Probably also 20% of the companies I've worked in. Tanya:              There's a ton of need to just be able to take the step from Excel to basic data munging in R & Python. And I've taught courses to non-coders, non-technical people on the basics of R. We've had great success with it. People enjoy it and they feel empowered to now go and do something a little bit more sophisticated than what they were previously able to do in Excel. And there's a lot of need for that. Just literally taking different sheets of data, joining them up, doing some basic QC, looking for missing data and be able to just do basic summary statistics. Like aggregating and finally maybe putting it in a Tableau Dashboard. And we've taught courses that are part-time over eight weeks that get you to that point. Tanya:              So people get intimidated and I can see why because the field hasn't been super welcoming to some extent, where you're not a real data scientist if you don't know every single sort algorithm in existence and can write it on a white board while someone stares at you. Or you're not a real data scientist if you don't have a PhD in statistics. Tanya:              So we need to start to compartmentalize and understand what we actually need into different skillsets. Hugo:               Yeah, and I do think this also speaks to a general statistical literacy and data literacy which, as you say, the people who are using ... You know the tens of millions or perhaps more people who use spreadsheets once a week or more, getting them up to speed on a bit more of the robust programing and statistical concepts that will help them do their job. Tanya:              Yep, exactly. Hugo:               So this leads nicely into my next question because we're really talking about the future now. I'm wondering what the future of data science looks like to you? And this is a prediction problem, right? Tanya:              Yeah. Let me build a model for you real quick. Hugo:               Yeah, great. Build me a dashboard. Tanya:              Wow. I mean I think we will start to see more advanced cases for predictive modeling, machine learning but like we said before, companies are just now starting to ask questions of their data. Seeing what they have, realizing it's dirty. The prediction part always comes later. So I do think you're gonna see an increase in that even though some people are starting to say the big data and machine learning hype is ... The wave has passed. But I think it's still just beginning. We were kind of ahead of our time when we were doing this type of work at a biotech company I worked at back in 2008 or so. Tanya:              But people are starting to get it. The more success stories you see with it. The more it's gonna catch on. I think you're gonna start seeing more uses across industries we might not have expected. In government, in small research labs, and even the army is starting to use some techniques. You're gonna see just a lot of different applications of using data to improve outcomes for any case you can really think of. I mean you can improve employee retention. You can improve ... You can try to cure cancer. There's a million different ways that we can use it more effectively. Hugo:               Absolutely. So in that sense, what the prediction is then that we're gonna see it move laterally into kind of all facets of life and society and business? Tanya:              Yeah. I just think it's gonna be more adopted, widespread and I think a big key indicator of that has actually been it's involvement in the recent election and in sports. With sports gambling actually being legalized here pretty soon, people are starting to get it. That's a big way I teach it too is we use sports data so that people have a little bit of fun and they understand it. And once you use analogies like that, it starts to click and people realize, ""Oh, wow. I could use this in accounting or to sell more inventory."" Hugo:               Yeah. That's one of the most important things for education and learning in general is making it relevant to the learner. Tanya:              Yeah. We always try to use relevant data sets at corporate training so that people get it and they understand the why. And when we do just these part-time, kind of fun ones, we'll use a fun data set and then we ask the students, ""Well how can you see this applying to your current work environment?"" And it's pretty interesting. 'Cause they can just see how it applies laterally once they've done ... You know you calculate Tom Brady's touchdown conversion percentage and then you go and calculate percentage of budget used. Hugo:               Yeah, that's fantastic. Hugo:               So what's one of your favorite data science-y techniques or methodologies? Something you enjoy doing. Tanya:              I love Shiny. I mentioned it before and I just think it's given data scientists such a cool way to put their work out there and express themselves and I used to just build R scripts in isolation that maybe generated a CSV file. But now, with R markdown and Shiny we can really showcase what we've done and make it something real that people can touch and see and that is one of my favorite things to do. Tanya:              I mean I build fantasy sports dashboards for fun. So it's definitely my favorite part of it I think is visualizing and putting together those dashboards. Hugo:               Great. And I don't know if ... I recall Mara Averick who's also in Boston area. She's a big fantasy sports fan as well, right? Tanya:              She is. We worked together actually on a project. It was a sports API that we built an R wrapper for and she's a huge hoops nerd. She knows more about basketball then probably anyone I know. And yeah I keep trying to team up with her to take over the sports betting market so we'll see. Hugo:               Fantastic. Well let me know how that pans out. Tanya:              I will. Hugo:               So Tanya, my final question is, do you have a final call to action for our listeners out there? Tanya:              Final call to action is just dive in, don't be afraid. R is not scary. Python's not even scary. But I honestly love R. I use both but just find a data set that you might be interested in to ... It could be anything. It could be about food. It could be about sports, movies. And look up some tutorials. DataCamp is a great place to start. I always plug you guys and - Hugo:               Awesome. I do too. Tanya:              Yeah. I'm not paid, I swear for this promotion. But yeah. It's just dive in and get your hands dirty. And the R community's very friendly and helpful. There's lots of places to go. There's slack channels you can join. So definitely just get started if you're interested in playing with data. Hugo:               Absolutely. And I always tell people, you know the R stats hashtag on Twitter is a great way to get involved. You'll ask something and most of the time you'll get an answer really quickly, in all honesty. Tanya:              Yeah. And Rstudio also just launched a community section where people ask questions and get answers. So if stack overflow isn't working out or you just don't even want to face the wrath of some stack over-flowers you could go to our studio community. Hugo:               And what we'll also do Tanya is post links to a whole bunch of pieces you've written. There's a great one on rapid prototyping. Another one on hiring in the data science space. We'll post all of these in the show notes as well. Tanya:              Cool, that'd be great. Then I'm always happy to answer questions if people want to reach out over Twitter or you can find my email at our website, tcbananyltics.com as well. Hugo:               Fantastic. Thank you so much for coming on the show, Tanya. It's been an absolute pleasure. Tanya:              Thank you, Hugo.","Keyword(freq): product(12), sport(10), question(8), dashboard(7), company(6), hour(4), industry(4), scientist(4), stakeholder(4), bioinformatic(3)"
"3","mastery",2018-09-07,"How to Develop a Probabilistic Forecasting Model to Predict Air Pollution Days","https://machinelearningmastery.com/how-to-develop-a-probabilistic-forecasting-model-to-predict-air-pollution-days/","Air pollution is characterized by the concentration of ground ozone. From meteorological measurements, such as wind speed and temperature, it is possible to forecast whether the ground ozone will be at a sufficiently high level tomorrow to issue a public air pollution warning. This is the basis behind a standard machine learning dataset used for time series classification dataset, called simply the ¡°ozone prediction problem¡°. This dataset describes meteorological observations over seven years in the Houston area and whether or not ozone levels were above a critical air pollution level, or not. In this tutorial, you will discover how to explore this data and to develop a probabilistic forecast model in order to predict air pollution in Houston, Texas. After completing this tutorial, you will know: Let¡¯s get started. How to Develop a Probabilistic Forecasting Model to Predict Air Pollution DaysPhoto by paramita, some rights reserved. This tutorial is divided into five parts; they are: Air pollution can be characterized as a high measure of ozone at the ground level, often characterized as ¡°bad ozone¡± to differentiate it from the ozone in the higher atmosphere. The ozone prediction problem is a time series classification prediction problem that involves predicting whether the next day will be a high air pollution day (ozone day) or not. The prediction of an ozone day can be used by meteorological organizations to warn the public such that they could take precautionary measures. The dataset was originally studied by Kun Zhang, et al. in their 2006 paper ¡°Forecasting Skewed Biased Stochastic Ozone Days: Analyses and Solutions,¡± then again in their follow-up paper ¡°Forecasting Skewed Biased Stochastic Ozone Days: analyses, solutions and beyond.¡± It is a challenging problem because the physical mechanisms that underlie high ozone levels are (or were not) completely understood, meaning that forecasts cannot be based on physical simulations as with other meteorological forecasts like temperature and rainfall. The dataset was used as the basis for developing predictive models that use a broad suite of variables that may or may not be relevant to predicting an ozone level, in addition to the few known to be relevant to the actual chemical processes involved. However, it is a common belief among environmental scientists that a significant large number of other features currently never explored yet are very likely useful in building highly accurate ozone prediction model. Yet, little is known on exactly what these features are and how they actually interact in the formation of ozone. [¡¦] none of today¡¯s environmental science knows as of yet how to use them. This provides a wonderful opportunities for data mining <U+2014> Forecasting Skewed Biased Stochastic Ozone Days: Analyses and Solutions, 2006. Forecasting a high level of ground ozone on a subsequent day is a challenging problem that is known to be stochastic in nature. This means that it is expected that there will be forecast errors. Therefore, it is desirable to model the prediction problem probabilistically and forecasting the probability of an ozone day or not given observations on the prior day or days. The dataset contains seven years of daily observations of meteorological variables (1998-2004 or 2,536 days) and whether there was an ozone day or not, taken in the Houston, Galveston, and Brazoria areas, Texas, USA. A total of 72 variables were observed each day, many of which are believed to be relevant to the prediction problem, and 10 of which have been confirmed to be relevant based on the physics. [¡¦] only about 10 features among these 72 features have been verified by environmental scientists to be useful and relevant, and there is neither empirical nor theoretical information as of yet on the relevance of the other 60 features. However, air quality control scientists have been speculating for a long time that some of these features might be useful, but just haven¡¯t been able to either develop the theory or use simulations to justify their relevance <U+2014> Forecasting Skewed Biased Stochastic Ozone Days: Analyses and Solutions, 2006. There are 24 variables that track the hourly wind speed and another 24 variables that track the temperature throughout each hour of the day. Two versions of the dataset are made available with different averaging periods for the measures, specifically 1-hourly and 8-hourly. What does appear to be missing and might be useful is the observed ozone for each day rather than the dichotomous ozone day/non-ozone day. Other measures used in the parametric model are also not available. Interestingly, a parametric ozone forecast model is used as a baseline, based on a description in ¡°Guideline For Developing An Ozone Forecasting Program,¡± 1999 EPA guideline. This document also describes standard methodological to verify ozone forecasting systems. In summary, it is a challenging prediction problem because: Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course The dataset is available from the UCI Machine Learning repository. We will only look at the 8-hour data in this tutorial. Download ¡°eighthr.data¡± and place it in your current working directory. Inspecting the data file, we can see observations with different scales. Skimming through the file, such as to the start of 2003, we can see that missing observations are marked with a ¡°?¡± value. First, we can load the data as a Pandas DataFrame using the read_csv() function. There is no data header and we can parse the dates in the first column and use them as an index; the complete example is listed below. Running the example confirms there are 2,534 days of data and 73 variables. We can also see the nature of the class imbalance where a little more than 93% of the days are non-ozone days and about 6% are ozone days. We can also create a line plot of the output variable over the seven years to get an idea if the ozone days occur at any particular time of year. Running the example creates a line plot of the output variable over seven years. We can see that there are clusters of ozone days in the middle of each year: the summer or warmer months in the northern hemisphere. Line plot of output variable over 7 years From briefly reviewing the observations, we can get some ideas of how we might prepare the data: We can perform some minimal data preparation. The example below loads the dataset, replaces the missing observations with 0.0, frames the data as a supervised learning problem (predict ozone tomorrow based on observations today), and splits the data into train and test sets, based on a rough number of days in two years. You could explore alternate approaches to replacing the missing values, such as imputing a mean value. Also, 2004 is a leap year, so the split of data into train and test sets is not a clean 5-2 year split, but is close enough for this tutorial. Running the example saves the train and test sets to CSV files and summarizes the shape of the two datasets. A naive model would predict the probability of an ozone day each day. This is a naive approach because it does not use any information other than the base rate of the event. In the verification of meteorological forecasts, this is called the climatological forecast. We can estimate the probability of an ozone day from the training dataset, as follows. We can then predict the naive probability of an ozone day for each day on the test dataset. Once we have a forecast, we can evaluate it. A useful measure for evaluating probabilistic forecast is the Brier score. This score can be thought of as the mean squared error of the predicted probabilities (e.g. 5%) from the expected probabilities (0% or 1%). It is the average of the errors made across each day in the test dataset. We are interested in minimizing the Brier score, smaller values are better, e.g. smaller error. We can evaluate the Brier score for a forecast using the brier_score_loss() function from the scikit-learn library. For a model to be skilful, it must have a score better than the score of the naive forecast. We can make this obvious by calculating a Brier Skill Score (BSS) that normalizes the Brier Score (BS) based on the naive forecast. We expect that the calculated BSS for the naive forecast would be 0.0. Going forward, we are interested in maximizing this score, e.g. larger BSS scores are better. The complete example for the naive forecast is listed below. Running the example, we can see the naive probability of an ozone day even is about 7.2%. Using the base rate as a forecast results in a Brier skill of 0.039 and the expected Brier Skill Score of 0.0 (ignore the sign). We are now ready to explore some machine learning methods to see if we can add skill to this forecast. Note that the original papers evaluated the skill of the approaches using precision and recall directly, a surprising approach for direct comparison between methods. Perhaps an alternative measure you could explore is the area under ROC curve (ROC AUC). Plotting ROC curves for final models would allow the operator of the model to choose a threshold that provides the desired level of balance between the true positive (hit) and false positive (false alarm) rates. The original paper reports some success with bagged decision trees. Though our choice of inductive learners are nonexhaustive, this paper has shown that inductive learning can be a method of choice for ozone level forecast, and ensemble-based probability trees provide better forecasts (higher recall and precision) than existing approaches. <U+2014> Forecasting Skewed Biased Stochastic Ozone Days: Analyses and Solutions, 2006. This is not surprising for a few reasons: This suggests a good place to start when testing machine learning algorithms on the problem. We can get started quickly by spot-checking the performance of a sample of standard ensemble tree methods from the scikit-learn library with their default configurations and the number of trees set to 100. Specifically, the methods: First, we must split the train and test datasets into input (X) and output (y) components so that we can fit sklearn models. We also require the Brier Score for the naive forecast so that we can correctly calculate the Brier Skill Scores for the new models. We can evaluate the skill of a single scikit-learn model generically. Below defines the function named evaluate_once() that fits and evaluates a given defined and configured scikit-learn model and returns the Brier Skill Score (BSS). Ensemble trees are a stochastic machine learning method. This means that they will make different predictions when the same configuration of the same model is trained on the same data. To correct for this, we can evaluate a given model multiple times, such as 10 times, and calculate the average skill across each of these runs. The function below will evaluate a given model 10 times, print the average BSS score, and return the population of scores for analysis. We are now ready to evaluate a suite of ensemble decision tree algorithms. The complete example is listed below. Running the example summarizes the average BSS for each model averaged across 10 runs. Given the stochastic nature of the algorithms, your specific results may differ, but the trends should be the same. From the mean BSS scores, it suggests that extra trees, stochastic gradient boosting, and random forest models are the most skillful. A box and whisker plot of the scores for each model is plotted. All of the models on all of their runs showed skill over the naive forecast (positive scores), which is very encouraging. The distribution of the BSS scores for Extra Trees, Stochastic Gradient Boosting, and Random Forest all look encouraging. Box and whisker plot of ensemble decision tree BSS scores on the test set Given that stochastic gradient boosting looks promising, it is worth exploring whether the performance of the model can be further lifted through some parameter tuning. There are many parameters to tune with the model, but some good heuristics for tuning the model include: Rather than grid searching values, we can spot check some arguments based on these principles. You can explore grid searching of these parameters yourself if you have the time and computational resources. We will compare four configurations of the GBM model: The complete example is listed below. Running the example prints the BSS for each model averaged across 10 runs for each configuration. The results suggest that the change to the learning rate and number of trees alone introduced some lift over the default configuration. The results also show that the ¡®all¡¯ configuration that included each change resulted in the best mean BSS. Box and whisker plots of the BSS scores from each configuration are created. We can see that the configuration that included all the changes was dramatically better than the baseline model and the other configuration combinations. Perhaps even further gains are possible with fine tuned arguments to the model. Box and whisker plot of tuned GBM models showing BSS scores on the test set It would be interesting to get a hold of the parametric model described in the paper and the data required to use it in order to compare its skill to the skill of this final model. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop a probabilistic forecast model to predict air pollution in Houston, Texas. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hi, I loved your blog post and find it to be a great starting point for this dataset and similar ones. I am a Data Scientist by profession, and yet, the conciseness, clarity and usefulness of your posts offer fantastic value for the time invested and I find myself eager for more. Thanks, I¡¯m happy the posts are useful! Really great case study. Is it included in your deep learning for time series book? Thanks. No, it¡¯s not in the book. Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning.","Keyword(freq): score(10), observation(8), model(7), tree(7), feature(6), variable(6), analysis(5), solution(5), forecast(4), method(4)"
"4","mastery",2018-09-05,"A Gentle Introduction to Probability Scoring Methods in Python","https://machinelearningmastery.com/how-to-score-probability-predictions-in-python/","Predicting probabilities instead of class labels for a classification problem can provide additional nuance and uncertainty for the predictions. The added nuance allows more sophisticated metrics to be used to interpret and evaluate the predicted probabilities. In general, methods for the evaluation of the accuracy of predicted probabilities are referred to as scoring rules or scoring functions. In this tutorial, you will discover three scoring methods that you can use to evaluate the predicted probabilities on your classification predictive modeling problem. After completing this tutorial, you will know: Let¡¯s get started. A Gentle Introduction to Probability Scoring Methods in PythonPhoto by Paul Balfe, some rights reserved. This tutorial is divided into four parts; they are: Log loss, also called ¡°logistic loss,¡± ¡°logarithmic loss,¡± or ¡°cross entropy¡± can be used as a measure for evaluating predicted probabilities. Each predicted probability is compared to the actual class output value (0 or 1) and a score is calculated that penalizes the probability based on the distance from the expected value. The penalty is logarithmic, offering a small score for small differences (0.1 or 0.2) and enormous score for a large difference (0.9 or 1.0). A model with perfect skill has a log loss score of 0.0. In order to summarize the skill of a model using log loss, the log loss is calculated for each predicted probability, and the average loss is reported. The log loss can be implemented in Python using the log_loss() function in scikit-learn. For example: In the binary classification case, the function takes a list of true outcome values and a list of probabilities as arguments and calculates the average log loss for the predictions. We can make a single log loss score concrete with an example. Given a specific known outcome of 0, we can predict values of 0.0 to 1.0 in 0.01 increments (101 predictions) and calculate the log loss for each. The result is a curve showing how much each prediction is penalized as the probability gets further away from the expected value. We can repeat this for a known outcome of 1 and see the same curve in reverse. The complete example is listed below. Running the example creates a line plot showing the loss scores for probability predictions from 0.0 to 1.0 for both the case where the true label is 0 and 1. This helps to build an intuition for the effect that the loss score has when evaluating predictions. Line Plot of Evaluating Predictions with Log Loss Model skill is reported as the average log loss across the predictions in a test dataset. As an average, we can expect that the score will be suitable with a balanced dataset and misleading when there is a large imbalance between the two classes in the test set. This is because predicting 0 or small probabilities will result in a small loss. We can demonstrate this by comparing the distribution of loss values when predicting different constant probabilities for a balanced and an imbalanced dataset. First, the example below predicts values from 0.0 to 1.0 in 0.1 increments for a balanced dataset of 50 examples of class 0 and 1. Running the example, we can see that a model is better-off predicting probabilities values that are not sharp (close to the edge) and are back towards the middle of the distribution. The penalty of being wrong with a sharp probability is very large. Line Plot of Predicting Log Loss for Balanced Dataset We can repeat this experiment with an imbalanced dataset with a 10:1 ratio of class 0 to class 1. Here, we can see that a model that is skewed towards predicting very small probabilities will perform well, optimistically so. The naive model that predicts a constant probability of 0.1 will be the baseline model to beat. The result suggests that model skill evaluated with log loss should be interpreted carefully in the case of an imbalanced dataset, perhaps adjusted relative to the base rate for class 1 in the dataset. Line Plot of Predicting Log Loss for Imbalanced Dataset The Brier score, named for Glenn Brier, calculates the mean squared error between predicted probabilities and the expected values. The score summarizes the magnitude of the error in the probability forecasts. The error score is always between 0.0 and 1.0, where a model with perfect skill has a score of 0.0. Predictions that are further away from the expected probability are penalized, but less severely as in the case of log loss. The skill of a model can be summarized as the average Brier score across all probabilities predicted for a test dataset. The Brier score can be calculated in Python using the brier_score_loss() function in scikit-learn. It takes the true class values (0, 1) and the predicted probabilities for all examples in a test dataset as arguments and returns the average Brier score. For example: We can evaluate the impact of prediction errors by comparing the Brier score for single probability forecasts in increasing error from 0.0 to 1.0. The complete example is listed below. Running the example creates a plot of the probability prediction error in absolute terms (x-axis) to the calculated Brier score (y axis). We can see a familiar quadratic curve, increasing from 0 to 1 with the squared error. Line Plot of Evaluating Predictions with Brier Score Model skill is reported as the average Brier across the predictions in a test dataset. As with log loss, we can expect that the score will be suitable with a balanced dataset and misleading when there is a large imbalance between the two classes in the test set. We can demonstrate this by comparing the distribution of loss values when predicting different constant probabilities for a balanced and an imbalanced dataset. First, the example below predicts values from 0.0 to 1.0 in 0.1 increments for a balanced dataset of 50 examples of class 0 and 1. Running the example, we can see that a model is better-off predicting middle of the road probabilities values like 0.5. Unlike log loss that is quite flat for close probabilities, the parabolic shape shows the clear quadratic increase in the score penalty as the error is increased. Line Plot of Predicting Brier Score for Balanced Dataset We can repeat this experiment with an imbalanced dataset with a 10:1 ratio of class 0 to class 1. Running the example, we see a very different picture for the imbalanced dataset. Like the average log loss, the average Brier score will present optimistic scores on an imbalanced dataset, rewarding small prediction values that reduce error on the majority class. In these cases, Brier score should be compared relative to the naive prediction (e.g. the base rate of the minority class or 0.1 in the above example) or normalized by the naive score. This latter example is common and is called the Brier Skill Score (BSS). Where BS is the Brier skill of model, and BS_ref is the Brier skill of the naive prediction. The Brier Skill Score reports the relative skill of the probability prediction over the naive forecast. A good update to the scikit-learn API would be to add a parameter to the brier_score_loss() to support the calculation of the Brier Skill Score. Line Plot of Predicting Brier Score for Imbalanced Dataset A predicted probability for a binary (two-class) classification problem can be interpreted with a threshold. The threshold defines the point at which the probability is mapped to class 0 versus class 1, where the default threshold is 0.5. Alternate threshold values allow the model to be tuned for higher or lower false positives and false negatives. Tuning the threshold by the operator is particularly important on problems where one type of error is more or less important than another or when a model is makes disproportionately more or less of a specific type of error. The Receiver Operating Characteristic, or ROC, curve is a plot of the true positive rate versus the false positive rate for the predictions of a model for multiple thresholds between 0.0 and 1.0. Predictions that have no skill for a given threshold are drawn on the diagonal of the plot from the bottom left to the top right. This line represents no-skill predictions for each threshold. Models that have skill have a curve above this diagonal line that bows towards the top left corner. Below is an example of fitting a logistic regression model on a binary classification problem and calculating and plotting the ROC curve for the predicted probabilities on a test set of 500 new data instances. Running the example creates an example of a ROC curve that can be compared to the no skill line on the main diagonal. Example ROC Curve The integrated area under the ROC curve, called AUC or ROC AUC, provides a measure of the skill of the model across all evaluated thresholds. An AUC score of 0.5 suggests no skill, e.g. a curve along the diagonal, whereas an AUC of 1.0 suggests perfect skill, all points along the left y-axis and top x-axis toward the top left corner. An AUC of 0.0 suggests perfectly incorrect predictions. Predictions by models that have a larger area have better skill across the thresholds, although the specific shape of the curves between models will vary, potentially offering opportunity to optimize models by a pre-chosen threshold. Typically, the threshold is chosen by the operator after the model has been prepared. The AUC can be calculated in Python using the roc_auc_score() function in scikit-learn. This function takes a list of true output values and predicted probabilities as arguments and returns the ROC AUC. For example: An AUC score is a measure of the likelihood that the model that produced the predictions will rank a randomly chosen positive example above a randomly chosen negative example. Specifically, that the probability will be higher for a real event (class=1) than a real non-event (class=0). This is an instructive definition that offers two important intuitions: Below, the example demonstrating the ROC curve is updated to calculate and display the AUC. Running the example calculates and prints the ROC AUC for the logistic regression model evaluated on 500 new examples. An important consideration in choosing the ROC AUC is that it does not summarize the specific discriminative power of the model, rather the general discriminative power across all thresholds. It might be a better tool for model selection rather than in quantifying the practical skill of a model¡¯s predicted probabilities. Predicted probabilities can be tuned to improve or even game a performance measure. For example, the log loss and Brier scores quantify the average amount of error in the probabilities. As such, predicted probabilities can be tuned to improve these scores in a few ways: Generally, it may be useful to review the calibration of the probabilities using tools like a reliability diagram. This can be achieved using the calibration_curve() function in scikit-learn. Some algorithms, such as SVM and neural networks, may not predict calibrated probabilities natively. In these cases, the probabilities can be calibrated and in turn may improve the chosen metric. Classifiers can be calibrated in scikit-learn using the CalibratedClassifierCV class. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered three metrics that you can use to evaluate the predicted probabilities on your classification predictive modeling problem. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of scikit-learn code Discover how in my new Ebook:Machine Learning Mastery With Python Covers self-study tutorials and end-to-end projects like:Loading data, visualization, modeling, tuning, and much more¡¦ Skip the Academics. Just Results. Click to learn more. Many thanks for this. I have calculated a Brier Skill score on my horse ratings. I did this by calculating the naive score by applying Brier to the fraction of winners in the data set which is 0.1055 or 10.55%. Using this with the Brier skill score formula and the raw Brier score I get a BSS of 0.0117 This is better than zero which is good but how good ? To be a valid score of model performance, you would calculate the score for all forecasts in a period. Yes I calculated the Brier base score for 0.1055 and then I calculated the Brier score for all my ratings thats 49,277 of them Thank for this, Jason.  I noticed something strange with the Brier score:brier_score_loss([1], [1], pos_label=1) returns 1 instead of 0.brier_score_loss([1], [0], pos_label=1) returns 0 instead of 1. Looking into the source code, it seems that brier_score_loss breaks like this only when y_true contains a single unique class (like [1]). This stems from a bug that is already reported here:https://github.com/scikit-learn/scikit-learn/issues/9300  A quick workaround for your code would be to replace this line:
losses = [brier_score_loss([1], [x], pos_label=[1]) for x in yhat] with the following:
losses = [2 * brier_score_loss([0, 1], [0, x], pos_label=[1]) for x in yhat] Interesting. I guess it might not make much sense to evaluate a single forecast using Brier. Brier score should be applicable for any number of forecasts.
Having a bug in sklearn shouldn¡¯t change that. <U+0001F642> ¡®An AUC score of 0.0 suggests no skill¡¯ <U+2013> here it should be 0.5 AUC, right? 0.0 would mean a perfect skill you just need to invert the classes. Nice article ! Very well explained. Thanks Thanks, fixed! Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning.","Keyword(freq): probability(26), prediction(16), value(13), model(5), example(4), forecast(4), score(4), threshold(4), argument(3), increment(3)"
"5","mastery",2018-09-03,"How and When to Use a Calibrated Classification Model with scikit-learn","https://machinelearningmastery.com/calibrated-classification-model-in-scikit-learn/","Instead of predicting class values directly for a classification problem, it can be convenient to predict the probability of an observation belonging to each possible class. Predicting probabilities allows some flexibility including deciding how to interpret the probabilities, presenting predictions with uncertainty, and providing more nuanced ways to evaluate the skill of the model. Predicted probabilities that match the expected distribution of probabilities for each class are referred to as calibrated. The problem is, not all machine learning models are capable of predicting calibrated probabilities. There are methods to both diagnose how calibrated predicted probabilities are and to better calibrate the predicted probabilities with the observed distribution of each class. Often, this can lead to better quality predictions, depending on how the skill of the model is evaluated. In this tutorial, you will discover the importance of calibrating predicted probabilities and how to diagnose and improve the calibration of models used for probabilistic classification. After completing this tutorial, you will know: Let¡¯s get started. How and When to Use a Calibrated Classification Model with scikit-learnPhoto by Nigel Howe, some rights reserved. This tutorial is divided into four parts; they are: A classification predictive modeling problem requires predicting or forecasting a label for a given observation. An alternative to predicting the label directly, a model may predict the probability of an observation belonging to each possible class label. This provides some flexibility both in the way predictions are interpreted and presented (choice of threshold and prediction uncertainty) and in the way the model is evaluated. Although a model may be able to predict probabilities, the distribution and behavior of the probabilities may not match the expected distribution of observed probabilities in the training data. This is especially common with complex nonlinear machine learning algorithms that do not directly make probabilistic predictions and instead use approximations. The distribution of the probabilities can be adjusted to better match the expected distribution observed in the data. This adjustment is referred to as calibration, as in the calibration of the model or the calibration of the distribution of class probabilities. [¡¦] we desire that the estimated class probabilities are reflective of the true underlying probability of the sample. That is, the predicted class probability (or probability-like value) needs to be well-calibrated. To be well-calibrated, the probabilities must effectively reflect the true likelihood of the event of interest. <U+2014> Page 249, Applied Predictive Modeling, 2013. There are two concerns in calibrating probabilities; they are diagnosing the calibration of predicted probabilities and the calibration process itself. A reliability diagram is a line plot of the relative frequency of what was observed (y-axis) versus the predicted probability frequency<U+00A0> (x-axis). Reliability diagrams are common aids for illustrating the properties of probabilistic forecast systems. They consist of a plot of the observed relative frequency against the predicted probability, providing a quick visual intercomparison when tuning probabilistic forecast systems, as well as documenting the performance of the final product <U+2014> Increasing the Reliability of Reliability Diagrams, 2007. Specifically, the predicted probabilities are divided up into a fixed number of buckets along the x-axis. The number of events (class=1) are then counted for each bin (e.g. the relative observed frequency). Finally, the counts are normalized. The results are then plotted as a line plot. These plots are commonly referred to as ¡®reliability¡® diagrams in forecast literature, although may also be called ¡®calibration¡® plots or curves as they summarize how well the forecast probabilities are calibrated. The better calibrated or more reliable a forecast, the closer the points will appear along the main diagonal from the bottom left to the top right of the plot. The position of the points or the curve relative to the diagonal can help to interpret the probabilities; for example: Probabilities, by definition, are continuous, so we expect some separation from the line, often shown as an S-shaped curve showing pessimistic tendencies over-forecasting low probabilities and under-forecasting high probabilities. Reliability diagrams provide a diagnostic to check whether the forecast value Xi is reliable. Roughly speaking, a probability forecast is reliable if the event actually happens with an observed relative frequency consistent with the forecast value. <U+2014> Increasing the Reliability of Reliability Diagrams, 2007. The reliability diagram can help to understand the relative calibration of the forecasts from different predictive models. The predictions made by a predictive model can be calibrated. Calibrated predictions may (or may not) result in an improved calibration on a reliability diagram. Some algorithms are fit in such a way that their predicted probabilities are already calibrated. Without going into details why, logistic regression is one such example. Other algorithms do not directly produce predictions of probabilities, and instead a prediction of probabilities must be approximated. Some examples include neural networks, support vector machines, and decision trees. The predicted probabilities from these methods will likely be uncalibrated and may benefit from being modified via calibration. Calibration of prediction probabilities is a rescaling operation that is applied after the predictions have been made by a predictive model. There are two popular approaches to calibrating probabilities; they are the Platt Scaling and Isotonic Regression. Platt Scaling is simpler and is suitable for reliability diagrams with the S-shape. Isotonic Regression is more complex, requires a lot more data (otherwise it may overfit), but can support reliability diagrams with different shapes (is nonparametric). Platt Scaling is most effective when the distortion in the predicted probabilities is sigmoid-shaped. Isotonic Regression is a more powerful calibration method that can correct any monotonic distortion. Unfortunately, this extra power comes at a price. A learning curve analysis shows that Isotonic Regression is more prone to overfitting, and thus performs worse than Platt Scaling, when data is scarce. <U+2014> Predicting Good Probabilities With Supervised Learning, 2005. Note, and this is really important: better calibrated probabilities may or may not lead to better class-based or probability-based predictions. It really depends on the specific metric used to evaluate predictions. In fact, some empirical results suggest that the algorithms that can benefit the more from calibrating predicted probabilities include SVMs, bagged decision trees, and random forests. [¡¦] after calibration the best methods are boosted trees, random forests and SVMs. <U+2014> Predicting Good Probabilities With Supervised Learning, 2005. The scikit-learn machine learning library allows you to both diagnose the probability calibration of a classifier and calibrate a classifier that can predict probabilities. You can diagnose the calibration of a classifier by creating a reliability diagram of the actual probabilities versus the predicted probabilities on a test set. In scikit-learn, this is called a calibration curve. This can be implemented by first calculating the calibration_curve() function. This function takes the true class values for a dataset and the predicted probabilities for the main class (class=1). The function returns the true probabilities for each bin and the predicted probabilities for each bin. The number of bins can be specified via the n_bins argument and default to 5. For example, below is a code snippet showing the API usage: A classifier can be calibrated in scikit-learn using the CalibratedClassifierCV class. There are two ways to use this class: prefit and cross-validation. You can fit a model on a training dataset and calibrate this prefit model using a hold out validation dataset. For example, below is a code snippet showing the API usage: Alternately, the CalibratedClassifierCV can fit multiple copies of the model using k-fold cross-validation and calibrate the probabilities predicted by these models using the hold out set. Predictions are made using each of the trained models. For example, below is a code snippet showing the API usage: The CalibratedClassifierCV class supports two types of probability calibration; specifically, the parametric ¡®sigmoid¡® method (Platt¡¯s method) and the nonparametric ¡®isotonic¡® method which can be specified via the ¡®method¡® argument. We can make the discussion of calibration concrete with some worked examples. In these examples, we will fit a support vector machine (SVM) to a noisy binary classification problem and use the model to predict probabilities, then review the calibration using a reliability diagram and calibrate the classifier and review the result. SVM is a good candidate model to calibrate because it does not natively predict probabilities, meaning the probabilities are often uncalibrated. A note on SVM: probabilities can be predicted by calling the decision_function() function on the fit model instead of the usual predict_proba() function. The probabilities are not normalized, but can be normalized when calling the calibration_curve() function by setting the ¡®normalize¡® argument to ¡®True¡®. The example below fits an SVM model on the test problem, predicted probabilities, and plots the calibration of the probabilities as a reliability diagram, Running the example creates a reliability diagram showing the calibration of the SVMs predicted probabilities (solid line) compared to a perfectly calibrated model along the diagonal of the plot (dashed line.) We can see the expected S-shaped curve of a conservative forecast. Uncalibrated SVM Reliability Diagram We can update the example to fit the SVM via the CalibratedClassifierCV class using 5-fold cross-validation, using the holdout sets to calibrate the predicted probabilities. The complete example is listed below. Running the example creates a reliability diagram for the calibrated probabilities. The shape of the calibrated probabilities is different, hugging the diagonal line much better, although still under-forecasting in the upper quadrant. Visually, the plot suggests a better calibrated model. Calibrated SVM Reliability Diagram We can make the contrast between the two models more obvious by including both reliability diagrams on the same plot. The complete example is listed below. Running the example creates a single reliability diagram showing both the calibrated (orange) and uncalibrated (blue) probabilities. It is not really an apples-to-apples comparison as the predictions made by the calibrated model are in fact a combination of five submodels. Nevertheless, we do see a marked difference in the reliability of the calibrated probabilities (very likely caused by the calibration process). Calibrated and Uncalibrated SVM Reliability Diagram This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered the importance of calibrating predicted probabilities and how to diagnose and improve the calibration of models used for probabilistic classification. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of scikit-learn code Discover how in my new Ebook:Machine Learning Mastery With Python Covers self-study tutorials and end-to-end projects like:Loading data, visualization, modeling, tuning, and much more¡¦ Skip the Academics. Just Results. Click to learn more. I wish you include the last 2 tutorials in ¡°Machine learning mastery with Python¡±. Thanks. They may be a little advanced for that beginner book. Thanks for this, very interesting but on my Gradient Descent Boosting UK horse racing ratings it did not improve performance sadly Thanks, nice one for trying! Thanks,  very useful. I¡¯m happy to hear that. Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning.","Keyword(freq): probability(55), prediction(12), diagram(8), model(7), algorithm(4), example(3), method(3), plot(3), result(3), svm(3)"
"6","vidhya",2018-09-06,"An End-to-End Guide to Understand the Math behind XGBoost","https://www.analyticsvidhya.com/blog/2018/09/an-end-to-end-guide-to-understand-the-math-behind-xgboost/","

MEGA-LAUNCH Offer on Computer Vision Using Deep Learning | Use COUPON CODE: CVLAUNCH60 for 60% Discount |
Course Starts 4th September | 
Buy Now 

 Ever since its introduction in 2014, XGBoost has been lauded as the holy grail of machine learning hackathons and competitions. From predicting ad click-through rates to classifying high energy physics events, XGBoost has proved its mettle in terms of performance <U+2013> and speed. I always turn to XGBoost as my first algorithm of choice in any ML hackathon. The accuracy it consistently gives, and the time it saves, demonstrates how useful it is. But how does it actually work? What kind of mathematics power XGBoost? We¡¯ll figure out the answers to these questions soon. Tianqi Chen, one of the co-creators of XGBoost, announced (in 2016) that the innovative system features and algorithmic optimizations in XGBoost have rendered it 10 times faster than most sought after machine learning solutions. A truly amazing technique! In this article, we will first look at the power of XGBoost, and then deep dive into the inner workings of this popular and powerful technique. It¡¯s good to be able to implement it in Python or R, but understanding the nitty-gritties of the algorithm will help you become a better data scientist. Note: We recommend going through the below article as well to fully understand the various terms and concepts mentioned in this article: The beauty of this powerful algorithm lies in its scalability, which drives fast learning through parallel and distributed computing and offers efficient memory usage. It¡¯s no wonder then that CERN recognized it as the best approach to classify signals from the Large Hadron Collider. This particular challenge posed by CERN required a solution that would be scalable to process data being generated at the rate of 3 petabytes per year and effectively distinguish an extremely rare signal from background noises in a complex physical process. XGBoost emerged as the most useful, straightforward and robust solution. Now, let¡¯s deep dive into the inner workings of XGBoost. XGBoost is an ensemble learning method. Sometimes, it may not be sufficient to rely upon the results of just one machine learning model. Ensemble learning offers a systematic solution to combine the predictive power of multiple learners. The resultant is a single model which gives the aggregated output from several models. The models that form the ensemble,<U+00A0>also known as base learners, could be either from the same learning algorithm or different learning algorithms. Bagging and boosting are two widely used ensemble learners. Though these two techniques can be used with several statistical models, the most predominant usage has been with decision trees. Let¡¯s briefly discuss bagging before taking a more detailed look at the concept of boosting. While decision trees are one of the most easily interpretable models, they exhibit highly variable behavior. Consider a single training dataset that we randomly split into two parts. Now, let¡¯s use each part to train a decision tree in order to obtain two models. When we fit both these models, they would yield different results. Decision trees are said to be associated with high variance due to this behavior. Bagging or boosting aggregation helps to reduce the variance in any learner. Several decision trees which are generated in parallel, form the base learners of bagging technique. Data sampled with replacement is fed to these learners for training. The final prediction is the averaged output from all the learners. In boosting, the trees are built sequentially such that each subsequent tree aims to reduce the errors of the previous tree. Each tree learns from its predecessors and updates the residual errors. Hence, the tree that grows next in the sequence will learn from an updated version of the residuals. The base learners in boosting are weak learners in which the bias is high, and the predictive power is just a tad better than random guessing. Each of these weak learners contributes some vital information for prediction, enabling the boosting technique to produce a strong learner by effectively combining these weak learners. The final strong learner brings down both the bias and the variance. In contrast to bagging techniques like Random Forest, in which trees are grown to their maximum extent, boosting makes use of trees with fewer splits. Such small trees, which are not very deep, are highly interpretable. Parameters like the number of trees or iterations, the rate at which the gradient boosting learns, and the depth of the tree, could be optimally selected through validation techniques like k-fold cross validation. Having a large number of trees might lead to overfitting. So, it is necessary to carefully choose the stopping criteria for boosting. Boosting consists of three simple steps: To improve the performance of F1, we could model after the residuals of F1 and create a new model F2: This can be done for ¡®m¡¯ iterations, until residuals have been minimized as much as possible: Here, the additive learners do not disturb the functions created in the previous steps. Instead, they impart information of their own to bring down the errors. Consider the following data where the years of experience is predictor variable and salary (in thousand dollars) is the target. Using regression trees as base learners, we can create a model to predict the salary. For the sake of simplicity, we can choose square loss as our loss function and our objective would be to minimize the square error. As the first step, the model should be initialized with a function F0(x). F0(x) should be a function which minimizes the loss function or MSE (mean squared error), in this case: Taking the first differential of the above equation with respect to ¥ã, it is seen that the function minimizes at the mean i=1nyin. So, the boosting model could be initiated with: F0(x) gives the predictions from the first stage of our model. Now, the residual error for each instance is (yi <U+2013> F0(x)). We can use the residuals from F0(x) to create h1(x). h1(x) will be a regression tree which will try and reduce the residuals from the previous step. The output of h1(x) won¡¯t be a prediction of y; instead, it will help in predicting the successive function F1(x) which will bring down the residuals. The additive model h1(x) computes the mean of the residuals (y <U+2013> F0) at each leaf of the tree. The boosted function F1(x) is obtained by summing F0(x) and h1(x). This way h1(x) learns from the residuals of F0(x) and suppresses it in F1(x). This can be repeated for 2 more iterations to compute h2(x) and h3(x). Each of these additive learners, hm(x), will make use of the residuals from the preceding function, Fm-1(x). The MSEs for F0(x), F1(x) and F2(x) are 875, 692 and 540. It¡¯s amazing how these simple weak learners can bring about a huge reduction in error! Note that each learner, hm(x), is trained on the residuals. All the additive learners in boosting are modeled after the residual errors at each step. Intuitively, it could be observed that the boosting learners make use of the patterns in residual errors. At the stage where maximum accuracy is reached by boosting, the residuals appear to be randomly distributed without any pattern. Plots of Fn<U+00A0>and hn In the case discussed above, MSE was the loss function. The mean minimized the error here. When MAE (mean absolute error) is the loss function, the median would be used as F0(x) to initialize the model. A unit change in y would cause a unit change in MAE as well. For MSE, the change observed would be roughly exponential. Instead of fitting hm(x) on the residuals, fitting it on the gradient of loss function, or the step along which loss occurs, would make this process generic and applicable across all loss functions.  Gradient descent helps us minimize any differentiable function. Earlier, the regression tree for hm(x) predicted the mean residual at each terminal node of the tree. In gradient boosting, the average gradient component would be computed. For each node, there is a factor ¥ã with which hm(x) is multiplied. This accounts for the difference in impact of each branch of the split. Gradient boosting helps in predicting the optimal gradient for the additive model, unlike classical gradient descent techniques which reduce error in the output at each iteration. The following steps are involved in gradient boosting: XGBoost is a popular implementation of gradient boosting. Let¡¯s discuss some features of XGBoost that make it so interesting. So that was all about the mathematics that power the popular XGBoost algorithm. If your basics are solid, this article must have been a breeze for you. It¡¯s such a powerful algorithm and while there are other techniques that have spawned from it (like CATBoost), XGBoost remains a game changer in the machine learning community. If you have any feedback on the article, or questions on any of the above concepts, connect with me in the comments section below. Ramya Bhaskar Sundaram <U+2013> Data Scientist, Noah Data It¡¯s safe to say my forte is advanced analytics. The charm and magnificence of statistics have enticed me, all through my journey as a Data Scientist. There is a definite beauty in how the simplest of statistical techniques can bring out the most intriguing insights from data. My fascination for statistics has helped me to continuously learn and expand my skill set in the domain.My experience spans across multiple verticals: Renewable Energy, Semiconductor, Financial Technology, Educational Technology, E-Commerce Aggregator, Digital Marketing, CRM, Fabricated Metal Manufacturing, Human Resources. Nice explanation ! Hi. Nice article. Thanks for sharing. Couple of clarification
1. what¡¯s the formula for calculating the h1(X)
2. How did the split happen x23. Hi Srinivas,","Keyword(freq): learner(16), residual(12), tree(11), model(6), technique(6), error(5), iteration(3), step(3), concept(2), feature(2)"
"7","vidhya",2018-09-02,"The 5 Best Machine Learning GitHub Repositories & Reddit Threads from August 2018","https://www.analyticsvidhya.com/blog/2018/09/best-machine-learning-github-repositories-reddit-threads-august-2018/","

MEGA-LAUNCH Offer on Computer Vision Using Deep Learning | Use COUPON CODE: CVLAUNCH60 for 60% Discount |
Course Starts 4th September | 
Buy Now 

 When I started using GitHub early last year, I had never imagined how useful it would become for me. Initially I only used it to upload my own code, assuming that was the extent to which GitHub would prove it¡¯s usefulness. But as I joined Analytics Vidhya and my scope of research expanded, I was enthralled by how vast this platform really is. Apart from allowing me access to open source codes and projects from top companies like Google, Microsoft, NVIDIA, Facebook, etc., it opened up avenues to collaborate on existing projects with fellow machine learning enthusiasts. I cannot tell you how amazing it feels to have contributed to a project that other people use. It¡¯s a feeling like no other. And this, of course, led me to write this monthly series which I hope you have found beneficial in your own line of work. This month¡¯s article contains some pretty sweet repositories. There¡¯s a project from NVIDIA which looks at video-to-video translations, a neat Google repository that makes reinforcement learning way easier to learn than ever before, and I¡¯ve also included a useful automated object detection library. There¡¯s a ton of more information below, including an entertaining R package. In our Reddit section, we have diverse discussions ranging from multiple expert reviews of Julia to real-life data leakage stories. As a data scientist, you need to be on top of your game at all times, and that includes being updated with all the latest developments. Reddit, and AVBytes, should definitely be on your go-to list. You can check out the top GitHub repositories and top Reddit discussions (from April onwards) we have covered each month below: There has been tremendous progress in the image-to-image translation field. However the video processing field has rarely seen many breakthroughs in recent times. Until now. NVIDIA, already leading the way in using deep learning for image and video processing, has open sourced a technique that does video-to-video translation, with mind-blowing results. They have open sourced their code on GitHub so you can get started with using this technique NOW. The code is a PyTorch implementation of vid2vid and you can use it for: Check out our coverage of this repository here. If you¡¯ve worked or researched in the field of reinforcement learning, you will have an idea of how difficult (if not impossible) it is to reproduce existing approaches. Dopemine is a TensorFlow framework that has been created and open sourced with the hope of accelerating progress in this field and making it more flexible and reproducible. If you¡¯ve been wanting to learn reinforcement learning but were scared by how complex it is, this repository comes as a golden opportunity. Available in just 15 Python files, the code comes with detailed documentation and a free dataset! You can additionally read up on this repository here. Object detection is thriving in the deep learning community, but it can be a daunting challenge for newcomers. How many pixels and frames to map? How to increase the accuracy of a very basic model? Where do you even begin? You don¡¯t need to fret too much about this anymore <U+2013> thanks to MIT¡¯s algorithm that automates object detection with stunning precision. Their approach is called ¡®Semantic Soft Segmentation (SSS)¡¯. What takes an expert, say 10 minutes to manually edit, you can now do in a matter of seconds! The above image is a nice illustration of how this algorithm works, and how it¡¯ll look when you implement it on your machine. View our coverage of this technique in more detail here. Pose estimation is seeing a ton of interest from researchers this year and publications like MIT have published studies marking progress in this field. From helping elderly people receive the right treatment to commercial applications like making a human virtually dance, pose estimation is poised to become the next best thing commercially. This repository is Microsoft¡¯s official PyTorch implementation of their popular paper <U+2013><U+00A0>Simple Baselines for Human Pose Estimation and Tracking. They have offered baseline models and benchmarks that are good enough to hopefully inspire new ideas in this line of research. This one is for all the R users out there. We usually download R packages from CRAN so I personally haven¡¯t felt the need to go to GitHub, but this package is one that I found very interesting. chorrrds helps you extract, analyze, and organize music chords. It even comes pre-loaded with several music datasets. You can actually directly install it from CRAN, or use the devtools package to download it from GitHub. Find out more about how to do this, and more details, in this article. In case you haven¡¯t been following OpenAI in the last couple of months, their team has been hard at work trying to hype up their latest innovation <U+2013> OpenAI Five. It¡¯s a team of five neural network working together to become better at playing Dota. And these neural networks were doing extremely well, until they ran into the first professional Dota playing team. This Reddit thread looks at the team¡¯s defeat from all angles, and the machine learning perspective really stands out. Even if you haven¡¯t read their research paper, this thread has enough information to get you up to speed in a jiffy. There are well over 100 comments on this topic, a truly knowledge-rich discussion. Most of us in the data science and machine learning space have used Notebooks for various tasks, like data cleaning, model building, etc. I¡¯m actually yet to meet someone who hasn¡¯t used Notebooks at some point in their data science journey. We don¡¯t usually question the limitations of these notebooks, do we? Now here¡¯s an interesting take on why Notebooks aren¡¯t actually as useful as we think. Make sure you scroll through the entire discussion, there are some curious as well as insightful comments from fellow data scientists. And as a bonus, you can also check out the really well made presentation deck. TensorFlow 2.0 was teased a couple of weeks ago by Google and is expected to be launched in the next few months. This thread is equal parts funny and serious. TensorFlow users from around the world have given their take on what they are expecting, and what they want to see added. Quite a lot of comments are around the usefulness of Eager Execution. This has been a long awaited update so big things are being expected. Will Google deliver? The Julia programming language has been doing the rounds on social media lately after a few articles were written on how it might replace Python in the future. I¡¯ve had requests to review the language and have directed everyone to this thread. What better place to check out the pros and cons of a programming language than a hardcore ML Reddit thread? Rather than reading one perspective, you get access to multiple reviews, each adding a unique point of view. What I liked about this discussion was that plenty of existing Julia users have added their two cents. The consensus seems to be that it is showing a lot of promise (especially the latest release, Julia 1.0), but it has a while to go before it catches up with Python. We are all caught up in trying to solve real-world problems that we tend to forget issues that might crop up in existing projects. You might be surprised at the kind of stories people have told here <U+2013> including one where they had duplicate entries for one row, and that was making the model overfit the training data massively. There are some useful links as well for further reading on the kind of data leakage problems that have come up in the industry. Have you ever been a victim of data leakage? Share your story in this Reddit thread and participate in the discussion!","Keyword(freq): notebook(4), comment(3), project(3), user(3), discussion(2), problem(2), repository(2), review(2), story(2), time(2)"
"8","vidhya",2018-09-02,"DataHack Radio Episode #9: Data Science at Airbnb & Lyft with Dr. Alok Gupta","https://www.analyticsvidhya.com/blog/2018/09/datahack-radio-lyft-dr-alok-gupta/","

MEGA-LAUNCH Offer on Computer Vision Using Deep Learning | Use COUPON CODE: CVLAUNCH60 for 60% Discount |
Course Starts 4th September | 
Buy Now 

 Airbnb and Lyft have transformed their respective industries in recent years using data science as their guiding light.<U+00A0>In episode 9 of our DataHack Radio series, Dr. Alok Gupta gave us some very interesting insights into how Airbnb and Lyft use data science.<U+00A0>For instance, did you know that Spark is Airbnb¡¯s machine learning tool of choice? Dr. Alok is currently working as the Director of Data Science and Head of Growth Science at Lyft. He has a deep passion for mathematics and has used that throughout his career, including his four year stint at Airbnb. You will learn a lot in this podcast about how a data science leader thinks about challenging problems, and how leading tech start-ups scale up their operations from the ground up. This article summarizes the key points Dr. Alok discussed during this podcast. This is another valuable addition to the DataHack Radio podcast series, and I highly recommend listening to it as soon as possible! Subscribe to DataHack Radio NOW and listen to this, as well as all previous episodes, on any of the below platforms: Dr. Alok completed his undergraduate in mathematics from Cambridge University and proceeded to do his Masters in Finance and Mathematics from Imperial College, London. During his time there, he developed an interest in stochastic finance and statistics and decided to pursue this as his Ph.D at Oxford University, which he successfully completed in 2010. During his Ph.D years, the infamous recession struck and created chaos in the industry so he wasn¡¯t sure which industry to apply to. He ended up in financial trading at Deutsche Bank where he had an opportunity to design and build algorithms with profit and loss objectives. As part of his role at Deutsche Bank, he moved from London to New York, where he worked for around a year and a half. He discovered the role of a data scientist while in New York, and realized the similarities between that, and his own role as a Quant Trader in finance. This led to him applying at a number of companies and he finally got his break in 2014 at Airbnb as a data scientist and the rest, as he said, is history. The overlaps between a data scientist and a quant trader were plenty, including understanding the problem and framing it in a way that made business sense. There were other intersections, like opportunity sizing, detective analysis, impact estimation, etc. Of course one of the most interesting commonalities was actually solving the problem <U+2013> deciding which mathematical, or statistical, techniques do we need to apply, what is the objective function, how do we get to the optimal solution, among others. But there were a couple of crucial differences between these two roles as well, as Alok discovered in his initial days at Airbnb. The metric that you¡¯re trying to optimize in finance is taken as given (for example, trying to optimize PnL is a concrete objective). Whereas in the technology space, this was vague and needed to be understood at a far more granular level before performing any data science task. Experimentation is another tricky and challenging aspect in technology (there are a number of assignment units, different methodologies for measurements, etc.), whereas in finance you run an algorithm, see how much money it makes, and you¡¯re done! When Alok joined Airbnb in 2014, the entire company was some 1,000 employees strong, with the data science team consisting of just 10 people (when he left earlier this year, the team had grown to around 110!). He started as the Data Scientist on their Risk and Safety fraud prediction team, where he built models for both online and offline fraud detection. One year into his role, Alok started to build his own data science team in the Customer Support optimization space. Airbnb has some 10,000 customer support employees globally that use channels like phone, chat, email, SMS, etc. to help their customers resolve issues. This, as you can see, was a challenge ripe for machine learning. Alok has explained how his team took this as an optimization problem in the podcast and the different features they considered for the final model. A very fascinating section, this. In his last 2 years at Airbnb he switched focus completely to work on acquisition of new guests. This included sourcing different marketing channels, working on search engine optimization, recommendation systems, etc. Alok has described the acquisition process in a lot of depth which will benefit anyone who works in data science, regardless of the industry. The way he and his team approached the problem and worked their way through it can serve as a roadmap for all aspiring data scientists. Most of the data scientists at Airbnb use tools and services like Amazon Web Services (AWS), HIVE, etc. to pull or extract the data they needed. Python and R are used to perform local analysis and Alok saw an increasing number of data scientists moving to Python as it¡¯s easier to productionize Python scripts. For building models and solutions when confronted with large datasets, Airbnb¡¯s machine learning tool of choice was Spark.<U+00A0>Airbnb has also invested in building it¡¯s own centralized machine learning platform that can enable non-data scientists and non-engineers to spin up their own ML models without needing to have a lot of programming experience. Alok led the way in pioneering a knowledge sharing tool within the organization which was shared between data scientists and non-data scientists. The idea behind it was to get everyone on the same page regarding the happenings internally, and it was almost always written in Python or R as a Markdown document. This also helped them get peer reviews on any technical stuff and the quality of analysis was raised to unprecedented levels. At Lyft, all the folks working in the analytics and data science domain are grouped under the umbrella of scientists. Acquisition, engagement, and retention (of passengers and drivers) are some of the problems they are currently working on simultaneously. In his role as the head of Growth Science, Alok has been exposed to the supply side of things, a new and exciting challenge for him. The data science team under Alok currently consists of 40 people (at the time of recording this podcast). Quiet a few challenges he is facing in his current role, which he started just four months ago, he has already seen at Airbnb, so he feels at home in that respect.","Keyword(freq): scientist(7), airbnb(3), mathematic(3), model(3), channel(2), employee(2), problem(2), service(2), algorithm(1), analytics(1)"
