"","site","date","headline","url_address","text","keyword"
"1","kaggle",2018-10-17,"Data Notes: The Secret of Academic Success","http://blog.kaggle.com/2018/10/17/data-notes-the-secret-of-academic-success/","From t-SNE to Sex and the City: Enjoy these new, intriguing, and overlooked datasets and kernels 1. <U+0001F4DA> What is the Secret of Academic Success? (link) 2. <U+0001F1F0>Analyse Your Kaggle Profile!! (link) 3. <U+0001F1E8><U+0001F1F3> Chinese News WordCloud EDA (link) 4. <U+0001F952> t-SNE: Amazon Fine Food Reviews (link) 5. <U+0001F454> Clustering FMNIST w/ Neural Networks (link) 6. <U+0001F613> IBM Attrition Analysis and Prediction (link) 7. <U+0001F30E> Geospatial Analysis of Brazilian E-Commerce (link) 8. <U+0001F69B> Dataset: Craigslist Cars & Trucks Data (link) 9. <U+0001F3D9><U+FE0F> Dataset: Every Sex and the City Script (link) 10. <U+0001F596> Dataset: Star Trek Scripts (link) Using CNNs? Try experimenting with different network architectures! 
Copyright ¨Ï 2018 Kaggle, All rights reserved.","Keyword(freq): architecture(1), car(1), cnn(1), dataset(1), kernel(1), network(1), review(1), right(1), script(1), truck(1)"
"2","datacamp",2018-10-15,"SQL Server, PostgreSQL, MySQL... what's the difference? Where do I start?","https://www.datacamp.com/community/blog/sql-differences","The diagram below contains information about columns in two tables in an example relational database. Both tables contain columns named customer_id, which establishes a relationship between the tables. As the company grows and records thousands (or millions) of orders, storing data in separate tables helps optimize for space and reduce the size of the database. SQL, or Structured Query Language, is the standard language for interacting with relational databases. With SQL, you can query, or ask questions of, the data in a relational database. Working with SQL and relational databases is an invaluable skill set for a data analyst, data engineer, or a data scientist. If you have started looking for ways to learn SQL, you may have noticed the many different dialects of SQL available to learn with some clear (and less clear) distinctions between the different dialects. So where do you begin? Which version of SQL is most helpful to you if you haven¡¯t used it before? In this article, we will focus on four of the most popular database management systems -- PostgreSQL, MySQL, SQLite, and SQL Server -- and their versions of SQL syntax. The graph below from Stack Overflow Trends provides a sense of how often each of these platforms is discussed -- each line represents the percentage of all Stack Overflow questions about each version of SQL. MySQL has consistently been the most popular version of SQL in Stack Overflow questions. Second in line is Microsoft SQL Server (including T-SQL, the name of Microsoft¡¯s dialect of SQL), which remains a consistently more popular tag than PostgreSQL and SQLite. This means that if you have a question specific to one of these systems, you¡¯re more likely to find that someone already asked your question. PostgreSQL, MySQL, and SQLite use very similar syntax, with some notable differences highlighted below. Microsoft SQL Server has the greatest contrast in SQL syntax, as well as a wide variety of functions not available in other platforms. The table below highlights some examples of basic differences between SQL platforms. For students who have little to no experience with SQL and are looking to gain the most broadly applicable skills, I recommend starting with PostgreSQL. Despite the overwhelming popularity of MySQL, PostgreSQL may be a better choice because its syntax most closely conforms to Standard SQL. This means that you can easily translate your skills to other database management systems such as MySQL or SQLite.
For example, the query below aggregates data from a database of sales information. It contains a join, an aggregate function, and a filter. This syntax will generate identical results in all three database systems. If you anticipate working with Microsoft SQL Server in your career, I recommend you start by learning T-SQL/Microsoft SQL. SQL Server continues to maintain a sizable market share and is an important database management system in many industries. If you are unsure which of these two is more important for you, I recommend browsing through job openings in your field to determine if there is a preference for a specific database system in specific roles or industries. At DataCamp, we currently offer two courses in SQL that cover introductory topics and joins in PostgreSQL. We have some exciting courses in development covering intermediate and advanced topics in PostgreSQL. We also have several SQL Server courses in development, including an Introduction to T-SQL/Microsoft SQL course that will provide you with a clear foundation for working with SQL Server. You can check out our course roadmap for more information. If you¡¯re looking to practice interacting with a PostgreSQL database on your own, I recommend exploring data sets on Kaggle. If you find something that interests you, go ahead and import into PostgreSQL (CSV or SQLite files will import into PostgreSQL) and start exploring! Are you interested in sharing your knowledge and expertise with our 3 million students on DataCamp? We are always looking for instructors to contribute to our growing course library. Our course wishlist highlights the skills we are hoping to make available to our students in the coming months. You can apply here at this link. We look forward to working with you!","Keyword(freq): system(4), table(4), platform(3), question(3), skill(3), student(3), column(2), databasis(2), dialect(2), difference(2)"
"3","datacamp",2018-10-15,"Project Jupyter and Interactive Computing (Transcript)","https://www.datacamp.com/community/blog/project-jupyter-interactive-computing","Here is the podcast link. Hugo:               Hi there, Brian, and welcome to DataFramed. Brian:               Hi, Hugo. Thanks so much for having me on today. Hugo:               It's such a pleasure to have you on the show. And we're here today to talk about Project Jupyter, about interactive computing, and in fact, you sent me a great slide deck today of yours that you've been giving recently. Something we're going to be focusing in on is actually a slide that you have there. And I'm just going to quote this before we get started. You wrote, ""We are entering an era where large, complex organizations need to scale interactive computing with data to their entire organization in a manner that is collaborative, secure, and human centered."" Now these are all touch points we're going to be speaking about during this conversation. But before we get into all of this and before we get into the conversation about Jupyter, Jupyter Notebooks, JupyterLab, and all of these things, I'd like to know a bit about you. So first maybe you could tell me a bit about what you're known for in the data community. Brian:               Yeah. Not a problem. So I'm a physics professor at Cal Poly for the last, close to 15 years. I've been involved in a number of open source projects in the scientific computing and data science space. In the early years I was involved in SymPy, which is a symbolic computer algebra library for Python. And then also IPython, the de facto, interactive shell for Python. And then in the more recent years I was one of the co-founders of Project Jupyter. And in the last few years I've also co-founded Altair with Jake VanderPlas, which is a statistical visualization library. So the big theme now is open-source tools for humans working with code and data. Hugo:               And speaking of Altair, you're actually currently working on an Altair course for DataCamp, right? Brian:               Yes. You're being a little bit optimistic about your verb tense there. It's been a little bit stalled with all the different activities we have going on in the Jupyter world, but yeah, I think I'm around maybe two-thirds to three-quarters done with the DataCamp course for Altair. Hugo:               And so as a project lead for Project Jupyter, I'm wondering what type of skills come into play there. Because I know you have a very strong background, you're a physicist. You have a lot of data analytic skills. A lot of design and engineering and entrepreneurial skills presumably come into this role as well. So I'm just wondering what type of things you need in order to do this job? Brian:               Yeah. It certainly has evolved over the years. In the sort of early days of IPython and Jupyter, we were spending most of our time doing software engineering. There was a very small amount of design work, UI/UX design work. When it's only a handful of people, in principle but there's organizational work and community work to be done, but it's at a very small scale that is in the background relative to the software engineering. As Jupyter has grown though, I would say the demand for more time and effort on the organizational and community side, as well as the design aspects of the project, have really increased. One of the challenges in working on open source is that projects like Jupyter or Altair tend to attract really top-notch developers and software engineers. And so that aspect of the project tends to be reasonably well staffed. That doesn't mean that we all have as much time to put into the projects on the software engineering side as we would like. However, as these projects get big, there's nothing in particular that attracts top-notch UI/UX designers, for example, to Jupyter. That continues to be a challenge for us and other open-source projects in terms of how do we build design into the process and figure out how to engage designers in the development of the projects. Hugo:               So in terms of, I mean you're speaking about a number of things here that include design but also hiring, structuring an organization, I know that you think a lot about getting funding for the project. You're talking about community development, which these are all things we think about at DataCamp a lot as well. So it sounds somewhat similar in several ways to running a company. Brian:               It probably is. I've never run a company but when I talk to other people who are in different roles leading companies, there's a lot of overlap there. And our business model doesn't involve selling things to people in the traditional sense, but most certainly we have customers. And our interaction with those customers is very similar to that of a company who has paying customers in terms of, we exist in a very dynamic, fast paced part of the economy. And it's the type of thing that if Jupyter were to sort of relax and begin to coast, there's hundreds of other open-source projects and for-profit companies building products, quickly put Jupyter in a position of becoming outdated. And so there's a lot of thinking we do and work that we do around looking ahead, our three to five year growth map, where we see data science, machine learning, and interactive computing going and how do we build the resources to tackle those ambitious things on those time frames but also build a sustainable community along the way. Hugo:               So how did you get interested in or involved in data science, as opposed to being a physics professor and researcher? How did you get into data science initially? Brian:               Yeah, that's a great question. So we began working on interactive computing as part of IPython. I was a classmate of Fernando Perez back in grad school at the University of Colorado, and Fernando created IPython in the early two thousands. And at the time the world of interactive computing was really something that was done in the scientific world, in academic research, in education. I'm sure there were some companies at the time that were doing little bits of it here or there, but it wasn't something that was pervasive like it is today. And so as we started to build IPython and Jupyter in the 2000s, initially we felt like we had, what we imagined was a very grand vision that everyone in the world of academic research and scientific computing would be using Python and the tools that we and many other people were building. What we didn't see is that the whole world was about to discover data and that really sort of opened up a whole new audience and set of users to open-source data science tools and scientific computing tools that we never imagined. Brian:               And so, honestly, my own journey is more that we were doing what we had always done in terms of scientific computing and then woke up to realize that we were sort of right in the middle of the data science community that was forming both in the academic research side but also on the commercial industry side as well. Hugo:               I want to delve into a bit more about the general nature of Project Jupyter in a minute. But before that, I'd like to speak a bit more about interactive computing. So I'm going to quote Project Jupyter. Project Jupyter states it ""exists to develop open source software, open standards and services for interactive computing, across dozens of programming languages."" And I'm wondering what a general working definition of interactive computing is and why is it important? Brian:               Yeah. This is a great question and I think in the history of computer science, interactive computing has not even really been a thing that's acknowledged in terms of a topic worthy of study and something that is worth really thinking about carefully and clarifying. And it's something that we've been doing over the years and really the Jupyter architecture is an expression of our thinking about interactive computing. And I'd say that the core idea of interactive computing is that there is a computer program that's running where there's a human in the loop. As the program runs, that human is both writing and running code on the fly but then looking at the output of the result of running that code and making decisions about what code to write and run subsequently. And so there's this sort of interactive mode of going back and forth between the human authorship of the code and then the computer running it and the human interacting with the result in an iterative manner. Hugo:               And this is ideal for so many aspects of the scientific research process, right? From exploratory data analysis to writing code embedded with in-line results and images and text and that type of stuff? Brian:               Absolutely. This is something that we really think a lot about, and that is that when humans are working with code and data, eventually, at some point, for their work to be meaningful and impactful, the code and data need to be embedded into what we think of as a narrative or story around the code and data that enables humans to interact with it, make decisions based on it, understand it. And it's really that human application towards decision making. It really makes a difference to have a human in the loop when you're working with data. Hugo:               And for all our listeners out there who may not know what IPython is and how it differs from Python, would you mind spelling that out for them? Brian:               Yeah. So Python's the programming language. IPython stands for interactive Python and it originally was a terminal-based interactive shell for working with Python interactively. It had a lot of, and continues to have a lot of nice features that you want when you're working interactively, such as nice tab completion, easy integration with the system shell, in-line help, features like rich interactive shell. And the interaction between IPython and Jupyter today is that originally when we built the Notebook we called it the IPython Notebook because it only worked with Python. And then over the years we realized that the same user interface and architecture would also work with other programming languages. Core developers of IPython then sort of spawned another project, namely Project Jupyter, that's the home for the language independent aspects of the architecture. And IPython continues to exist today as the main way that people are using Python within Project Jupyter. And so it continues to be a project that we're still working on. Hugo:               So could you give us a high level overview of what Project Jupyter is and what it entails? Brian:               Yeah. So you've read a good summary of Project Jupyter in that it's focused around open-source software, open standards and services for interactive computing. And I think a lot of people are familiar with some of the software projects that we have created, namely the Jupyter Notebook, and I'm sure we'll get to talk more about that here in this conversation. But underneath the Jupyter Notebook is a set of open standards for interactive computing. I think when we think about Jupyter and its impact, it's really those open standards that are at the core of that. And one way to think about it is, it's a similar situation as to the modern internet where, yes, there's individual web browsers and websites, but underneath all of that there's a set of open standards, namely HTTP, TCP/IP, HTML, that enable all of those things to work together. The open standards that Jupyter has built for interactive computing serve a similar role as those other protocols do in the context of the broader internet. Hugo:               So I want to find out a bit about the scale and reach of Project Jupyter, but I want to preface this by saying... it's a story both you and I know but for our listeners, recently I attended a talk you gave at JupyterCon here in New York City. And before you started speaking, you asked people to put up their hand if 10 or less people in their organization used some aspect of Project Jupyter, then asked people to put their hand up if 50 or less, a hundred or less, 500 or less, a thousand or less and so on. And people put their hands up at every point. And then you asked: is there anybody in an organization which has over 10 thousand people using some aspect of Project Jupyter, and a number of people put their hands up. And that was a really large aha moment for me, thinking about the scale and reach of the project as a whole. So with that as a kind of intro, I'm wondering what you can tell us about the scale and reach of the project? Brian:               Yeah. So this is something that's been really fun to be a part of over the last few years, to see the usage of Jupyter literally explode and take off in ways that we never imagined. And there's a number of different ways of thinking about this. Being an open-source project, we don't have an accurate, precise way of tracking how many users we have. Our users obtain and install Jupyter through a number of different means and that does make it a challenge. One nice thing that we're watching is the number of notebooks on GitHub. And this can be obtained by querying the GitHub APIs. We have an open-source project where we're tracking that over time. And as of this summer the total number of public notebooks is on the order of two and a half million. And from talking with the GitHub folks that we know, it looks like there's roughly another similar amount of private Jupyter notebooks that are not visible to the world. Brian:               And so the interesting thing there, obviously the absolute number currently is interesting. I think what's more telling is that over time we're seeing an exponential increase in the number of notebooks and the doubling period right now is around nine or 10 months. So that really points to very strong current numbers as well as growth. It's difficult to put a number on the total number of people that are using Jupyter worldwide. Some of what makes it challenging is that most of our staff is in the US and Europe and yet we know from our Google Analytics traffic that Asia right now is one of the most popular continents that's using Jupyter. And so we don't have many contacts with people there. We don't know how Jupyter is being used but we see a very strong signal that it's being used heavily. Hugo:               And how about in terms of contributions and amount of developers working on the project? Brian:               Yeah. So along with the usage there's definitely been an increase in the number of contributors. I think that the total number of contributors is somewhere over 500 and it's a fairly large-scale open-source project. We have over a hundred different repositories on GitHub spread across a number of different orgs. The core team right now that are core maintainers of the project, many of whom work mostly full time on the project, is around 25 people. The Jupyter Steering Council is a key part of the leadership of the project and I think there's currently 15 steering council members. There's a number of new people who joined the steering council this summer is why I don't remember the precise number. Brian:               And one thing that I want to emphasize with this is that sort of what is the right narrative to have about the different contributions of people to Project Jupyter? I want to sort of make an analogy to Hollywood in terms of, if Jupyter were a movie, what type of movie would it be? And I think it's important to note that it would not be a movie where there is a single superhero who comes and saves the day. So sort of like a Superman narrative really doesn't fit the reality of how Jupyter has been built. A movie that I think that would be a better analogy to how Jupyter is built would be something like Infinity Wars, where you have a bunch of different superheroes, all very diverse in skills and strengths, contributing to the overall project. Brian:               I think it's really important to note that, yes, I'm the one that's here talking to you today but I am one among many people who have done absolutely amazing work on the project. Hugo:               And for our listeners out there who would like to get involved in perhaps contributing to the project, what are good ways to get involved and what are, I hesitate to say bad ways to get involved, but what are less good ways to get involved? Brian:               Yeah. So this is one thing that I talked about at JupyterCon in terms of, and in that context it was more thinking about what are healthy and productive ways for large companies to engage with open source. So for individuals, I would say one of the best ways would be to find a part of the project that you're interested in and then come on to GitHub and begin to interact with us. A lot of our popular GitHub repos have issues that are tagged for first-time contributors. And so we're working hard to try to make the project a welcoming place for new contributors. We welcome people to come and talk to us there. Brian:               We also have chat rooms that are public, on Gitter.im. This is an online, web-based chat platform that is integrated with GitHub. And so, for example, the Jupyter Notebook, JupyterLab, JupyterHub, Jupyter Widgets, all have chat rooms on Gitter. And both the core contributors as well as the broader community hang out in those contexts. So that's a great way for people to get involved. Hugo:               And how about organizations that want to contribute to the project, Brian? Brian:               I think in this case it's really helpful to have a good mental model of how open-source projects work and how contributions function. My favorite mental model is actually from Brett Cannon, who's one of the core Python devs and works at Microsoft. In a Tweet he said something like, ""Submitting a pull request in open-source project is like giving someone a puppy. And that is, you have to understand that the person accepting the pull request is essentially agreeing to care for that puppy for the rest of its life."" And one of the patterns that we see in organizations, companies that want to contribute to open source, is that they're interested in particular features and so they have their employees contribute to open source in a way that generates a lot of large new pull requests for those new features. Brian:               And I think this is where the puppy mental model really helps. And that is, oftentimes the core maintainers of open source projects are completely overwhelmed with just the maintenance of the project. That could include bug fixes, releasing issue triage and managing issues, but also reviewing other contributors' pull requests. And so one of the most helpful things that we're trying to cultivate is basically a balanced perspective of contributions that includes not just submitting pull requests that have new features but also includes reviewing other people's pull requests, involves helping other users with particular issues, and even fixing bugs. Brian:               One really nice thing that GitHub has done recently is in their contributor user interface, they have a new user interface for expressing someone's contributions to a particular GitHub repository and there's sort of an X, Y coordinate system and four directions around that. And it shows someone's contributions to, I think it's code review, pull requests, issues, and there's one other. From that you can get a perspective on how balanced someone's contributions are to an open-source project. Brian:               And so a simple way of putting it is, encouraging people to have a balanced way of contributing to open source. Now we also want to specifically address first time or new contributors. And there I think the idea again is balance but in a way where the core contributors and existing people working on the project can help new contributors to come along and begin to contribute in different ways. And so even for new contributors, those contributions don't necessarily have to be pull requests. Even checking out existing pull requests, just testing them locally, is really, really helpful for open-source projects. Hugo:               And it's incredible that GitHub now has the feature you discussed which kind of facilitates just figuring out this balance, right? Brian:               Oh, absolutely. I was thrilled to see them release that and I think it's happened in the last month. Off hand I don't even remember exactly what they're calling it. Hugo:               As we've been discussing, the scale and reach of Project Jupyter is massive. So I'm sure there are so many different uses of notebooks and the Project in general. But I'm wondering, to your mind, what the main uses of Jupyter Notebooks for data science and related work are? Brian:               Yeah. In terms of numbers of people using Jupyter for a particular purpose, I would say interactive computing with data, so data science, machine learning, AI, is one of the most popular ways that Jupyter's being used. Both by practitioners, so people who are working in the industry on data science and machine learning teams, but also in educational contexts. So within universities, with online programs, with boot camps, you have instructors and students doing those activities around data science and machine learning but in an educational context. I think that really captures the bigger picture of Jupyter's usage. Hugo:               Yeah. And in fact, we use them at DataCamp for our projects infrastructure, which, as you know, we teach a lot of skills in our courses. In our projects we teach kind of end-to-end data science workflows using Jupyter Notebooks. Brian:               That project style workflow is something that I've seen when I have taught data science at Cal Poly, my university, in that oftentimes it's helpful to start with students in a very highly scripted manner where the exercises are very small scale and focus on a particular aspect of a particular concept. And then eventually transition to more open ended project based work. I know in those course, towards the end of the quarter when the students have an opportunity to do sort of end-to-end data science that's a little more open ended, the learning really increases a lot and the students get a lot out of it. So I'm thrilled to see that DataCamp has that type of experience as well. Hugo:               And of course, we see notebooks pop up everywhere. From in the slide deck that we discussed earlier, you have a great slide on the Large Synoptic Survey Telescope. On top of that of course the gravitational waves were discovered by the LIGO project. And they've actually published all of their Jupyter notebooks. So this is in basic scientific research, right? I mean there's a lot of stuff happening at Netflix with Notebooks now. So it's across the board, right? Brian:               Yeah. And this is really a pattern that we've seen emerge in the last two years, and that is the transition from ad hoc usage by individuals in organizations to official organization-wide deployments at scale. And so we're starting to see a lot more organizations adopt Jupyter in a similar way to LIGO or LSST or Netflix where it is officially deployed and maintained by the organization and many, many users are using Jupyter on a regular basis. Some of the larger deployments that we're aware of are many thousands, or even on the order of 10 thousand or more people. So the scale is definitely getting large in these organizations. Hugo:               I'm going to say two things that I think are facts, and correct me if I'm wrong. Netflix runs over a hundred thousand automated notebook jobs a day. And at least two, either contributors or core contributors to Project Jupyter, work full time at Netflix as well. Brian:               Yes, absolutely. So Kyle Kelley and M Pacer are on the Notebook team, I don't know if that's exactly the name of their team, but they're one of the tools teams at Netflix. They work both with us on some of the core Project Jupyter projects but also they have a number of other open-source projects that work with the different Jupyter protocols. One of those is InterAct, which is another user interface from working with Jupyter Notebooks that has a focus on simplicity and personas, where individuals do want to work some with code but they're not living and breathing in code all the time. Business analysts would be a great example of the type of persona that InterAct is targeting. And then, as you mentioned, Netflix has really innovated in the area of using notebooks in a programmatic way, running them at batch jobs every day. And I think your number of around a hundred thousand batch jobs that are notebooks a day sounds about right from what I remember. Brian:               And then a number of open-source projects out to help with those type of workflows. One of those is Papermill, the other is Commuter. And I think one of the things I love about what's going on at Netflix, and I think this really comes from the leadership of Kyle Kelly there, and that is a deep understanding of the value of Jupyter's open protocols. And that is sort of a recognition that the different software projects that we've built on top of those protocols are sort of like a Lego set that you get. You bring it home from the store and there's a default instruction set to build something out of the box. But then realizing that the same pieces can be reassembled in different ways to build whatever your organization needs. I love how that thinking has really sort of seeped into all the different ways that Netflix is working with data. And I think they're doing really interesting things as a result. Hugo:               And the Notebook team at Netflix actually published a really interesting blog post article recently, which we'll link to in the show notes along with a lot of other things that we're talking about. Brian:               Yeah. And they also gave a number of talks at JupyterCon and those talks will be posted on the JupyterCon YouTube channel here in the coming month I think. Hugo:               Okay. Fantastic. So when we have 2.5 million public notebooks on GitHub, I'm sure there's a lot of surprising stuff happening out there. This may be a bit of a curve ball, but I'm wondering if you've seen any uses Jupyter Notebook that have surprised you, you've been like, ""Oh, wow, that's interesting."" So what is the most surprising use of a Notebook you've seen? Brian:               Yeah. I mean one of the fun things about working on Project Jupyter is to follow all the amazing things that our users are doing. And I think seeing the impact that Jupyter's having in the world of scientific research is something that we're really proud of. So to see large-scale science such as LIGO and Virgo winning a Nobel Prize in physics and as part of that publishing Jupyter Notebooks that anyone in the world can use to completely reproduce their analysis all the way from the raw data to the final publication-ready visualizations. That makes us really proud. I don't know that surprise is the right word to use there. Some of that is that that's the community that we came out of and so we've always worked really hard to make sure that Jupyter was useful for those usage cases. Brian:               In terms of surprise, the most surprising or shocking usage of Jupyter was by Cambridge Analytica and SCL Elections to build machine-learning models to manipulate the 2016 elections. Hugo:               Right. And I do think surprising is one word there. Shocking is another word. And I actually remember the first... I saw a Tweet, and I think it was Wes, it was Wes McKinney who tweeted words to that effect. And we saw a screenshot of a Jupyter Notebook with pandas DataFrame with some scikit-learn fit and predicts or something like that. And that was a moment where I also stepped back and really thought, all these tools can be used for all types of purposes, right? Brian:               So Cambridge Analytica, all of their web presence and GitHub presence is gone. SCL Elections, which worked with them, hasn't taken their stuff down, or hasn't taken all of their stuff down from GitHub. And so there's a project called JupyterStream. You can tell that the people working at SCL Elections were typical data scientists who were excited to use these tools to do data science. And the thing that's scary is if you look in the demo subdirectory in notebooks, there is a notebook there and it's very clear the type of things they were doing. Now in this particular case, it's nothing particularly sensitive. It looks like they're tracking voter registration counts by calendar week and working with a pandas DataFrame with that. But we were certainly ... again, I'm not quite sure what the right word is, surprised doesn't quite capture it. Brian:               I think it was really a moment of waking up for us and realizing that having an open-source project with a very free and liberal open-source license is very similar to free speech in that it literally is a licensed open-source project can and will be used by just about anyone, and that includes people doing really good things, but also people doing really evil things. Hugo:               It is creepy how this repo also says Jupyter to the rescue, exclamation point. Brian:               It's really a trip. I mean that was the original interview that Christopher Wylie did. So he was one of the data scientists at Cambridge Analytica. And in that first interview that came out, I think it was in The Guardian, he used the phrase ¡°build models¡±. And immediately I thought, ""Wait, hold on a second. This is a data scientist. They're talking about building models. What they're really saying there is import scikit-learn in a Jupyter Notebook."" And initially it was sort of like, ""Yeah, they might have used it. Or maybe they used RStudio."" But then over time it's become very clear that they certainly were using Jupyter at some point. Hugo:               So this is a nice segue into my next question which may seem like that's an answer to but I suppose it isn't necessarily. My next question is, adoption of Notebooks, as we've discussed, have been huge and I'm wondering if there are places you see Notebooks used where they shouldn't be used? Brian:               Yeah. I'm not quite sure I would phrase it where shouldn't be used. But certainly I think there's a little bit of an effect where the Notebook is a hammer, and so everything starts to look like a nail. In particular the type of workflow where Notebooks begin to be rather painful is when exploratory data science and machine learning becomes more software engineering and more about data engineering. And in those usage cases, it's not a fantastic software engineering environment. It's not designed for that purpose. Now this is something we're hearing from our users that right now there's sort of a very steep incline between working interactively in a notebook and software engineering. And as someone moves across that transition, at some point today they get to the point where really the right thing for them to do is stop using Jupyter and open up their favorite IDE and start to do traditional software engineering. And that can be rather painful in that most of the IDEs that people love are not web-based. Brian:               And so if anyone's working with significant amounts of data and running in the Cloud, those IDEs may not even really be a great option. And so we are working a lot to improve the experience at that boundary between interactive data science and software engineering. We don't envision that Jupyter's ever going to replace full-blown IDEs, but it's really at that boundary where we're seeing a lot of user pain currently where notebooks themselves start to be not the best tool. Hugo:               Yeah. And that dovetails nicely into my next question, which is around the fact that there are several common criticisms of Notebook, such as they may encourage bad software engineering practices. And I suppose most famously recently, JupyterCon accepted Joel Grus' talk, I Don't Like Notebooks, to be presented at JupyterCon. I'm just wondering what you consider the most important or relevant or valuable or insightful criticisms that can help moving the project forward? Brian:               Yeah. So I think there's, I really appreciate the talk that Joel gave at JupyterCon. It was a really well received talk and we want to hear things like this. It's really important for us. Some of the criticisms that he had about Project Jupyter are in the category of things that we fix. So existing user experiences or features that we offer or don't offer or could improve. And most of the things he brought up in that category I think the whole core Jupyter team is more or less on the same page. Brian:               The other aspect that he was bringing up gets more to the heart of interactive computing with Jupyter Notebooks. I think it's helpful to bring those things up as it really forces us to clarify the value proposition of that type of workflow in a notebook compared to just traditional software engineering. And so I think the discussion that has emerged out of that has been really helpful and something that is helping us to clarify, when should you use Jupyter Notebooks or why would you use them and why should you not use them in some circumstances. Hugo:               Absolutely. So we've discussed Notebooks, of course, but something I'm really excited about, and I know it's something you're incredibly excited about, is the next generation user interface for Project Jupyter, which is JupyterLab! So maybe you can tell us what JupyterLab is and why working data scientists would find it useful? Brian:               Yeah. JupyterLab is definitely something that I and many other people in the core team are excited about. JupyterLab is a next generation user interface for Project Jupyter. We've been working on JupyterLab for around four years now and just in the last month it left beta, so it is ready for production. Hugo:               And congratulations. Brian:               Thank you very much. We're really pleased to get through that hurdle. It's still not at at 1.0 release because some of the developer oriented extension APIs are still stabilizing. One of the big things we heard of users of the classic Notebook is that people wanted the ability to customize and extend and embed aspects of the Notebook with other applications. And the original code base in the classic Notebook just wasn't designed in a way that made that easy. So one of the core design ideas in JupyterLab is that everything in JupyterLab is an extension and all those extensions are in PM packages. And the idea there is that the core Jupyter team can't build everything for everyone and a lot of different individuals in organizations will come along and add new things to JupyterLab. Those extension APIs, which are the public developer oriented APIs of JupyterLab, enable those extensions to be built and we're still in the process of stabilizing some of those APIs. Brian:               But I want to emphasize that from a user's perspective, for people who are using Jupyter on a daily basis, JupyterLab is fully stable and production ready and in many ways, at this point, I would say it's a better user experience and more stable than the classic Notebook. Hugo:               Great. And what type of features do you have in JupyterLab that you don't get in the classic Notebook? Brian:               One of them is the ability to work with multiple different activities or building blocks for interactive computing at the same time. So the classic Notebook, each notebook or terminal or text editor, worked on a separate browser tab. And that made it very difficult for us to integrate those different activities with each other. So an example of how that integration would work in JupyterLab is, if you have multiple notebooks open side by side, you can just drag a cell between those two notebooks. Another example would be if you have a markdown file open, you can right click on the markdown file and open live markdown preview and then also open a code console, attached to that markdown file and start running code in any of the different languages that Jupyter supports in a manner that's more similar to an experience like RStudio. So having the different building blocks, places to type code, outputs, terminals, notebooks, integrated in different ways to support some of these other workflows that come up. Hugo:               And also a CSV viewer, right? Brian:               Yes. So another big idea, design idea in JupyterLab, is the idea of more direct manipulation of user interfaces. And so in many cases, writing code is the most effective way of interacting with data. However, there's many situations where writing code is a bit painful. And a great example of that is, if you have a new CSV file, you don't know what's in it, and you simply want to look at it. Of course you can open up a notebook, import Pandas and start to look at the CSV file. But in many cases, more direct modes of interaction are highly productive and useful. So JupyterLab's file system access is based around the idea of the possibility of multiple viewers and editors for a given file type. Brian:               And so for example, for a CSV file, you can open it in a text editor and edit it as a plain text CSV file, or you can open it in this new grid or sort of tabular view of it we have, and that viewer's the default for CSV files. So you can just double click on a CSV in JupyterLab and it will immediately open in a form that looks like a table. Hugo:               And I recall from one demonstration that it supports wildly large CSV files as well, right? Brian:               Yeah. So one of our core contributors, Chris Colbert, spent a lot of time building a well designed data model and a viewer on top of that. So the data model for this grid viewer does not assume that all of the data is loaded into memory. So it has an API that allows you to request data from a model on an as needed basis. And where that's used is in the view that sits on top of that, if you have a really large CSV or tabular data model, the view is only going to request the portions of the data that are visible to the user at a given time. And so for example, right now, some of the demos that we're doing, you can just double click on a CSV file, it has over a million rows and it's big enough, those files are big enough that they don't open successfully in Microsoft Excel on the same laptop. And they open just fine in JupyterLab anr the viewer or the renderer that Chris wrote, it uses Canvas, so it's a very high performance tabular data viewer. Brian:               And to keep ourselves honest, we've tested it with synthetic data sets. So these are not concrete data sets, they're generated on the fly but they have a trillion rows and a trillion columns. And the tabular dataset viewer works really well and you can scroll through the dataset just fine. Brian:               And I think another side effect of direct interaction with data is that when you make it easy for users to interact with data in those ways, they're going to do that, right? So if you can double click on a CSV file, a user's going to find, ""Wow, that's useful,"" and they're going to do that. And you have to spend a lot of time making sure the underlying architecture doesn't let them do things that are going to have adverse side effects. We're trying to build an architecture that has both good user experience but also can deal with the realities of large data. Hugo:               And there are several other features that we could discuss, but I'm just going to pick one which I think is very attractive and fantastic, which is the ability to collaboratively work on Jupyter Notebooks with colleagues and collaborators. Brian:               Yes. So this is something that we've been working on for a while now. Our first take on this was a JupyterLab extension post talk that UC Berkeley wrote, Ian Rose. And this provided integration with Google Drive and the Google Realtime APIs, which enable multiple people to open a single notebook and collaborate in real time on that notebook and see the other people working and editing the notebook at the same time. Brian:               And then in the last year and a half, we've started a new effort to build a real time data model and data store for JupyterLab for two reasons. One is that the Google Realtime API has been discontinued. And then the other is that we've heard very clearly from our users that there's many organizations for whom sending all their data to Google APIs is a no go. And so it's become really important for us to have a high performance, really well designed real time data storage. We've been working on that for the last 18 months. Again, Chris Colbert, who did the data grids, is the person working on that. Hugo:               Great. And listeners out there, this has been kind of a whirlwind introduction to a bunch of features in JupyterLab. I'd urge you to go and play with it yourself and check out some of the demos online as well if you haven't yet. Brian:               And I want to clarify, the version of JupyterLab that's out today, does not yet have the real time collaboration. Hugo:               Okay, that's right. Brian:               Still not quite released yet. Hugo:               So we've discussed IPython, we've discussed Jupyter Notebooks, we've JupyterLab. What else exists in the Jupyter ecosystem? Could you give us just a brief rundown of a couple of the other things? Brian:               Yeah, absolutely. Probably the biggest other constellation of projects is the JupyterHub project. JupyterHub is its own organization on GitHub and there's a number of different separate repos and projects there. And JupyterHub provides basically the ability for organizations to deploy Jupyter at scale to many users. With the patterns of adoption that we're seeing right now, that usage case is really, really important. As a result of that, JupyterHub has seen both a lot of interest from people contributing and also organizations using it. Hugo:               We discussed earlier that the talk you recently gave at JupyterCon in which you stated that, ""Project Jupyter is undergoing a phase transition from having individual users in organizations to having large scale institutional adoption."" I'm wondering what the unique challenges the project is now facing due to this transition? Brian:               Yeah. So there's both organizational and technical challenges we're facing. On the organizational side, I would say the big challenge is that we're seeing an increasing number of organizations coming to us and wanting to interact with us rather than just individuals in those organizations. And that really changes the type of people you're talking to in the organizations. So in many cases in the past, it may have been data scientists or machine learning researchers. And increasingly it's managers, project managers, and other decision makers who are thinking about the broader data strategy at the organizations. Brian:               From a technical perspective, it brings out a lot of new usage cases, in particular in JupyterHub, to address the needs of large organizations. Some examples of those are security, security is a really important thing for large organizations, particularly when there's sensitive data in the mix. Another aspect of that is that in these large organizations there are typically a wide range of different skill sets, responsibilities, roles, access permissions, and priorities of the people working with Jupyter. And so it's not necessarily just people who are living and breathing code all day long, but a lot of other people in the organization working with data that don't necessarily want to look at code all the time, or even most of the time. And so there's a lot of work we're doing thinking about, how would Jupyter need to evolve to address those usage cases? Hugo:               Absolutely. So Brian, as my last question, I'm wondering if you have a final call to action for all our listeners out there who may have used Jupyter Notebooks, may not have, but may be interested in doing so? Brian:               Yeah. So I think there's a couple different calls of action. One is for people to engage with open source, individuals. If you're a data scientist or someone doing machine learning at a company or a student learning about these tools and techniques, engage with the open-source projects. Find an open-source project you're interested in, understand more about the project, maybe help with documentation, and a lot of what we've found is that innovation happens when diverse groups of people come together and talk to each other and work towards common goals. And so the more people we have joining the projects and contributing and helping us think about these things, the better off and more healthy the open-source projects will be, but also the users of those projects will be better served. Hugo:               Agreed. Brian:               And a second call to action would be for people working in large organizations that are using open source tools in this space, I think it's important to note that many of the open-source projects, in particular those that are community driven, like Jupyter and many of the other NUMFOCUS focus projects, we continue to struggle with the long term sustainability. And there are many core contributors to these projects that continue to lack long term funding and the ability to focus on working on the projects. So if you're in an organization using these tools, I would really encourage you to talk to the people in the organization, to think about and understand how you can support the core contributors and the broader sustainability, both in the sense of community but also particular the financial sustainability of these efforts. That would be really, really helpful. Hugo:               And I'll add one other final call to action there, which is, I started using JupyterLab all the time instead of Notebooks mid last year I think. I would urge anyone out there who still uses the classic Notebook to jump into JupyterLab. I think you have no reason not to these days. It's such a wonderful place to use Notebooks among many other things. Brian:               Yes. That's a great point, Hugo. Even for the part of my job where I get to use the Jupyter Notebook, I transitioned, in particular in teaching and some research, to using JupyterLab back in January and it's worked really, really well in this context. At this point I'm using JupyterLab basically all the time. And so I echo what you said and I appreciate the kind word. Hugo:               Fantastic. I'm glad you agree. And, Brian, thank you so much for coming on the show. I always love our conversations and it's been an absolute pleasure formalizing this and putting it out there. Brian:               Yes. And thank you so much, Hugo, for working on this podcast. I know a lot of people really appreciate it and thanks for having me on.","Keyword(freq): notebook(32), project(27), organization(19), contributor(15), user(13), term(10), tool(10), feature(9), contribution(8), case(7)"
"4","mastery",2018-10-19,"How to Develop Machine Learning Models for Multivariate Multi-Step Air Pollution Time Series Forecasting","https://machinelearningmastery.com/how-to-develop-machine-learning-models-for-multivariate-multi-step-air-pollution-time-series-forecasting/","Real-world time series forecasting is challenging for a whole host of reasons not limited to problem features such as having multiple input variables, the requirement to predict multiple time steps, and the need to perform the same type of prediction for multiple physical sites. The EMC Data Science Global Hackathon dataset, or the ¡®Air Quality Prediction¡¯ dataset for short, describes weather conditions at multiple sites and requires a prediction of air quality measurements over the subsequent three days. Machine learning algorithms can be applied to time series forecasting problems and offer benefits such as the ability to handle multiple input variables with noisy complex dependencies. In this tutorial, you will discover how to develop machine learning models for multi-step time series forecasting of air pollution data. After completing this tutorial, you will know: Let¡¯s get started. How to Develop Machine Learning Models for Multivariate Multi-Step Air Pollution Time Series ForecastingPhoto by Eric Schmuttenmaer, some rights reserved. This tutorial is divided into nine parts; they are: The Air Quality Prediction dataset describes weather conditions at multiple sites and requires a prediction of air quality measurements over the subsequent three days. Specifically, weather observations such as temperature, pressure, wind speed, and wind direction are provided hourly for eight days for multiple sites. The objective is to predict air quality measurements for the next 3 days at multiple sites. The forecast lead times are not contiguous; instead, specific lead times must be forecast over the 72 hour forecast period. They are: Further, the dataset is divided into disjoint but contiguous chunks of data, with eight days of data followed by three days that require a forecast. Not all observations are available at all sites or chunks and not all output variables are available at all sites and chunks. There are large portions of missing data that must be addressed. The dataset was used as the basis for a short duration machine learning competition (or hackathon) on the Kaggle website in 2012. Submissions for the competition were evaluated against the true observations that were withheld from participants and scored using Mean Absolute Error (MAE). Submissions required the value of -1,000,000 to be specified in those cases where a forecast was not possible due to missing data. In fact, a template of where to insert missing values was provided and required to be adopted for all submissions (what a pain). A winning entrant achieved a MAE of 0.21058 on the withheld test set (private leaderboard) using random forest on lagged observations. A writeup of this solution is available in the post: In this tutorial, we will explore how to develop naive forecasts for the problem that can be used as a baseline to determine whether a model has skill on the problem or not. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course Before we can evaluate naive forecasting methods, we must develop a test harness. This includes at least how the data will be prepared and how forecasts will be evaluated. The first step is to download the dataset and load it into memory. The dataset can be downloaded for free from the Kaggle website. You may have to create an account and log in, in order to be able to download the dataset. Download the entire dataset, e.g. ¡°Download All¡± to your workstation and unzip the archive in your current working directory with the folder named ¡®AirQualityPrediction¡®. Our focus will be the ¡®TrainingData.csv¡® file that contains the training dataset, specifically data in chunks where each chunk is eight contiguous days of observations and target variables. We can load the data file into memory using the Pandas read_csv() function and specify the header row on line 0. We can group data by the ¡®chunkID¡¯ variable (column index 1). First, let¡¯s get a list of the unique chunk identifiers. We can then collect all rows for each chunk identifier and store them in a dictionary for easy access. Below defines a function named to_chunks() that takes a NumPy array of the loaded data and returns a dictionary of chunk_id to rows for the chunk. The complete example that loads the dataset and splits it into chunks is listed below. Running the example prints the number of chunks in the dataset. Now that we know how to load the data and split it into chunks, we can separate into train and test datasets. Each chunk covers an interval of eight days of hourly observations, although the number of actual observations within each chunk may vary widely. We can split each chunk into the first five days of observations for training and the last three for test. Each observation has a row called ¡®position_within_chunk¡® that varies from 1 to 192 (8 days * 24 hours). We can therefore take all rows with a value in this column that is less than or equal to 120 (5 * 24) as training data and any values more than 120 as test data. Further, any chunks that don¡¯t have any observations in the train or test split can be dropped as not viable. When working with the naive models, we are only interested in the target variables, and none of the input meteorological variables. Therefore, we can remove the input data and have the train and test data only comprised of the 39 target variables for each chunk, as well as the position within chunk and hour of observation. The split_train_test() function below implements this behavior; given a dictionary of chunks, it will split each into a list of train and test chunk data. We do not require the entire test dataset; instead, we only require the observations at specific lead times over the three day period, specifically the lead times: Where, each lead time is relative to the end of the training period. First, we can put these lead times into a function for easy reference: Next, we can reduce the test dataset down to just the data at the preferred lead times. We can do that by looking at the ¡®position_within_chunk¡® column and using the lead time as an offset from the end of the training dataset, e.g. 120 + 1, 120 +2, etc. If we find a matching row in the test set, it is saved, otherwise a row of NaN observations is generated. The function to_forecasts() below implements this and returns a NumPy array with one row for each forecast lead time for each chunk. We can tie all of this together and split the dataset into train and test sets and save the results to new files. The complete code example is listed below. Running the example first comments that chunk 69 is removed from the dataset for having insufficient data. We can then see that we have 42 columns in each of the train and test sets, one for the chunk id, position within chunk, hour of day, and the 39 training variables. We can also see the dramatically smaller version of the test dataset with rows only at the forecast lead times. The new train and test datasets are saved in the ¡®naive_train.csv¡® and ¡®naive_test.csv¡® files respectively. Once forecasts have been made, they need to be evaluated. It is helpful to have a simpler format when evaluating forecasts. For example, we will use the three-dimensional structure of [chunks][variables][time], where variable is the target variable number from 0 to 38 and time is the lead time index from 0 to 9. Models are expected to make predictions in this format. We can also restructure the test dataset to have this dataset for comparison. The prepare_test_forecasts() function below implements this. We will evaluate a model using the mean absolute error, or MAE. This is the metric that was used in the competition and is a sensible choice given the non-Gaussian distribution of the target variables. If a lead time contains no data in the test set (e.g. NaN), then no error will be calculated for that forecast. If the lead time does have data in the test set but no data in the forecast, then the full magnitude of the observation will be taken as error. Finally, if the test set has an observation and a forecast was made, then the absolute difference will be recorded as the error. The calculate_error() function implements these rules and returns the error for a given forecast. Errors are summed across all chunks and all lead times, then averaged. The overall MAE will be calculated, but we will also calculate a MAE for each forecast lead time. This can help with model selection generally as some models may perform differently at different lead times. The evaluate_forecasts() function below implements this, calculating the MAE and per-lead time MAE for the provided predictions and expected values in [chunk][variable][time] format. Once we have the evaluation of a model, we can present it. The summarize_error() function below first prints a one-line summary of a model¡¯s performance then creates a plot of MAE per forecast lead time. We are now ready to start exploring the performance of naive forecasting methods. Machine Learning Modeling The problem can be modeled with machine learning. Most machine learning models do not directly support the notion of observations over time. Instead, the lag observations must be treated as input features in order to make predictions. This is a benefit of machine learning algorithms for time series forecasting. Specifically, that they are able to support large numbers of input features. These could be lag observations for one or multiple input time series. Other general benefits of machine learning algorithms for time series forecasting over classical methods include: A challenge with this dataset is the need to make multi-step forecasts. There are two main approaches that machine learning methods can be used to make multi-step forecasts; they are: The recursive approach can make sense when forecasting a short contiguous block of lead times, whereas the direct approach may make more sense when forecasting discontiguous lead times. The direct approach may be more appropriate for the air pollution forecast problem given that we are interested in forecasting a mixture of 10 contiguous and discontiguous lead times over a three-day period. The dataset has 39 target variables, and we develop one model per target variable, per forecast lead time. That means that we require (39 * 10) 390 machine learning models. Key to the use of machine learning algorithms for time series forecasting is the choice of input data. We can think about three main sources of data that can be used as input and mapped to each forecast lead time for a target variable; they are: Data can be drawn from across all chunks, providing a rich dataset for learning a mapping from inputs to the target forecast lead time. The 39 target variables are actually comprised of 12 variables across 14 sites. Because of the way the data is provided, the default approach to modeling is to treat each variable-site as independent. It may be possible to collapse data by variable and use the same models for a variable across multiple sites. Some variables have been purposely mislabeled (e.g different data used variables with the same identifier). Nevertheless, perhaps these mislabeled variables can be identified and excluded from multi-site models. Before we can explore machine learning models of this dataset, we must prepare the data in such a way that we can fit models. This requires two data preparation steps: For now, we will focus on the 39 target variables and ignore the meteorological and metadata. Chunks are comprised of five days or less of hourly observations for 39 target variables. Many of the chunks do not have all five days of data, and none of the chunks have data for all 39 target variables. In those cases where a chunk has no data for a target variable, a forecast is not required. In those cases where a chunk does have some data for a target variable, but not all five days worth, there will be gaps in the series. These gaps may be a few hours to over a day of observations in length, sometimes even longer. Three candidate strategies for dealing with these gaps are as follows: We could ignore the gaps. A problem with this would be that that data would not be contiguous when splitting data into inputs and outputs. When training a model, the inputs will not be consistent, but could mean the last n hours of data, or data spread across the last n days. This inconsistency will make learning a mapping from inputs to outputs very noisy and perhaps more difficult for the model than it needs to be. We could use only the data without gaps. This is a good option. A risk is that we may not have much or enough data with which to fit a model. Finally, we could fill the gaps. This is called data imputation and there are many strategies that could be used to fill the gaps. Three methods that may perform well include: In this tutorial, we will use the latter approach and fill the gaps by using the median for the time of day across chunks. This method seems to result in more training samples and better model performance after a little testing. For a given variable, there may be missing observations defined by missing rows. Specifically, each observation has a ¡®position_within_chunk¡®. We expect each chunk in the training dataset to have 120 observations, with ¡®positions_within_chunk¡® from 1 to 120 inclusively. Therefore, we can create an array of 120 NaN values for each variable, mark all observations in the chunk using the ¡®positions_within_chunk¡® values, and anything left will be marked<U+00A0>NaN. We can then plot each variable and look for gaps. The variable_to_series() function below will take the rows for a chunk and a given column index for the target variable and will return a series of 120 time steps for the variable with all available data marked with the value from the chunk. We need to calculate a parallel series of the hour of day for each chunk that we can use for imputing hour specific data for each variable in the chunk. Given a series of partially filled hours of day, the interpolate_hours() function below will fill in the missing hours of day. It does this by finding the first marked hour, then counting forward, filling in the hour of day, then performing the same operation backwards. We can call the same variable_to_series() function (above) to create the series of hours with missing values (column index 2), then call interpolate_hours() to fill in the gaps. We can then pass the hours to any impute function that may make use of it. We can now try filling in missing values in a chunk with values within the same series with the same hour. Specifically, we will find all rows with the same hour on the series and calculate the median value. The impute_missing() below takes all of the rows in a chunk, the prepared sequence of hours of the day for the chunk, and the series with missing values for a variable and the column index for a variable. It first checks to see if the series is all missing data and returns immediately if this is the case as no impute can be performed. It then enumerates over the time steps of the series and when it detects a time step with no data, it collects all rows in the series with data for the same hour and calculates the median value. We need to transform the series for each target variable into rows with inputs and outputs so that we can fit supervised machine learning algorithms. Specifically, we have a series, like: When forecasting the lead time of +1 using 2 lag variables, we would split the series into input (X) and output (y) patterns as follows: This first requires that we choose a number of lag observations to use as input. There is no right answer; instead, it is a good idea to test different numbers and see what works. We then must perform the splitting of the series into the supervised learning format for each of the 10 forecast lead times. For example, forecasting +24 with 2 lag observations might look like: This process is then repeated for each of the 39 target variables. The patterns prepared for each lead time for each target variable can then be aggregated across chunks to provide a training dataset for a model. We must also prepare a test dataset. That is, input data (X) for each target variable for each chunk so that we can use it as input to forecast the lead times in the test dataset. If we chose a lag of 2, then the test dataset would be comprised of the last two observations for each target variable for each chunk. Pretty straightforward. We can start off by defining a function that will create input-output patterns for a given complete (imputed) series. The supervised_for_lead_time() function below will take a series, a number of lag observations to use as input, and a forecast lead time to predict, then will return a list of input/out rows drawn from the series. It is important to understand this piece. We can test this function and explore different numbers of lag variables and forecast lead times on a small test dataset. Below is a complete example that generates a series of 20 integers and creates a series with two input lags and forecasts the +6 lead time. Running the example prints the resulting patterns showing lag observations and their associated forecast lead time. Experiment with this example to get comfortable with this data transform as it is key to modeling time series using machine learning algorithms. We can now call supervised_for_lead_time() for each forecast lead time for a given target variable series. The target_to_supervised() function below implements this. First the target variable is converted into a series and imputed using the functions developed in the previous section. Then training samples are created for each target lead time. A test sample for the target variable is also created. Both the training data for each forecast lead time and the test input data are then returned for this target variable. We have the pieces; we now need to define the function to drive the data preparation process. This function builds up the train and test datasets. The approach is to enumerate each target variable and gather the training data for each lead time from across all of the chunks. At the same time, we collect the samples required as input when making a prediction for the test dataset. The result is a training dataset that has the dimensions [var][lead time][sample] where the final dimension are the rows of training samples for a forecast lead time for a target variable. The function also returns the test dataset with the dimensions [chunk][var][sample] where the final dimension is the input data for making a prediction for a target variable for a chunk. The data_prep() function below implements this behavior and takes the data in chunk format and a specified number of lag observations to use as input. We can tie everything together and prepare a train and test dataset with a supervised learning format for machine learning algorithms. We will use the prior 12 hours of lag observations as input when predicting each forecast lead time. The resulting train and test datasets are then saved as binary NumPy arrays. The complete example is listed below. Running the example may take a minute. The result are two binary files containing the train and test datasets that we can load in the following sections for training and evaluating machine learning algorithms on the problem. Before we can start evaluating algorithms, we need some more elements of the test harness. First, we need to be able to fit a scikit-learn model on training data. The fit_model() function below will make a clone of the model configuration and fit it on the provided training data. We will need to fit many (360) versions of each configured model, so this function will be called a lot. Next, we need to fit a model for each variable and forecast lead time combination. We can do this by enumerating the training dataset first by the variables and then by the lead times. We can then fit a model and store it in a list of lists with the same structure, specifically: [var][time][model]. The fit_models() function below implements this. Fitting models is the slow part and could benefit from being parallelized, such as with the Joblib library. This is left as an extension. Once the models are fit, they can be used to make predictions for the test dataset. The prepared test dataset is organized first by chunk, and then by target variable. Making predictions is fast and involves first checking that a prediction can be made (we have input data) and if so, using the appropriate models for the target variable. Each of the 10 forecast lead times for the variable will then be predicted with each of the direct models for those lead times. The make_predictions() function below implements this, taking the list of lists of models and the loaded test dataset as arguments and returning an array of forecasts with the structure [chunks][var][time]. We need a list of models to evaluate. We can define a generic get_models() function that is responsible for defining a dictionary of model-names mapped to configured scikit-learn model objects. Finally, we need a function to drive the model evaluation process. Given the dictionary of models, enumerate the models, first fitting the matrix of models on the training data, making predictions of the test dataset, evaluating the predictions, and summarizing the results. The evaluate_models() function below implements this. We now have everything we need to evaluate machine learning models. In this section, we will spot check a suite of linear machine learning algorithms. Linear algorithms are those that assume that the output is a linear function of the input variables. This is much like the assumptions of classical time series forecasting models like ARIMA. Spot checking means evaluating a suite of models in order to get a rough idea of what works. We are interested in any models that outperform a simple autoregression model AR(2) that achieves a MAE error of about 0.487. We will test eight linear machine learning algorithms with their default configuration; specifically: We can define these models in the get_models() function. The complete code example is listed below. Running the example prints the MAE for each of the evaluated algorithms. We can see that many of the algorithms show skill compared to a simple AR model, achieving a MAE below 0.487. Huber regression seems to perform the best (with default configuration), achieving a MAE of 0.434. This is interesting as Huber regression, or robust regression with Huber loss, is a method that is designed to be robust to outliers in the training dataset. It may suggest that the other methods may perform better with a little more data preparation, such as standardization and/or outlier removal. We can use the same framework to evaluate the performance of a suite of nonlinear and ensemble machine learning algorithms. Specifically: Nonlinear Algorithms Ensemble Algorithms The get_models() function below defines these nine models. The complete code listing is provided below. Running the example, we can see that many algorithms performed well compared to the baseline of an autoregression algorithm, although none performed as well as Huber regression in the previous section. Both support vector regression and perhaps gradient boosting machines may be worth further investigation of achieving MAEs of 0.437 and 0.450 respectively. In the previous spot check experiments, the number of lag observations was arbitrarily fixed at 12. We can vary the number of lag observations and evaluate the effect on MAE. Some algorithms may require more or fewer prior observations, but general trends may hold across algorithms. Prepare the supervised learning dataset with a range of different numbers of lag observations and fit and evaluate the HuberRegressor on each. I experimented with the following number of lag observations: The results were as follows: A plot of these results is provided below. Line Plot of Number of Lag Observations vs MAE for Huber Regression We can see a general trend of decreasing overall MAE with the increase in the number of lag observations, at least to a point after which error begins to rise again. The results suggest, at least for the HuberRegressor algorithm, that 36 lag observations may be a good configuration achieving a MAE of 0.422. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop machine learning models for multi-step time series forecasting of air pollution data. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Wow, that¡¯s a complete tutorial! Tnks Thanks! i d like to apply something similar to the lottery viewed as time series with multiple time steps You cannot predict the lottery:https://machinelearningmastery.com/faq/single-faq/can-i-use-machine-learning-to-predict-the-lottery Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): observation(34), model(28), variable(24), algorithm(20), chunk(18), time(18), row(12), gap(10), hour(9), implement(9)"
"5","mastery",2018-10-17,"How to Develop Autoregressive Forecasting Models for Multi-Step Air Pollution Time Series Forecasting","https://machinelearningmastery.com/how-to-develop-autoregressive-forecasting-models-for-multi-step-air-pollution-time-series-forecasting/","Real-world time series forecasting is challenging for a whole host of reasons not limited to problem features such as having multiple input variables, the requirement to predict multiple time steps, and the need to perform the same type of prediction for multiple physical sites. The EMC Data Science Global Hackathon dataset, or the ¡®Air Quality Prediction¡¯ dataset for short, describes weather conditions at multiple sites and requires a prediction of air quality measurements over the subsequent three days. Before diving into sophisticated machine learning and deep learning methods for time series forecasting, it is important to find the limits of classical methods, such as developing autoregressive models using the AR or ARIMA method. In this tutorial, you will discover how to develop autoregressive models for multi-step time series forecasting for a multivariate air pollution time series. After completing this tutorial, you will know: Let¡¯s get started. Impact of Dataset Size on Deep Learning Model Skill And Performance EstimatesPhoto by Eneas De Troya, some rights reserved. This tutorial is divided into six parts; they are: The Air Quality Prediction dataset describes weather conditions at multiple sites and requires a prediction of air quality measurements over the subsequent three days. Specifically, weather observations such as temperature, pressure, wind speed, and wind direction are provided hourly for eight days for multiple sites. The objective is to predict air quality measurements for the next 3 days at multiple sites. The forecast lead times are not contiguous; instead, specific lead times must be forecast over the 72 hour forecast period. They are: Further, the dataset is divided into disjoint but contiguous chunks of data, with eight days of data followed by three days that require a forecast. Not all observations are available at all sites or chunks and not all output variables are available at all sites and chunks. There are large portions of missing data that must be addressed. The dataset was used as the basis for a short duration machine learning competition (or hackathon) on the Kaggle website in 2012. Submissions for the competition were evaluated against the true observations that were withheld from participants and scored using Mean Absolute Error (MAE). Submissions required the value of -1,000,000 to be specified in those cases where a forecast was not possible due to missing data. In fact, a template of where to insert missing values was provided and required to be adopted for all submissions (what a pain). A winning entrant achieved a MAE of 0.21058 on the withheld test set (private leaderboard) using random forest on lagged observations. A writeup of this solution is available in the post: In this tutorial, we will explore how to develop naive forecasts for the problem that can be used as a baseline to determine whether a model has skill on the problem or not. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course Before we can evaluate naive forecasting methods, we must develop a test harness. This includes at least how the data will be prepared and how forecasts will be evaluated. The first step is to download the dataset and load it into memory. The dataset can be downloaded for free from the Kaggle website. You may have to create an account and log in, in order to be able to download the dataset. Download the entire dataset, e.g. ¡°Download All¡± to your workstation and unzip the archive in your current working directory with the folder named ¡®AirQualityPrediction¡®. Our focus will be the ¡®TrainingData.csv¡® file that contains the training dataset, specifically data in chunks where each chunk is eight contiguous days of observations and target variables. We can load the data file into memory using the Pandas read_csv() function and specify the header row on line 0. We can group data by the ¡®chunkID¡¯ variable (column index 1). First, let¡¯s get a list of the unique chunk identifiers. We can then collect all rows for each chunk identifier and store them in a dictionary for easy access. Below defines a function named to_chunks() that takes a NumPy array of the loaded data and returns a dictionary of chunk_id to rows for the chunk. The complete example that loads the dataset and splits it into chunks is listed below. Running the example prints the number of chunks in the dataset. Now that we know how to load the data and split it into chunks, we can separate into train and test datasets. Each chunk covers an interval of eight days of hourly observations, although the number of actual observations within each chunk may vary widely. We can split each chunk into the first five days of observations for training and the last three for test. Each observation has a row called ¡®position_within_chunk¡® that varies from 1 to 192 (8 days * 24 hours). We can therefore take all rows with a value in this column that is less than or equal to 120 (5 * 24) as training data and any values more than 120 as test data. Further, any chunks that don¡¯t have any observations in the train or test split can be dropped as not viable. When working with the naive models, we are only interested in the target variables, and none of the input meteorological variables. Therefore, we can remove the input data and have the train and test data only comprised of the 39 target variables for each chunk, as well as the position within chunk and hour of observation. The split_train_test() function below implements this behavior; given a dictionary of chunks, it will split each into a list of train and test chunk data. We do not require the entire test dataset; instead, we only require the observations at specific lead times over the three day period, specifically the lead times: Where, each lead time is relative to the end of the training period. First, we can put these lead times into a function for easy reference: Next, we can reduce the test dataset down to just the data at the preferred lead times. We can do that by looking at the ¡®position_within_chunk¡® column and using the lead time as an offset from the end of the training dataset, e.g. 120 + 1, 120 +2, etc. If we find a matching row in the test set, it is saved, otherwise a row of NaN observations is generated. The function to_forecasts() below implements this and returns a NumPy array with one row for each forecast lead time for each chunk. We can tie all of this together and split the dataset into train and test sets and save the results to new files. The complete code example is listed below. Running the example first comments that chunk 69 is removed from the dataset for having insufficient data. We can then see that we have 42 columns in each of the train and test sets, one for the chunk id, position within chunk, hour of day, and the 39 training variables. We can also see the dramatically smaller version of the test dataset with rows only at the forecast lead times. The new train and test datasets are saved in the ¡®naive_train.csv¡® and ¡®naive_test.csv¡® files respectively. Once forecasts have been made, they need to be evaluated. It is helpful to have a simpler format when evaluating forecasts. For example, we will use the three-dimensional structure of [chunks][variables][time], where variable is the target variable number from 0 to 38 and time is the lead time index from 0 to 9. Models are expected to make predictions in this format. We can also restructure the test dataset to have this dataset for comparison. The prepare_test_forecasts() function below implements this. We will evaluate a model using the mean absolute error, or MAE. This is the metric that was used in the competition and is a sensible choice given the non-Gaussian distribution of the target variables. If a lead time contains no data in the test set (e.g. NaN), then no error will be calculated for that forecast. If the lead time does have data in the test set but no data in the forecast, then the full magnitude of the observation will be taken as error. Finally, if the test set has an observation and a forecast was made, then the absolute difference will be recorded as the error. The calculate_error() function implements these rules and returns the error for a given forecast. Errors are summed across all chunks and all lead times, then averaged. The overall MAE will be calculated, but we will also calculate a MAE for each forecast lead time. This can help with model selection generally as some models may perform differently at different lead times. The evaluate_forecasts() function below implements this, calculating the MAE and per-lead time MAE for the provided predictions and expected values in [chunk][variable][time] format. Once we have the evaluation of a model, we can present it. The summarize_error() function below first prints a one-line summary of a model¡¯s performance then creates a plot of MAE per forecast lead time. We are now ready to start exploring the performance of naive forecasting methods. The first step in fitting classical time series models to this data is to take a closer look at the data. There are 208 (actually 207) usable chunks of data, and each chunk has 39 time series to fit; that is a total of 8,073 separate models that would need to be fit on the data. That is a lot of models, but the models are trained on a relatively small amount of data, at most (5 * 24) or 120 observations and the model is linear so it will find a fit quickly. We have choices about how to configure models to the data; for example: We will investigate the simplest approach of one model configuration for all series, but you may want to explore one or more of the other approaches. This section is divided into three parts; they are: Classical time series methods require that the time series to be complete, e.g. that there are no missing values. Therefore the first step is to investigate how complete or incomplete the target variables are. For a given variable, there may be missing observations defined by missing rows. Specifically, each observation has a ¡®position_within_chunk¡®. We expect each chunk in the training dataset to have 120 observations, with ¡®positions_within_chunk¡® from 1 to 120 inclusively. Therefore, we can create an array of 120 nan values for each variable, mark all observations in the chunk using the ¡®positions_within_chunk¡® values, and anything left will be marked NaN. We can then plot each variable and look for gaps. The variable_to_series() function below will take the rows for a chunk and a given column index for the target variable and will return a series of 120 time steps for the variable with all available data marked with the value from the chunk. We can then call this function for each target variable in one chunk and create a line plot. The function below named plot_variables() will implement this and create a figure with 39 line plots stacked horizontally. Tying this together, the complete example is listed below. A plot of all variables in the first chunk is created. Running the example creates a figure with 39 line plots, one for each target variable in the first chunk. We can see a seasonal structure in many of the variables. This suggests it may be beneficial to perform a 24-hour seasonal differencing of each series prior to modeling. The plots are small, and you may need to increase the size of the figure to clearly see the data. We can see that there are variables for which we have no data. These can be detected and ignored as we cannot model or forecast them. We can see gaps in many of the series, but the gaps are short, lasting for a few hours at most. These could be imputed either with persistence of previous values or values at the same hours within the same series. Line Plots for All Targets in Chunk 1 With Missing Values Marked Looking at a few other chunks randomly, many result in plots with much the same observations. This is not always the case though. Update the example to plot the 4th chunk in the dataset (index 3). The result is a figure that tells a very different story. We see gaps in the data that last for many hours, perhaps up to a day or more. These series will require dramatic repair before they can be used to fit a classical model. Imputing the missing data using persistence or observations within the series with the same hour will likely not be sufficient. They may have to be filled with average values taken across the entire training dataset. Line Plots for All Targets in Chunk 4 With Missing Values Marked There are many ways to impute the missing data, and we cannot know which is best a priori. One approach would be to prepare the data using multiple different imputation methods and use the skill of the models fit on the data to help guide the best approach. Some imputation approaches already suggested include: It may also be useful to use combinations, e.g. persist or fill from the series for small gaps and draw from the whole dataset for large gaps. We can also investigate the effect of imputing methods by filling in the missing data and looking at plots to see if the series looks reasonable. It¡¯s crude, effective, and fast. First, we need to calculate a parallel series of the hour of day for each chunk that we can use for imputing hour-specific data for each variable in the chunk. Given a series of partially filled hours of day, the interpolate_hours() function below will fill in the missing hours of day. It does this by finding the first marked hour, then counting forward, filling in the hour of day, then performing the same operation backwards. I¡¯m sure there is a more Pythonic way to write this function, but I wanted to lay it all out to make it obvious what was going on. We can test this out on a mock list of hours with missing data. The complete example is listed below. Running the example first prints the hour data with missing values, then the same sequence with all of the hours filled in correctly. We can use this function to prepare a series of hours for a chunk that can be used to fill in missing values for a chunk using hour-specific information. We can call the same variable_to_series() function from the previous section to create the series of hours with missing values (column index 2), then call interpolate_hours() to fill in the gaps. We can then pass the hours to any impute function that may make use of it. Let¡¯s try filling in missing values in a chunk with values within the same series with the same hour. Specifically, we will find all rows with the same hour on the series and calculate the median value. The impute_missing() below takes all of the rows in a chunk, the prepared sequence of hours of the day for the chunk, and the series with missing values for a variable and the column index for a variable. It first checks to see if the series is all missing data and returns immediately if this is the case as no impute can be performed. It then enumerates over the time steps of the series and when it detects a time step with no data, it collects all rows in the series with data for the same hour and calculates the median value. To see the impact of this impute strategy, we can update the plot_variables() function from the previous section to first plot the imputed series then plot the original series with missing values. This will allow the imputed values to shine through in the gaps of the original series and we can see if the results look reasonable. The updated version of the plot_variables() function is listed below with this change, calling the impute_missing() function to create the imputed version of the series and taking the hours series as an argument. Tying all of this together, the complete example is listed below. Running the example creates a single figure with 39 line plots: one for each target variable in the first chunk in the training dataset. We can see that the series is orange, showing the original data and the gaps have been imputed and are marked in blue. The blue segments seem reasonable. Line Plots for All Targets in Chunk 1 With Imputed Missing Values We can try the same approach on the 4th chunk in the dataset that has a lot more missing data. Running the example creates the same kind of figure, but here we can see the large missing segments filled in with imputed values. Again, the sequences seem reasonable, even showing daily seasonal cycle structure where appropriate. Line Plots for All Targets in Chunk 4 With Imputed Missing Values This looks like a good start; you can explore other imputation strategies and see how they compare either in terms of line plots or on the resulting model skill. Now that we know how to fill in the missing values, we can take a look at autocorrelation plots for the series data. Autocorrelation plots summarize the relationship of each observation with observations at prior time steps. Together with partial autocorrelation plots, they can be used to determine the configuration for an ARMA model. The statsmodels library provides the plot_acf() and plot_pacf() functions that can be used to plot ACF and PACF plots respectively. We can update the plot_variables() to create these plots, one of each type for each of the 39 series. That is a lot of plots. We will stack all ACF plots on the left vertically and all PACF plots on the right vertically. That is two columns of 39 plots. We will limit the lags considered by the plot to 24 time steps (hours) and ignore the correlation of each variable with itself as it is redundant. The updated plot_variables() function for plotting ACF and PACF plots is listed below. The complete example is listed below. Running the example creates a figure with a lot of plots for the target variables in the first chunk of the training dataset. You may need to increase the size of the plot window to better see the details of each plot. We can see on the left that most ACF plots show significant correlations (dots above the significance region) at lags 1-2 steps, maybe lags 1-3 steps in some cases, with a slow, steady decrease over the lags Similarly, on the right, we can see significant lags in the PACF plot at 1-2 time steps with steep fall-off. This strongly suggests an autocorrelation process with an order of perhaps 1, 2, or 3, e.g. AR(3). In the ACF plots on the left we can also see a daily cycle in the correlations. This may suggest some benefit in a seasonal differencing of the data prior to modeling or the use of an AR model capable of seasonal differencing. ACF and PACF Plots for Target Variables in Chunk 1 We can repeat this analysis of the target variables for other chunks and we see much the same picture. It suggests we may be able to get away with a general AR model configuration for all series across all chunks. In this section, we will develop an autoregressive model for the imputed target series data. The first step is to implement a general function for making a forecast for each chunk. The function tasks the training dataset and the input columns (chunk id, position in chunk, and hour) for the test set and returns forecasts for all chunks with the expected 3D format of [chunk][variable][time]. The function enumerates the chunks in the forecast, then enumerates the 39 target columns, calling another new function named forecast_variable() in order to make a prediction for each lead time for a given target variable. The complete function is listed below. We can now implement a version of the forecast_variable(). For each variable, we first check if there is no data (e.g. all NaNs) and if so, we return a forecast that is a NaN for each forecast lead time. We then create a series from the variable using the variable_to_series() and then impute the missing values using the median within the series by calling impute_missing(), both of which were developed in the previous section. Finally, we call a new function named fit_and_forecast() that fits a model and predicts the 10 forecast lead times. We will fit an AR model to a given imputed series. To do this, we will use the statsmodels ARIMA class. We will use ARIMA instead of AR to offer some flexibility if you would like to explore any of the family of ARIMA models. First, we must define the model, including the order of the autoregressive process, such as AR(1). Next, the model is fit on the imputed series. We turn off the verbose information during the fit by setting disp to False. The fit model is then used to forecast the next 72 hours beyond the end of the series. We are only interested in specific lead times, so we prepare an array of those lead times, subtract 1 to turn them into array indices, then use them to select the values at the 10 forecast lead times in which we are interested. The statsmodels ARIMA models use linear algebra libraries to fit the model under the covers, and sometimes the fit process can be unstable on some data. As such, it can throw an exception or report a lot of warnings. We will trap exceptions and return a NaN forecast, and ignore all warnings during the fit and evaluation. The fit_and_forecast() function below ties all of this together. We are now ready to evaluate an autoregressive process for each of the 39 series in each of the 207 training chunks. We will start off by testing an AR(1) process. The complete code example is listed below. Running the example first reports the overall MAE for the test set, followed by the MAE for each forecast lead time. We can see that the model achieves a MAE of about 0.492, which is less than a MAE 0.520 achieved by a naive persistence model. This shows that indeed the approach has some skill. A line plot of MAE per forecast lead time is created, showing the linear increase in forecast error with the increase in forecast lead time. MAE vs Forecast Lead Time for AR(1) We can change the code to test other AR models. Specifically the order of the ARIMA model in the fit_and_forecast() function. An AR(2) model can be defined as: Running the code with this update shows a further drop in error to an overall MAE of about 0.490. We can also try an AR(3): Re-running the example with the update shows an increase in the overall MAE compared to an AR(2). An AR(2) might be a good global level configuration to use, although it is expected that models tailored to each variable or each series may perform better overall. We can evaluate the AR(2) model with an alternate imputation strategy. Instead of calculating the median value for the same hour across the series in the chunk, we can calculate the same value across the variable in all chunks. We can update the impute_missing() to take all training chunks as an argument, then collect rows from all chunks for a given hour in order to calculate the median value used to impute. The updated version of the function is listed below. In order to pass the train_chunks to the impute_missing() function, we must update the forecast_variable() function to also take train_chunks as an argument and pass it along, and in turn update the forecast_chunks() function to pass train_chunks. The complete example using a global imputation strategy is listed below. Running the example shows a further drop in the overall MAE to about 0.487. It may be interesting to explore imputation strategies that alternate the method used to fill in missing values based on how much missing data a series has or the gap being filled. A line plot of MAE vs. forecast lead time is also created. MAE vs Forecast Lead Time for AR(2) Impute With Global Strategy This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop autoregressive models for multi-step time series forecasting for a multivariate air pollution time series. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hi Dr. Brownlee! Awesome job done as always! Thank you so much for your contribution.
I wonder is it possible to implement some kind of Machine Learning methods to fill missing data by neighboring stations. I mean if there are several stations with correlated observed timeseries and I want to use them to fill missing timeseries data in the station that I use for modeling. Usually pairwise correlation method used for such tasks and it is statistically based approach, maybe it would be better to use Neural Networks. What do you think? Thanks. Sure, try it and let me know how you go. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): value(26), plot(25), chunk(21), observation(18), model(17), variable(16), hour(15), time(13), row(10), gap(9)"
"6","mastery",2018-10-15,"How to Develop Baseline Forecasts for Multi-Site Multivariate Air Pollution Time Series Forecasting","https://machinelearningmastery.com/how-to-develop-baseline-forecasts-for-multi-site-multivariate-air-pollution-time-series-forecasting/","Real-world time series forecasting is challenging for a whole host of reasons not limited to problem features such as having multiple input variables, the requirement to predict multiple time steps, and the need to perform the same type of prediction for multiple physical sites. The EMC Data Science Global Hackathon dataset, or the ¡®Air Quality Prediction¡® dataset for short, describes weather conditions at multiple sites and requires a prediction of air quality measurements over the subsequent three days. An important first step when working with a new time series forecasting dataset is to develop a baseline in model performance by which the skill of all other more sophisticated strategies can be compared. Baseline forecasting strategies are simple and fast. They are referred to as ¡®naive¡¯ strategies because they assume very little or nothing about the specific forecasting problem. In this tutorial, you will discover how to develop naive forecasting methods for the multistep multivariate air pollution time series forecasting problem. After completing this tutorial, you will know: Let¡¯s get started. How to Develop Baseline Forecasts for Multi-Site Multivariate Air Pollution Time Series ForecastingPhoto by DAVID HOLT, some rights reserved. This tutorial is divided into six parts; they are: The Air Quality Prediction dataset describes weather conditions at multiple sites and requires a prediction of air quality measurements over the subsequent three days. Specifically, weather observations such as temperature, pressure, wind speed, and wind direction are provided hourly for eight days for multiple sites. The objective is to predict air quality measurements for the next 3 days at multiple sites. The forecast lead times are not contiguous; instead, specific lead times must be forecast over the 72 hour forecast period. They are: Further, the dataset is divided into disjoint but contiguous chunks of data, with eight days of data followed by three days that require a forecast. Not all observations are available at all sites or chunks and not all output variables are available at all sites and chunks. There are large portions of missing data that must be addressed. The dataset was used as the basis for a short duration machine learning competition (or hackathon) on the Kaggle website in 2012. Submissions for the competition were evaluated against the true observations that were withheld from participants and scored using Mean Absolute Error (MAE). Submissions required the value of -1,000,000 to be specified in those cases where a forecast was not possible due to missing data. In fact, a template of where to insert missing values was provided and required to be adopted for all submissions (what a pain). A winning entrant achieved a MAE of 0.21058 on the withheld test set (private leaderboard) using random forest on lagged observations. A writeup of this solution is available in the post: In this tutorial, we will explore how to develop naive forecasts for the problem that can be used as a baseline to determine whether a model has skill on the problem or not. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course A baseline in forecast performance provides a point of comparison. It is a point of reference for all other modeling techniques on your problem. If a model achieves performance at or below the baseline, the technique should be fixed or abandoned. The technique used to generate a forecast to calculate the baseline performance must be easy to implement and naive of problem-specific details. The principle is that if a sophisticated forecast method cannot outperform a model that uses little or no problem-specific information, then it does not have skill. There are problem-agnostic forecast methods that can and should be used first, followed by naive methods that use a modicum of problem-specific information. Two examples of problem agnostic naive forecast methods that could be used include: The data is divided into chunks, or intervals, of time. Each chunk of time has multiple variables at multiple sites to forecast. The persistence forecast method makes sense at this chunk-level of organization of the data. Other persistence methods could be explored; for example: These are desirable baseline methods to explore, but the large amount of missing data and discontiguous structure of most of the data chunks make them challenging to implement without non-trivial data preparation. Forecasting the average observations for each series can be elaborated further; for example: A three-day forecast is required for each series with different start-times, e.g. times of day. As such, the forecast lead times for each chunk will fall on different hours of the day. A further elaboration of forecasting the average value is to incorporate the hour of day that is being forecasted; for example: Many variables are measured at multiple sites; as such, it may be possible to use information across series, such as in the calculation of averages or averages per hour of day for forecast lead times. These are interesting, but may exceed the mandate of naive. This is a good starting point, although there may be further elaborations of the naive methods that you may want to consider and explore as an exercise. Remember, the goal is to use very little problem specific information in order to develop a forecast baseline. In summary, we will investigate five different naive forecasting methods for this problem, the best of which will provide a lower-bound on performance by which other models can be compared. They are: Before we can evaluate naive forecasting methods, we must develop a test harness. This includes at least how the data will be prepared and how forecasts will be evaluated. The first step is to download the dataset and load it into memory. The dataset can be downloaded for free from the Kaggle website. You may have to create an account and log in, in order to be able to download the dataset. Download the entire dataset, e.g. ¡°Download All¡± to your workstation and unzip the archive in your current working directory with the folder named ¡®AirQualityPrediction¡®. Our focus will be the ¡®TrainingData.csv¡® file that contains the training dataset, specifically data in chunks where each chunk is eight contiguous days of observations and target variables. We can load the data file into memory using the Pandas read_csv() function and specify the header row on line 0. We can group data by the ¡®chunkID¡¯ variable (column index 1). First, let¡¯s get a list of the unique chunk identifiers. We can then collect all rows for each chunk identifier and store them in a dictionary for easy access. Below defines a function named to_chunks() that takes a NumPy array of the loaded data and returns a dictionary of chunk_id to rows for the chunk. The complete example that loads the dataset and splits it into chunks is listed below. Running the example prints the number of chunks in the dataset. Now that we know how to load the data and split it into chunks, we can separate into train and test datasets. Each chunk covers an interval of eight days of hourly observations, although the number of actual observations within each chunk may vary widely. We can split each chunk into the first five days of observations for training and the last three for test. Each observation has a row called ¡®position_within_chunk¡® that varies from 1 to 192 (8 days * 24 hours). We can therefore take all rows with a value in this column that is less than or equal to 120 (5 * 24) as training data and any values more than 120 as test data. Further, any chunks that don¡¯t have any observations in the train or test split can be dropped as not viable. When working with the naive models, we are only interested in the target variables, and none of the input meteorological variables. Therefore, we can remove the input data and have the train and test data only comprised of the 39 target variables for each chunk, as well as the position within chunk and hour of observation. The split_train_test() function below implements this behavior; given a dictionary of chunks, it will split each into a list of train and test chunk data. We do not require the entire test dataset; instead, we only require the observations at specific lead times over the three day period, specifically the lead times: Where, each lead time is relative to the end of the training period. First, we can put these lead times into a function for easy reference: Next, we can reduce the test dataset down to just the data at the preferred lead times. We can do that by looking at the ¡®position_within_chunk¡® column and using the lead time as an offset from the end of the training dataset, e.g. 120 + 1, 120 +2, etc. If we find a matching row in the test set, it is saved, otherwise a row of NaN observations is generated. The function to_forecasts() below implements this and returns a NumPy array with one row for each forecast lead time for each chunk. We can tie all of this together and split the dataset into train and test sets and save the results to new files. The complete code example is listed below. Running the example first comments that chunk 69 is removed from the dataset for having insufficient data. We can then see that we have 42 columns in each of the train and test sets, one for the chunk id, position within chunk, hour of day, and the 39 training variables. We can also see the dramatically smaller version of the test dataset with rows only at the forecast lead times. The new train and test datasets are saved in the ¡®naive_train.csv¡® and ¡®naive_test.csv¡® files respectively. Once forecasts have been made, they need to be evaluated. It is helpful to have a simpler format when evaluating forecasts. For example, we will use the three-dimensional structure of [chunks][variables][time], where variable is the target variable number from 0 to 38 and time is the lead time index from 0 to 9. Models are expected to make predictions in this format. We can also restructure the test dataset to have this dataset for comparison. The prepare_test_forecasts() function below implements this. We will evaluate a model using the mean absolute error, or MAE. This is the metric that was used in the competition and is a sensible choice given the non-Gaussian distribution of the target variables. If a lead time contains no data in the test set (e.g. NaN), then no error will be calculated for that forecast. If the lead time does have data in the test set but no data in the forecast, then the full magnitude of the observation will be taken as error. Finally, if the test set has an observation and a forecast was made, then the absolute difference will be recorded as the error. The calculate_error() function implements these rules and returns the error for a given forecast. Errors are summed across all chunks and all lead times, then averaged. The overall MAE will be calculated, but we will also calculate a MAE for each forecast lead time. This can help with model selection generally as some models may perform differently at different lead times. The evaluate_forecasts() function below implements this, calculating the MAE and per-lead time MAE for the provided predictions and expected values in [chunk][variable][time] format. Once we have the evaluation of a model, we can present it. The summarize_error() function below first prints a one-line summary of a model¡¯s performance then creates a plot of MAE per forecast lead time. We are now ready to start exploring the performance of naive forecasting methods. In this section, we will explore naive forecast methods that use all data in the training dataset, not constrained to the chunk for which we are making a prediction. We will look at two approaches: The first step is to implement a general function for making a forecast for each chunk. The function takes the training dataset and the input columns (chunk id, position in chunk, and hour) for the test set and returns forecasts for all chunks with the expected 3D format of [chunk][variable][time]. The function enumerates the chunks in the forecast, then enumerates the 39 target columns, calling another new function named forecast_variable() in order to make a prediction for each lead time for a given target variable. The complete function is listed below. We can now implement a version of the forecast_variable() that calculates the mean for a given series and forecasts that mean for each lead time. First, we must collect all observations in the target column across all chunks, then calculate the average of the observations while also ignoring the NaN values. The nanmean() NumPy function will calculate the mean of an array and ignore NaN values. The forecast_variable() function below implements this behavior. We now have everything we need. The complete example of forecasting the global mean for each series across all forecast lead times is listed below. Running the example first prints the overall MAE of 0.634, followed by the MAE scores for each forecast lead time. A line plot is created showing the MAE scores for each forecast lead time from +1 hour to +72 hours. We cannot see any obvious relationship in forecast lead time to forecast error as we might expect with a more skillful model. MAE by Forecast Lead Time With Global Mean We can update the example to forecast the global median instead of the mean. The median may make more sense to use as a central tendency than the mean for this data given the non-Gaussian like distribution the data seems to show. NumPy provides the nanmedian() function that we can use in place of nanmean() in the forecast_variable() function. The complete updated example is listed below. Running the example shows a drop in MAE to about 0.59, suggesting that indeed using the median as the central tendency may be a better baseline strategy. A line plot of MAE per lead time is also created. MAE by Forecast Lead Time With Global Median We can update the naive model for calculating a central tendency by series to only include rows that have the same hour of day as the forecast lead time. For example, if the +1 lead time has the hour 6 (e.g. 0600 or 6AM), then we can find all other rows in the training dataset across all chunks for that hour and calculate the median value for a given target variable from those rows. We record the hour of day on the test dataset and make it available to the model when making forecasts. One wrinkle is that in some cases the test dataset did not have a record for a given lead time and one had to be invented with NaN values, including a NaN value for the hour. In these cases, no forecast is required so we will skip them and forecast a NaN value. The forecast_variable() function below implements this behavior, returning forecasts for each lead time for a given variable. It is not very efficient, and it might be a lot more efficient to pre-calculate the median values for each hour for each variable first and then forecast using a lookup table. Efficiency is not a concern at this point as we are looking for a baseline of model performance. The complete example of forecasting the global median value by hour of the day across is listed below. Running the example summarizes the performance of the model with a MAE of 0.567, which is an improvement over the global median for each series. A line plot of the MAE by forecast lead time is also created showing that +72 had the lowest overall forecast error. This is interesting, and may suggest that hour-based information may be useful in more sophisticated models. MAE by Forecast Lead Time With Global Median By Hour of Day It is possible that using information specific to the chunk may have more predictive power than using global information from the entire training dataset. We can explore this with three local or chunk-specific naive forecasting methods; they are: The last two of which are the chunk-specific version of the global strategies that were evaluated in the previous section. Forecasting the last non-NaN observation for a chunk is perhaps the simplest model, classically called the persistence model or the naive model. The forecast_variable() function below implements this forecast strategy. The complete example for evaluating the persistence forecast strategy on the test set is listed below. Running the example prints the overall MAE and the MAE per forecast lead time. We can see that the persistence forecast appears to out-perform all of the global strategies evaluated in the previous section. This adds some support that the reasonable assumption that chunk-specific information is important in modeling this problem. A line plot of MAE per forecast lead time is created. Importantly, this plot shows the expected behavior of increasing error with the increase in forecast lead time. Namely, the further one predicts into the future, the more challenging it is, and in turn, the more error one would be expected to make. MAE by Forecast Lead Time via Persistence Instead of persisting the last observation for the series, we can persist the average value for the series using only the data in the chunk. Specifically, we can calculate the median of the series, which as we found in the previous section seems to lead to better performance. The forecast_variable() implements this local strategy. The complete example is listed below. Running the example summarizes the performance of this naive strategy, showing a MAE of about 0.568, which is worse than the above persistence strategy. A line plot of MAE per forecast lead time is also created showing the familiar increasing curve of error per lead time. MAE by Forecast Lead Time via Local Median Finally, we can dial in the persistence strategy by using the average value per series for the specific hour of day at each forecast lead time. This approach was found to be effective at the global strategy. It may be effective using only the data from the chunk, although at the risk of using a much smaller data sample. The forecast_variable() function below implements this strategy, first finding all rows with the hour of the forecast lead time, then calculating the median of those rows for the given target variable. The complete example is listed below. Running the example prints the overall MAE of about 0.574, which is worse than the global variation of the same strategy. As suspected, this is likely due to the small sample size, that is at most five rows of training data contributing to each forecast. A line plot of MAE per forecast lead time is also created showing the familiar increasing curve of error per lead time. MAE by Forecast Lead Time via Local Median By Hour of Day We can summarize the performance of all of the naive forecast methods reviewed in this tutorial. The example below lists each method using a shorthand of ¡®g¡® and ¡®l¡® for global and local and ¡®h¡® for the hour-of-day variations. The example creates a bar chart so that we can compare the naive strategies based on their relative performance. Running the example creates a bar chart comparing the MAE for each of the six strategies. We can see that the persistence strategy was better than all of the other methods and that the second best strategy was the global median for each series that used the hour of day. Models evaluated on this train/test separation of the dataset must achieve an overall MAE lower than 0.520 in order to be considered skillful. Bar Chart with Summary of Naive Forecast Methods This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop naive forecasting methods for the multistep multivariate air pollution time series forecasting problem. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Exactly how does this:
     chunk_ids = unique(values[:, 1]) develop a list of unique identifiers?   When I see ¡®unique¡¯ I think set().  I am not following the above line of code. Yes, a set, in this case a list of unique values in column 1. Hi,
Thanks for a great tutorial. Just a small remark <U+2013> to me, it would be much easier/quicker to understand steps if use pandas dataframes.
All the best! Thanks. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): chunk(17), method(16), mae(15), observation(14), time(14), variable(11), row(10), forecast(9), site(9), implement(8)"
"7","vidhya",2018-10-18,"Deep Learning in the Trenches: Understanding Inception Network from Scratch","https://www.analyticsvidhya.com/blog/2018/10/understanding-inception-network-from-scratch/","

FLASH SALE: Course On Introduction To Data Science | Use COUPON CODE: FLASH25 for 25% Discount| Work on 4 real-life projects |
Buy Now 

 Deep learning is rapidly gaining steam as more and more research papers emerge from around the world. These papers undoubtedly contain a ton of information, but they can often be difficult to parse through. And to understand them, you might have to go through that paper multiple number of times (and perhaps even other dependent papers!). This is truly a daunting task for non-academicians like us. Personally, I find the task of going through a research paper, interpreting the crux behind it, and implementing the code as an important skill every deep learning enthusiast and practitioner should possess. Practically implementing research ideas brings out the thought process of the author and also helps transform those ideas into real-world industry applications. So in this article (and the subsequent series of articles) my motive of writing is two-fold: This article assumes that you have a good grasp on the basics of deep learning. In case you don¡¯t, or simply need a refresher, check out the below articles first and then head back here pronto: This article focuses on the paper ¡°Going deeper with convolutions¡± from which the hallmark idea of inception network came out. Inception network was once considered a state-of-the-art deep learning architecture (or model) for solving image recognition and detection problems. It put forward a breakthrough performance on the ImageNet Visual Recognition Challenge (in 2014), which is a reputed platform for benchmarking image recognition and detection algorithms. Along with this, it set off a ton of research in the creation of new deep learning architectures with innovative and impactful ideas. We will go through the main ideas and suggestions propounded in the aforementioned paper and try to grasp the techniques within. In the words of the author: ¡°In this paper, we will focus on an efficient deep neural network architecture for computer vision, code named Inception, which derives its name from (¡¦) the famous ¡°we need to go deeper¡± internet meme.¡± That does sound intriguing, doesn¡¯t it? Well, read on then! There¡¯s a simple but powerful way of creating better deep learning models. You can just make a bigger model, either in terms of deepness, i.e., number of layers, or the number of neurons in each layer. But as you can imagine, this can often create complications: A solution for this, as the paper suggests, is to move on to sparsely connected network architectures which will replace fully connected network architectures, especially inside convolutional layers. This idea can be conceptualized in the below images: Densely connected architecture Sparsely connected architecture This paper proposes a new idea of creating deep architectures. This approach lets you maintain the ¡°computational budget¡±, while increasing the depth and width of the network. Sounds too good to be true! This is how the conceptualized idea looks: Let us look at the proposed architecture in a bit more detail. The paper proposes a new type of architecture <U+2013> GoogLeNet or Inception v1. It is basically a convolutional neural network (CNN) which is 27 layers deep. Below is the model summary: Notice in the above image that there is a layer called inception layer. This is actually the main idea behind the paper¡¯s approach. The inception layer is the core concept of a sparsely connected architecture. Idea of an Inception module Let me explain in a bit more detail what an inception layer is all about. Taking an excerpt from the paper: ¡°(Inception Layer) is a combination of all those layers (namely, 1¡¿1 Convolutional layer, 3¡¿3 Convolutional layer, 5¡¿5 Convolutional layer) with their output filter banks concatenated into a single output vector forming the input of the next stage.¡± Along with the above-mentioned layers, there are two major add-ons in the original inception layer: Inception Layer To understand the importance of the inception layer¡¯s structure, the author calls on the Hebbian principle from human learning. This says that ¡°neurons that fire together, wire together¡±. The author suggests that when creating a subsequent layer in a deep learning model, one should pay attention to the learnings of the previous layer. Suppose, for example, a layer in our deep learning model has learned to focus on individual parts of a face. The next layer of the network would probably focus on the overall face in the image to identify the different objects present there. Now to actually do this, the layer should have the appropriate filter sizes to detect different objects. This is where the inception layer comes to the fore. It allows the internal layers to pick and choose which filter size will be relevant to learn the required information. So even if the size of the face in the image is different (as seen in the images below), the layer works accordingly to recognize the face. For the first image, it would probably take a higher filter size, while it¡¯ll take a lower one for the second image. The overall architecture, with all the specifications, looks like this: Note that this architecture came about largely due to the authors participating in an image recognition and detection challenge. Hence there are quite a few ¡°bells and whistles¡± which they have explained in the paper. These include: Among this, the auxiliary training done by the authors is quite interesting and novel in nature. So we will focus on that for now. The details for the rest of the techniques can be taken from the paper itself, or in the implementation which we will see below. To prevent the middle part of the network from ¡°dying out¡±, the authors introduced two auxiliary classifiers (the purple boxes in the image). They essentially applied softmax to the outputs of two of the inception modules, and computed an auxiliary loss over the same labels. The total loss function is a weighted sum of the auxiliary loss and the real loss. The weight value used in the paper was 0.3 for each auxiliary loss. Now that you have understood the architecture of GoogLeNet and the intuition behind it, it¡¯s time to power up Python and implement our learnings using Keras! We will use the CIFAR-10 dataset for this purpose. CIFAR-10 is a popular image classification dataset. It consists of 60,000 images of 10 classes (each class is represented as a row in the above image). The dataset is divided into 50,000 training images and 10,000 test images. Note that you must have the required libraries installed to implement the code we will see in this section. This includes Keras and TensorFlow (as a back-end for Keras). You can refer to the official installation guide<U+00A0>in case you don¡¯t have Keras already installed on your machine. Now that we have dealt with the prerequisites, we can finally start coding the theory we covered in the earlier sections. The first thing we need to do is import all the necessary libraries and modules which we will use throughout the code. Our model gave an impressive 80%+ accuracy on the validation set, which proves that this model architecture is truly worth checking out! This was a really enjoyable article to write and I hope you found it equally useful. Inception v1 was the focal point on this article, wherein I explained the nitty gritty of what this framework is about and demonstrated how to implement it from scratch in Keras.","Keyword(freq): layer(7), image(5), architecture(4), idea(4), kera(4), author(3), article(2), learning(2), library(2), module(2)"
"8","vidhya",2018-10-17,"Must Read Books<U+00A0>for Beginners on Machine Learning and Artificial Intelligence","https://www.analyticsvidhya.com/blog/2018/10/read-books-for-beginners-machine-learning-artificial-intelligence/","

FLASH SALE: Course On Introduction To Data Science | Use COUPON CODE: FLASH25 for 25% Discount| Work on 4 real-life projects |
Buy Now 

 This article was originally published on October 25, 2015, and updated on October 17, 2018. Machine Learning has granted incredible power to humans. The power to run tasks in an automated manner, the power to make our lives comfortable,<U+00A0>the power to improve things continuously by studying decisions at a large scale, and the power to create species who think better than humans. This list can go on and on. Still sceptical about AI and ML? Read what Google¡¯s CEO Mr. Sundar Pichai had to say all the way back in 2015: ¡®Machine learning is a core, transformative way by which we¡¯re rethinking everything we¡¯re doing. We¡¯re thoughtfully applying it across all our products, be it search, ads, YouTube, or Play. We¡¯re in the early days, but you¡¯ll see us in a systematic way think about how we can apply machine learning to all these areas.¡¯ <U+2013> Sundar Pichai, CEO, Google Those who know of these advancements are keen to master this<U+00A0>concept, including us at Analytics Vidhya. When we started with this mission, we found various forms of digitized study material. They seemed promising and comprehensive, yet lacked a perspective. Our curiosity didn¡¯t let us rest for long and we resorted to books. When Elon Musk, one of the busiest men on the planet, was asked about his secret of success, he replied, ¡®I used to read books. A LOT¡¯. Later, Kimbal Musk, Elon¡¯s brother, said, ¡®He would even complete two books in a day¡¯. In this article, we¡¯ve listed some of the must-read books on Machine Learning and Artificial Intelligence. These books are in no particular rank or order. The motive of this article is not to promote any particular book, but to make you aware of a world which<U+00A0>exists beyond video tutorials, blogs and podcasts. I encourage you to check out these 10 Free E-books on Machine Learning as well which are a great starting point (or a refresher) for anyone in this field. We had to start with a book by the great Andrew Ng. It is still a work in progress, but several chapters have been released and can be downloaded FOR FREE today. This book will help the reader get up to speed with building AI systems. It will effectively<U+00A0>teach you how to make the various decisions required with organizing a machine learning project. There¡¯s no better person to start off this list, in our opinion. You can sign up on the site to receive updates as soon as a new chapter is released. Programming Collective Intelligence, PCI as it is popularly known, is one of the best books to start learning machine learning.<U+00A0>If there is one book to choose on machine learning <U+2013> it is this one. I haven¡¯t met a data scientist yet who has read this book and does not recommend to keep it on your bookshelf. A lot of them have re-read this book multiple times. The book was written long before data science and machine learning acquired the cult status they have today <U+2013> but the topics and chapters are entirely relevant even today! Some of the topics covered in the book are collaborative filtering techniques, search engine features, Bayesian filtering and Support vector machines.<U+00A0>If you don¡¯t have a copy of this book <U+2013> order it as soon as you finish reading this article!<U+00A0>The book uses Python to deliver machine learning in a fascinating manner. This book is written by Drew Conway and John Myles White. It is majorly based on data analysis in R. This book is best suited for beginners having a basic knowledge and grasp of R. It covers the use of advanced R in data wrangling. It has interesting case studies which will help you to understand the importance of using machine learning algorithms. After you¡¯ve read the above books, you are good to dive into the world of machine learning. And this is a great introductory book to start your journey. It<U+00A0>provides a nice overview of ML theorems<U+00A0>with pseudocode summaries of their algorithms. Apart from case studies,<U+00A0>Tom has used basic examples to help you understand these algorithms easily. Most of the experts you ask in this field never fail to mention this book which helped them at the start of their careers. It¡¯s such a well-written and explained book that we feel it should be made mandatory in every machine learning course! This is quite a popular book. It was written by Trevor Hastie, Robert Tibshirani and Jerome Friedman. This book aptly explains various machine learning algorithms mathematically from a statistical perspective. It provides a powerful world created by statistics and machine learning. This books lays emphasis on mathematical derivations to define the underlying logic behind an algorithm. Keep in mind that you need to have a rudimentary understanding of linear algebra before picking this up. There¡¯s a beginner friendly version of these concepts in a book by some of the same authors, called ¡®Introduction to Statistical Learning¡¯. Make sure you check that out if this one is too complex for you right now. Free PDF Link: Download This book is written by<U+00A0>Yaser Abu Mostafa, Malik Magdon-Ismail and Hsuan-Tien Lin. It provides a perfect introduction to machine learning.<U+00A0>This book prepares you to understand complex areas of machine learning. Yaser, a very popular and brilliant professor, has provided ¡®to the point¡¯ explanations instead of lengthy and go-around explanations. If you choose this book, I¡¯d suggest you to refer to online tutorials of Yaser Abu Mostafa as well. They¡¯re awesome. Free PDF Link: Download This book is written by Christopher M Bishop. This book serves as a excellent reference<U+00A0>for students keen to understand the use of statistical techniques in machine learning and pattern recognition. This books assumes the knowledge of linear algebra and multivariate calculus. It provides a comprehensive<U+00A0>introduction to statistical pattern recognition techniques using practice exercises. Free PDF Link: Download Folks interested in getting into Natural Language Processing (NLP) should read this book. It¡¯s written in a lucid and clear manner with extremely well-presented codes in Python. Readers are given access to well-annotated datasets to analyse and deal with unstructured data, linguistic structure in text, among other NLP things. The authors of this book are Steven Bird, Ewan Klein, and Edward Loper. A ML superteam! Who better to learn AI from than the great Peter Norvig? You have to take a course from Norvig to understand his style of teaching. But once you do, you will remember it for a long, long time. This book is written by Stuart Russell and Peter Norvig. It is best suited<U+00A0>for people new to A.I. More than just providing an overview of artificial intelligence,<U+00A0>this book<U+00A0>thoroughly covers subjects from search algorithms and reducing problems to search problems, working with logic, planning, and more advanced topics in AI, such as reasoning with partial observability, machine learning and language processing. Make it the first book on A.I in your book shelf. Free PDF Link: Download This book is written by Jeff Heaton. It teaches basic artificial intelligence algorithms such as<U+00A0>dimensionality, distance metrics, clustering, error calculation, hill climbing, Nelder Mead, and linear regression. It explains these algorithms using interesting examples and cases. Needless to say, this book requires good commands over mathematics. Otherwise, you¡¯ll have tough time deciphering the equations. Another one by Peter Norvig! This book teaches advanced common lisp techniques to build major A.I systems. It delves deep into the practical aspects of A.I and teaches its readers the method to build and debug robust practical programs. It also demonstrates superior programming style and essential AI concepts. I¡¯d recommend reading this book, if you are serious about a career in A.I specially. This book is written by Nils J Nilsson. After reading the above 3 books, you¡¯d like something which could challenge your mind. Here¡¯s what you are looking for. This books covers topics such as<U+00A0>Neural networks, genetic programming, computer vision, heuristic search, knowledge representation and reasoning, Bayes networks and explains them with great ease. I wouldn¡¯t recommend this book for a beginner. However, it¡¯s a must read for advanced level user. Nick Bostrom has authored (or co-authored) over 200 publications, including this book called Superintelligence. Most of the world is enthralled and captivated by what AI can do and it¡¯s potential to change the world. But how many of us stop to think about how AI will affect our society? Are we considering the human aspect at all when building AI products and services? If not, we really should. In this thought-provoking book, Nick Bostrom lays down a future scenario where machines reach the superintelligent stage and deliberately or accidentally lead to the extinction of humans. This might sound like a sci-fi movie plot, but the way Mr. Bostrom has laid down his arguments and the thinking behind them will definitely sway you and make you take him seriously. We consider this a must-read for everyone working in the AI space. Similar to the above idea propounded by Nick Bostrom, Ray Kurzweil¡¯s ¡®Singularity is Near¡¯ delves into the thick depths of superintelligent machines. It is a slightly long read, but well worth it in the end. The way Mr. Ray has described the Singularity is breathtaking and will make you stop in your tracks. Singularity, as<U+00A0> Ray Kurzweil has described it, is the point where humans and the intelligence of machines will merge. Once this happens, machines will be far more intelligent than all of the human species combined. It¡¯s NOT science fiction but a truly poignant description of what might happen in the future if we aren¡¯t careful with what and how we work with AI. When Stephan Hawking endorses a book, one sits up and listens. This book by Max Tegmark is an international bestseller and deals with the topic of superintelligence. Some of the basic questions this book asks (and answers) are (taken from Amazon¡¯s summary): How can we grow our prosperity through automation, without leaving people lacking income or purpose? How can we ensure that future AI systems do what we want without crashing, malfunctioning or getting hacked? Should we fear an arms race in lethal autonomous weapons? Will AI help life flourish as never before, or will machines eventually outsmart us at all tasks, and even, perhaps, replace us altogether? This is one of our favorite books in this list. Can there be just one algorithm that deals with all the aspects of technology? Instead of building AI products for specific functions, can we build one single algorithm for all functions? This thought is quite similar to what Albert Einstein spent the latter years of his life trying to discover. Pedro Domingos is a masterful writer, and he deals with the intricacies of his subject extremely well. Make sure you add this to your reading list! Disclosure:<U+00A0>The Amazon links in this article are affiliate links. If you buy a book through this link, we would get paid through Amazon. This is one of the ways for us to cover our costs while we continue to create these awesome articles. Further, the list reflects our recommendation based on content of book and is no way influenced by the commission. This is just the tip of the iceberg. Books are a wonderful source of knowledge for anyone willing to learn from them. This collection spans various aspects of AI and ML <U+2013> from the mathematics and statistics side to the intangible factors like ethics and impact of society. All of these should be considered together when working on an AI and ML project. Having said that, there is truly no substitute for experience. Once you¡¯ve devoured all these books can provide, always apply your learning to real-world problems and challenges. And as always, if you have any questions or suggestions for us on this article, feel free to share them in the comments section below. We look forward to connecting with you! Really good. It would be great if the pdf links are shared.. Thanks. I¡¯ll compile the links and share them with you shortly. Please paste the link of some of their legal pdf¡¯s uploaded by their authors. I¡¯ll compile the links and share them with you shortly. may be in a google drive. Please mail me the links of the pdfs of these books and any other data science related books materials which you have.
It would be great and a treasure house if these are uploaded in a drive and granted access. Loved your suggestion. I¡¯m working on it. I¡¯ll share the link as soon as pdf links are compiled. I second DK Samuel¡¯s comment. A reputed site like AV should be sharing only those (free) PDFs which are made available by the authors or publishers (e.g. ESL by Hastie et al.). Agree with your suggestion. I¡¯ve already mentioned this part in the end notes section. Great article! By the way, there are actually 3 authors for Learning from Data, not only Prof. Yaser Abu Mostafa. You forgot to mention the other 2 authors. I missed them! My Bad. Thank you so much for highlighting this error, I would have never known otherwise. Appreciate it.
Added now. Quite helpful information on Machine Learning. Do share the links to PDFs. Hi. Thanks for this useful post, especially for beginers. It¡¯ll be great if you can share those pdfs. Quite fascinating list, please share the links for legal pdf¡¯s uploaded by their authors.. And would it be possible to compile the list of books on Business/Data Analytics. Thanks for this great list! Could you share the pdfs please? I think some of the more honourable mentions are Kevin Murphy¡¯s Machine Learning, Pattern Classification by Duda, Hart, et, al and Hal Daume¡¯s A course in Machine learning. Hello all, I have added the pdf download links below their respective books in this article. Keep Learning Machine Learning and AI. <U+0001F642> I¡¯d recommend ¡°An Introduction to Statistical Learning with Applications in R¡± by James, Witten, Hastie, Tibshirani before ¡°The Elements of Statistical Learning.¡± I wouldn¡¯t consider the latter an intro book. What do you think about Applied Predictive Modeling? After you study machine learning by yourself by reading book, you might need to study some practical exercises to be better. I would recommend to take the machine leaning courses that I took, because it covered almost everything that I wanted. http://www.thedevmasters.com/machine-learning-in-python/ One great ML book that is not in your list: ML A probabilistic perspective by Kevin P Murphy I am beginners with the basic knowledge on python and want to work more on ML. I do not know the right direction. Please share me ur thoughts as a beginners This are the indeed the best resource for machine learning. A well structured article which clearly explains the relevance of each book. Boy, this site is serious about giving people information. Good going boys. Cheers. The download links for most of them are not working. Can you kindly share. Thanks","Keyword(freq): book(19), link(11), algorithm(7), author(7), pdf(7), machine(6), technique(4), thank(4), topic(4), aspect(3)"
"9","vidhya",2018-10-16,"An NLP Approach to Mining Online Reviews using Topic Modeling (with Python codes)","https://www.analyticsvidhya.com/blog/2018/10/mining-online-reviews-topic-modeling-lda/","

FLASH SALE: Course On Introduction To Data Science | Use COUPON CODE: FLASH25 for 25% Discount| Work on 4 real-life projects |
Buy Now 

 E-commerce has revolutionized the way we shop. That phone you¡¯ve been saving up to buy for months? It¡¯s just a search and a few clicks away.<U+00A0>Items are delivered within a matter of days (sometimes even the next day!). For online retailers, there are no constraints related to inventory management or space management They can sell as many different products as they want. Brick and mortar stores can keep only a limited number of products due to the finite space they have available. I remember when I used to place orders for books at my local bookstore, and it used to take over a week for the book to arrive. It seems like a story from the ancient times now! Source: http://www.yeebaplay.com.br But online shopping comes with its own caveats. One of the biggest challenges is verifying the authenticity of a product. Is it as good as advertised on the e-commerce site? Will the product last more than a year? Are the reviews given by other customers really true or are they false advertising? These are important questions customers need to ask before splurging their money. This is a great place to experiment and apply Natural Language Processing (NLP) techniques.<U+00A0>This article will help you understand the significance of harnessing online product reviews with the help of Topic Modeling. Please go through the below articles in case you need a quick refresher on Topic Modeling: A few days back, I took the e-commerce plunge and purchased a smartphone online. It was well within my budget, and it had an above decent rating of 4.5 out of 5. Unfortunately, it turned out to be a bad decision as the battery backup was well below par. I didn¡¯t go through the reviews of the product and made a hasty decision to buy it based on its ratings alone. And I know I¡¯m not the only one out there who made this mistake! Ratings alone do not give a complete picture of the products we wish to purchase, as I found to my detriment. So, as a precautionary measure, I always advise people to read a product¡¯s reviews before deciding whether to buy it or not. But then an interesting problem comes up. What if the number of reviews is in the hundreds or thousands? It¡¯s just not feasible to go through all those reviews, right? And this is where natural language processing comes up trumps. A problem statement is the seed from which your analysis blooms. Therefore, it is really important to have a solid, clear and well-defined problem statement. How we can analyze a large number of online reviews using Natural Language Processing (NLP)? Let¡¯s define this problem. Online product reviews are a great source of information for consumers. From the sellers¡¯ point of view, online reviews can be used to gauge the consumers¡¯ feedback on the products or services they are selling. However, since these online reviews are quite often overwhelming in terms of numbers and information, an intelligent system, capable of finding key insights (topics) from these reviews, will be of great help for both the consumers and the sellers. This system will serve two purposes: To solve this task, we will use the concept of Topic Modeling (LDA) on Amazon Automotive Review data. You can download it from this<U+00A0>link. Similar datasets for other categories of products can be found here. As the name suggests, Topic Modeling is a process to automatically identify topics present in a text object and to derive hidden patterns exhibited by a text corpus. Topic Models are very useful for multiple purposes, including: A good topic model, when trained on some<U+00A0>text about the stock market, should result in topics like ¡°bid¡±, ¡°trading¡±, ¡°dividend¡±, ¡°exchange¡±, etc. The below image illustrates how a typical topic model works: In our case, instead of text documents, we have thousands of online product reviews for the items listed under the ¡®Automotive¡¯ category. Our aim here is to extract a certain number of groups of important words from the reviews. These groups of words are basically the topics which would help in ascertaining what the consumers are actually talking about in the reviews. In this section, we¡¯ll power up our Jupyter notebooks (or any other IDE you use for Python!). Here we¡¯ll work on the problem statement defined above to extract useful topics from our online reviews dataset using the concept of Latent Dirichlet Allocation (LDA). Note: As I mentioned in the introduction, I highly recommend going through this article to understand what LDA is and how it works. Let¡¯s first load all the necessary libraries: To import the data, first extract the data to your working directory and then use the<U+00A0>read_json( ) function of pandas to read it into a pandas dataframe. As you can see, the data contains the following columns: For the scope of our analysis and this article, we will be using only the reviews column, i.e.,<U+00A0>reviewText. Data preprocessing and cleaning<U+00A0>is an important step before any text mining task, in this step, we will remove the punctuations, stopwords and normalize the reviews as much as possible. After every preprocessing step, it is a good practice to check the most frequent words in the data. Therefore, let¡¯s define a function that would plot a bar graph of n most frequent words in the data. Let¡¯s try this function and find out which are the most common words in our reviews dataset. Most common words are ¡®the¡¯, ¡®and¡¯, ¡®to¡¯, so on and so forth. These words are not so important for our task and they do not tell any story. We¡¯ have to get rid of these kinds of words. Before that let¡¯s remove the punctuations and numbers from our text data. Let¡¯s try to remove the stopwords<U+00A0>and short words (<2 letters) from the reviews. Let¡¯s again plot the most frequent words and see if the more significant words have come out. We can see some improvement here. Terms like ¡®battery¡¯, ¡®price¡¯, ¡®product¡¯, ¡®oil¡¯ have come up which are quite relevant for the Automotive category. However, we still have neutral terms like ¡®the¡¯, ¡®this¡¯, ¡®much¡¯, ¡®they¡¯ which are not that relevant. To further remove noise from the text we can use lemmatization from the<U+00A0>spaCy library. It reduces any given word to its base form thereby reducing multiple forms of a word to a single word. Let¡¯s tokenize the reviews and then lemmatize the tokenized reviews. As you can see, we have not just lemmatized the words but also filtered only nouns and adjectives. Let¡¯s de-tokenize the lemmatized reviews and plot the most common words. It seems that now most frequent terms in our data are relevant. We can now go ahead and start building our topic model. We will start by creating the term dictionary of our corpus, where every unique term is assigned an index Then we will convert the list of reviews (reviews_2) into a Document Term Matrix using the dictionary prepared above. The code above will take a while. Please note that I have specified the number of topics as 7 for this model using the num_topics parameter. You can specify any number of topics using the same parameter. Let¡¯s print out the topics that our LDA model has learned. The fourth topic Topic 3<U+00A0>has terms like ¡®towel¡¯, ¡®clean¡¯, ¡®wax¡¯, ¡®water¡¯, indicating that the topic is very much related to car-wash. Similarly, Topic 6<U+00A0>seems to be about the overall value of the product as it has<U+00A0>terms like ¡®price¡¯, ¡®quality¡¯, and ¡®worth¡¯. To visualize our topics in a 2-dimensional space we will use the pyLDAvis library. This visualization is interactive in nature and displays topics along with the most relevant words. Apart from topic modeling, there are many other NLP methods as well which are used for analyzing and understanding online reviews. Some of them are listed below: Information retrieval saves us from the labor of going through product reviews one by one. It gives us a fair idea of what other consumers are talking about the product. However, it does not tell us whether the reviews are positive, neutral, or negative. This becomes an extension of the problem of information retrieval where we don¡¯t just have to extract the topics, but also determine the sentiment. This is an interesting task which we will cover in the next article. Topic modeling is one of the most popular NLP techniques with several real-world applications such as dimensionality reduction, text summarization, recommendation engine, etc.. The purpose of this article was to demonstrate the application of LDA on a raw, crowd-generated text data. I encourage you to implement the code on other datasets and share your findings. If you have any<U+00A0>suggestion, doubt, or anything else that you wish to share regarding topic modeling, then please feel free to use the comments section below. Full code is available<U+00A0>here. Thanks man Thanks pratik. That was a nice article to read. Is there any techniques to find out the synonyms and antonyms in NLP? Hi Gokul, You can use nltk library and WordNet database to find synonym/antonym of a word. Thanks!","Keyword(freq): review(26), topic(11), product(6), consumer(5), term(5), technique(3), thank(3), customer(2), dataset(2), item(2)"
"10","vidhya",2018-10-14,"DataHack Radio #12: Exploring the Nuts and Bolts of Natural Language Processing with Sebastian Ruder","https://www.analyticsvidhya.com/blog/2018/10/datahack-radio-podcast-nlp-research-sebastian-ruder/","

FLASH SALE: Course On Introduction To Data Science | Use COUPON CODE: FLASH25 for 25% Discount| Work on 4 real-life projects |
Buy Now 

 There¡¯s text everywhere around us, from digital sources like social media to physical objects like books and print media. The amount of text data being generated every day is mind boggling and yet we¡¯re not even close to harnessing the full power of natural language processing. I see a ton of aspiring data scientists interested in this field, but they often turn away daunted by the challenges NLP presents. It¡¯s such a niche line of work, and we at Analytics Vidhya would love to see more of our community actively participate in ground-breaking work in this field. So we thought what better way to do this than get an NLP expert on our DataHack Radio podcast? Yes, we¡¯ve got none other than Sebastian Ruder in Episode 12! This podcast is a knowledge goldmine for NLP enthusiasts, so make sure you tune in. It¡¯s catapulted to the top of my machine learning recommended podcasts. I strongly feel every aspiring and established data science professional should take the time to hear Sebastian talk about the diverse and often complex NLP domain. If you¡¯re looking for a place to get started with NLP, you¡¯ve landed at the right place! Check out Analytics Vidhya¡¯s Natural Language Processing using Python course and enrol yourself TODAY! Subscribe to DataHack Radio today and listen to this, as well as all previous episodes, on any of the below platforms: Sebastian¡¯s background is in computational linguistics, which is essentially a combination of computer science and linguistics. His interest in mathematics and languages piqued in high school and he carved a career out of that. He completed his graduation in Germany in this field as well, and has been immersed in the research side of things ever since. This is a very niche field so there are a lot of things Sebastian had to pick up from scratch and learn on his own. Quite an inspiring story for folks looking to transition into machine learning <U+2013> a healthy dose of passion added to tons of dedication and an inquisitive mind. He is currently working at Aylien as a research scientist and also pursuing a Ph.D. from the National University of Ireland in, you guessed it, Natural Language Processing.<U+00A0>Relationship extraction, named entity recognition and sentiment analysis are some of the areas where Sebastian has worked during his initial Ph.D. years. There¡¯s a certain bias in the machine learning world when it comes to NLP. When someone mentions text, the first language that pops into our mind is English. How difficult is it to transfer a NLP model between languages? Next to impossible, as it turns out. Sebastian explained this with an example between his native tongue German and English. German is more syntactically richer than English, while in the latter you can go a long way with techniques like tokenization and building models on the word-level. In German, you need to be more careful of how words are composed. This is important in computational linguistics as hierarchy on words matters. And of course, a common difference is how the words as used. Because of this, there are different rules in place for individual languages which is what makes working with text so challenging. For sentiment analysis, Sebastian mentioned that the primary example is looking at different categories of product reviews. These are usually already well-defined and getting training data is comparatively easier. Apart from this, social media data (especially Twitter) is popularly used to mine text and extract sentiments. Other sources that are referred to include print media, like digital newspaper articles, magazines, blogs, among others. If you¡¯re applying deep learning techniques, then scanned images of text can also be used for training your model. Sebastian recently co-authored a fascinating research paper with the great Jeremy Howard called ¡®Universal Language Model Language Fine-tuning¡¯ (ULMFiT). The paper made waves in the NLP community as soon as it was released, and the techniques are available in the fastai library. ULMFiT is an effective transfer learning method that can be applied to almost any NLP task. At the time of the release, it outperformed six state-of-the-art text classification tasks. Sebastian and Jeremy have done all the hard work for you <U+2013> they have released pretrained models that you can plug into your existing project and generate incredibly accurate text classification results. You can read the paper in full here. The most prominent challenge in most machine learning applications is first getting the data you need to train the model, and then find the right amount of computational resources to actually do the training. This has often proved to be a step too far for a lot of project. So Sebastian introduced us to the idea of increased sample efficiency wherein you can train models without having to collect millions of text data points. In addition to this, the trained model should not overfit on this relatively smaller sample, and should generalize well. Another challenge, which we touched on earlier, is the lack of datasets in non-English languages. The majority of data, and subsequently algorithms, are from English origins. We should seriously think about democratizing data from other languages to reduce this gap and eliminate the current state of bias.","Keyword(freq): language(5), media(4), linguistic(3), model(3), technique(3), analytics(2), source(2), algorithm(1), application(1), article(1)"
