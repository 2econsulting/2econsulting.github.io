"site","date","headline","url_address","text"
"mastery",2018-07-27,"How to Configure the Number of Layers and Nodes in a Neural Network","https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/","Artificial neural networks have two main hyperparameters that control the architecture or topology of the network: the number of layers and the number of nodes in each hidden layer. You must specify values for these parameters when configuring your network. The most reliable way to configure these hyperparameters for your specific predictive modeling problem is via systematic experimentation with a robust test harness. This can be a tough pill to swallow for beginners to the field of machine learning, looking for an analytical way to calculate the optimal number of layers and nodes, or easy rules of thumb to follow. In this post, you will discover the roles of layers and nodes and how to approach the configuration of a multilayer perceptron neural network for your predictive modeling problem. After reading this post, you will know: Let¡¯s get started. How to Configure the Number of Layers and Nodes in a Neural NetworkPhoto by Ryan, some rights reserved. This post is divided into four sections; they are: A node, also called a neuron or Perceptron, is a computational unit that has one or more weighted input connections, a transfer function that combines the inputs in some way, and an output connection. Nodes are then organized into layers to comprise a network. A single-layer artificial neural network, also called a single-layer, has a single layer of nodes, as its name suggests. Each node in the single layer connects directly to an input variable and contributes to an output variable. Single-layer networks have just one layer of active units. Inputs connect directly to the outputs through a single layer of weights. The outputs do not interact, so a network with N outputs can be treated as N separate single-output networks. <U+2014> Page 15, Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999. A single-layer network can be extended to a multiple-layer network, referred to as a Multilayer Perceptron. A Multilayer Perceptron, or MLP for sort, is an artificial neural network with more than a single layer. It has an input layer that connects to the input variables, one or more hidden layers, and an output layer that produces the output variables. The standard multilayer perceptron (MLP) is a cascade of single-layer perceptrons. There is a layer of input nodes, a layer of output nodes, and one or more intermediate layers. The interior layers are sometimes called ¡°hidden layers¡± because they are not directly observable from the systems inputs and outputs. <U+2014> Page 31, Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999. We can summarize the types of layers in an MLP as follows: Finally, there are terms used to describe the shape and capability of a neural network; for example: Traditionally, there is some disagreement about how to count the number of layers. The disagreement centers around whether or not the input layer is counted. There is an argument to suggest it should not be counted because the inputs are not active; they are simply the input variables. We will use this convention; this is also the convention recommended in the book ¡°Neural Smithing¡°. Therefore, an MLP that has an input layer, one hidden layer, and one output layer is a 2-layer MLP. The structure of an MLP can be summarized using a simple notation. This convenient notation summarizes both the number of layers and the number of nodes in each layer. The number of nodes in each layer is specified as an integer, in order from the input layer to the output layer, with the size of each layer separated by a forward-slash character (¡°/¡±). For example, a network with two variables in the input layer, one hidden layer with eight nodes, and an output layer with one node would be described using the notation: 2/8/1. I recommend using this notation when describing the layers and their size for a Multilayer Perceptron neural network. Before we look at how many layers to specify, it is important to think about why we would want to have multiple layers. A single-layer neural network can only be used to represent linearly separable functions. This means very simple problems where, say, the two classes in a classification problem can be neatly separated by a line. If your problem is relatively simple, perhaps a single layer network would be sufficient. Most problems that we are interested in solving are not linearly separable. A Multilayer Perceptron can be used to represent convex regions. This means that in effect, they can learn to draw shapes around examples in some high-dimensional space that can separate and classify them, overcoming the limitation of linear separability. In fact, there is a theoretical finding by Lippmann in the 1987 paper ¡°An introduction to computing with neural nets¡± that shows that an MLP with two hidden layers is sufficient for creating classification regions of any desired shape. This is instructive, although it should be noted that no indication of how many nodes to use in each layer or how to learn the weights is given. A further theoretical finding and proof has shown that MLPs are universal approximators. That with one hidden layer, an MLP can approximate any function that we require. Specifically, the universal approximation theorem states that a feedforward network with a linear output layer and at least one hidden layer with any ¡°squashing¡± activation function (such as the logistic sigmoid activation function) can approximate any Borel measurable function from one finite-dimensional space to another with any desired non-zero amount of error, provided that the network is given enough hidden units. <U+2014> Page 198, Deep Learning, 2016. This is an often-cited theoretical finding and there is a ton of literature on it. In practice, we again have no idea how many nodes to use in the single hidden layer for a given problem nor how to learn or set their weights effectively. Further, many counterexamples have been presented of functions that cannot directly be learned via a single one-hidden-layer MLP or require an infinite number of nodes. Even for those functions that can be learned via a sufficiently large one-hidden-layer MLP, it can be more efficient to learn it with two (or more) hidden layers. Since a single sufficiently large hidden layer is adequate for approximation of most functions, why would anyone ever use more? One reason hangs on the words ¡°sufficiently large¡±. Although a single hidden layer is optimal for some functions, there are others for which a single-hidden-layer-solution is very inefficient compared to solutions with more layers. <U+2014> Page 38, Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999. With the preamble of MLPs out of the way, let¡¯s get down to your real question. How many layers should you use in your Multilayer Perceptron and how many nodes per layer? In this section, we will enumerate five approaches to solving this problem. In general, when I¡¯m asked how many layers and nodes to use for an MLP, I often reply: I don¡¯t know. Use systematic experimentation to discover what works best for your specific dataset. I still stand by this answer. In general, you cannot analytically calculate the number of layers or the number of nodes to use per layer in an artificial neural network to address a specific real-world predictive modeling problem. The number of layers and the number of nodes in each layer are model hyperparameters that you must specify. You are likely to be the first person to attempt to address your specific problem with a neural network. No one has solved it before you. Therefore, no one can tell you the answer of how to configure the network. You must discover the answer using a robust test harness and controlled experiments. For example, see the post: Regardless of the heuristics you might encounter, all answers will come back to the need for careful experimentation to see what works best for your specific dataset. The network can be configured via intuition. For example, you may have an intuition that a deep network is required to address a specific predictive modeling problem. A deep model provides a hierarchy of layers that build up increasing levels of abstraction from the space of the input variables to the output variables. Given an understanding of the problem domain, we may believe that a deep hierarchical model is required to sufficiently solve the prediction problem. In which case, we may choose a network configuration that has many layers of depth. Choosing a deep model encodes a very general belief that the function we want to learn should involve composition of several simpler functions. This can be interpreted from a representation learning point of view as saying that we believe the learning problem consists of discovering a set of underlying factors of variation that can in turn be described in terms of other, simpler underlying factors of variation. <U+2014> Page 201, Deep Learning, 2016. This intuition can come from experience with the domain, experience with modeling problems with neural networks, or some mixture of the two. In my experience, intuitions are often invalidated via experiments. In their important textbook on deep learning, Goodfellow, Bengio, and Courville highlight that empirically, on problems of interest, deep neural networks appear to perform better. Specifically, they state the choice of using deep neural networks as a statistical argument in cases where depth may be intuitively beneficial. Empirically, greater depth does seem to result in better generalization for a wide variety of tasks. [¡¦] This suggests that using deep architectures does indeed express a useful prior over the space of functions the model learns. <U+2014> Page 201, Deep Learning, 2016. We may use this argument to suggest that using deep networks, those with many layers, may be a heuristic approach to configuring networks for challenging predictive modeling problems. This is similar to the advice for starting with Random Forest and Stochastic Gradient Boosting on a predictive modeling problem with tabular data to quickly get an idea of an upper-bound on model skill prior to testing other methods. A simple, but perhaps time consuming approach, is to leverage findings reported in the literature. Find research papers that describe the use of MLPs on instances of prediction problems similar in some way to your problem. Note the configuration of the networks used in those papers and use them as a starting point for the configurations to test on your problem. Transferability of model hyperparameters that result in skillful models from one problem to another is a challenging open problem and the reason why model hyperparameter configuration is more art than science. Nevertheless, the network layers and number of nodes used on related problems is a good starting point for testing ideas. Design an automated search to test different network configurations. You can seed the search with ideas from literature and intuition. Some popular search strategies include: This can be challenging with large models, large datasets and combinations of the two. Some ideas to reduce or manage the computational burden include: I recommend being systematic if time and resources permit. I have seen countless heuristics of how to estimate the number of layers and either the total number of neurons or the number of neurons per layer. I do not want to enumerate them; I¡¯m skeptical that they add practical value beyond the special cases on which they are demonstrated. If this area is interesting to you, perhaps start with ¡°Section 4.4 Capacity versus Size¡± in the book ¡°Neural Smithing¡°. It summarizes a ton of findings in this area. The book is dated from 1999, so there are another nearly 20 years of ideas to wade through in this area if you¡¯re up for it. Also, see some of the discussions linked in the Further Reading section (below). Did I miss your favorite method for configuring a neural network? Or do you know a good reference on the topic?
Let me know in the comments below. This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered the role of layers and nodes and how to configure a multilayer perceptron neural network. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of Python Discover how in my new Ebook: Deep Learning With Python It covers self-study tutorials and end-to-end projects on topics like:Multilayer Perceptrons,<U+00A0>Convolutional Nets and<U+00A0>Recurrent Neural Nets, and more¡¦ Skip the Academics. Just<U+00A0>Results. Click to<U+00A0>learn more. Thanks for the blog post.
There is indeed a large number of recent research going to answer this question automatically, dubbed (neural) architecture search. Here a list of papers which I maintain:https://www.automl.org/automl/literature-on-neural-architecture-search/ Thanks. Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning."
"mastery",2018-07-25,"How to Calculate McNemar¡¯s Test to Compare Two Machine Learning Classifiers","https://machinelearningmastery.com/mcnemars-test-for-machine-learning/","The choice of a statistical hypothesis test is a challenging open problem for interpreting machine learning results. In his widely cited 1998 paper, Thomas Dietterich recommended the McNemar¡¯s test in those cases where it is expensive or impractical to train multiple copies of classifier models. This describes the current situation with deep learning models that are both very large and are trained and evaluated on large datasets, often requiring days or weeks to train a single model. In this tutorial, you will discover how to use the McNemar¡¯s statistical hypothesis test to compare machine learning classifier models on a single test dataset. After completing this tutorial, you will know: Let¡¯s get started. How to Calculate McNemar¡¯s Test for Two Machine Learning ClassifiersPhoto by Mark Kao, some rights reserved. This tutorial is divided into five parts; they are: Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In his important and widely cited 1998 paper on the use of statistical hypothesis tests to compare classifiers titled ¡°Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms¡°, Thomas Dietterich recommends the use of the McNemar¡¯s test. Specifically, the test is recommended in those cases where the algorithms that are being compared can only be evaluated once, e.g. on one test set, as opposed to repeated evaluations via a resampling technique, such as k-fold cross-validation. For algorithms that can be executed only once, McNemar¡¯s test is the only test with acceptable Type I error. <U+2014> Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithm, 1998. Specifically, Dietterich¡¯s study was concerned with the evaluation of different statistical hypothesis tests, some operating upon the results from resampling methods. The concern of the study was low Type I error, that is, the statistical test reporting an effect when in fact no effect was present (false positive). Statistical tests that can compare models based on a single test set is an important consideration for modern machine learning, specifically in the field of deep learning. Deep learning models are often large and operate on very large datasets. Together, these factors can mean that the training of a model can take days or even weeks on fast modern hardware. This precludes the practical use of resampling methods to compare models and suggests the need to use a test that can operate on the results of evaluating trained models on a single test dataset. The McNemar¡¯s test may be a suitable test for evaluating these large and slow-to-train deep learning models. The McNemar¡¯s test operates upon a contingency table. Before we dive into the test, let¡¯s take a moment to understand how the contingency table for two classifiers is calculated. A contingency table is a tabulation or count of two categorical variables. In the case of the McNemar¡¯s test, we are interested in binary variables correct/incorrect or yes/no for a control and a treatment or two cases. This is called a 2¡¿2 contingency table. The contingency table may not be intuitive at first glance. Let¡¯s make it concrete with a worked example. Consider that we have two trained classifiers. Each classifier makes binary class prediction for each of the 10 examples in a test dataset. The predictions are evaluated and determined to be correct or incorrect. We can then summarize these results in a table, as follows: We can see that Classifier1 got 6 correct, or an accuracy of 60%, and Classifier2 got 5 correct, or 50% accuracy on the test set. The table can now be reduced to a contingency table. The contingency table relies on the fact that both classifiers were trained on exactly the same training data and evaluated on exactly the same test data instances. The contingency table has the following structure: In the case of the first cell in the table, we must sum the total number of test instances that Classifier1 got correct and Classifier2 got correct. For example, the first instance that both classifiers predicted correctly was instance number 5. The total number of instances that both classifiers predicted correctly was 4. Another more programmatic way to think about this is to sum each combination of Yes/No in the results table above. The results organized into a contingency table are as follows: McNemar¡¯s test is a paired nonparametric or distribution-free statistical hypothesis test. It is also less intuitive than some other statistical hypothesis tests. The McNemar¡¯s test is checking if the disagreements between two cases match. Technically, this is referred to as the homogeneity of the contingency table (specifically the marginal homogeneity). Therefore, the McNemar¡¯s test is a type of homogeneity test for contingency tables. The test is widely used in medicine to compare the effect of a treatment against a control. In terms of comparing two binary classification algorithms, the test is commenting on whether the two models disagree in the same way (or not). It is not commenting on whether one model is more or less accurate or error prone than another. This is clear when we look at how the statistic is calculated. The McNemar¡¯s test statistic is calculated as: Where Yes/No is the count of test instances that Classifier1 got correct and Classifier2 got incorrect, and No/Yes is the count of test instances that Classifier1 got incorrect and Classifier2 got correct. This calculation of the test statistic assumes that each cell in the contingency table used in the calculation has a count of at least 25. The test statistic has a Chi-Squared distribution with 1 degree of freedom. We can see that only two elements of the contingency table are used, specifically that the Yes/Yes and No/No elements are not used in the calculation of the test statistic. As such, we can see that the statistic is reporting on the different correct or incorrect predictions between the two models, not the accuracy or error rates. This is important to understand when making claims about the finding of the statistic. The default assumption, or null hypothesis, of the test is that the two cases disagree to the same amount. If the null hypothesis is rejected, it suggests that there is evidence to suggest that the cases disagree in different ways, that the disagreements are skewed. Given the selection of a significance level, the p-value calculated by the test can be interpreted as follows: It is important to take a moment to clearly understand how to interpret the result of the test in the context of two machine learning classifier models. The two terms used in the calculation of the McNemar¡¯s Test capture the errors made by both models. Specifically, the No/Yes and Yes/No cells in the contingency table. The test checks if there is a significant difference between the counts in these two cells. That is all. If these cells have counts that are similar, it shows us that both models make errors in much the same proportion, just on different instances of the test set. In this case, the result of the test would not be significant and the null hypothesis would not be rejected. Under the null hypothesis, the two algorithms should have the same error rate ¡¦ <U+2014> Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithm, 1998. If these cells have counts that are not similar, it shows that both models not only make different errors, but in fact have a different relative proportion of errors on the test set. In this case, the result of the test would be significant and we would reject the null hypothesis. So we may reject the null hypothesis in favor of the hypothesis that the two algorithms have different performance when trained on the particular training <U+2014> Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithm, 1998. We can summarize this as follows: After performing the test and finding a significant result, it may be useful to report an effect statistical measure in order to quantify the finding. For example, a natural choice would be to report the odds ratios, or the contingency table itself, although both of these assume a sophisticated reader. It may be useful to report the difference in error between the two classifiers on the test set. In this case, be careful with your claims as the significant test does not report on the difference in error between the models, only the relative difference in the proportion of error between the models. Finally, in using the McNemar¡¯s test, Dietterich highlights two important limitations that must be considered. They are: Generally, model behavior varies based on the specific training data used to fit the model. This is due to both the interaction of the model with specific training instances and the use of randomness during learning. Fitting the model on multiple different training datasets and evaluating the skill, as is done with resampling methods, provides a way to measure the variance of the model. The test is appropriate if the sources of variability are small. Hence, McNemar¡¯s test should only be applied if we believe these sources of variability are small. <U+2014> Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithm, 1998. The two classifiers are evaluated on a single test set, and the test set is expected to be smaller than the training set. This is different from hypothesis tests that make use of resampling methods as more, if not all, of the dataset is made available as a test set during evaluation (which introduces its own problems from a statistical perspective). This provides less of an opportunity to compare the performance of the models. It requires that the test set is an appropriately representative of the domain, often meaning that the test dataset is large. The McNemar¡¯s test can be implemented in Python using the mcnemar() Statsmodels function. The function takes the contingency table as an argument and returns the calculated test statistic and p-value. There are two ways to use the statistic depending on the amount of data. If there is a cell in the table that is used in the calculation of the test statistic that has a count of less than 25, then a modified version of the test is used that calculates an exact p-value using a binomial distribution. This is the default usage of the test: Alternately, if all cells used in the calculation of the test statistic in the contingency table have a value of 25 or more, then the standard calculation of the test can be used. We can calculate the McNemar¡¯s on the example contingency table described above. This contingency table has a small count in both the disagreement cells and as such the exact method must be used. The complete example is listed below. Running the example calculates the statistic and p-value on the contingency table and prints the results. We can see that the test strongly confirms that there is very little difference in the disagreements between the two cases. The null hypothesis not rejected. As we are using the test to compare classifiers, we state that there is no statistically significant difference in the disagreements between the two models. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to use the McNemar¡¯s test statistical hypothesis test to compare machine learning classifier models on a single test dataset. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦by writing lines of code in python Discover how in my new Ebook:Statistical Methods for Machine Learning It provides self-study tutorials on topics like:Hypothesis Tests, Correlation, Nonparametric Stats, Resampling, and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hi Jason, Thanks for this nice post. Any practical python post about 5¡¿2 CV + paired t-test coming soon? Best,
Elie Good question. 5¡¿2 is straight forward with sklearn. I do have a post on how to code the t-test from scratch scheduled. It can be modified with the suggestions from:https://machinelearningmastery.com/statistical-significance-tests-for-comparing-machine-learning-algorithms/ Thanks Jason. I found a nice Kaggle kernel treating 5¡¿2 CV t-test: https://www.kaggle.com/ogrellier/parameter-tuning-5-x-2-fold-cv-statistical-test Nice. Thank you Jason.
I¡¯ve learnt alot from you I¡¯m glad to hear that. Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning."
"mastery",2018-07-23,"When to Use MLP, CNN, and RNN Neural Networks","https://machinelearningmastery.com/when-to-use-mlp-cnn-and-rnn-neural-networks/","What neural network is appropriate for your predictive modeling problem? It can be difficult for a beginner to the field of deep learning to know what type of network to use. There are so many types of networks to choose from and new methods being published and discussed every day. To make things worse, most neural networks are flexible enough that they work (make a prediction) even when used with the wrong type of data or prediction problem. In this post, you will discover the suggested use for the three main classes of artificial neural networks. After reading this post, you will know: Let¡¯s get started. When to Use MLP, CNN, and RNN Neural NetworksPhoto by PRODAVID S. FERRY III,DDS, some rights reserved. This post is divided into five sections; they are: Deep learning is the application of artificial neural networks using modern hardware. It allows the development, training, and use of neural networks that are much larger (more layers) than was previously thought possible. There are thousands of types of specific neural networks proposed by researchers as modifications or tweaks to existing models. Sometimes wholly new approaches. As a practitioner, I recommend waiting until a model emerges as generally applicable. It is hard to tease out the signal of what works well generally from the noise of the vast number of publications released daily or weekly. There are three classes of artificial neural networks that I recommend that you focus on in general. They are: These three classes of networks provide a lot of flexibility and have proven themselves over decades to be useful and reliable in a wide range of problems. They also have many subtypes to help specialize them to the quirks of different framings of prediction problems and different datasets. Now that we know what networks to focus on, let¡¯s look at when we can use each class of neural network. Multilayer Perceptrons, or MLPs for short, are the classical type of neural network. They are comprised of one or more layers of neurons. Data is fed to the input layer, there may be one or more hidden layers providing levels of abstraction, and predictions are made on the output layer, also called the visible layer. For more details on the MLP, see the post: Model of a Simple Network MLPs are suitable for classification prediction problems where inputs are assigned a class or label. They are also suitable for regression prediction problems where a real-valued quantity is predicted given a set of inputs. Data is often provided in a tabular format, such as you would see in a CSV file or a spreadsheet. Use MLPs For: They are very flexible and can be used generally to learn a mapping from inputs to outputs. This flexibility allows them to be applied to other types of data. For example, the pixels of an image can be reduced down to one long row of data and fed into a MLP. The words of a document can also be reduced to one long row of data and fed to a MLP. Even the lag observations for a time series prediction problem can be reduced to a long row of data and fed to a MLP. As such, if your data is in a form other than a tabular dataset, such as an image, document, or time series, I would recommend at least testing an MLP on your problem. The results can be used as a baseline point of comparison to confirm that other models that may appear better suited add value. Try MLPs On: Convolutional Neural Networks, or CNNs, were designed to map image data to an output variable. They have proven so effective that they are the go-to method for any type of prediction problem involving image data as an input. For more details on CNNs, see the post: The benefit of using CNNs is their ability to develop an internal representation of a two-dimensional image. This allows the model to learn position and scale in variant structures in the data, which is important when working with images. Use CNNs For: More generally, CNNs work well with data that has a spatial relationship. The CNN input is traditionally two-dimensional, a field or matrix, but can also be changed to be one-dimensional, allowing it to develop an internal representation of a one-dimensional sequence. This allows the CNN to be used more generally on other types of data that has a spatial relationship. For example, there is an order relationship between words in a document of text. There is an ordered relationship in the time steps of a time series. Although not specifically developed for non-image data, CNNs achieve state-of-the-art results on problems such as document classification used in sentiment analysis and related problems. Try CNNs On: Recurrent Neural Networks, or RNNs, were designed to work with sequence prediction problems. Sequence prediction problems come in many forms and are best described by the types of inputs and outputs supported. Some examples of sequence prediction problems include: The Many-to-Many problem is often referred to as sequence-to-sequence, or seq2seq for short. For more details on the types of sequence prediction problems, see the post: Recurrent neural networks were traditionally difficult to train. The Long Short-Term Memory, or LSTM, network is perhaps the most successful RNN because it overcomes the problems of training a recurrent network and in turn has been used on a wide range of applications. For more details on RNNs, see the post: RNNs in general and LSTMs in particular have received the most success when working with sequences of words and paragraphs, generally called natural language processing. This includes both sequences of text and sequences of spoken language represented as a time series. They are also used as generative models that require a sequence output, not only with text, but on applications such as generating handwriting. Use RNNs For: Recurrent neural networks are not appropriate for tabular datasets as you would see in a CSV file or spreadsheet. They are also not appropriate for image data input. Don¡¯t Use RNNs For: RNNs and LSTMs have been tested on time series forecasting problems, but the results have been poor, to say the least. Autoregression methods, even linear methods often perform much better. LSTMs are often outperformed by simple MLPs applied on the same data. For more on this topic, see the post: Nevertheless, it remains an active area. Perhaps Try RNNs on: A CNN or RNN model is rarely used alone. These types of networks are used as layers in a broader model that also has one or more MLP layers. Technically, these are a hybrid type of neural network architecture. Perhaps the most interesting work comes from the mixing of the different types of networks together into hybrid models. For example, consider a model that uses a stack of layers with a CNN on the input, LSTM in the middle, and MLP at the output. A model like this can read a sequence of image inputs, such as a video, and generate a prediction. This is called a CNN LSTM architecture. The network types can also be stacked in specific architectures to unlock new capabilities, such as the reusable image recognition models that use very deep CNN and MLP networks that can be added to a new LSTM model and used for captioning photos. Also, the encoder-decoder LSTM networks that can be used to have input and output sequences of differing lengths. It is important to think clearly about what you and your stakeholders require from the project first, then seek out a network architecture (or develop one) that meets your specific project needs. For a good framework to help you think about your data and prediction problems, see the post: This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered the suggested use for the three main classes of artificial neural networks. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of Python Discover how in my new Ebook: Deep Learning With Python It covers self-study tutorials and end-to-end projects on topics like:Multilayer Perceptrons,<U+00A0>Convolutional Nets and<U+00A0>Recurrent Neural Nets, and more¡¦ Skip the Academics. Just<U+00A0>Results. Click to<U+00A0>learn more. Very nice article on neural networks.  I love to work on data using neural networks. The human brain is clearly the baseline for many computer programs and artificial intelligence approaches. Artificial neural networks algorithm are focused on replicating the thought and reasoning patterns of the human brain which makes it an intriguing algorithm to use. Thanks. Hi Dr. Brownlee, Firstly, thanks for all your posts, they¡¯ve been a useful reference for me since I began getting involved with ML problems about a year ago. Right now I¡¯m working with the problem of audio classification using conventional and neural network approaches. I¡¯m actually using the three NNs you mention above. The idea of a hybrid model fascinates me but 1. I don¡¯t know if it can work properly for my audio problem and 2. I don¡¯t have any experience designing these hybrid models.  I appreciate any advice on this! Try it and see how it goes. What hybrid do you want to try? I have many on the blog, a good start might be here:https://machinelearningmastery.com/keras-functional-api-deep-learning/ Thanks   jason it is useful I¡¯m glad to hear that. Thank you Dr. Jason.
Your tutorials have given me an inroad to ML and Data Mining.
Thank you I¡¯m glad to hear that. Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning."
