"site","date","headline","url_address","text"
"vidhya",2018-11-01,"Top 5 Machine Learning GitHub Repositories & Reddit Discussions (October 2018)","https://www.analyticsvidhya.com/blog/2018/11/best-machine-learning-github-repositories-reddit-threads-october-2018/","¡°Should I use GitHub for my projects?¡± <U+2013> I¡¯m often asked this question by aspiring data scientists. There¡¯s only one answer to this <U+2013> ¡°Absolutely!¡±. GitHub is an invaluable platform for data scientists looking to stand out from the crowd. It¡¯s an online resume for displaying your code to recruiters and other fellow professionals. The fact that GitHub hosts open-source projects from the top tech behemoths like Google, Facebook, IBM, NVIDIA, etc. is what adds to the gloss of an already shining offering. If you¡¯re a beginner in data science, or even an established professional, you should have a GitHub account. And to save you the time of looking for the most interesting repositories out there (and there are plenty), I am delighted to scour the platform and bring them straight to you in this monthly series. This month¡¯s collection comes from a variety of use cases <U+2013> computer vision (object detection and segmentation), PyTorch implementation of Google AI¡¯s record-breaking BERT framework for NLP, extracting the latest research papers with their summaries, among others. Scroll down to start learning! Why do we include Reddit discussions in this series? I have personally found Reddit an incredibly rewarding platform for a number of reasons <U+2013> rich content, top machine learning/deep learning experts taking the time to propound their thoughts, a stunning variety of topics, open-source resources, etc. I could go on all day, but suffice to say I highly recommend going through these threads I have shortlisted <U+2013> they are unique and valuable in their own way. You can check out the top GitHub repositories and Reddit discussions (from April onwards) we have covered each month below: Computer vision has become so incredibly popular these days that organizations are rushing to implement and integrate the latest algorithms in their products. Sounds like a pretty compelling reason to jump on the bandwagon, right? Of course, object detection is easily the most sought-after skill to learn in this domain. So here¡¯s a really cool project from Facebook that aims to provide the building blocks for creating segmentation and detection models using the their popular PyTorch 1.0 framework. Facebook claims that this is upto two times faster than it¡¯s Detectron framework, and comes with pre-trained models. Enough resources and details to get started! I encourage you to check out a step-by-step introduction to the basic object detection algorithms if you need a quick refresher. And if you¡¯re looking to get familiar with the basics of PyTorch, check out this awesome beginner-friendly tutorial. This repository is a goldmine for all deep learning enthusiasts. Intrigued by the heading? Just wait till you check out some numbers about this dataset:<U+00A0>17,609,752 training and 88,739 validation image URLs, which are annotated with up to 11,166 categories. Incredible! This project also include a pre-trained Resnet-101 model, which has so far achieved a 80.73% accuracy on ImageNet via transfer learning. The repository contains exhaustive details and code on how and where to get started. This is a significant step towards making high quality data available to the community. Oh, and did I mentioned that these images are annotated? What are you waiting for, go ahead and download it NOW! Wait, another PyTorch entry? Just goes to show how popular this framework has become. For those who haven¡¯t heard of BERT, it¡¯s a language representation model that stands for<U+00A0>Bidirectional Encoder Representations from Transformers. It sounds like a mouthful, but it has been making waves in the machine learning community. BERT has set all sorts of new benchmarks in 11 natural language processing (NLP) tasks. A pre-trained language model being used on a wide range of NLP tasks might sound outlandish to some, but the BERT framework has transformed it into reality. It even emphatically outperformed humans on the popular SQuAD question answering test. This repository contains the PyTorch code for implementing BERT on your own machine. As Google Brain¡¯s Research Scientist Thang Luong tweeted, this could well by the beginning of a new era in NLP. In case you¡¯re interested in reading the research paper, that¡¯s also available here. And in case you¡¯re eager (like me) to see the official Google code, bookmark (or star)<U+00A0>this repository. How can we stay on top of the latest research in machine learning? It seems we see breakthroughs on an almost weekly basis and keeping up with them is a daunting, if not altogether impossible, challenge. Most top researchers post their full papers on arxiv.org so is there any way of sorting through the latest ones? Yes, there is! This repository uses Python (v3.x) to return the latest results by scraping arxiv papers and summarizing their abstracts. This is an really useful tool, as it will help us stay in touch with the latest papers and let us pick the one(s) we want to read. As mentioned in the repository, you can run the below command to search for a keyword: The script returns five results by default if you fail to specify how many instances you want. I always try to include at least one reinforcement learning repository in these lists <U+2013> primarily because I feel everyone in this field should be aware of the latest advancements in this space. And this month¡¯s entry is a fascinating one <U+2013> motion imitation with deep reinforcement learning. This repository in an implementation of the ¡°DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills¡± paper presented at SIGGRAPH 2018. Quoting from the repository, ¡°The framework uses reinforcement learning to train a simulated humanoid to imitate a variety of motion skills¡±. Check out the above project link which includes videos and code on how to implement this framework on your own. I just couldn¡¯t leave out this incredibly useful repository. AdaNet is a lightweight and scalable TensorFlow-based framework for automatically learning high-quality models. The best part about it is that you don¡¯t need to intervene too much <U+2013> the framework is smart and flexible enough for building better models. You can read more about AdaNet here. Google, as usual, does a great job of explaining complex concepts. Ah, the question on everybody¡¯s mind. Will autoML be ruling the roost? How will the hardware have advanced? Will there finally be official rules and policies around ethics? Will machine learning have integrated itself into the very fabric of society? Will reinforcement learning finally have found a place in the industry? These are just some of the many thoughts propounded in this discussion. Individuals have their own predictions about what they expect and what they want to see, and this discussion does an excellent job of combining the two. The conversation varies between technical and non-technical topics so you have the luxury of choosing which ones you prefer reading about. Interesting topic. We¡¯ve seen this trend before <U+2013> a non-ML person is assigned to lead a team of ML experts and it usually ends in frustration for both parties. Due to various reasons (time constraints being top of that list), it often feels like things are at an impasse. I implore all project managers, leaders, CxOs, etc. to take the time and go through this discussion thread. There are some really useful ideas that you can implement in your own projects as soon as possible. Getting all the technical and non-technical folks on the same page is a crucial cog in the overall project¡¯s success so it¡¯s important that the leader(s) sets the right example. Looking for a new project to experiment with? Or need ideas for your thesis? You¡¯ve landed at the right place. This is a collection of ideas graduate students are working on to hone and fine tune their machine learning skills. Some of the ones that stood out for me are: This is where Reddit becomes so useful <U+2013> you can pitch your idea in this discussion and you¡¯ll receive feedback from the community on how you can approach the challenge. This one is a fully technical discussion as you might have gathered from the heading. This is an entirely subjective question and answers vary depending on the level of experience the reader has and how well the researcher has put across his/her thoughts. I like this discussion because there very specific examples of linked research papers so you can explore them and form your own opinion. It¡¯s a well known (and accepted) fact that quite a lot of papers have math and findings all cobbled together <U+2013> not everyone has the patience, willingness or even the ability to present their study in a lucid manner. It¡¯s always a good idea to work on your presentation skills while you can. How do established professionals feel when their field starts getting tons of attention from newbies? It¡¯s an interesting question that potentially spans domains, but this thread focuses on machine learning. This is not a technical discussion per se, but it¡¯s interesting to note how top data scientists and applied machine learning professionals feel about the recent spike in interest in this field. The discussion, with over 120+ comments, is rich in thought and suggestions. Things get especially interesting when the topic of how to deal with non-technical leaders and team members comes up. There are tons of ideas to steal from here!"
"vidhya",2018-11-01,"An Introduction to Text Summarization using the TextRank Algorithm (with Python implementation)","https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/","Text Summarization is one of those applications of Natural Language Processing (NLP) which is bound to have a huge impact on our lives. With growing digital media and ever growing publishing <U+2013> who has the time to go through entire articles / documents / books to decide whether they are useful or not? Thankfully <U+2013> this technology is already here. Have you come across the mobile app inshorts? It¡¯s an innovative news app that converts news articles into a 60-word summary. And that is exactly what we are going to learn in this article<U+00A0><U+2014><U+00A0>Automatic Text Summarization. Automatic Text Summarization is one of the most challenging and interesting problems in the field of Natural Language Processing (NLP). It is a process of generating a concise and meaningful summary of text from multiple text resources such as books, news articles, blog posts, research papers, emails, and tweets. The demand for automatic text summarization systems is spiking these days thanks to the availability of large amounts of textual data.  Through this article, we will explore the realms of text summarization. We will understand how the TextRank algorithm works, and will also implement it in Python. Strap in, this is going to be a fun ride! Automatic Text Summarization gained attention as early as the 1950¡¯s. A research paper, published by Hans Peter Luhn in the late 1950s, titled ¡°The automatic creation of literature abstracts¡±, used features such as word frequency and phrase frequency to extract important sentences from the text for summarization purposes. Another important research, done by Harold P Edmundson in the late 1960¡¯s, used methods like the presence of cue words, words used in the title appearing in the text, and the location of sentences, to extract significant sentences for text summarization. Since then, many important and exciting studies have been published to address the challenge of automatic text summarization. Text summarization can broadly be divided into two categories <U+2014> Extractive Summarization and Abstractive Summarization. In this article, we will be focusing on the extractive summarization technique. Before getting started with the TextRank algorithm, there¡¯s another algorithm which we should become familiar with <U+2013> the PageRank algorithm. In fact, this actually inspired TextRank! PageRank is used primarily for ranking web pages in online search results. Let¡¯s quickly understand the basics of this algorithm with the help of an example. Source: http://www.scottbot.net/HIAL/ Suppose we have 4 web pages <U+2014> w1, w2, w3, and w4. These pages contain links pointing to one another. Some pages might have no link <U+2013> these are called dangling pages. In order to rank these pages, we would have to compute a score called the PageRank score. This score is the probability of a user visiting that page.  To capture the probabilities of users navigating from one page to another, we will create a square matrix M, having n rows and n columns, where n is the number of web pages. Each element of this matrix denotes the probability of a user transitioning from one web page to another. For example, the highlighted cell below contains the probability of transition from w1 to w2. The initialization of the probabilities is explained in the steps below:<U+00A0> Hence, in our case, the matrix M will be initialized as follows: Finally, the values in this matrix will be updated in an iterative fashion to arrive at the web page rankings. Let¡¯s understand the TextRank algorithm, now that we have a grasp on PageRank. I have listed the similarities between these two algorithms below: TextRank is an extractive and unsupervised text summarization technique. Let¡¯s take a look at the flow of the TextRank algorithm that we will be following: So, without further ado, let¡¯s fire up our Jupyter Notebooks and start coding! Note: If you want to learn more about Graph Theory, then I¡¯d recommend checking out this article. Being a major tennis buff, I always try to keep myself updated with what¡¯s happening in the sport by religiously going through as many online tennis updates as possible. However, this has proven to be a rather difficult job! There are way too many resources and time is a constraint. Therefore, I decided to design a system that could prepare a bullet-point summary for me by scanning through multiple articles. How to go about doing this? That¡¯s what I¡¯ll show you in this tutorial. We will apply the TextRank algorithm on a dataset of scraped articles with the aim of creating a nice and concise summary. Please note that this is essentially a single-domain-multiple-documents summarization task, i.e., we will take multiple articles as input and generate a single bullet-point<U+00A0>summary. Multi-domain text summarization is not covered in this article, but feel free to try that out at your end. So, without any further ado, fire up your Jupyter Notebooks and let¡¯s implement what we¡¯ve learned so far. First, import the libraries we¡¯ll be leveraging for this challenge. Now let¡¯s read our dataset. I have provided the link to download the data in the previous section (in case you missed it). Let¡¯s take a quick glance at the data. 
We have 3 columns in our dataset <U+2014> ¡®article_id¡¯, ¡®article_text¡¯, and ¡®source¡¯. We are most interested in the ¡®article_text¡¯ column as it contains the text of the articles.<U+00A0>Let¡¯s print some of the values of the variable just to see what they look like. Output: Now we have 2 options <U+2013> we can either summarize each article individually, or we can generate a single summary for all the articles. For our purpose, we will go ahead with the latter. Now the next step is to break the text into individual sentences. We will use the<U+00A0>sent_tokenize( )<U+00A0>function of the<U+00A0>nltk library to do this. Let¡¯s print a few elements of the list sentences. Output: GloVe word embeddings are vector representation of words. These word embeddings will be used to create vectors for our sentences. We could have also used the Bag-of-Words or TF-IDF approaches to create features for our sentences, but these methods ignore the order of the words (and the number of features is usually pretty large). We will be using the pre-trained Wikipedia 2014 + Gigaword 5 GloVe vectors available here. Heads up <U+2013> the size of these word embeddings is 822 MB. Let¡¯s extract the words embeddings or word vectors. We now have word vectors for 400,000 different terms stored in the dictionary <U+2013> ¡®word_embeddings¡¯. It is always a good practice to make your textual data noise-free as much as possible. So, let¡¯s do some basic text cleaning. Get rid of the<U+00A0>stopwords (commonly used words of a language <U+2013> is, am, the, of, in, etc.) present in the sentences. If you have not downloaded nltk-stopwords, then execute the following line of code: Now we can import the stopwords. Let¡¯s define a function to remove these stopwords from our dataset. We will use clean_sentences to create vectors for sentences in our data with the help of the GloVe word vectors. Now, let¡¯s create vectors for our sentences. We will first fetch vectors (each of size 100 elements) for the constituent words in a sentence and then take mean/average of those vectors to arrive at a consolidated vector for the sentence. The next step is to find similarities between the sentences, and we will use the cosine similarity approach for this challenge. Let¡¯s create an empty similarity matrix for this task and populate it with cosine similarities of the sentences. Let¡¯s first define a zero matrix of dimensions (n * n).<U+00A0> We will initialize this matrix with cosine similarity scores of the sentences. Here,<U+00A0>n is the number of sentences. We will use Cosine Similarity to compute the similarity between a pair of sentences. And initialize the matrix with cosine similarity scores. Before proceeding further, let¡¯s convert the similarity matrix sim_mat into a graph. The nodes of this graph will represent the sentences and the edges will represent the similarity scores between the sentences. On this graph, we will apply the PageRank algorithm to arrive at the sentence rankings. Finally, it¡¯s time to extract the top N sentences based on their rankings for summary generation. And there we go! An awesome, neat, concise, and useful summary for our articles. Automatic Text Summarization is a<U+00A0>hot topic of research, and in this article, we have covered just the tip of the iceberg. Going forward, we will explore the abstractive text summarization technique where deep learning plays a big role. In addition, we can also look into the following summarization tasks: Problem-specific Algorithm-specific I hope this post helped you in understanding the concept of automatic text summarization. It has a variety of use cases and has spawned extremely successful applications. Whether it¡¯s for leveraging in your business, or just for your own knowledge, text summarization is an approach all NLP enthusiasts should be familiar with. I will try to cover the<U+00A0>abstractive text summarization technique using advanced techniques in a future article. Meanwhile, feel free to use the comments section below to let me know your thoughts or ask any questions you might have on this article. Hi Prateek, Thanks for sharing. Since I¡¯m an absolute beginner, hope you don¡¯t me asking. Why did I get this error & how do I fix this? Thanks. NameError Traceback (most recent call last)
in ()
1 for s in df [¡®article_text¡¯]:
<U+2014>-> 2 sentences.append (sent_tokenize(s))
3 sentences = [y for x in sentences for y in x] #flatten list NameError: name ¡®sentences¡¯ is not defined"
"vidhya",2018-10-29,"DataHack Radio #13: Data Science and AI in the Oil & Gas Industry with Yogendra Pandey, Ph.D.","https://www.analyticsvidhya.com/blog/2018/10/datahack-radio-podcast-oil-gas-ai/","Did you know that the oil and gas industry is currently only using close to 1% of the data it generates? A mind-boggling figure, and not one we usually think about when talking about artificial intelligence and machine learning applications. In episode #13 of the DataHack Radio podcast, we are joined by Yogendra Narayan Pandey, Ph.D, as he takes us on a knowledge-rich journey in the world of oil and gas. This is not a field that grabs a lot of headlines in the AI and ML community, but as you¡¯ll find out in this podcast, the potential applications are vast. The amount of data collected in a typical oil and gas exploration and production process is staggeringly high, and that in turn spawns multiple use cases where machine learning techniques like regression, clustering and neural networks can be applied. Yogendra has done a phenomenal job of condensing the end-to-end oil and gas life-cycle into byte-sized knowledge for you and me to capture. It¡¯s well worth spending your time listening to this podcast and broadening your horizons. Happy listening! Subscribe to DataHack Radio today and listen to this, as well as all previous episodes, on any of the below platforms: Yogendra is the Founder and Managing Consultant at PRABUDDHA, an organization that provides AI solutions for the oil and energy industry. He<U+00A0>is a chemical engineer from IIT-Varanasi and successfully completed his Ph.D. from the University of Houston in the same field (his dissertation topic was ¡°A Simulation Approach to Thermodynamics in Interfacial Phenomena¡±). In his professional career, Yogendra has worked for organizations like Halliburton, Innowatts, and W.D. Von Gonten Laboratories. His role in all these organizations has been in the capacity of a data scientist. His passion for the oil and gas industry has driven him to pursue and make a mark in this field. In the initial part of the podcast, Yogendra has described his work in this fascinating space following his Ph.D. Anyone with an interest in data science and the energy field will really appreciate this episode! Oil and gas is a high-risk industry, so this makes the validation phase longer than usual. Decision makers have to be far more cautious, and this is one of the primary reasons why AI and ML have seen a slow adoption rate in this domain. But as Yogendra mentioned, this scenario is starting to change as technological advancements gather pace. One of the most important use cases of AI in oil and gas are predictive maintenance and equipment failure analytics. Another application is around autonomous drilling rigs, which means designing an end-to-end fully automatic drilling system. This system is smart enough to understand where to drill (optimal well path), how to drill, and the optimal duration required to finish the job. Like most AI applications these days, these autonomous rigs aim to augment the manual effort workers put in, rather than replace their jobs. To give you a very high-level overview, we can broadly divide the end-to-end oil and gas life-cycle (starting from a drop of oil found thousands of feet beneath the surface) into three major segments: For drilling operations, a large setup offshore can generate up to 1-2 terabytes of data everyday. The same goes for a large downstream refinery <U+2013> it can generate up to 1 terabyte of data per day. So if you were wondering where and how much data this industry can come up, this is a pretty good place to start! Each segment mentioned above has been explained eloquently and in detail with multiple examples by Yogendra in the podcast and trust me, the entire process is incredibly enthralling. My favorite part was about how a model can tell you whether a certain region has oil in it or not with a remarkably high accuracy rate (a probabilistic model). This helps the organization(s) decide whether it¡¯s worth drilling in that region. Unsupervised learning techniques like clustering are heavily leveraged in this process. Other algorithms used by data scientists in this domain for forecasting include regression, Hidden Markov Models for time series, Recurrent Neural Networks, Gated Recurrent Units (GRUs), and Long Short Term Memory (LSTMs), among others."
"vidhya",2018-10-29,"An Intuitive Guide to Interpret a Random Forest Model using fastai library (Machine Learning for Programmers <U+2013> Part 2)","https://www.analyticsvidhya.com/blog/2018/10/interpret-random-forest-model-machine-learning-programmers/","Machine Learning is a fast evolving field <U+2013> but a few things would remain as they were years ago. One such thing is ability to interpret and explain your machine learning models. If you build a model and can not explain it to your business users <U+2013> it is very unlikely that it will see the light of the day. Can you imagine integrating a model into your product without understanding how it works? Or which features are impacting your final result? In addition to backing from stakeholders, we as data scientists benefit from interpreting our work and improving upon it. It¡¯s a win-win situation all around! The first article of this fast.ai machine learning course saw an incredible response from our community. I¡¯m delighted to share part 2 of this series, which primarily deals with how you can interpret a random forest model. We will understand the theory and also implement it in Python to solidify our grasp on this critical concept. As always, I encourage you to replicate the code on your own machine while you go through the article. Experiment with the code and see how different your results are from what I have covered in this article. This will help you understand the different facets of both the random forest algorithm and the importance of interpretability. Before we dive into the next lessons of this course, let¡¯s quickly recap what we covered in the first two lessons. This will give you some context as to what to expect moving forward. We will continue working on the same dataset in this article. We will have a look at what are the different variables in the dataset and how can we build a random forest model to make valuable interpretations. Alright, it¡¯s time to fire up our Jupyter notebooks and dive right in to lesson#3! You can access the notebook for this lesson here. This notebook will be used for all the three lessons covered in this video. You can watch the entire lesson in the below video (or just scroll down and start implementing things right away): NOTE: Jeremy Howard regularly provides various tips that can be used for solving a certain problem more efficiently, as we saw in the previous article as well. A part of this video is about how to deal with very large datasets. I have included this in the last section of the article so we can focus on the topic at hand first.  Let¡¯s continue from where we left off at the end of lesson 2. We had created new features using the date column and dealt with the categorical columns as well. We will load the processed dataset which includes our newly engineered features and the<U+00A0>log of the<U+00A0>saleprice<U+00A0>variable (since the evaluation metric is RMSLE): We will define the necessary functions which we¡¯ll be frequently<U+00A0>using throughout our implementation. The next step will be to implement a random forest model and interpret the results to understand our dataset better. We have so far learned that random forest is a group of many trees, each trained on a different subset of data points and features. Each individual tree is as different as possible, capturing unique relations from the dataset. We make predictions by running each row through each tree and taking the average of the values at the leaf node. This average is taken as the final prediction for the row. While interpreting the results, it is necessary that the process is interactive and takes lesser time to run. To make this happen, we will make two changes in the code (as compared to what we implemented in the previous article): We¡¯re only using a sample as working with the entire data will take a long time to run. An important thing to note here is that the sample should not be very small. This might end up giving a different result and that¡¯ll be detrimental to our entire project. A sample size of 50,000 works well. Previously, we made predictions for each row using every single tree and then we calculated the mean of the results and the standard deviation. You might have noticed that this works in a sequential manner. Instead, we can call the predict function on multiple trees in parallel! This can be achieved using the<U+00A0>parallel_trees function in the fastai library. The time taken here is less and the results are exactly the same! We will now create a copy of the data so that any changes we make do not affect the original dataset. Once we have the predictions, we can calculate the RMSLE to determine how well the model is performing. But the overall value does not help us identify how close the predicted values are for a particular row or how confident we are that the predictions are correct. We will look at the standard deviation for the rows in this case. If a row is different from those present in the train set, each tree will give different values as predictions. This consequently means means that the standard deviation will be high. On the other hand, the trees would make almost similar predictions for a row that is quite similar to the ones present in the train set, t, i.e., the standard deviation will be low. So, based on the value of the standard deviations we can decide how confident we are about the predictions. Let¡¯s save these predictions and standard deviations: Now, let¡¯s take up a variable from the dataset and visualization it¡¯s distribution and understand what it actually represents. We¡¯ll begin with the<U+00A0>Enclosure variable. The actual sale price and the prediction values are almost similar in three categories <U+2013> ¡®EROPS¡¯, ¡®EROPS w AC¡¯, ¡®OROPS¡¯ (the remaining have null values). Since these null value columns do not add any extra information, we will drop them and visualize the plots for salesprice and prediction: Note that the small black bars represent standard deviation. In the same way, let¡¯s look at another variable <U+2013> ProductSize. We will take a ratio of the standard deviation values and the sum of predictions in order to compare which category has a higher deviation. The standard deviation is higher for the ¡®Large¡¯ and ¡®Compact¡¯ categories. Why do you that is? Take a moment to ponder the answer before reading on. Have a look at the bar plot of values for each category in ProductSize.<U+00A0>Found the reason? We have a lesser number of rows for these two categories. Thus, the model is giving a relatively poor prediction accuracy for these variables. Using this information, we can say that we are more confident about the predictions for the mini, medium and medium/large product size, and less confident about the small, compact and large ones. Feature importance is one of the key aspects of a machine learning model. Understanding which variable is contributing the most to a model is critical to interpreting the results. This is what data scientists strive for when building models that need to be explained to non-technical stakeholders. Our dataset has multiple features and it is often difficult to understand which feature is dominant. This is where the feature importance function of random forest is so helpful. Let¡¯s look at the top 10 most important features for our current model (including visualizing them by their importance): That¡¯s a pretty intuitive plot. Here¡¯s a bar plot visualization of the top 30 features: Clearly<U+00A0>YearMade is the most important feature, followed by Coupler_System.<U+00A0>The majority of the features seems to have little importance in the final model. Let¡¯s verify this statement by removing these features and checking whether this affects the model¡¯s performance. So, we will build a random forest model using only the features that have a feature importance greater than 0.005: When you think about it, removing redundant columns should not decrease the model score, right? And in this case, the model performance has slightly improved. Some of the features we dropped earlier might have been highly collinear with others, so removing them did not affect the model adversely. Let¡¯s check feature importance again to verify our hypothesis: The difference between the feature importance of the<U+00A0>YearMade and Coupler_System<U+00A0>variables is more significant. From the list of features removed, some features were highly collinear to YearMade, resulting in distribution of feature importance between them. On removing these features, we can see that the difference between the importance of YearMade and CouplerSystem has increased from the previous plot. Here is a detailed explanation of how feature importance is actually calculated: And that wraps up the implementation of lesson #3! I encourage you to try out these codes and experiment with them on your own machine to truly understand how each aspect of a random forest model works. In this lesson, Jeremy Howard gives a quick overview of lesson 3 initially before introducing a few important concepts like One Hot Encoding, Dendrogram, and Partial Dependence. Below is the YouTube video of the lecture (or you can jump straight to the implementation below): In the first article of the series, we learned that a lot of machine learning models cannot deal with categorical variables. Using proc_df, we converted the categorical variables into numeric columns. For example, we have a variable UsageBand,<U+00A0>which has three levels -¡®High¡¯, ¡®Low¡¯, and ¡®Medium¡¯. We replaced these categories with numbers (0, 1, 2) to make things easier for ourselves. Surely there must be another way of handling this that takes a significantly less effort on our end? There is! Instead of converting these categories into numbers, we can create separate columns for each category. The column UsageBand can be replaced with three columns: Each of these has 1s and 0s as the values. This is called one-hot encoding. What happens when there are far more than 3 categories? What if we have more than 10? Let¡¯s take an example to understand this. Assume we have a column ¡®zip_code¡¯ in the dataset which has a unique value for every row. Using one-hot encoding here will not be beneficial for the model, and will end up increasing the run time (a lose-lose scenario). Using proc_df in fastai, we can perform one-hot encoding by passing a parameter max_n_cat. Here, we have set the max_n_cat=7, which means that variables having levels more than 7 (such as zip code) will not be encoded, while all the other variables will be one-hot encoded. This can be helpful in determining if a particular level in a particular column is important or not. Since we have separated each level for the categorical variables, plotting feature importance will show us comparisons between them as well: Earlier,<U+00A0>YearMade was the most important feature in the dataset, but EROPS w AC has a higher feature importance in the above chart. Curious what this variable is? Don¡¯t worry, we will discuss what EROPS w AC actually represents in the following section. So far, we¡¯ve understood that having a high number of features can affect the performance of the model and also make it difficult to interpret the results. In this section, we will see how we can identify redundant features and remove them from the data. We will use cluster analysis, more specifically hierarchical clustering, to identify similar variables. In this technique, we look at every object and identify which of them are the closest in terms of features. These variables are then replaced by their midpoint. To understand this better, let us have a look at the cluster plot for our dataset: From the above dendrogram plot, we can see that the variables SaleYear and SaleElapsed are very similar to each other and tend to represent the same thing.<U+00A0>Similarly, Grouser_Tracks, Hydraulics_Flow, and Coupler_System<U+00A0>are highly correlated. The same happens with ProductGroup & ProductGroupDesc and fiBaseModel & fiModelDesc. We will remove each of these features one by one and see how it affects the model performance. First, we define a function to calculate the Out of Bag (OOB) score (to avoid repeating the same lines of code): For the sake of comparison, below is the original OOB score before dropping any feature: We will now drop one variable at a time and calculate the score: This hasn¡¯t heavily affected the OOB score. Let us now remove one variable from each pair and check the overall score: The score has changed from 0.8901 to 0.8885. We will use these selected features on the complete dataset and see how our model performs: Once these variables are removed from the original dataframe, the model¡¯s score turns out to be 0.907 on the validation set. I¡¯ll introduce another technique here that has the potential to help us understand the data better. This technique is called Partial Dependence and it¡¯s used to find out how features are related to the target variable. Let us compare YearMade and SalePrice. If you create a scatter plot for YearMade and SaleElapsed, you¡¯d notice that some vehicles were created in the year 1000, which is not practically possible. These could be the values which were initially missing and have been replaced with 1,000. To keep things practical, we will focus on values that are greater than 1930 for the YearMade variable<U+00A0>and create a plot using the popular ggplot package. This plot shows that the sale price is higher for more recently made vehicles, except for one drop between 1991 and 1997. There could be various reasons for this drop <U+2013> recession, customers preferred vehicles of lower price, or some other external factor. To understand this, we will create a plot that shows the relationship between YearMade and SalePrice, given that all other feature values are the same. This plot is obtained by fixing the YearMade for each row to 1960, then 1961, and so on. In simple words, we take a set of rows and calculate SalePrice for each row when YearMade is 1960. Then we take the whole set again and calculate SalePrice by setting YearMade to 1962. We repeat this multiple times, which results in the multiple blue lines we see in the above plot. The dark black line represents the average. This confirms our hypothesis that the sale price increases for more recently manufactured vehicles. Similarly, you can check for other features like SaleElapsed, or YearMade and SaleElpased together. Performing the same step for the categories under Enclosure (since Enclosure_EROPS w AC proved to be one of the most important features), the resulting plot looks like this: Enclosure_EROPS w AC seems to have a higher sale price as compared to the other two variables (which have almost equal values). So what in the world is EROPS? It¡¯s an enclosed rollover protective structure which can be with or without an AC. And obviously, EROPS with an AC will have a higher sale price. Tree interpreter in another interesting technique that analyzes each individual row in the dataset. We have seen so far how to interpret a model, and how each feature (and the levels in each categorical feature) affect the model predictions. So we will now use this tree interpreter concept and visualize the predictions for a particular row. Let¡¯s import the tree interpreter library and evaluate the results for the first row in the validation set. These are the original values for first row (and it¡¯s every column) in the validation set. Using tree interpreter, we will make predictions for the same using a random forest model. Tree interpreter gives three results <U+2013> prediction, bias and contribution. The value of Coupler_System < 0.5 increased the value from 10.189 to 10.345 and enclosure less than 0.2 reduced the value from 10.345 to 9.955, and so on. So the contributions will represent this change in the predicted values. To understand this in a better way, take a look at the table below: In this table, we have stored the value against each feature and the split point (verify from the image above). The change is the difference between the value before and after the split. These are plotted using a waterfall chart in Excel. The change seen here is for an individual tree. An average of change across all the trees in the random forest is given by contribution in the tree interpreter. Printing the prediction and bias for the first row in our validation set: The value of contribution of each feature in the dataset for this first row: Note: If you are watching the video simultaneously with this article, the values may differ. This is because initially the values were sorted based on index which presented incorrect information. This was corrected in the later video and also in the notebook we have been following throughout the lesson. You should have a pretty good understanding of the random forest algorithm at this stage. In lesson #5, we will focus on how to identify whether model is generalizing well or not. Jeremy Howard also talks about tree interpreters, contribution, and understanding the same using a waterfall chart (which we have already covered in the previous lesson, so will not elaborate on this further).<U+00A0> The primary focus of the video is on Extrapolation and understanding how we can build a random forest algorithm from scratch. A model might not perform well if it¡¯s built on data spanning four years and then used to predict the values for the next one year. In other words, the model does not extrapolate. We have previously seen that there is a significant difference between the training score and validation score, which might be because our validation set consists of a set of recent data points (and the model is using time dependent variables for making predictions). Also, the validation score is worse than the OOB score<U+00A0>which should not be the case, right? A detailed explanation of the OOB score has been given in part 1 of the series. One way of fixing this problem is by attacking it directly <U+2013> deal with the time dependent variables. To figure out which variables are time dependent, we will create a random forest model that tries to predict if a particular row is in the validation set or not. Then we will check which variable has the highest contribution in making a successful prediction. Defining the target variable: The model is able to separate the train and validation sets with a r-square value 0.99998, and the most important features are SaleID, SaleElapsed, MachineID.  It is evident from the tables above that the mean value of these three variables is significantly different. We will drop these variables, fit the random forest again and check the feature importance: Although these variables are obviously time dependent, they can also be important for making the predictions. Before we drop these variables, we need to check how they affect the OOB score. The initial OOB score in a sample is calculated for comparison: Dropping each feature one by one: Looking at the results, age, MachineID and SaleDayofYear actually improved the score while others did not. So, we will remove the remaining variables and fit the random forest on the complete dataset. After removing the time dependent variables, the validation score (0.915) is now better than the OOB score (0.909). We can now play around with other parameters like n_estimator on max_features. To create the final model, Jeremy increased the number of trees to 160 and here are the results: The validation score is 0.92 while the RMSE drops to 0.21. A great improvement indeed! We have learned about how a random forest model actually works, how the features are selected and how predictions are eventually made. In this section, we will create our own random forest model from absolute scratch. Here is the notebook for this section : Random Forest from scratch. We¡¯ll start with importing the basic libraries: We¡¯ll just use two variables to start with. Once we are confident that the model works well with these selected variables, we can use the complete set of features. We have loaded the dataset, split it into train and validation sets, and selected two features <U+2013><U+00A0>YearMade and MachineHoursCurrentMeter.<U+00A0>The first thing to think about while building any model from scratch is <U+2013> what information do we need? So, for a random forest, we need: Let¡¯s define a class with the inputs as mentioned above and set the random seed to 42. We have created a function create_trees that will be called as many times as the number assigned to n_trees. The function create_trees generates<U+00A0>a randomly shuffled set of rows (of size = sample_sz) and returns DecisionTree. We¡¯ll see DecisionTree in a while, but first let¡¯s figure out how predictions are created and saved. We learned earlier that in a random forest model, each single tree makes a prediction for each row and the final prediction is calculated by taking the average of all the predictions. So we will create a predict function, where .predict is used on every tree to create a list of predictions and the mean of this list is calculated as our final value. The final step is to create the DecisionTree. We first select a feature and split point that gives the least error. At present, this code is only for a single decision. We can make this recursive if the code runs successfully. self.n defines the number of rows used in each tree and self.c is the number of columns. Self.val calculates the mean of predictions for each index. This code is still incomplete and will be continued in the next lesson. Yes, part 3 is coming soon! I consider this one of the most important articles in this ongoing series. I cannot stress enough on how important model interpretability is. In real-life industry scenarios, you will quite often face the situation of having to explain the model¡¯s results to the stakeholder (who is usually a non-technical person). Your chances of getting the model approved will lie in how well you are able to explain how and why the model is behaving the way it is. Plus it¡¯s always a good idea to always explain any model¡¯s performance to yourself in a way that a layman will understand <U+2013> this is always a good practice! Use the comments section below to let me know your thoughts or ask any questions you might have on this article. And as I mentioned, part 3 is coming soon so stay tuned! The link to the dataset is not working."
"vidhya",2018-10-29,"MADRaS: A Multi-Agent DRiving Simulator for Autonomous Driving Research","https://www.analyticsvidhya.com/blog/2018/10/madras-multi-agent-driving-simulator/","In this article, we present MADRaS: Multi-Agent DRiving Simulator. It is a multi-agent version of<U+00A0>TORCS, a racing simulator popularly used for autonomous driving research by the reinforcement learning and imitation learning communities. You can read more about TORCS in the below resources: MADRaS is a multi-agent extension of<U+00A0>Gym-TORCS<U+00A0>and is open source, lightweight, easy to install, and has the OpenAI Gym API, which makes it ideal for beginners in autonomous driving research. It enables independent control of tens of agents within the same environment, opening up a prolific direction of research in multi-agent reinforcement learning and imitation learning research aimed at acquiring human-like negotiation skills in complicated traffic situations<U+2014>a major challenge in autonomous driving that all major players are racing to solve. Most open-source autonomous driving simulators (like<U+00A0>CARLA*,<U+00A0>DeepDrive,<U+00A0>AirSim, and<U+00A0>Udacity* SDC) innately support only egocentric control; that is, single agent behavior, and have preprogrammed behaviors for the other agents. The difficulty in introducing agents with custom behaviors in these simulators restricts the diversity of real-world scenarios that can be simulated. To address this issue, we developed MADRaS, wherein each car on the racing track can be independently controlled, enabling the creation of rich, custom-made traffic scenarios, and learning the policy of control of multiple agents simultaneously. The task of negotiation in traffic can be posed as that of finding the winning strategy in a multi-agent game, wherein multiple entities (cars, buses, two-wheelers, and pedestrians) are trying to achieve their objectives of getting from one place to another fast, yet safely and reliably. Imitation learning algorithms like Behavioral Cloning, Active Learning, and Apprenticeship Learning (Inverse Reinforcement Learning followed by Reinforcement Learning) have proved to be effective for learning such sophisticated behaviors, under a multitude of simplifying assumptions and constraining conditions. A major portion of the contemporary literature makes the single-agent assumption; that is, the agent acts in an environment with a plethora of other agents<U+2014>similar or different<U+2014>but does not<U+00A0>interact<U+00A0>with any of them, robbing it of data and information that could potentially be extremely useful in decision making, at both the egocentric and collaborative levels. Driving, however, is inherently multi-agent, and the following is a partial list of things that become possible once we get rid of the single-agent assumption. Source: eDriving One of the earliest instances of multi-agent systems being deployed in vehicles (starting<U+00A0>way back in 1993!) was in the use of platooning, wherein vehicles travel at highway speeds with small inter-vehicle spacing to reduce congestion and still achieve high throughput without compromising safety. Now it seems obvious that autonomous cars in the near future will communicate, cooperate, and form platoons over intersecting lengths of their commutes. Source: phys.org Apart from transferring information about pile-ups and possible diversions ahead to all the vehicles in the geographical vicinity, this power of reliable communication can be used to pool together the<U+00A0>knowledge<U+00A0>of multiple learning agents. An intuitive motivation could be to consider a large gridworld. With a single learning agent, one could<U+00A0>solve<U+00A0>the gridworld in<U+00A0>n<U+00A0>hours of training. With multiple learning agents pooling their experiences, we could cut down the training time significantly, possibly even linearly! There¡¯s a host of untapped literature<U+00A0>on communication among multiple agents in various environments (not autonomous driving¡¦ yet.) See: Now this raises important questions about the reliability of the communication between vehicles. With the imminent advent of 5G,1<U+00A0>fast and reliable communication between vehicles can help lead to the training and deployment of completely hands-free autonomous cars. Drivers on the road constantly anticipate the potential actions of fellow drivers. As an example, for close maneuvering in car parks and intersections, eye contact is made to ensure a shared understanding. Defense Advanced Research Projects Agency (DARPA) stated that traffic vehicle drivers, unnerved by being unable to make eye contact with the robots, had resorted to watching the front wheels of the robots for an indication of their intent. Source: The Star Multi-agent learning comes with its own share of complications: But remember why we started solving fully autonomous driving (FAD) in the first place. Writing for<U+00A0>Technology Review, Will Knight<U+00A0>outlines the possibilities of our driverless car future: The list goes on.. So, today we¡¯re excited to release MADRaS for the community to kickstart research into making FAD a reality. With the ability of introducing multiple learning agents in the environment at the same time, this simulator, built on top of<U+00A0>TORCS, can be used to benchmark and try out existing and new multi-agent learning algorithms for self-driving cars such as: Multi-Agent Deep Deterministic Policy Gradient (MADDPG),<U+00A0>PSMADDPG, and the lot. And since this extends TORCS, it supports the deployment of all the single-agent learning algorithms as well. Scripts for training a DDPG agent are provided as a sample. Check out the following video for an overview of the features and the general interface. This project was developed by<U+00A0>Abhishek Naik<U+00A0>and<U+00A0>Anirban Santara<U+00A0>(an Intel¢ç Student Ambassador for AI) during their internship at the Parallel Computing Lab, Intel Labs, Bangalore, India. This project was driven by Intel¡¯s urge to address the absence of an open source multi-agent autonomous driving simulator that can be utilized by machine learning (particularly, reinforcement learning) scientists to rapidly prototype and evaluate their ideas. Although the system was developed and optimized entirely on the Intel¢ç<U+00A0>Core¢â i7 processor and Intel¢ç<U+00A0>Xeon¢ç<U+00A0>processors, we believe that it would run smoothly on other<U+00A0>x86 platforms, too. Currently, we are working on integrating MADRaS with the<U+00A0>Intel¢ç<U+00A0>Nervana¢âplatform Reinforcement Learning Coach<U+00A0>and we invite the community to participate in its development."
"vidhya",2018-10-29,"A Computer Vision Approach to Hand Gesture Recognition","https://www.analyticsvidhya.com/blog/2018/10/computer-vision-approach-hand-gesture-recognition/","Soldiers communicate with each other through gestures. But sometimes those gestures are not visible due to obstructions or poor lighting. For that purpose, an instrument is required to record the gesture and send it to the fellow soldiers. The two options for gesture recognition are through Computer Vision and through some sensors attached to the hands. The first option is not viable in this case as proper lighting is required for recognition through Computer Vision. Hence the second option of using sensors for recognitions has been used. We present a system which recognizes the gestures given in this<U+00A0>link. The given gestures include motions of fingers, wrist, and elbow. To detect any changes in them we have used flex sensors which detect the amount by which it has been bent at each of these joints. To take into account for the dynamic gestures an Inertial Measurement Unit (IMU-MPU-9250) was used. The parameters used from the IMU are acceleration, gyroscopic acceleration, and angles in all three axes. An Arduino* Mega was used to receive the signals from the sensors and send it to the processor. A flex sensor is a strip which has a resistance proportional to the amount of strain in the sensor. Thus it gives out a variable voltage value according to the strain. An IMU (MPU-6050) gives out linear acceleration and gyroscopic acceleration in all three axes (x, y, z). The gestures can be classified into two sub-classes: The number of features primarily used for both the sub classes differ First of all the angles have to be calculated from the acceleration values using these formulae. The angle values have some noise in them and thus have to be filtered out in order to get smooth values out of it. Thus we have used a Kalman filter for filtering the values. Then both the flex sensor values and angles are fed into a pre-trained Support Vector Machine (SVM) with Radial Basis Function (Gaussian) Kernel. And thus the output is obtained. Figure 1: Principal Component Analysis of the dataset using all the features. Each of the colored cluster represents a particular gesture. As accelerations are also included the clusters are quite elongated. Figure 2: Principal Component Analysis of the dataset using just flex sensor values and angles. Here each colored cluster represents a particular gesture. Also these clusters are classifiable. The angles, liner accelerations, and gyroscopic accelerations are filtered using a Kalman Filter. The values are stores in a temporary file with each line representing one time point. Then every value is normalized column-wise. Then 50 time points are sampled out of them. After that they are linearized into one single vector of 800 dimensions. Then it is fed into a SVM with Radial Basis Function kernel (Gaussian). Because some gestures like ¡®Column Formation¡¯, ¡®Vehicle¡¯, ¡®Ammunition¡¯, and ¡®Rally-Point¡¯ are similar to each other we have grouped such similar features as one class. If the first SVM classifies into one of these groups then they are fed into another SVM which is trained just to classify the gestures in that group. Figure 3: Two samples of graph of x-axis acceleration the gesture door. Salient Features of the system: *Since we were told to show the output on a screen we have not used a Raspberry Pi Zero (microprocessor) for processing purposes. But it can be used for that and we have checked the feasibility of the algorithm¡¯s speed in that processor also."
