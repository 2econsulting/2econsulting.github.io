"site","date","headline","url_address","text"
"mastery",2018-10-12,"How to Load, Visualize, and Explore a Complex Multivariate Multistep Time Series Forecasting Dataset","https://machinelearningmastery.com/how-to-load-visualize-and-explore-a-complex-multivariate-multistep-time-series-forecasting-dataset/","Real-world time series forecasting is challenging for a whole host of reasons not limited to problem features such as having multiple input variables, the requirement to predict multiple time steps, and the need to perform the same type of prediction for multiple physical sites. The EMC Data Science Global Hackathon dataset, or the ¡®Air Quality Prediction¡® dataset for short, describes weather conditions at multiple sites and requires a prediction of air quality measurements over the subsequent three days. In this tutorial, you will discover and explore the Air Quality Prediction dataset that represents a challenging multivariate, multi-site, and multi-step time series forecasting problem. After completing this tutorial, you will know: Let¡¯s get started. How to Load, Visualize, and Explore a Complex Multivariate Multistep Time Series Forecasting DatasetPhoto by H Matthew Howarth, some rights reserved. This tutorial is divided into seven parts; they are: The EMC Data Science Global Hackathon dataset, or the ¡®Air Quality Prediction¡® dataset for short, describes weather conditions at multiple sites and requires a prediction of air quality measurements over the subsequent three days. Specifically, weather observations such as temperature, pressure, wind speed, and wind direction are provided hourly for eight days for multiple sites. The objective is to predict air quality measurements for the next three days at multiple sites. The forecast lead times are not contiguous; instead, specific lead times must be forecast over the 72 hour forecast period; they are: Further, the dataset is divided into disjoint but contiguous chunks of data, with eight days of data followed by three days that require a forecast. Not all observations are available at all sites or chunks and not all output variables are available at all sites and chunks. There are large portions of missing data that must be addressed. The dataset was used as the basis for a short duration machine learning competition (or hackathon) on the Kaggle website in 2012. Submissions for the competition were evaluated against the true observations that were withheld from participants and scored using Mean Absolute Error (MAE). Submissions required the value of -1,000,000 to be specified in those cases where a forecast was not possible due to missing data. In fact, a template of where to insert missing values was provided and required to be adopted for all submissions (what a pain). A winning entrant achieved a MAE of 0.21058 on the withheld test set (private leaderboard) using random forest on lagged observations. A writeup of this solution is available in the post: In this tutorial, we will explore this dataset in order to better understand the nature of the forecast problem and suggest approaches for how it may be modeled. The first step is to download the dataset and load it into memory. The dataset can be downloaded for free from the Kaggle website. You may have to create an account and log in, in order to be able to download the dataset. Download the entire dataset, e.g. ¡°Download All¡± to your workstation and unzip the archive in your current working directory with the folder named ¡®AirQualityPrediction¡® You should have five files in the AirQualityPrediction/ folder; they are: Our focus will be the ¡®TrainingData.csv¡® that contains the training dataset, specifically data in chunks where each chunk is eight contiguous days of observations and target variables. The test dataset (remaining three days of each chunk) is not available for this dataset at the time of writing. Open the ¡®TrainingData.csv¡® file and review the contents. The unzipped data file is relatively small (21 megabytes) and will easily fit into RAM. Reviewing the contents of the file, we can see that the data file contains a header row. We can also see that missing data is marked with the ¡®NA¡® value, which Pandas will automatically convert to NumPy.NaN. We can see that the ¡®weekday¡® column contains the day as a string, whereas all other data is numeric. Below are the first few lines of the data file for reference. We can load the data file into memory using the Pandas read_csv() function and specify the header row on line 0. We can also get a quick idea of how much missing data there is in the dataset. We can do that by first trimming the first few columns to remove the string weekday data and convert the remaining columns to floating point values. We can then calculate the total number of missing observations and the percentage of values that are missing. The complete example is listed below. Running the example first prints the shape of the loaded dataset. We can see that we have about 37,000 rows and 95 columns. We know these numbers are misleading given that the data is in fact divided into chunks and the columns are divided into the same observations at different sites. We can also see that a little over 40% of the data is missing. This is a lot. The data is very patchy and we are going to have to understand this well before modeling the problem. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course A good starting point is to look at the data in terms of the chunks. We can group data by the ¡®chunkID¡¯ variable (column index 1). If each chunk is eight days and the observations are hourly, then we would expect (8 * 24) or 192 rows of data per chunk. If there are 37,821 rows of data, then there must be chunks with more or less than 192 hours as 37,821/192 is about 196.9 chunks. Let¡¯s first split the data into chunks. We can first get a list of the unique chunk identifiers. We can then collect all rows for each chunk identifier and store them in a dictionary for easy access. Below defines a function named to_chunks() that takes a NumPy array of the loaded data and returns a dictionary of chunk_id to rows for the chunk. The ¡®position_within_chunk¡® in the data file indicates the order of a row within a chunk. At this stage, we assume that rows are already ordered and do not need to be sorted. A skim of the raw data file seems to confirm this assumption. Once the data is sorted into chunks, we can calculate the number of rows in each chunk and have a look at the distribution, such as with a box and whisker plot. The complete example that ties all of this together is listed below Running the example first prints the number of chunks in the dataset. We can see that there are 208, which suggests that indeed the number of hourly observations must vary across the chunks. A box and whisker plot and a histogram plot of chunk durations is created. We can see that indeed the median is 192, meaning that most chunks have eight days of observations or close to it. We can also see a long tail of durations down to about 25 rows. Although there are not many of these cases, we would expect that will be challenging to forecast given the lack of data. The distribution also raises questions about how contiguous the observations within each chunk may be. Box and whisker plot and Histogram plot of chunk duration in hours It may be helpful to get an idea of how contiguous (or not) the observations are within those chunks that do not have the full eight days of data. One approach to considering this is to create a line plot for each discontiguous chunk and show the gaps in the observations. We can do this on a single plot. Each chunk has a unique identifier, from 1 to 208, and we can use this as the value for the series and mark missing observations within the eight day interval via NaN values that will not appear on the plot. Inverting this, we can assume that we have NaN values for all time steps within a chunk, then use the ¡®position_within_chunk¡® column (index 2) to determine the time steps that do have values and mark them with the chunk id. The plot_discontinuous_chunks() below implements this behavior, creating one series or line for each chunk with missing rows all on the same plot. The expectation is that breaks in the line will help us see how contiguous or discontiguous these incomplete chunks happen to be. The complete example is listed below. Running the example creates a single figure with one line for each of the chunks with missing data. The number and lengths of the breaks in the line for each chunk give an idea of how discontiguous the observations within each chunk happen to be. Many of the chunks do have long stretches of contiguous data, which is a good sign for modeling. There are cases where chunks have very few observations and those observations that are present are in small contiguous patches. These may be challenging to model. Further, not all of these chunks have observations at the end of chunk: the period right before a forecast is required. These specifically will be a challenge for those models that seek to persist recent observations. The discontinuous nature of the series data within the chunks will also make it challenging to evaluate models. For example, one cannot simply split chunk data in half, train on the first half and test on the second when the observations are patchy. At least, when the incomplete chunk data is considered. Line plots of chunks with discontinuous observations The discontiguous nature of the chunks also suggests that it may be important to look at the hours covered by each chunk. The time of day is important in environmental data, and models that assume that each chunk covers the same daily or weekly cycle may stumble if the start and end time of day vary across chunks. We can quickly check this by plotting the distribution of the first hour (in a 24 hour day) of each chunk. The number of bins in the histogram is set to 24 so we can clearly see the distribution for each hour of the day in 24-hour time. Further, when collecting the first hour of the chunk, we are careful to only collect it from those chunks that have all eight days of data, in case a chunk with missing data does not have observations at the beginning of the chunk, which we know happens. The complete example is listed below. Running the example creates a box and whisker plot and a histogram of the first hour within each chunk. We can see a reasonably uniform distribution of the start time across the 24 hours in the day. Further, this means that the interval to be forecast for each chunk will also vary across the 24 hour period. This adds a wrinkle for models that might expect a standard three day forecast period (midnight to midnight). Distribution of first hour in observations within each chunk Now that we have some idea of the chunk-structure of the data, let¡¯s take a closer look at the input variables that describe the meteorological observations. There are 56 input variables. The first six (indexes 0 to 5) are metadata information for the chunk and time of the observations. They are: The remaining 50 describe meteorological information for specific sites; they are: Really, there are only eight meteorological input variables: These variables are recorded across 23 unique sites; they are: The data is beautifully complex. Not all variables are recorded at all sites. There is some overlap in the site identifiers used in the target variables, such as 1, 50, 64, etc. There are site identifiers used in the target variables that are not used in the input variables, such as 4002. There are also site identifiers that are used in the input that are not used in the target identifiers, such as 15. This suggests, at the very least, that not all variables are recorded at all locations. That recording stations are heterogeneous across sites. Further, there might be something special about sites that only collect measures of a given type or collect all measurements. Let¡¯s take a closer look at the data for the input variables. We can start off by looking at the structure and distribution of inputs per chunk. The first few chunks that have all eight days of observations have the chunkId of 1, 3, and 5. We can enumerate all of the input columns and create one line plot for each. This will create a time series line plot for each input variable to give a rough idea of how each varies across time. We can repeat this for a few chunks to get an idea how the temporal structure may differ across chunks. The function below named plot_chunk_inputs() takes the data in chunk format and a list of chunk ids to plot. It will create a figure with 50 line plots, one for each input variable, and n lines per plot, one for each chunk. The complete example is listed below. Running the example creates a single figure with 50 line plots, one for each of the meteorological input variables. The plots are hard to see, so you may want to increase the size of the created figure. We can see that the observations for the first five variables look pretty complete; these are solar radiation, wind speed, and wind direction. The rest of the variables appear pretty patchy, at least for this chunk. Parallel Time Series Line Plots For All Input Variables for 1 Chunks We can update the example and plot the input variables for the first three chunks with the full eight days of observations. Running the example creates the same 50 line plots, each with three series or lines per plot, one for each chunk. Again, the figure makes the individual plots hard to see, so you may need to increase the size of the figure to better review the patterns. We can see that these three figures do show similar structures within each line plot. This is helpful finding as it suggests that it may be useful to model the same variables across multiple chunks. Parallel Time Series Line Plots For All Input Variables for 3 Chunks It does raise the question as to whether the distribution of the variables differs greatly across sites. We can look at the distribution of input variables crudely using box and whisker plots. The plot_chunk_input_boxplots() below will create one box and whisker per input feature for the data for one chunk. The complete example is listed below. Running the example creates 50 boxplots, one for each input variable for the observations in the first chunk in the training dataset. We can see that variables of the same type may have the same spread of observations, and each group of variables appears to have differing units. Perhaps degrees for wind direction, hectopascales for pressure, degrees Celsius for temperature, and so on. Box and whisker plots of input variables for one chunk It may be interesting to further investigate the distribution and spread of observations for each of the eight variable types. This is left as a further exercise. We have some rough ideas about the input variables, and perhaps they may be useful in predicting the target variables. We cannot be sure. We can now turn our attention to the target variables. The goal of the forecast problem is to predict multiple variables across multiple sites for three days. There are 39 time series variables to predict. From the column header, they are: The naming convention for these column headers is: We can convert these column headers into a small dataset of variable ids and site ids with a little regex. The results are as follows: Helpfully, the targets are grouped by variable id. We can see that one variable may have to be predicted across multiple sites; for example, variable 11 predicted at sites 1, 32, 50, and so on: We can see that different variables may need to be predicted for a given site. For example, site 50 requires variables 11, 3, and 4: We can save the small dataset of targets to a file called ¡®targets.txt¡® and load it up for some quick analysis. Running the example prints the number of unique variables and sites. We can see that 39 target variables is far less than (12*14) 168 if we were predicting all variables for all sites. Let¡¯s take a closer look at the data for the target variables. We can start off by looking at the structure and distribution of targets per chunk. The first few chunks that have all eight days of observations have the chunkId of 1, 3, and 5. We can enumerate all of the target columns and create one line plot for each. This will create a time series line plot for each target variable to give a rough idea of how it varies across time. We can repeat this for a few chunks to get a rough idea of how the temporal structure may vary across chunks. The function below, named plot_chunk_targets(), takes the data in chunk format and a list of chunk ids to plot. It will create a figure with 39 line plots, one for each target variable, and n lines per plot, one for each chunk. The complete example is listed below. Running the example creates a single figure with 39 line plots for chunk identifier ¡°1¡±. The plots are small, but give a rough idea of the temporal structure for the variables. We can see that there are more than a few variables that have no data for this chunk. These cannot be forecasted directly, and probably not indirectly. This suggests that in addition to not having all variables for all sites, that even those specified in the column header may not be present for some chunks. We can also see breaks in some of the series for missing values. This suggests that even though we may have observations for each time step within the chunk, that we may not have a contiguous series for all variables in the chunk. There is a cyclic structure to many of the plots. Most have eight peaks, very likely corresponding to the eight days of observations within the chunk. This seasonal structure could be modeled directly, and perhaps removed from the data when modeling and added back to the forecasted interval. There does not appear to be any trend to the series. Parallel Time Series Line Plots For All Target Variables for 1 Chunk We can re-run the example and plot the target variables for the first three chunks with complete data. Running the example creates a figure with 39 plots and three time series per plot, one for the targets for each chunk. The plot is busy, and you may want to increase the size of the plot window to better see the comparison across the chunks for the target variables. For many of the variables that have a cyclic daily structure, we can see the structure repeated across the chunks. This is encouraging as it suggests that modeling a variable for a site may be helpful across chunks. Further, plots 3-to-10 correspond to variable 11 across seven different sites. The string similarity in temporal structure across these plots suggest that modeling the data per variable which is used across sites may be beneficial. Parallel Time Series Line Plots For All Target Variables for 3 Chunks It is also useful to take a look at the distribution of the target variables. We can start by taking a look at the distribution of each target variable for one chuck by creating box and whisker plots for each target variable. A separate boxplot can be created for each target side-by-side, allowing the shape and range of values to be directly compared on the same scale. The complete example is listed below. Running the example creates a figure containing 39 boxplots, one for each of the 39 target variables for the first chunk. We can see that many of the variables have a median close to zero or one; we can also see a large asymmetrical spread for most variables, suggesting the variables likely have a skew with outliers. It is encouraging that the boxplots from 4-10 for variable 11 across seven sites show a similar distribution. This is further supporting evidence that data may be grouped by variable and used to fit a model that could be used across sites. Box and whisker plots of target variables for one chunk We can re-create this plot using data across all chunks to see dataset-wide patterns. The complete example is listed below. Running the example creates a new figure showing 39 box and whisker plots for the entire training dataset regardless of chunk. It is a little bit of a mess, where the circle outliers obscure the main data distributions. We can see that outlier values do extend into the range 5-to-10 units. This suggests there might be some use in standardizing and/or rescaling the targets when modeling. Perhaps the most useful finding is that there are some targets that do not have any (or very much) data regardless of chunk. These columns probably should be excluded from the dataset. Box and whisker plots of target variables for all training data We can investigate the apparent missing data further by creating a bar chart of the ratio of missing data per column, excluding the metadata columns at the beginning (e.g. the first five columns). The plot_col_percentage_missing() function below implements this. The complete example is listed below. Running the example first prints the column id (zero offset) and the ratio of missing data, if the ratio is above 90%. We can see that there are in fact no columns with zero non-NaN data, but perhaps two dozen (12) that have above 90% missing data. Interestingly, seven of these are target variables (index 56 or higher). A bar chart of column index number to ratio of missing data is created. We can see that there might be some stratification to the ratio of missing data, with a cluster below 10%, a cluster around 70%, and a cluster above 90%. We can also see a separation between input variable and target variables where the former are quite regular as they show the same variable type measured across different sites. Such small amounts of data for some target variables suggest the need to leverage other factors besides past observations in order to make predictions. Bar Chart of Percentage of Missing Data Per Column The distribution of the target variables are not neat and may be non-Gaussian at the least, or highly multimodal at worst. We can check this by looking at histograms of the target variables, for the data for a single chunk. A problem with the hist() function in matplotlib is that it is not robust to NaN values. We can overcome this by checking that each column has non-NaN values prior to plotting and excluding the rows with NaN values. The function below does this and creates one histogram for each target variable for one or more chunks. The complete example is listed below. Running the example creates a figure with 39 histograms, one for each target variable for the first chunk. The plot is hard to read, but the large number of bins goes to show the distribution of the variables. It might be fair to say that perhaps none of the target variables have an obvious Gaussian distribution. Many may have a skewed distribution with a long right tail. Other variables have what appears to be quite a discrete distribution that might be an artifact of the chosen measurement device or measurement scale. Histograms for each target variable for one chunk We can re-create the same plot with target variables for all chunks. The complete example is listed below. Running the example creates a figure with 39 histograms, one for each of the target variables in the training dataset. We can see fuller distributions, which are more insightful. The first handful of plots perhaps show a highly skewed distribution, the core of which may or may not be Gaussian. We can see many Gaussian-like distributions with gaps, suggesting discrete measurements imposed on a Gaussian-distributed continuous variable. We can also see some variables that show an exponential distribution. Together, this suggests either the use of power transforms to explore reshaping the data to be more Gaussian, and/or the use of nonparametric modeling methods that are not dependent upon a Gaussian distribution of the variables. For example, classical linear methods may be expected to have a hard time. Histograms for each target variable for the entire training dataset After the end of the competition, the person who provided the data, David Chudzicki, summarized the true meaning of the 12 output variables. This was provided in a form post titled ¡°what the target variables really were¡°, reproduced partially below: This is interesting as we can see that the target variables are meteorological in nature and related to air quality as the name of the competition suggests. A problem is that there are 15 variables and only 12 different types of target variables in the dataset. The cause of this problem is that the same target variable in the dataset may be used to represent different target variables. Specifically: From the names of the variables, the doubling-up of data into the same target variable was done so with variables with differing chemical characters and perhaps even measures, e.g. it appears to be accidental rather than strategic. It is not clear, but it is likely that a target represents one variable within a chunk but may represent different variables across chunks. Alternately, it may be possible that the variables differ across sites within each chunk. In the former case, it means that models that expect consistency in these target variables across chunks, which is a very reasonable assumption, may have difficulty. In the latter, models can treat the variable-site combinations as distinct variables. It may be possible to tease out the differences by comparing the distribution and scales of these variables across chunks. This is disappointing, and depending on how consequential it is to model skill, it may require the removal of these variables from the dataset, which are a lot of the target variables (20 of 39). In this section, we will harness what we have discovered about the problem and suggest some approaches to modeling this problem. I like this dataset; it is messy, realistic, and resists naive approaches. This section is divided into four sections; they are: The problem is generally framed as a multivariate multi-step time series forecasting problem. Further, the multiple variables are required to be forecasted across multiple sites, which is a common structural breakdown for time series forecasting problems, e.g. predict the variable thing at different physical locations such as stores or stations. Let¡¯s walk through some possible framings of the data. A first-cut approach might be to treat each variable at each site as a univariate time series forecasting problem. A model is given eight days of hourly observations for a variable and is asked to forecast three days, from which a specific subset of forecast lead times are taken and used or evaluated. It may be possible in a few select cases, and this could be confirmed with some further data analysis. Nevertheless, the data generally resists this framing because not all chunks have eight days of observations for each target variable. Further, the time series for the target variable can be dramatically discontiguous, if not mostly (90%-to-95%) incomplete. We could relax the expectation of the structure and amount of prior data required by the model, designing the model to make use of whatever is available. This approach would require 39 models per chunk and a total of (208 * 39) or 8,112 separate models. It sounds possible, but perhaps less scalable than we may prefer from an engineering perspective. The variable-site combinations could be modeled across chunks, requiring only 39 models. The target variables can be aggregated across sites. We can also relax what lag lead times are used to make a forecast and present what is available either with zero-padding or imputing for missing values, or even lag observations that disregard lead time. We can then frame the problem as given some prior observations for a given variable, forecast the following three days. The models may have more to work with, but will disregard any variable differences based on site. This may or may not be reasonless and could be checked by comparing variable distributions across sites. There are 12 unique variables. We could model each variable per chunk, giving (208 * 12) or 2,496 models. It might make more sense to model the 12 variables across chunks, requiring only 12 models. Perhaps one or more target variables are dependent on one or more of the meteorological variables, or even on the other target variables. This could be explored by investigating the correlation between each target variable and each input variable, as well as with the other target variables. If such dependencies exist, or could be assumed, it may be possible to not only forecast the variables with more complete data, but also those target variables with above 90% missing data. Such models could use some subset of prior meteorological observations and/or target variable observations as input. The discontiguous nature of the data may require the relaxing of the traditional lag temporal structure for the input variables, allowing the model to use whatever was available for a specific forecast. Depending on the choice of model, the input and target variables may benefit from some data preparation, such as: To address the missing data, in some cases imputing may be required with simple persistence or averaging. In other cases, and depending on the choice of model, it may be possible to learn directly from the NaN values as observations (e.g. XGBoost can do this) or to fill with 0 values and mask the inputs (e.g. LSTMs can do this). It may be interesting to investigate downscaling input to 2, 4, or 12, hourly data or similar in an attempt to fill the gaps in discontiguous data, e.g. forecast hourly from 12 hourly data. Modeling may require some prototyping to discover what works well in terms of methods and chosen input observations. There may be rare examples of chunks with complete data where classical methods like ETS or SARIMA could be used for univariate forecasting. Generally, the problem resists the classical methods. A good choice would be the use of nonlinear machine learning methods that are agnostic about the temporal structure of the input data, making use of whatever is available. Such models could be used in a recursive or direct strategy to forecast the lead times. A direct strategy may make more sense, with one model per required lead time. There are 10 lead times, and 39 target variables, in which case a direct strategy would require (39 * 10) or 390 models. A downside of the direct approach to modeling the problem is the inability of the model to leverage any dependencies between target variables in the forecast interval, specifically across sites, across variables, or across lead times. If these dependencies exist (and some surely do), it may be possible to add a flavor of them in using a second-tier of of ensemble models. Feature selection could be used to discover the variables and/or the lag lead times that may provide the most value in forecasting each target variable and lead time. This approach would provide a lot of flexibility, and as was shown in the competition, ensembles of decision trees perform well with little tuning. Like machine learning methods, deep learning methods may be able to use whatever multivariate data is available in order to make a prediction. Two classes of neural networks may be worth exploring for this problem: CNNs are capable of distilling long sequences of multivariate input time series data into small feature maps, and in essence learn the features from the sequences that are most relevant for forecasting. Their ability to handle noise and feature invariance across the input sequences may be useful. Like other neural networks, CNNs can output a vector in order to predict the forecast lead times. LSTMs are designed to work with sequence data and can directly support missing data via masking. They too are capable of automatic feature learning from long input sequences and alone or combined with CNNs may perform well on this problem. Together with an encoder-decoder architecture, the LSTM network can be used to natively forecast multiple lead times. A naive approach that mirrors that used in the competition might be best for evaluating models. That is, splitting each chunk into train and test sets, in this case using the first five days of data for training and the remaining three for test. It may be possible and interesting to finalize a model by training it on the entire dataset and submitting a forecast to the Kaggle website for evaluation on the held out test dataset. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered and explored the Air Quality Prediction dataset that represents a challenging multivariate, multi-site, and multi-step time series forecasting problem. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Comment  Name (required)  Email (will not be published) (required)  Website"
"mastery",2018-10-10,"How to Develop LSTM Models for Multi-Step Time Series Forecasting of Household Power Consumption","https://machinelearningmastery.com/how-to-develop-lstm-models-for-multi-step-time-series-forecasting-of-household-power-consumption/","Given the rise of smart electricity meters and the wide adoption of electricity generation technology like solar panels, there is a wealth of electricity usage data available. This data represents a multivariate time series of power-related variables that in turn could be used to model and even forecast future electricity consumption. Unlike other machine learning algorithms, long short-term memory recurrent neural networks are capable of automatically learning features from sequence data, support multiple-variate data, and can output a variable length sequences that can be used for multi-step forecasting. In this tutorial, you will discover how to develop long short-term memory recurrent neural networks for multi-step time series forecasting of household power consumption. After completing this tutorial, you will know: Let¡¯s get started. How to Develop LSTM Models for Multi-Step Time Series Forecasting of Household Power ConsumptionPhoto by Ian Muttoo, some rights reserved. This tutorial is divided into nine parts; they are: The ¡®Household Power Consumption¡® dataset is a multivariate time series dataset that describes the electricity consumption for a single household over four years. The data was collected between December 2006 and November 2010 and observations of power consumption within the household were collected every minute. It is a multivariate series comprised of seven variables (besides the date and time); they are: Active and reactive energy refer to the technical details of alternative current. A fourth sub-metering variable can be created by subtracting the sum of three defined sub-metering variables from the total active energy as follows: The dataset can be downloaded from the UCI Machine Learning repository as a single 20 megabyte .zip file: Download the dataset and unzip it into your current working directory. You will now have the file ¡°household_power_consumption.txt¡± that is about 127 megabytes in size and contains all of the observations. We can use the read_csv() function to load the data and combine the first two columns into a single date-time column that we can use as an index. Next, we can mark all missing values indicated with a ¡®?¡® character with a NaN value, which is a float. This will allow us to work with the data as one array of floating point values rather than mixed types (less efficient.) We also need to fill in the missing values now that they have been marked. A very simple approach would be to copy the observation from the same time the day before. We can implement this in a function named fill_missing() that will take the NumPy array of the data and copy values from exactly 24 hours ago. We can apply this function directly to the data within the DataFrame. Now we can create a new column that contains the remainder of the sub-metering, using the calculation from the previous section. We can now save the cleaned-up version of the dataset to a new file; in this case we will just change the file extension to .csv and save the dataset as ¡®household_power_consumption.csv¡®. Tying all of this together, the complete example of loading, cleaning-up, and saving the dataset is listed below. Running the example creates the new file ¡®household_power_consumption.csv¡® that we can use as the starting point for our modeling project. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this section, we will consider how we can develop and evaluate predictive models for the household power dataset. This section is divided into four parts; they are: There are many ways to harness and explore the household power consumption dataset. In this tutorial, we will use the data to explore a very specific question; that is: Given recent power consumption, what is the expected power consumption for the week ahead? This requires that a predictive model forecast the total active power for each day over the next seven days. Technically, this framing of the problem is referred to as a multi-step time series forecasting problem, given the multiple forecast steps. A model that makes use of multiple input variables may be referred to as a multivariate multi-step time series forecasting model. A model of this type could be helpful within the household in planning expenditures. It could also be helpful on the supply side for planning electricity demand for a specific household. This framing of the dataset also suggests that it would be useful to downsample the per-minute observations of power consumption to daily totals. This is not required, but makes sense, given that we are interested in total power per day. We can achieve this easily using the resample() function on the pandas DataFrame. Calling this function with the argument ¡®D¡® allows the loaded data indexed by date-time to be grouped by day (see all offset aliases). We can then calculate the sum of all observations for each day and create a new dataset of daily power consumption data for each of the eight variables. The complete example is listed below. Running the example creates a new daily total power consumption dataset and saves the result into a separate file named ¡®household_power_consumption_days.csv¡®. We can use this as the dataset for fitting and evaluating predictive models for the chosen framing of the problem. A forecast will be comprised of seven values, one for each day of the week ahead. It is common with multi-step forecasting problems to evaluate each forecasted time step separately. This is helpful for a few reasons: The units of the total power are kilowatts and it would be useful to have an error metric that was also in the same units. Both Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) fit this bill, although RMSE is more commonly used and will be adopted in this tutorial. Unlike MAE, RMSE is more punishing of forecast errors. The performance metric for this problem will be the RMSE for each lead time from day 1 to day 7. As a short-cut, it may be useful to summarize the performance of a model using a single score in order to aide in model selection. One possible score that could be used would be the RMSE across all forecast days. The function evaluate_forecasts() below will implement this behavior and return the performance of a model based on multiple seven-day forecasts. Running the function will first return the overall RMSE regardless of day, then an array of RMSE scores for each day. We will use the first three years of data for training predictive models and the final year for evaluating models. The data in a given dataset will be divided into standard weeks. These are weeks that begin on a Sunday and end on a Saturday. This is a realistic and useful way for using the chosen framing of the model, where the power consumption for the week ahead can be predicted. It is also helpful with modeling, where models can be used to predict a specific day (e.g. Wednesday) or the entire sequence. We will split the data into standard weeks, working backwards from the test dataset. The final year of the data is in 2010 and the first Sunday for 2010 was January 3rd. The data ends in mid November 2010 and the closest final Saturday in the data is November 20th. This gives 46 weeks of test data. The first and last rows of daily data for the test dataset are provided below for confirmation. The daily data starts in late 2006. The first Sunday in the dataset is December 17th, which is the second row of data. Organizing the data into standard weeks gives 159 full standard weeks for training a predictive model. The function split_dataset() below splits the daily data into train and test sets and organizes each into standard weeks. Specific row offsets are used to split the data using knowledge of the dataset. The split datasets are then organized into weekly data using the NumPy split() function. We can test this function out by loading the daily dataset and printing the first and last rows of data from both the train and test sets to confirm they match the expectations above. The complete code example is listed below. Running the example shows that indeed the train dataset has 159 weeks of data, whereas the test dataset has 46 weeks. We can see that the total active power for the train and test dataset for the first and last rows match the data for the specific dates that we defined as the bounds on the standard weeks for each set. Models will be evaluated using a scheme called walk-forward validation. This is where a model is required to make a one week prediction, then the actual data for that week is made available to the model so that it can be used as the basis for making a prediction on the subsequent week. This is both realistic for how the model may be used in practice and beneficial to the models allowing them to make use of the best available data. We can demonstrate this below with separation of input data and output/predicted data. The walk-forward validation approach to evaluating predictive models on this dataset is provided below named evaluate_model(). The train and test datasets in standard-week format are provided to the function as arguments. An additional argument n_input is provided that is used to define the number of prior observations that the model will use as input in order to make a prediction. Two new functions are called: one to build a model from the training data called build_model() and another that uses the model to make forecasts for each new standard week called forecast(). These will be covered in subsequent sections. We are working with neural networks, and as such, they are generally slow to train but fast to evaluate. This means that the preferred usage of the models is to build them once on historical data and to use them to forecast each step of the walk-forward validation. The models are static (i.e. not updated) during their evaluation. This is different to other models that are faster to train where a model may be re-fit or updated each step of the walk-forward validation as new data is made available. With sufficient resources, it is possible to use neural networks this way, but we will not in this tutorial. The complete evaluate_model() function is listed below. Once we have the evaluation for a model, we can summarize the performance. The function below named summarize_scores() will display the performance of a model as a single line for easy comparison with other models. We now have all of the elements to begin evaluating predictive models on the dataset. Recurrent neural networks, or RNNs, are specifically designed to work, learn, and predict sequence data. A recurrent neural network is a neural network where the output of the network from one time step is provided as an input in the subsequent time step. This allows the model to make a decision as to what to predict based on both the input for the current time step and direct knowledge of what was output in the prior time step. Perhaps the most successful and widely used RNN is the long short-term memory network, or LSTM for short. It is successful because it overcomes the challenges involved in training a recurrent neural network, resulting in stable models. In addition to harnessing the recurrent connection of the outputs from the prior time step, LSTMs also have an internal memory that operates like a local variable, allowing them to accumulate state over the input sequence. For more information about Recurrent Neural Networks, see the post: For more information about Long Short-Term Memory networks, see the post: LSTMs offer a number of benefits when it comes to multi-step time series forecasting; they are: Further, specialized architectures have been developed that are specifically designed to make multi-step sequence predictions, generally referred to as sequence-to-sequence prediction, or seq2seq for short. This is useful as multi-step time series forecasting is a type of seq2seq prediction. An example of a recurrent neural network architecture designed for seq2seq problems is the encoder-decoder LSTM. An encoder-decoder LSTM is a model comprised of two sub-models: one called the encoder that reads the input sequences and compresses it to a fixed-length internal representation, and an output model called the decoder that interprets the internal representation and uses it to predict the output sequence. The encoder-decoder approach to sequence prediction has proven much more effective than outputting a vector directly and is the preferred approach. Generally, LSTMs have been found to not be very effective at auto-regression type problems. These are problems where forecasting the next time step is a function of recent time steps. For more on this issue, see the post: One-dimensional convolutional neural networks, or CNNs, have proven effective at automatically learning features from input sequences. A popular approach has been to combine CNNs with LSTMs, where the CNN is as an encoder to learn features from sub-sequences of input data which are provided as time steps to an LSTM. This architecture is called a CNN-LSTM. For more information on this architecture, see the post: A power variation on the CNN LSTM architecture is the ConvLSTM that uses the convolutional reading of input subsequences directly within an LSTM¡¯s units. This approach has proven very effective for time series classification and can be adapted for use in multi-step time series forecasting. In this tutorial, we will explore a suite of LSTM architectures for multi-step time series forecasting. Specifically, we will look at how to develop the following models: The models will be developed and demonstrated on the household power prediction problem. A model is considered skillful if it achieves performance better than a naive model, which is an overall RMSE of about 465 kilowatts across a seven day forecast. We will not focus on the tuning of these models to achieve optimal performance; instead, we will stop short at skillful models as compared to a naive forecast. The chosen structures and hyperparameters are chosen with a little trial and error. The scores should be taken as just an example rather than a study of the optimal model or configuration for the problem. Given the stochastic nature of the models, it is good practice to evaluate a given model multiple times and report the mean performance on a test dataset. In the interest of brevity and keeping the code simple, we will instead present single-runs of models in this tutorial. We cannot know which approach will be the most effective for a given multi-step forecasting problem. It is a good idea to explore a suite of methods in order to discover what works best on your specific dataset. We will start off by developing a simple or vanilla LSTM model that reads in a sequence of days of total daily power consumption and predicts a vector output of the next standard week of daily power consumption. This will provide the foundation for the more elaborate models developed in subsequent sections. The number of prior days used as input defines the one-dimensional (1D) subsequence of data that the LSTM will read and learn to extract features. Some ideas on the size and nature of this input include: There is no right answer; instead, each approach and more can be tested and the performance of the model can be used to choose the nature of the input that results in the best model performance. These choices define a few things: A good starting point would be to use the prior seven days. An LSTM model expects data to have the shape: One sample will be comprised of seven time steps with one feature for the seven days of total daily power consumed. The training dataset has 159 weeks of data, so the shape of the training dataset would be: This is a good start. The data in this format would use the prior standard week to predict the next standard week. A problem is that 159 instances is not a lot to train a neural network. A way to create a lot more training data is to change the problem during training to predict the next seven days given the prior seven days, regardless of the standard week. This only impacts the training data, and the test problem remains the same: predict the daily power consumption for the next standard week given the prior standard week. This will require a little preparation of the training data. The training data is provided in standard weeks with eight variables, specifically in the shape [159, 7, 8]. The first step is to flatten the data so that we have eight time series sequences. We then need to iterate over the time steps and divide the data into overlapping windows; each iteration moves along one time step and predicts the subsequent seven days. For example: We can do this by keeping track of start and end indexes for the inputs and outputs as we iterate across the length of the flattened data in terms of time steps. We can also do this in a way where the number of inputs and outputs are parameterized (e.g. n_input, n_out) so that you can experiment with different values or adapt it for your own problem. Below is a function named to_supervised() that takes a list of weeks (history) and the number of time steps to use as inputs and outputs and returns the data in the overlapping moving window format. When we run this function on the entire training dataset, we transform 159 samples into 1,099; specifically, the transformed dataset has the shapes X=[1099, 7, 1] and y=[1099, 7]. Next, we can define and fit the LSTM model on the training data. This multi-step time series forecasting problem is an autoregression. That means it is likely best modeled where that the next seven days is some function of observations at prior time steps. This and the relatively small amount of data means that a small model is required. We will develop a model with a single hidden LSTM layer with 200 units. The number of units in the hidden layer is unrelated to the number of time steps in the input sequences. The LSTM layer is followed by a fully connected layer with 200 nodes that will interpret the features learned by the LSTM layer. Finally, an output layer will directly predict a vector with seven elements, one for each day in the output sequence. We will use the mean squared error loss function as it is a good match for our chosen error metric of RMSE. We will use the efficient Adam implementation of stochastic gradient descent and fit the model for 70 epochs with a batch size of 16. The small batch size and the stochastic nature of the algorithm means that the same model will learn a slightly different mapping of inputs to outputs each time it is trained. This means results may vary when the model is evaluated. You can try running the model multiple times and calculate an average of model performance. The build_model() below prepares the training data, defines the model, and fits the model on the training data, returning the fit model ready for making predictions. Now that we know how to fit the model, we can look at how the model can be used to make a prediction. Generally, the model expects data to have the same three dimensional shape when making a prediction. In this case, the expected shape of an input pattern is one sample, seven days of one feature for the daily power consumed: Data must have this shape when making predictions for the test set and when a final model is being used to make predictions in the future. If you change the number if input days to 14, then the shape of the training data and the shape of new samples when making predictions must be changed accordingly to have 14 time steps. It is a modeling choice that you must carry forward when using the model. We are using walk-forward validation to evaluate the model as described in the previous section. This means that we have the observations available for the prior week in order to predict the coming week. These are collected into an array of standard weeks called history. In order to predict the next standard week, we need to retrieve the last days of observations. As with the training data, we must first flatten the history data to remove the weekly structure so that we end up with eight parallel time series. Next, we need to retrieve the last seven days of daily total power consumed (feature index 0). We will parameterize this as we did for the training data so that the number of prior days used as input by the model can be modified in the future. Next, we reshape the input into the expected three-dimensional structure. We then make a prediction using the fit model and the input data and retrieve the vector of seven days of output. The forecast() function below implements this and takes as arguments the model fit on the training dataset, the history of data observed so far, and the number of input time steps expected by the model. That¡¯s it; we now have everything we need to make multi-step time series forecasts with an LSTM model on the daily total power consumed univariate dataset. We can tie all of this together. The complete example is listed below. Running the example fits and evaluates the model, printing the overall RMSE across all seven days, and the per-day RMSE for each lead time. Your specific results may vary given the stochastic nature of the algorithm. You may want to try running the example a few times. We can see that in this case, the model was skillful as compared to a naive forecast, achieving an overall RMSE of about 399 kilowatts, less than 465 kilowatts achieved by a naive model. A plot of the daily RMSE is also created. The plot shows that perhaps Tuesdays and Fridays are easier days to forecast than the other days and that perhaps Saturday at the end of the standard week is the hardest day to forecast. Line Plot of RMSE per Day for Univariate LSTM with Vector Output and 7-day Inputs We can increase the number of prior days to use as input from seven to 14 by changing the n_input variable. Re-running the example with this change first prints a summary of performance of the model. Your specific results may vary; try running the example a few times. In this case, we can see a further drop in the overall RMSE to about 370 kilowatts, suggesting that further tuning of the input size and perhaps the number of nodes in the model may result in better performance. Comparing the per-day RMSE scores we see some are better and some are worse than using seven-day inputs. This may suggest benefit in using the two different sized inputs in some way, such as an ensemble of the two approaches or perhaps a single model (e.g. a multi-headed model) that reads the training data in different ways. Line Plot of RMSE per Day for Univariate LSTM with Vector Output and 14-day Inputs In this section, we can update the vanilla LSTM to use an encoder-decoder model. This means that the model will not output a vector sequence directly. Instead, the model will be comprised of two sub models, the encoder to read and encode the input sequence, and the decoder that will read the encoded input sequence and make a one-step prediction for each element in the output sequence. The difference is subtle, as in practice both approaches do in fact predict a sequence output. The important difference is that an LSTM model is used in the decoder, allowing it to both know what was predicted for the prior day in the sequence and accumulate internal state while outputting the sequence. Let¡¯s take a closer look at how this model is defined. As before, we define an LSTM hidden layer with 200 units. This is the decoder model that will read the input sequence and will output a 200 element vector (one output per unit) that captures features from the input sequence. We will use 14 days of total power consumption as input. We will use a simple encoder-decoder architecture that is easy to implement in Keras, that has a lot of similarity to the architecture of an LSTM autoencoder. First, the internal representation of the input sequence is repeated multiple times, once for each time step in the output sequence. This sequence of vectors will be presented to the LSTM decoder. We then define the decoder as an LSTM hidden layer with 200 units. Importantly, the decoder will output the entire sequence, not just the output at the end of the sequence as we did with the encoder. This means that each of the 200 units will output a value for each of the seven days, representing the basis for what to predict for each day in the output sequence. We will then use a fully connected layer to interpret each time step in the output sequence before the final output layer. Importantly, the output layer predicts a single step in the output sequence, not all seven days at a time, This means that we will use the same layers applied to each step in the output sequence. It means that the same fully connected layer and output layer will be used to process each time step provided by the decoder. To achieve this, we will wrap the interpretation layer and the output layer in a TimeDistributed wrapper that allows the wrapped layers to be used for each time step from the decoder. This allows the LSTM decoder to figure out the context required for each step in the output sequence and the wrapped dense layers to interpret each time step separately, yet reusing the same weights to perform the interpretation. An alternative would be to flatten all of the structure created by the LSTM decoder and to output the vector directly. You can try this as an extension to see how it compares. The network therefore outputs a three-dimensional vector with the same structure as the input, with the dimensions [samples, timesteps, features]. There is a single feature, the daily total power consumed, and there are always seven features. A single one-week prediction will therefore have the size: [1, 7, 1]. Therefore, when training the model, we must restructure the output data (y) to have the three-dimensional structure instead of the two-dimensional structure of [samples, features] used in the previous section. We can tie all of this together into the updated build_model() function listed below. The complete example with the encoder-decoder model is listed below. Running the example fits the model and summarizes the performance on the test dataset. Your specific results may vary given the stochastic nature of the algorithm. You may want to try running the example a few times. We can see that in this case, the model is skillful, achieving an overall RMSE score of about 372 kilowatts. A line plot of the per-day RMSE is also created showing a similar pattern in error as was seen in the previous section. Line Plot of RMSE per Day for Univariate Encoder-Decoder LSTM with 14-day Inputs In this section, we will update the Encoder-Decoder LSTM developed in the previous section to use each of the eight time series variables to predict the next standard week of daily total power consumption. We will do this by providing each one-dimensional time series to the model as a separate sequence of input. The LSTM will in turn create an internal representation of each input sequence that will together be interpreted by the decoder. Using multivariate inputs is helpful for those problems where the output sequence is some function of the observations at prior time steps from multiple different features, not just (or including) the feature being forecasted. It is unclear whether this is the case in the power consumption problem, but we can explore it nonetheless. First, we must update the preparation of the training data to include all of the eight features, not just the one total daily power consumed. It requires a single line change: The complete to_supervised() function with this change is listed below. We also must update the function used to make forecasts with the fit model to use all eight features from the prior time steps. Again, another small change: The complete forecast()<U+00A0>function with this change is listed below: The same model architecture and configuration is used directly, although we will increase the number of training epochs from 20 to 50 given the 8-fold increase in the amount of input data. The complete example is listed below. Running the example fits the model and summarizes the performance on the test dataset. Experimentation found that this model appears less stable than the univariate case and may be related to the differing scales of the input eight variables. Your specific results may vary given the stochastic nature of the algorithm. You may want to try running the example a few times. We can see that in this case, the model is skillful, achieving an overall RMSE score of about 376 kilowatts. A line plot of the per-day RMSE is also created. Line Plot of RMSE per Day for Multivariate Encoder-Decoder LSTM with 14-day Inputs A convolutional neural network, or CNN, can be used as the encoder in an encoder-decoder architecture. The CNN does not directly support sequence input; instead, a 1D CNN is capable of reading across sequence input and automatically learning the salient features. These can then be interpreted by an LSTM decoder as per normal. We refer to hybrid models that use a CNN and LSTM as CNN-LSTM models, and in this case we are using them together in an encoder-decoder architecture. The CNN expects the input data to have the same 3D structure as the LSTM model, although multiple features are read as different channels that ultimately have the same effect. We will simplify the example and focus on the CNN-LSTM with univariate input, but it can just as easily be updated to use multivariate input, which is left as an exercise. As before, we will use input sequences comprised of 14 days of daily total power consumption. We will define a simple but effective CNN architecture for the encoder that is comprised of two convolutional layers followed by a max pooling layer, the results of which are then flattened. The first convolutional layer reads across the input sequence and projects the results onto feature maps. The second performs the same operation on the feature maps created by the first layer, attempting to amplify any salient features. We will use 64 feature maps per convolutional layer and read the input sequences with a kernel size of three time steps. The max pooling layer simplifies the feature maps by keeping 1/4 of the values with the largest (max) signal. The distilled feature maps after the pooling layer are then flattened into one long vector that can then be used as input to the decoding process. The decoder is the same as was defined in previous sections. The only other change is to set the number of training epochs to 20. The build_model() function with these changes is listed below. We are now ready to try the encoder-decoder architecture with a CNN encoder. The complete code listing is provided below. Running the example fits the model and summarizes the performance on the test dataset. A little experimentation showed that using two convolutional layers made the model more stable than using just a single layer. Your specific results may vary given the stochastic nature of the algorithm. You may want to try running the example a few times. We can see that in this case the model is skillful, achieving an overall RMSE score of about 372 kilowatts. A line plot of the per-day RMSE is also created. Line Plot of RMSE per Day for Univariate Encoder-Decoder CNN LSTM with 14-day Inputs A further extension of the CNN-LSTM approach is to perform the convolutions of the CNN (e.g. how the CNN reads the input sequence data) as part of the LSTM for each time step. This combination is called a Convolutional LSTM, or ConvLSTM for short, and like the CNN-LSTM is also used for spatio-temporal data. Unlike an LSTM that reads the data in directly in order to calculate internal state and state transitions, and unlike the CNN-LSTM that is interpreting the output from CNN models, the ConvLSTM is using convolutions directly as part of reading input into the LSTM units themselves. For more information for how the equations for the ConvLSTM are calculated within the LSTM unit, see the paper: The Keras library provides the ConvLSTM2D class that supports the ConvLSTM model for 2D data. It can be configured for 1D multivariate time series forecasting. The ConvLSTM2D class, by default, expects input data to have the shape: Where each time step of data is defined as an image of (rows * columns) data points. We are working with a one-dimensional sequence of total power consumption, which we can interpret as one row with 14 columns, if we assume that we are using two weeks of data as input. For the ConvLSTM, this would be a single read: that is, the LSTM would read one time step of 14 days and perform a convolution across those time steps. This is not ideal. Instead, we can split the 14 days into two subsequences with a length of seven days. The ConvLSTM can then read across the two time steps and perform the CNN process on the seven days of data within each. For this chosen framing of the problem, the input for the ConvLSTM2D would therefore be: Or: You can explore other configurations, such as providing 21 days of input split into three subsequences of seven days, and/or providing all eight features or channels as input. We can now prepare the data for the ConvLSTM2D model. First, we must reshape the training dataset into the expected structure of [samples, timesteps, rows, cols, channels]. We can then define the encoder as a ConvLSTM hidden layer followed by a flatten layer ready for decoding. We will also parameterize the number of subsequences (n_steps) and the length of each subsequence (n_length) and pass them as arguments. The rest of the model and training is the same. The build_model() function with these changes is listed below. This model expects five-dimensional data as input. Therefore, we must also update the preparation of a single sample in the forecast() function when making a prediction. The forecast() function with this change and with the parameterized subsequences is provided below. We now have all of the elements for evaluating an encoder-decoder architecture for multi-step time series forecasting where a ConvLSTM is used as the encoder. The complete code example is listed below. Running the example fits the model and summarizes the performance on the test dataset. A little experimentation showed that using two convolutional layers made the model more stable than using just a single layer. We can see that in this case the model is skillful, achieving an overall RMSE score of about 367 kilowatts. A line plot of the per-day RMSE is also created. Line Plot of RMSE per Day for Univariate Encoder-Decoder ConvLSTM with 14-day Inputs This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop long short-term memory recurrent neural networks for multi-step time series forecasting of household power consumption. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hi Jason, Thanks for another great article.  I¡¯ve got a question about your thoughts about Attention based networks and how do they compere to LSTMs. I heard many voices in favor of the first ones, but I would like to know how this looks in real situations and not competitions-world <U+0001F609> Thanks,
Konrad Attention-based models can offer a lot of benefit on challenging sequence prediction problems. I have not used attention for time series forecasting though, sorry. Id on¡¯t have good off the cuff advice. Ok, sure, thanks for reply! <U+0001F642> # model.add(LSTM(200, activation=¡¯relu¡¯, input_shape=(n_timesteps, n_features)))
# model.add(Dense(100, activation=¡¯relu¡¯)) how do we choose LSTM unit and dense unit? for example, here 200 units for LSTM and 100 units for Dense have been used.  is there any formula out there? should we guess?  it would be great if you could explain!  Thanks in advance. Trial and error. I explain more here:https://machinelearningmastery.com/faq/single-faq/how-many-layers-and-nodes-do-i-need-in-my-neural-network How to calculate the accuracy of the Convolutional LSTM model of the electricity consumption dataset. Can you please provide the code for that? It is a regression problem, we cannot calculate accuracy for a regression problem. Comment  Name (required)  Email (will not be published) (required)  Website"
"mastery",2018-10-08,"How to Develop Convolutional Neural Networks for Multi-Step Time Series Forecasting","https://machinelearningmastery.com/how-to-develop-convolutional-neural-networks-for-multi-step-time-series-forecasting/","Given the rise of smart electricity meters and the wide adoption of electricity generation technology like solar panels, there is a wealth of electricity usage data available. This data represents a multivariate time series of power-related variables that in turn could be used to model and even forecast future electricity consumption. Unlike other machine learning algorithms, convolutional neural networks are capable of automatically learning features from sequence data, support multiple-variate data, and can directly output a vector for multi-step forecasting. As such, one-dimensional CNNs have been demonstrated to perform well and even achieve state-of-the-art results on challenging sequence prediction problems. In this tutorial, you will discover how to develop 1D convolutional neural networks for multi-step time series forecasting. After completing this tutorial, you will know: Let¡¯s get started. How to Develop Convolutional Neural Networks for Multi-Step Time Series ForecastingPhoto by Banalities, some rights reserved. This tutorial is divided into seven parts; they are: The ¡®Household Power Consumption¡® dataset is a multivariate time series dataset that describes the electricity consumption for a single household over four years. The data was collected between December 2006 and November 2010 and observations of power consumption within the household were collected every minute. It is a multivariate series comprised of seven variables (besides the date and time); they are: Active and reactive energy refer to the technical details of alternative current. A fourth sub-metering variable can be created by subtracting the sum of three defined sub-metering variables from the total active energy as follows: The dataset can be downloaded from the UCI Machine Learning repository as a single 20 megabyte .zip file: Download the dataset and unzip it into your current working directory. You will now have the file ¡°household_power_consumption.txt¡± that is about 127 megabytes in size and contains all of the observations. We can use the read_csv() function to load the data and combine the first two columns into a single date-time column that we can use as an index. Next, we can mark all missing values indicated with a ¡®?¡® character with a NaN value, which is a float. This will allow us to work with the data as one array of floating point values rather than mixed types (less efficient.) We also need to fill in the missing values now that they have been marked. A very simple approach would be to copy the observation from the same time the day before. We can implement this in a function named fill_missing() that will take the NumPy array of the data and copy values from exactly 24 hours ago. We can apply this function directly to the data within the DataFrame. Now we can create a new column that contains the remainder of the sub-metering, using the calculation from the previous section. We can now save the cleaned-up version of the dataset to a new file; in this case we will just change the file extension to .csv and save the dataset as ¡®household_power_consumption.csv¡®. Tying all of this together, the complete example of loading, cleaning-up, and saving the dataset is listed below. Running the example creates the new file ¡®household_power_consumption.csv¡® that we can use as the starting point for our modeling project. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this section, we will consider how we can develop and evaluate predictive models for the household power dataset. This section is divided into four parts; they are: There are many ways to harness and explore the household power consumption dataset. In this tutorial, we will use the data to explore a very specific question; that is: Given recent power consumption, what is the expected power consumption for the week ahead? This requires that a predictive model forecast the total active power for each day over the next seven days. Technically, this framing of the problem is referred to as a multi-step time series forecasting problem, given the multiple forecast steps. A model that makes use of multiple input variables may be referred to as a multivariate multi-step time series forecasting model. A model of this type could be helpful within the household in planning expenditures. It could also be helpful on the supply side for planning electricity demand for a specific household. This framing of the dataset also suggests that it would be useful to downsample the per-minute observations of power consumption to daily totals. This is not required, but makes sense, given that we are interested in total power per day. We can achieve this easily using the resample() function on the pandas DataFrame. Calling this function with the argument ¡®D¡® allows the loaded data indexed by date-time to be grouped by day (see all offset aliases). We can then calculate the sum of all observations for each day and create a new dataset of daily power consumption data for each of the eight variables. The complete example is listed below. Running the example creates a new daily total power consumption dataset and saves the result into a separate file named ¡®household_power_consumption_days.csv¡®. We can use this as the dataset for fitting and evaluating predictive models for the chosen framing of the problem. A forecast will be comprised of seven values, one for each day of the week ahead. It is common with multi-step forecasting problems to evaluate each forecasted time step separately. This is helpful for a few reasons: The units of the total power are kilowatts and it would be useful to have an error metric that was also in the same units. Both Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) fit this bill, although RMSE is more commonly used and will be adopted in this tutorial. Unlike MAE, RMSE is more punishing of forecast errors. The performance metric for this problem will be the RMSE for each lead time from day 1 to day 7. As a short-cut, it may be useful to summarize the performance of a model using a single score in order to aide in model selection. One possible score that could be used would be the RMSE across all forecast days. The function evaluate_forecasts() below will implement this behavior and return the performance of a model based on multiple seven-day forecasts. Running the function will first return the overall RMSE regardless of day, then an array of RMSE scores for each day. We will use the first three years of data for training predictive models and the final year for evaluating models. The data in a given dataset will be divided into standard weeks. These are weeks that begin on a Sunday and end on a Saturday. This is a realistic and useful way for using the chosen framing of the model, where the power consumption for the week ahead can be predicted. It is also helpful with modeling, where models can be used to predict a specific day (e.g. Wednesday) or the entire sequence. We will split the data into standard weeks, working backwards from the test dataset. The final year of the data is in 2010 and the first Sunday for 2010 was January 3rd. The data ends in mid November 2010 and the closest final Saturday in the data is November 20th. This gives 46 weeks of test data. The first and last rows of daily data for the test dataset are provided below for confirmation. The daily data starts in late 2006. The first Sunday in the dataset is December 17th, which is the second row of data. Organizing the data into standard weeks gives 159 full standard weeks for training a predictive model. The function split_dataset() below splits the daily data into train and test sets and organizes each into standard weeks. Specific row offsets are used to split the data using knowledge of the dataset. The split datasets are then organized into weekly data using the NumPy split() function. We can test this function out by loading the daily dataset and printing the first and last rows of data from both the train and test sets to confirm they match the expectations above. The complete code example is listed below. Running the example shows that indeed the train dataset has 159 weeks of data, whereas the test dataset has 46 weeks. We can see that the total active power for the train and test dataset for the first and last rows match the data for the specific dates that we defined as the bounds on the standard weeks for each set. Models will be evaluated using a scheme called walk-forward validation. This is where a model is required to make a one week prediction, then the actual data for that week is made available to the model so that it can be used as the basis for making a prediction on the subsequent week. This is both realistic for how the model may be used in practice and beneficial to the models, allowing them to make use of the best available data. We can demonstrate this below with separation of input data and output/predicted data. The walk-forward validation approach to evaluating predictive models on this dataset is provided below, named evaluate_model(). The train and test datasets in standard-week format are provided to the function as arguments. An additional argument, n_input, is provided that is used to define the number of prior observations that the model will use as input in order to make a prediction. Two new functions are called: one to build a model from the training data called build_model() and another that uses the model to make forecasts for each new standard week, called forecast(). These will be covered in subsequent sections. We are working with neural networks and as such they are generally slow to train but fast to evaluate. This means that the preferred usage of the models is to build them once on historical data and to use them to forecast each step of the walk-forward validation. The models are static (i.e. not updated) during their evaluation. This is different to other models that are faster to train, where a model may be re-fit or updated each step of the walk-forward validation as new data is made available. With sufficient resources, it is possible to use neural networks this way, but we will not in this tutorial. The complete evaluate_model() function is listed below. Once we have the evaluation for a model, we can summarize the performance. The function below, named summarize_scores(), will display the performance of a model as a single line for easy comparison with other models. We now have all of the elements to begin evaluating predictive models on the dataset. Convolutional Neural Network models, or CNNs for short, are a type of deep neural network that was developed for use with image data, such as handwriting recognition. They are proven very effective on challenging computer vision problems when trained at scale for tasks such as identifying and localizing objects in images and automatically describing the content of images. They are a model that are comprised of two main types of elements: convolutional layers and pooling layers. Convolutional layers read an input, such as a 2D image or a 1D signal using a kernel that reads in small segments at a time and steps across the entire input field. Each read results in an interpretation of the input that is projected onto a filter map and represents an interpretation of the input. Pooling layers take the feature map projections and distill them to the most essential elements, such as using a signal averaging or signal maximizing process. The convolution and pooling layers can be repeated at depth, providing multiple layers of abstraction of the input signals. The output of these networks is often one or more fully-connected layers that interpret what has been read and maps this internal representation to a class value. For more information on convolutional neural networks, you can see the post: Convolutional neural networks can be used for multi-step time series forecasting. The key benefits of the approach are the automatic feature learning and the ability of the model to output a multi-step vector directly. CNNs can be used in either a recursive or direct forecast strategy, where the model makes one-step predictions and outputs are fed as inputs for subsequent predictions, and where one model is developed for each time step to be predicted. Alternately, CNNs can be used to predict the entire output sequence as a one-step prediction of the entire vector. This is a general benefit of feed-forward neural networks. An important secondary benefit of using CNNs is that they can support multiple 1D inputs in order to make a prediction. This is useful if the multi-step output sequence is a function of more than one input sequence. This can be achieved using two different model configurations. In this tutorial, we will explore how to develop three different types of CNN models for multi-step time series forecasting; they are: The models will be developed and demonstrated on the household power prediction problem. A model is considered skillful if it achieves performance better than a naive model, which is an overall RMSE of about 465 kilowatts across a seven day forecast. We will not focus on the tuning of these models to achieve optimal performance; instead we will sill stop short at skillful models as compared to a naive forecast. The chosen structures and hyperparameters are chosen with a little trial and error. In this section, we will develop a convolutional neural network for multi-step time series forecasting using only the univariate sequence of daily power consumption. Specifically, the framing of the problem is: Given some number of prior days of total daily power consumption, predict the next standard week of daily power consumption. The number of prior days used as input defines the one-dimensional (1D) subsequence of data that the CNN will read and learn to extract features. Some ideas on the size and nature of this input include: There is no right answer; instead, each approach and more can be tested and the performance of the model can be used to choose the nature of the input that results in the best model performance. These choices define a few things about the implementation, such as: A good starting point would be to use the prior seven days. A 1D CNN model expects data to have the shape of: One sample will be comprised of seven time steps with one feature for the seven days of total daily power consumed. The training dataset has 159 weeks of data, so the shape of the training dataset would be: This is a good start. The data in this format would use the prior standard week to predict the next standard week. A problem is that 159 instances is not a lot for a neural network. A way to create a lot more training data is to change the problem during training to predict the next seven days given the prior seven days, regardless of the standard week. This only impacts the training data, the test problem remains the same: predict the daily power consumption for the next standard week given the prior standard week. This will require a little preparation of the training data. The training data is provided in standard weeks with eight variables, specifically in the shape [159, 7, 8]. The first step is to flatten the data so that we have eight time series sequences. We then need to iterate over the time steps and divide the data into overlapping windows; each iteration moves along one time step and predicts the subsequent seven days. For example: We can do this by keeping track of start and end indexes for the inputs and outputs as we iterate across the length of the flattened data in terms of time steps. We can also do this in a way where the number of inputs and outputs are parameterized (e.g. n_input, n_out) so that you can experiment with different values or adapt it for your own problem. Below is a function named to_supervised() that takes a list of weeks (history) and the number of time steps to use as inputs and outputs and returns the data in the overlapping moving window format. When we run this function on the entire training dataset, we transform 159 samples into 1,099; specifically, the transformed dataset has the shapes X=[1099, 7, 1] and y=[1099, 7]. Next, we can define and fit the CNN model on the training data. This multi-step time series forecasting problem is an autoregression. That means it is likely best modeled where that the next seven days is some function of observations at prior time steps. This and the relatively small amount of data means that a small model is required. We will use a model with one convolution layer with 16 filters and a kernel size of 3. This means that the input sequence of seven days will be read with a convolutional operation three time steps at a time and this operation will be performed 16 times. A pooling layer will reduce these feature maps by 1/4 their size before the internal representation is flattened to one long vector. This is then interpreted by a fully connected layer before the output layer predicts the next seven days in the sequence. We will use the mean squared error loss function as it is a good match for our chosen error metric of RMSE. We will use the efficient Adam implementation of stochastic gradient descent and fit the model for 20 epochs with a batch size of 4. The small batch size and the stochastic nature of the algorithm means that the same model will learn a slightly different mapping of inputs to outputs each time it is trained. This means results may vary when the model is evaluated. You can try running the model multiple times and calculating an average of model performance. The build_model() below prepares the training data, defines the model, and fits the model on the training data, returning the fit model ready for making predictions. Now that we know how to fit the model, we can look at how the model can be used to make a prediction. Generally, the model expects data to have the same three dimensional shape when making a prediction. In this case, the expected shape of an input pattern is one sample, seven days of one feature for the daily power consumed: Data must have this shape when making predictions for the test set and when a final model is being used to make predictions in the future. If you change the number of input days to 14, then the shape of the training data and the shape of new samples when making predictions must be changed accordingly to have 14 time steps. It is a modeling choice that you must carry forward when using the model. We are using walk-forward validation to evaluate the model as described in the previous section. This means that we have the observations available for the prior week in order to predict the coming week. These are collected into an array of standard weeks, called history. In order to predict the next standard week, we need to retrieve the last days of observations. As with the training data, we must first flatten the history data to remove the weekly structure so that we end up with eight parallel time series. Next, we need to retrieve the last seven days of daily total power consumed (feature number 0). We will parameterize as we did for the training data so that the number of prior days used as input by the model can be modified in the future. Next, we reshape the input into the expected three-dimensional structure. We then make a prediction using the fit model and the input data and retrieve the vector of seven days of output. The forecast() function below implements this and takes as arguments the model fit on the training dataset, the history of data observed so far, and the number of inputs time steps expected by the model. That¡¯s it; we now have everything we need to make multi-step time series forecasts with a CNN model on the daily total power consumed univariate dataset. We can tie all of this together. The complete example is listed below. Running the example fits and evaluates the model, printing the overall RMSE across all seven days, and the per-day RMSE for each lead time. Your specific results may vary given the stochastic nature of the algorithm. You may want to try running the example a few times. We can see that in this case, the model was skillful as compared to a naive forecast, achieving an overall RMSE of about 404 kilowatts, less than 465 kilowatts achieved by a naive model. A plot of the daily RMSE is also created. The plot shows that perhaps Tuesdays and Fridays are easier days to forecast than the other days and that perhaps Saturday at the end of the standard week is the hardest day to forecast. Line Plot of RMSE per Day for Univariate CNN with 7-day Inputs We can increase the number of prior days to use as input from seven to 14 by changing the n_input variable. Re-running the example with this change first prints a summary of the performance of the model. Your specific results may vary; try running the example a few times. In this case, we can see a further drop in the overall RMSE, suggesting that further tuning of the input size and perhaps the kernel size of the model may result in better performance. Comparing the per-day RMSE scores, we see some are better and some are worse than using seventh inputs. This may suggest a benefit in using the two different sized inputs in some way, such as an ensemble of the two approaches or perhaps a single model (e.g. a multi-headed model) that reads the training data in different ways. Line Plot of RMSE per Day for Univariate CNN with 14-day Inputs In this section, we will update the CNN developed in the previous section to use each of the eight time series variables to predict the next standard week of daily total power consumption. We will do this by providing each one-dimensional time series to the model as a separate channel of input. The CNN will then use a separate kernel and read each input sequence onto a separate set of filter maps, essentially learning features from each input time series variable. This is helpful for those problems where the output sequence is some function of the observations at prior time steps from multiple different features, not just (or including) the feature being forecasted. It is unclear whether this is the case in the power consumption problem, but we can explore it nonetheless. First, we must update the preparation of the training data to include all of the eight features, not just the one total daily power consumed. It requires a single line: The complete to_supervised() function with this change is listed below. We also must update the function used to make forecasts with the fit model to use all eight features from the prior time steps. Again, another small change: The complete forecast() with this change is listed below: We will use 14 days of prior observations across eight of the input variables as we did in the final section of the prior section that resulted in slightly better performance. Finally, the model used in the previous section does not perform well on this new framing of the problem. The increase in the amount of data requires a larger and more sophisticated model that is trained for longer. With a little trial and error, one model that performs well uses two convolutional layers with 32 filter maps followed by pooling, then another convolutional layer with 16 feature maps and pooling. The fully connected layer that interprets the features is increased to 100 nodes and the model is fit for 70 epochs with a batch size of 16 samples. The updated build_model() function that defines and fits the model on the training dataset is listed below. We now have all of the elements required to develop a multi-channel CNN for multivariate input data to make multi-step time series forecasts. The complete example is listed below. Running the example fits and evaluates the model, printing the overall RMSE across all seven days, and the per-day RMSE for each lead time. Your specific results may vary given the stochastic nature of the algorithm. You may want to try running the example a few times. We can see that in this case, the use of all eight input variables does result in another small drop in the overall RMSE score. For the daily RMSE scores, we do see that some are better and some are worse than the univariate CNN from the previous section. The final day, Saturday, remains a challenging day to forecast, and Friday an easy day to forecast. There may be some benefit in designing models to focus specifically on reducing the error of the harder to forecast days. It may be interesting to see if the variance across daily scores could be further reduced with a tuned model or perhaps an ensemble of multiple different models. It may also be interesting to compare the performance for a model that uses seven or even 21 days of input data to see if further gains can be made. Line Plot of RMSE per Day for a Multichannel CNN with 14-day Inputs We can further extend the CNN model to have a separate sub-CNN model or head for each input variable, which we can refer to as a multi-headed CNN model. This requires a modification to the preparation of the model, and in turn, modification to the preparation of the training and test datasets. Starting with the model, we must define a separate CNN model for each of the eight input variables. The configuration of the model, including the number of layers and their hyperparameters, were also modified to better suit the new approach. The new configuration is not optimal and was found with a little trial and error. The multi-headed model is specified using the more flexible functional API for defining Keras models. We can loop over each variable and create a sub-model that takes a one-dimensional sequence of 14 days of data and outputs a flat vector containing a summary of the learned features from the sequence. Each of these vectors can be merged via concatenation to make one very long vector that is then interpreted by some fully connected layers before a prediction is made. As we build up the submodels, we keep track of the input layers and flatten layers in lists. This is so that we can specify the inputs in the definition of the model object and use the list of flatten layers in the merge layer. When the model is used, it will require eight arrays as input: one for each of the submodels. This is required when training the model, when evaluating the model, and when making predictions with a final model. We can achieve this by creating a list of 3D arrays, where each 3D array contains [samples, timesteps, 1], with one feature. We can prepare the training dataset in this format as follows: The updated build_model() function with these changes is listed below. When the model is built, a diagram of the structure of the model is created and saved to file. Note: the call to plot_model() requires that pygraphviz and pydot are installed. If this is a problem, you can comment out this line. The structure of the network looks as follows. Structure of the Multi Headed Convolutional Neural Network Next, we can update the preparation of input samples when making a prediction for the test dataset. We must perform the same change, where an input array of [1, 14, 8] must be transformed into a list of eight 3D arrays each with [1, 14, 1]. The forecast() function with this change is listed below. That¡¯s it. We can tie all of this together; the complete example is listed below. Running the example fits and evaluates the model, printing the overall RMSE across all seven days, and the per-day RMSE for each lead time. Your specific results may vary given the stochastic nature of the algorithm. You may want to try running the example a few times. We can see that in this case, the overall RMSE is skillful compared to a naive forecast, but with the chosen configuration may not perform better than the multi-channel model in the previous section. We can also see a different, more pronounced profile for the daily RMSE scores where perhaps Mon-Tue and Thu-Fri are easier for the model to predict than the other forecast days. These results may be useful when combined with another forecast model. It may be interesting to explore alternate methods in the architecture for merging the output of each sub-model. Line Plot of RMSE per Day for a Multi-head CNN with 14-day Inputs This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop 1D convolutional neural networks for multi-step time series forecasting. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hi Jason, thanks for the tutorial. One naive question: for last complete code, how to make prediction fixed / reproducible ? I tried ¡°from numpy.random import seed¡± ¡°seed(1)¡±, but the scores are still varied when running twice. This is a common question that I answer here:https://machinelearningmastery.com/faq/single-faq/what-value-should-i-set-for-the-random-number-seed Great article <U+2013> thank you for the code examples.  I really enjoy looking at your work since I¡¯m trying to learn ML/DL.  Question <U+2013> what if I wanted to forecast out all of the variables separately rather than one total daily power consumption?   Thank you in advance. This would be a multi-step multi-variate problem. I show how in my book. I would recommend treating it like a seq2seq problem and forecast n variables for each step in the output sequence. An encoder-decoder model would be appropriate with a CNN or LSTM input model. Thank you for the response Jason.  Which one of your books are you referring to? This book:https://machinelearningmastery.com/deep-learning-for-time-series-forecasting/ Hi. When you increase the number of training example by overlapping your data, do you not run the risk of overfitting your model? You are essentially giving the model the same data multiple times. It may, this is why we are so reliant on the model validation method. Please teach us capsnet. Thanks for the suggestion.  Why do you want to use capsule networks? They seem fringe to me. Thank you <U+2013> I have purchased your ebook. Comment  Name (required)  Email (will not be published) (required)  Website"
