"site","date","headline","url_address","text"
"vidhya",2018-10-04,"5 Amazing Machine Learning GitHub Repositories & Reddit Threads from September 2018","https://www.analyticsvidhya.com/blog/2018/10/best-machine-learning-github-repositories-reddit-threads-september-2018/","

FLASH SALE: Course On Introduction To Data Science | Use COUPON CODE: FLASH25 for 25% Discount| Work on 4 real-life projects |
Buy Now 

 Welcome to the September edition of our popular GitHub repositories and Reddit discussions series! GitHub repositories continue to change the way teams code and collaborate on projects. They¡¯re a great source of knowledge for anyone willing to tap into their infinite potential. As more and more professionals are vying to break into the machine learning field, everyone needs to keep themselves updated with the latest breakthroughs and frameworks. GitHub serves as a gateway to learn from the best in the business. And as always, Analytics Vidhya is at the forefront of bringing the best of the bunch straight to you. This month¡¯s GitHub collection is awe-inspiring. Ever wanted to convert a research paper into code? We have you covered. How about implementing the top object detection algorithms using a framework of your choosing? Sure, we have that as well. And the fun doesn¡¯t stop there! Scroll down to check out this, and other top repositories, launched in September. On the Reddit front, I have included the most thought-provoking discussions in this field. My suggestion is to not only read through these threads but also actively participate in them to enhance and supplement your existing knowledge. You can check out the top GitHub repositories and top Reddit discussions (from April onwards) we have covered each month below: How many times have you come across research papers and wondered how to implement them on your own? I have personally struggled on multiple occasions to convert a paper into code. Well, the painstaking process of scouring the internet for specific pieces of code is over! Hundreds of machine learning and deep learning research papers and their respective codes are included here. This repository is truly stunning in its scope and is a treasure trove of knowledge for a data scientist. New links are added weekly and the NIPS 2018 conference papers have been added as well! If there¡¯s one GitHub repository you bookmark, make sure it¡¯s this one. Object detection is quickly becoming commonplace in the deep learning universe. And why wouldn¡¯t it? It¡¯s a fascinating concept with tons of real-life applications, ranging from games to surveillance. So how about a one-stop shop where you can find all the top object detection algorithms designed since 2014? Yes, you landed in the right place. This repository, much like the one above, contains links to the full research papers and the accompanying object detection code to implement the approach mentioned in them. And the best part? The code is available for multiple frameworks! So whether you¡¯re a TensorFlow, Keras, PyTorch, or Caffe user, this repository has something for everyone. At the time of publishing this article, 43 different papers were listed here. Yes, you really can train a model on the ImageNet dataset in under 18 minutes. The great Jeremy Howard and his team of students designed an algorithm that outperformed even Google, according to the popular DAWNBench benchmark.<U+00A0>This benchmark measures the training time, cost and other aspects of deep learning models. And now you can reproduce their results on your own machine! You need to have Python 3.6 (or higher) to get started. Go ahead and dive right in. Source: North Concepts Data engineering is a critical function in any machine learning project. Most aspiring data scientists these days tend to skip over this part, preferring to focus on the model building side of things. Not a great idea! You need to be aware (and even familiar) with how data pipelines work, what role Hadoop, Spark and Dask have to play, etc. Sounds daunting? Then check out this repository. Pypeline is a simple yet very effective Python library for creating concurrent data pipelines. The aim of this library is to solve low to medium data tasks (that involve concurrency or parallelism) where the use of Spark might feel unnecessary. This repository contains codes, benchmarks, documentation and other resources to help you become a data pipeline expert! This one is a personal favourite. I covered the release of the research paper back in August and have continued to be in awe of this technique. It is capable of transferring motion between human objects in different videos. I high recommend checking out the video available in the above link, it will blow your mind! This repository contains a PyTorch implementation of this approach. The sheer amount of details this algorithm can pick up and replicate are staggering. I can¡¯t wait to try this on my own machine! This thread continues our theme of implementing research papers. It¡¯s an ideal spot for beginners in AI looking for a new challenge. There¡¯a two fold advantage of checking out this thread: Don¡¯t you love the open source community? A keen-eyed Redditor recently found a flaw in one of the CVPR (Computer Vision and Pattern Recognition) 2018 research papers. This is quite a big thing since the paper had already been accepted by the conference committee and successfully presented to the community. The original author of the paper took time out to respond to this mistake. It led to a very civil and thought-provoking discussion between the top ML folks on what should be done when a mistake like this is unearthed. Should the paper be retracted or amended with the corrections? There are over 100 comments in this thread and render this a must-read for everyone. We all get stuck at some point while going through a research paper. The math can often be difficult to understand, and the approach used can bamboozle the best of us. So why not reach out to the community and ask for help? That¡¯s exactly what this thread aims to do. Make sure you follow the format mentioned in the original post and your queries will be answered. There are plenty of Q&As already there so you can browse through them to get a feel for how the process works. A pertinent question. A lot of people I speak to are interested in getting into the research side of ML, without having a clue of what to expect. Is a background in mathematics and statistics enough? Or should you be capable enough of cracking open all research papers and making sense of them on your own? The answer lies more towards the latter. Research is a field where the experts can guide you, but no one really knows the right answer until someone figures it out. There¡¯s no single book or course that can prepare you for such a role. Needless to say, this thread is an enlightening one with different takes on what the prerequisites are. This is a controversial topic, but one I feel everyone should be aware of. Researchers release the paper and mention that the code will follow soon in order to make the reviewers happy. But sometimes this just doesn¡¯t happen. The paper gets accepted to the conference, but the code is never released to the community. It¡¯s a question of ethics than anything else. Why not mention that the data is private and can only be shared with a select few? If your code cannot be validated, what¡¯s the point of presenting it to the audience? This is a microcosm of the questions asked in this thread. Curating this list and writing about each repository and discussion thread was quite a thrill. It filled with me a sense of wonder and purpose <U+2013> there is so much knowledge out there and most of it is open-source. It would be highly negligent of us to not learn from it and put it to good use. If there are any other links you feel the community should know about, feel free to let us know in the comments section below. I need recommendations about this . Hi Praveen,"
"vidhya",2018-10-03,"DataHack Radio #11: Decision Intelligence with Google Cloud¡¯s Chief Decision Scientist, Cassie Kozyrkov","https://www.analyticsvidhya.com/blog/2018/10/datahack-radio-decision-intelligence-google-cloud-cassie-kozyrkov/","

FLASH SALE: Course On Introduction To Data Science | Use COUPON CODE: FLASH25 for 25% Discount| Work on 4 real-life projects |
Buy Now 

 What is decision intelligence? How does it tie into the world of data science? And what does Google have to do with it all? Click on the above SoundCloud link and find out! Welcome to episode #11 of DataHack Radio, where we were joined by Google Cloud¡¯s Chief Decision Scientist, Cassie Kozyrkov! Cassie is a well-known speaker in the data science sphere, and often pens down her thoughts in articulate fashion in this field. She takes us on a journey into her life at Google and how she went from being a Statistician at Google to her current role. This is a short summary of Cassie in conversation with Kunal. Give the podcast a listen and find out how a top Google scientist thinks, works, and structures her thoughts! As a bonus, there are some brilliant quotable quotes in this episode, which you will find yourself chuckling and nodding to. Subscribe to DataHack Radio NOW and listen to this, as well as all previous episodes, on any of the below platforms: The story behind Cassie¡¯s shift from using MATLAB to R is a fascinating one. While working with MATLAB, she wanted to make a particular kind of chart. After spending hours trying to figure it out, she decided to try her hand at R. It took her half an hour to design the plot she wanted! And the rest, as they say, is history. She¡¯s a big R fan, and turns to Python if absolutely necessary. Cassie joined Google in 2014 as a Statistician and one of her early projects was getting rid of duplicate entries in Google Maps. It was a far more challenging task than one might think. How do you actually define duplicate entries? You need to define good processes for measuring and verifying each duplicate entry. On a global scale, this is not a straightforward task. There were a lot of statistical techniques involved in this process, like hypothesis testing. But she wasn¡¯t the only statistician on board this project, which meant coordinating and collaborating with others. Just getting people to agree on one definition of a duplicate entry was a long winding process (anyone who has worked on a project staffed with over 50 employees will be able to relate to this!). ¡°It¡¯s quite risky for data scientists to join teams that don¡¯t quite know what they¡¯re doing.¡± Decision Intelligence (DI) augments data science with theory from social science, decision theory, and managerial science, among other applications. DI provides a framework for best practices in organizational decision-making and processes for applying machine learning and AI at scale. Cassie finds herself these days working on multiple projects, especially in the initial stages. This way she can assign the task of particular things to the correct people, instead of letting projects get bogged down due to teams not being aware of what the next step should be. If you just rely on data science in a project, the whole thing just might flop. You need to add some extra muscle, which decision intelligence supplies. This is quite a fascinating concept and worth listening to in the podcast. She also aims to help the outside world (outside Google, that is) do some of this stuff, in a more organised and better manner. Check out some of her talks in various global forums to get an idea of what she means by that. ¡°When it comes to AI, I think the whole world is making a mistake of talking about it as some form of holy water, when it¡¯s just water.¡± What Cassie means by the above quote is that when you start thinking about it as holy water, it instantly means it¡¯s accessible only to a select few. This is absolutely not true for AI and there¡¯s no special magic attached to it. ¡°Think of it as a different way of communicating your wishes to a computer.¡± You can use it to power your business and improve your results, without needing to rely on intuition and good old luck. Cassie encourages everyone to explore programming and machine learning, at least at a basic level. It¡¯s such a wonderful gateway to a whole new world where you have the power to change results, so why shouldn¡¯t you leverage that? Coming to the future of this field, Cassie is most excited about the applied side of machine learning and AI. She uses her popular analogy of a microwave and other kitchen appliances to explain this <U+2013> a truly innovative way of thinking about this domain! I personally had only vaguely heard about decision intelligence before listening to this podcast, so it was quite an eye-opener. It¡¯s a intriguing discipline and one I feel anyone in the data science field should explore. This is one those podcasts you just don¡¯t want to end, it has so much knowledge packed into an hour! I hope you enjoy listening to it as much as I did."
"vidhya",2018-10-01,"Text Mining 101: A Stepwise Introduction to Topic Modeling using Latent Semantic Analysis (using Python)","https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/","

FLASH SALE: Course On Introduction To Data Science | Use COUPON CODE: FLASH25 for 25% Discount| Work on 4 real-life projects |
 Buy Now 

 Have you ever been inside a well-maintained library? I¡¯m always incredibly impressed with the way the librarians keep everything organized, by name, content, and other topics. But if you gave these librarians thousands of books and asked them to arrange each book on the basis of their genre, they will struggle to accomplish this task in a day, let alone an hour! However, this won¡¯t happen to you if these books came in a digital format, right? All the arrangement seems to happen in a matter of seconds, without requiring any manual effort. All hail Natural Language Processing (NLP). Source: confessionsofabookgeek.com Have a look at the below text snippet: As you might gather from the highlighted text, there are three topics (or concepts) <U+2013> Topic 1, Topic 2, and Topic 3. A good topic model will identify similar words and put them under one group or topic. The most dominant topic in the above example is Topic 2, which indicates that this piece of text is primarily about fake videos. Intrigued, yet? Good! In this article, we will learn about a text mining approach called Topic Modeling. It is an extremely useful technique for extracting topics, and one you will work with a lot when faced with NLP challenges. Note: I highly recommend going through this article<U+00A0>to understand terms like SVD and UMAP. They are leveraged in this article so having a basic understanding of them will help solidify these concepts. A Topic Model can be defined as an unsupervised technique to discover topics across various text documents. These topics are abstract in nature, i.e., words which are related to each other form a topic. Similarly, there can be multiple topics in an individual document. For the time being, let¡¯s understand a topic model as a black box, as illustrated in the below figure: This black box (topic model) forms clusters of similar and related words which are called topics. These topics have a certain distribution in a document, and every topic is defined by the proportion of different words it contains. Recall the example we saw earlier of arranging similar books together. Now suppose you have to perform a similar task with a few digital text documents. You would be able to manually accomplish this, as long as the number of documents is manageable (aka not too many of them). But what happens when there¡¯s an impossible number of these digital text documents? That¡¯s where NLP techniques come to the fore. And for this particular task, topic modeling is the technique we will turn to. Source: topix.io/tutorial/tutorial.html Topic modeling helps in exploring large amounts of text data, finding clusters of words, similarity between documents, and discovering abstract topics. As if these reasons weren¡¯t compelling enough, topic modeling is also used in search engines wherein the search string is matched with the results. Getting interesting, isn¡¯t it? Well, read on then! All languages have their own intricacies and nuances which are quite difficult for a machine to capture (sometimes they¡¯re even misunderstood by us humans!). This can include different words that mean the same thing, and also the words which have the same spelling but different meanings. For example, consider the following two sentences: In the first sentence, the word ¡®novel¡¯ refers to a book, and in the second sentence it means new or fresh. We can easily distinguish between these words because we are able to understand the context behind these words. However, a machine would not be able to capture this concept as it cannot understand the context in which the words have been used. This is where Latent Semantic Analysis (LSA) comes into play as it attempts to leverage the context around the words to capture the hidden concepts, also known as topics.  So, simply mapping words to documents won¡¯t really help. What we really need is to figure out the hidden concepts or topics behind the words. LSA is one such technique that can find these hidden topics. Let¡¯s now deep dive into the inner workings of LSA. Let¡¯s say we have m number of text documents with n number of total unique terms (words). We wish to extract k topics from all the text data in the documents. The number of topics, k, has to be specified by the user. It¡¯s time to power up Python and understand how to implement LSA in a topic modeling problem. Once your Python environment is open, follow the steps I have mentioned below. Let¡¯s load the required libraries before proceeding with anything else. In this article, we will use the ¡¯20 Newsgroup¡¯ dataset from sklearn. You can download the dataset here, and follow along with the code. Output: 11,314 The dataset has 11,314 text documents distributed across 20 different newsgroups. To start with, we will try to clean our text data as much as possible. The idea is to remove the punctuations, numbers, and special characters all in one step using the regex replace(¡°[^a-zA-Z#]¡±, ¡± ¡°), which will replace everything, except alphabets with space. Then we will remove shorter words because they usually don¡¯t contain useful information. Finally, we will make all the text lowercase to nullify case sensitivity. It¡¯s good practice to remove the stop-words from the text data as they are mostly clutter and hardly carry any information. Stop-words are terms like ¡®it¡¯, ¡®they¡¯, ¡®am¡¯, ¡®been¡¯, ¡®about¡¯, ¡®because¡¯, ¡®while¡¯, etc. To remove stop-words from the documents, we will have to tokenize the text, i.e., split the string of text into individual tokens or words. We will stitch the tokens back together once we have removed the stop-words. This is the first step towards topic modeling. We will use sklearn¡¯s TfidfVectorizer to create a document-term matrix with 1,000 terms. We could have used all the terms to create this matrix but that would need quite a lot of computation time and resources. Hence, we have restricted the number of features to 1,000. If you have the computational power, I suggest trying out all the terms. The next step is to represent each and every term and document as a vector. We will use the document-term matrix and decompose it into multiple matrices. We will use sklearn¡¯s TruncatedSVD to perform the task of matrix decomposition. Since the data comes from 20 different newsgroups, let¡¯s try to have 20 topics for our text data. The number of topics can be specified by using the<U+00A0>n_components<U+00A0>parameter. The components of svd_model are our topics, and we can access them using svd_model.components_. Finally, let¡¯s print a few most important words in each of the 20 topics and see how our model has done. To find out how distinct our topics are, we should visualize them. Of course, we cannot visualize more than 3 dimensions, but there are techniques like PCA and t-SNE which can help us visualize high dimensional data into lower dimensions. Here we will use a relatively new technique called UMAP (Uniform Manifold Approximation and Projection). As you can see above, the result is quite beautiful. Each dot represents a document and the colours represent the 20 newsgroups. Our LSA model seems to have done a good job. Feel free to play around with the parameters of UMAP to see how the plot changes its shape. The entire code for this article can be found in this GitHub repository. Latent Semantic Analysis can be very useful as we saw above, but it does have its limitations. It¡¯s important to understand both the sides of LSA so you have an idea of when to leverage it and when to try something else. Pros: Cons: Apart from LSA, there are other advanced and efficient topic modeling techniques such as<U+00A0>Latent Dirichlet Allocation (LDA) and<U+00A0>lda2Vec. We have a wonderful article on LDA which you can check out here. lda2vec is a much more advanced topic modeling which is based on word2vec word embeddings. If you want to find out more about it, let me know in the comments section below and I¡¯ll be happy to answer your questions/."
"vidhya",2018-10-01," Text Mining 101: A Stepwise Introduction to Topic Modeling using Latent Semantic Analysis (using Python)","https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/","

FLASH SALE: Course On Introduction To Data Science | Use COUPON CODE: FLASH25 for 25% Discount| Work on 4 real-life projects |
 Buy Now 

 Have you ever been inside a well-maintained library? I¡¯m always incredibly impressed with the way the librarians keep everything organized, by name, content, and other topics. But if you gave these librarians thousands of books and asked them to arrange each book on the basis of their genre, they will struggle to accomplish this task in a day, let alone an hour! However, this won¡¯t happen to you if these books came in a digital format, right? All the arrangement seems to happen in a matter of seconds, without requiring any manual effort. All hail Natural Language Processing (NLP). Source: confessionsofabookgeek.com Have a look at the below text snippet: As you might gather from the highlighted text, there are three topics (or concepts) <U+2013> Topic 1, Topic 2, and Topic 3. A good topic model will identify similar words and put them under one group or topic. The most dominant topic in the above example is Topic 2, which indicates that this piece of text is primarily about fake videos. Intrigued, yet? Good! In this article, we will learn about a text mining approach called Topic Modeling. It is an extremely useful technique for extracting topics, and one you will work with a lot when faced with NLP challenges. Note: I highly recommend going through this article<U+00A0>to understand terms like SVD and UMAP. They are leveraged in this article so having a basic understanding of them will help solidify these concepts. A Topic Model can be defined as an unsupervised technique to discover topics across various text documents. These topics are abstract in nature, i.e., words which are related to each other form a topic. Similarly, there can be multiple topics in an individual document. For the time being, let¡¯s understand a topic model as a black box, as illustrated in the below figure: This black box (topic model) forms clusters of similar and related words which are called topics. These topics have a certain distribution in a document, and every topic is defined by the proportion of different words it contains. Recall the example we saw earlier of arranging similar books together. Now suppose you have to perform a similar task with a few digital text documents. You would be able to manually accomplish this, as long as the number of documents is manageable (aka not too many of them). But what happens when there¡¯s an impossible number of these digital text documents? That¡¯s where NLP techniques come to the fore. And for this particular task, topic modeling is the technique we will turn to. Source: topix.io/tutorial/tutorial.html Topic modeling helps in exploring large amounts of text data, finding clusters of words, similarity between documents, and discovering abstract topics. As if these reasons weren¡¯t compelling enough, topic modeling is also used in search engines wherein the search string is matched with the results. Getting interesting, isn¡¯t it? Well, read on then! All languages have their own intricacies and nuances which are quite difficult for a machine to capture (sometimes they¡¯re even misunderstood by us humans!). This can include different words that mean the same thing, and also the words which have the same spelling but different meanings. For example, consider the following two sentences: In the first sentence, the word ¡®novel¡¯ refers to a book, and in the second sentence it means new or fresh. We can easily distinguish between these words because we are able to understand the context behind these words. However, a machine would not be able to capture this concept as it cannot understand the context in which the words have been used. This is where Latent Semantic Analysis (LSA) comes into play as it attempts to leverage the context around the words to capture the hidden concepts, also known as topics.  So, simply mapping words to documents won¡¯t really help. What we really need is to figure out the hidden concepts or topics behind the words. LSA is one such technique that can find these hidden topics. Let¡¯s now deep dive into the inner workings of LSA. Let¡¯s say we have m number of text documents with n number of total unique terms (words). We wish to extract k topics from all the text data in the documents. The number of topics, k, has to be specified by the user. It¡¯s time to power up Python and understand how to implement LSA in a topic modeling problem. Once your Python environment is open, follow the steps I have mentioned below. Let¡¯s load the required libraries before proceeding with anything else. In this article, we will use the ¡¯20 Newsgroup¡¯ dataset from sklearn. You can download the dataset here, and follow along with the code. Output: 11,314 The dataset has 11,314 text documents distributed across 20 different newsgroups. To start with, we will try to clean our text data as much as possible. The idea is to remove the punctuations, numbers, and special characters all in one step using the regex replace(¡°[^a-zA-Z#]¡±, ¡± ¡°), which will replace everything, except alphabets with space. Then we will remove shorter words because they usually don¡¯t contain useful information. Finally, we will make all the text lowercase to nullify case sensitivity. It¡¯s good practice to remove the stop-words from the text data as they are mostly clutter and hardly carry any information. Stop-words are terms like ¡®it¡¯, ¡®they¡¯, ¡®am¡¯, ¡®been¡¯, ¡®about¡¯, ¡®because¡¯, ¡®while¡¯, etc. To remove stop-words from the documents, we will have to tokenize the text, i.e., split the string of text into individual tokens or words. We will stitch the tokens back together once we have removed the stop-words. This is the first step towards topic modeling. We will use sklearn¡¯s TfidfVectorizer to create a document-term matrix with 1,000 terms. We could have used all the terms to create this matrix but that would need quite a lot of computation time and resources. Hence, we have restricted the number of features to 1,000. If you have the computational power, I suggest trying out all the terms. The next step is to represent each and every term and document as a vector. We will use the document-term matrix and decompose it into multiple matrices. We will use sklearn¡¯s TruncatedSVD to perform the task of matrix decomposition. Since the data comes from 20 different newsgroups, let¡¯s try to have 20 topics for our text data. The number of topics can be specified by using the<U+00A0>n_components<U+00A0>parameter. The components of svd_model are our topics, and we can access them using svd_model.components_. Finally, let¡¯s print a few most important words in each of the 20 topics and see how our model has done. To find out how distinct our topics are, we should visualize them. Of course, we cannot visualize more than 3 dimensions, but there are techniques like PCA and t-SNE which can help us visualize high dimensional data into lower dimensions. Here we will use a relatively new technique called UMAP (Uniform Manifold Approximation and Projection). As you can see above, the result is quite beautiful. Each dot represents a document and the colours represent the 20 newsgroups. Our LSA model seems to have done a good job. Feel free to play around with the parameters of UMAP to see how the plot changes its shape. The entire code for this article can be found in this GitHub repository. Latent Semantic Analysis can be very useful as we saw above, but it does have its limitations. It¡¯s important to understand both the sides of LSA so you have an idea of when to leverage it and when to try something else. Pros: Cons: Apart from LSA, there are other advanced and efficient topic modeling techniques such as<U+00A0>Latent Dirichlet Allocation (LDA) and<U+00A0>lda2Vec. We have a wonderful article on LDA which you can check out here. lda2vec is a much more advanced topic modeling which is based on word2vec word embeddings. If you want to find out more about it, let me know in the comments section below and I¡¯ll be happy to answer your questions/."
