"","site","date","headline","url_address","text","keyword"
"1","kaggle",2018-09-20,"Data Notes: How Do Autoencoders Work?","http://blog.kaggle.com/2018/09/20/data-notes-how-do-autoencoders-work/","Autoencoders, convolutions, and predicting Google's stock price: Enjoy these new, intriguing, and overlooked datasets and kernels 1.<U+00A0><U+0001F916> How Autoencoders Work: Intro and Use Cases (link) 2.<U+00A0><U+0001F54B> 3D Convolutions: Understanding and Implementation (link) 3.<U+00A0><U+0001F551> Hourly Time Series Forecasting with Prophet (link) 4.<U+00A0><U+0001F4C8> Google Stock Price Prediction - RNN (link) 5. #<U+FE0F><U+20E3> Extract Entities From Social Media Posts (link) 6.<U+00A0><U+0001F3AE> Video Games Report IGN - Visualization & Analysis (link) 7.<U+00A0><U+0001F6B2> A Brief Tour of Tour de France in Numbers (link) 8.<U+00A0><U+0001F33D> Dataset: Soybean Prices 1962-2018 (link) 9.<U+00A0><U+0001F4FA> Dataset: Top 5000 Youtube Channels (link) 10.<U+00A0><U+0001F45F> Dataset: 5,000 #JustDoIt! Tweets (link) Want to understand your model¡¯s prediction? Demystify the random forest! Copyright ¨Ï 2018 Kaggle, All rights reserved. 
  

","Keyword(freq): autoencoder(2), convolution(2), case(1), channel(1), dataset(1), entity(1), kernel(1), media(1), model(1), number(1)"
"2","kaggle",2018-09-19,"Help! I can¡¯t reproduce a machine learning project!","http://blog.kaggle.com/2018/09/19/help-i-cant-reproduce-a-machine-learning-project/","Have you ever sat down with the code and data for an existing machine learning project, trained the same model, checked your results¡¦ and found that they were different from the original results? Not being able to reproduce someone else¡¯s results is super frustrating. Not being able to reproduce your own results is frustrating and embarrassing. And tracking down the exact reason that you aren¡¯t able to reproduce results can take ages; it took me a solid week to reproduce this NLP paper, even with the original authors¡¯ exact code and data. But there's good news: Reproducibility breaks down in three main places: the code, the data and the environment. I¡¯ve put together this guide to help you narrow down where your reproducibility problems are, so you can focus on fixing them. Let¡¯s go through the three potential offenders one by one, talk about what kind of problems arise and then see how to fix them. I¡¯ve called this section ¡°non-deterministic code¡± rather than ¡°differences in code¡± because in a lot of machine learning or statistical applications you can end up with completely different results from the same code. This is because many machine learning and statistical algorithms are non-deterministic: randomness is an essential part of how they work.  If you come from a more traditional computer science or software engineering background, this can be pretty surprising: imagine if a sorting algorithm intentionally returned inputs in a slightly different order every time you ran it! In machine learning and statistical computing, however, randomness shows up in many places, including: <U+00A0> Randomness is your friend when it comes to things like escaping local minima, but it can throw a wrench in the works when you¡¯re trying to reproduce those same results later. In order to make machine learning results reproducible, you need to make sure that the random elements are the same every time you run your code. In other words, you need to make sure you ¡°randomly¡± generate the same set of random numbers. You can do this by making sure to set every random seed that your code relies on. Random seed: The number used by a pseudorandom generator to determine the order in which numbers are generated. If the same generator is given the same seed, it will generate the same sequence every time it restarts. Unfortunately, if you¡¯re working from a project where the random seed was never set in the first place, you probably won¡¯t be able to get those same results again. In that case, your best bet is to retrain a new model that does have a seed set. Here¡¯s a quick primer on how to do that: In R: Most packages depend on the global random seed, which you can set using `set.seed()`. The exceptions are packages that are actually wrappers around software written in other languages, like XGBoost or some of the packages that rely heavily on rcpp.  In Python: You can set the global random seed in Python using `seed()` from the `random` module. Unfortunately, most packages in the Python data science ecosystem tend to have their own internal random seed. My best piece of advice is to quickly search the documentation for each package you¡¯re using and see if it has a function for setting the random seed. (I¡¯ve compiled a list of the methods for some of the most common packages in the ¡°Controlling randomness¡± section of this notebook.) Another thing that can potentially break reproducibility is differences in the data. While this happens less often, it can be especially difficult to pin down. (This is one of the reasons Kaggle datasets have versioning and why we¡¯ve recently added notes on what¡¯s changed between versions. This dataset is a good example: scroll down to the ¡°History¡± section.) You might not be lucky enough to be working from a versioned datasets, however. If you have access to the original data files, there are some ways you can check to make sure that you¡¯re working with the same data the original project used: Another thing that can be helpful is knowing what can introduce differences in your data files. By far the biggest culprit here is opening data in word processing or spreadsheet software. I¡¯ve personally been bitten in the butt by this one more than once. A lot of the nice helpful changes made to improve the experience of working with data files for humans can be just enough to end up breaking reproducibility. Here¡¯s are two of the biggest sneaky problem areas. Automatic date formatting: This is actually a huge problem for scientific researchers. One study found that gene names have been automatically converted to dates in one-fifth of published genomics papers. In addition to changing non-dates into dates like that, the format of your dates will sometimes be edited to be more in line with your computer locale, like 6/4/2018 being changed to 4/6/2018. <U+00A0> Character encodings: This is an especially sneaky one because a lot of text editors will open files with different character encodings with no problems¡¦ but then save them using whatever your system default character encoding is. That means that your text might not look any different in the editor, but all the underlying bytes have been completely changed. Of course if you always use UTF-8 this isn¡¯t generally a problem, but that¡¯s not always an option.  Because of these problems, I strongly recommend that you don¡¯t check or edit your data files in word processors or spreadsheet software. Or, if you do, do it with a second copy of your data that you can discard later. I tend to use a text editor to check out datasets instead. (I like Gedit or Notepad++ but I know better than to wade into an argument about which text editor is better than the other. <U+0001F609> If you¡¯re comfortable working in the command line, you can also check your data there. So you¡¯ve triple-double-checked your code and data, and you¡¯re 100% sure that they aren¡¯t accounting for differences between your runs. What¡¯s left? The answer is the computational environment. This includes everything needed to run the code, including things like what packages and package versions were used to run the code and, if you reference them, file directory structures. Getting a lot of ¡°File not found¡± errors? They¡¯re probably showing up because you¡¯re trying to run code that references a specific directory structure in a directory with a different structure. Problems related to the file directory structures are pretty easy to fix: just make sure you¡¯re using relative rather than absolute paths and configure your . (If you¡¯re unsure what that means, this notebook goes into more details.) You may have to go back and redo some things by hand, but it¡¯s a pretty easy fix once you know what you¡¯re looking for.  It¡¯s much harder to fix problems that show up because of dependency mismatches. If you¡¯re trying to reproduce your own work, hopefully you still have access to the environment you originally ran your code in. If so, check out the ¡°Computing environment¡± section of this notebook to learn how to get you can quickly get information on what packages and their versions you used to run code. You can then use that list to make sure you¡¯re using the same packages versions in your current environment. Pro tip: Make sure you check the language version too! While major versions will definitely break reproducibility (looking at you, Python 2 vs. Python 3), even subversion updates can introduce problems. In particular, differences in the subversion can make it difficult or impossible to load serialized data formats, like pickles in Python or .RData files in R. The amount of information you have about the environment used to run the code will determine how difficult is to reproduce. You might have... (This taxonomy is discussed in depth in this paper, if you¡¯re curious. ) But what if you don¡¯t already have access to any information about the original environment? If it¡¯s your own project and you don¡¯t have access to it anymore because you dropped it into a lake or something, you may be out of luck. If you¡¯re trying to reproduce someone else¡¯s work, through, the best advice I can give you is to reach out to the person whose work you¡¯re trying to reproduce. I¡¯ve actually had a fair amount of success reaching out to researchers on this front, especially if I¡¯m very polite and specific about what I¡¯m asking. _____ In the majority of cases, you should be able to track down problems in reproducibility to one of these three places: the code, data or environment. It can take a while to laser in on the exact problems with your project, but having a rough guide should help you narrow down your search.  That said, there are a small number of cases where it¡¯s literally impossible to reproduce modeling results. Perhaps the best example is deep-learning projects that rely on the cuDNN library, which is part of the NVIDIA Deep Learning SDK. Some key methods used for CNNs and bi-directional LSTMs are currently non-deterministic. (But check the documentation for up-to-date information). My advice is to consider not using CUDA if reproducibility is a priority. The tradeoff here is that not using CUDA is that your models might take much longer to train, so if you¡¯re prioritizing speed instead this will be an important consideration for you. Reproducibility is one of those areas where the whole ¡°an ounce of prevention is worth a pound of cure¡± idea is very applicable. The easiest way to avoid running into problems with reproducibility is for everyone to make sure their work is reproductible from the get-go. If this is something you¡¯re willing to invest some time in, I¡¯ve written a notebook that walks through best practices for reproducible research here to get you started. ","Keyword(freq): problem(10), result(10), package(8), file(6), difference(5), version(5), date(4), dataset(3), case(2), else(2)"
"3","datacamp",2018-09-19,"New Skill Track: Tidyverse Fundamentals with R","https://www.datacamp.com/community/blog/skill-track-tidyverse-r","Here is the track link. In this track, you¡¯ll learn the skills needed to get you up and running with data science in R using the tidyverse. The tidyverse is a collection of R packages that share a common design philosophy and are designed to seamlessly work together, all with novices to data science professionals equally benefiting. You¡¯ll begin by hopping into data wrangling and data visualization with the gapminder dataset in the Introduction to the Tidyverse course. Next, in Working with Data in the Tidyverse, you¡¯ll learn about ¡°tidy data¡± and see how to get data into tidy format with fun datasets from the British reality television series ¡°Great British Bake Off¡± allowing you to see what¡¯s cooking behind the scenes of so many R data analyses. After that, you¡¯ll ease into general modeling concepts via regression using tidyverse principles in Modeling with Data in the Tidyverse. There you¡¯ll explore Seattle housing prices and how different variables can be used to explore patterns in these prices. Throughout these first three courses, you¡¯ll use the dplyr, ggplot2, and tidyr packages that serve as the powerhouses of the tidyverse allowing you to see the power of readable code. In Communicating with Data in the Tidyverse, you¡¯ll learn how to further customize your ggplot2 graphics and use R Markdown to write reproducible reports while working with data from the International Labour Organization in Europe. The track closes with Categorical Data in the Tidyverse that explores ways to handle the sometimes tricky concept of factors in data science with R using datasets from Kaggle¡¯s Data Science and Machine Learning Survey and FiveThirtyEight.com. The goal of the track is for you to gain experience using the tools and techniques of the whole data science pipeline made famous by Hadley Wickham and Garrett Grolemund as shown below. You¡¯ll gain exposure to each component of this pipeline from a variety of different perspectives in this track. We look forward to seeing you in the track! This is an introduction to the programming language R, focused on a powerful set of tools known as the ""tidyverse"". In the course, you'll learn the intertwined processes of data manipulation and visualization through the tools dplyr and ggplot2. You'll learn to manipulate data by filtering, sorting and summarizing a real dataset of historical country data in order to answer exploratory questions. You'll then learn to turn this processed data into informative line plots, bar plots, histograms, and more with the ggplot2 package. This gives a taste both of the value of exploratory data analysis and the power of tidyverse tools. This is a suitable introduction for people who have no previous experience in R and are interested in learning to perform data analysis. This course was even approved by Hadley himself:
 In this course, you'll learn to work with data using tools from the tidyverse in R. By data, we mean your own data, other people's data, messy data, big data, small data - any data with rows and columns that comes your way! By work, we mean doing most of the things that sound hard to do with R, and that need to happen before you can analyze or visualize your data. But work doesn't mean that it is not fun - you will see why so many people love working in the tidyverse as you learn how to explore, tame, tidy, and transform your data. Throughout this course, you'll work with data from a popular television baking competition called ""The Great British Bake Off."" In this course, you will learn to model with data. Models attempt to capture the relationship between an outcome variable of interest and a series of explanatory/predictor variables. Such models can be used for both explanatory purposes, e.g. ""Does knowing professors' ages help explain their teaching evaluation scores?"", and predictive purposes, e.g., ""How well can we predict a house's price based on its size and condition?"" You will leverage your tidyverse skills to construct and interpret such models. This course centers around the use of linear regression, one of the most commonly-used and easy to understand approaches to modeling. Such modeling and thinking is used in a wide variety of fields, including statistics, causal inference, machine learning, and artificial intelligence. They say that a picture is worth a thousand words. Indeed, successfully promoting your data analysis is not only a matter of accurate and effective graphics, but also of aesthetics and uniqueness. This course teaches you how to leverage the power of ggplot2 themes for producing publication-quality graphics that stick out from the mass of boilerplate plots out there. It shows you how to tweak and get the most out of ggplot2 in order to produce unconventional plots that draw attention on social media. In the end, you will combine that knowledge to produce a slick and custom-styled report with RMarkdown and CSS <U+2013> all of that within the powerful tidyverse. As a data scientist, you will often find yourself working with non-numerical data, such as job titles, survey responses, or demographic information. This type of data is qualitative and can be ordinal if they have an order to them, or categorical/nominal, if they don't. R has a special way of representing them, called factors, and this course will help you master working with them using the tidyverse package forcats. We¡¯ll also work with other tidyverse packages, including ggplot2, dplyr, stringr, and tidyr and use real-world datasets, such as the FiveThirtyEight flight dataset and Kaggle¡¯s State of Data Science and ML Survey. Following this course, you¡¯ll be able to identify and manipulate factor variables, quickly and efficiently visualize your data, and effectively communicate your results. Get ready to categorize!","Keyword(freq): tool(5), plot(4), dataset(3), graphic(3), model(3), package(3), variable(3), factor(2), kaggle(2), price(2)"
"4","datacamp",2018-09-17,"Becoming a Data Scientist (Transcript)","https://www.datacamp.com/community/blog/blog-becoming-data-scientist","Here is a link to the podcast. Hugo:    Hi there, Renee, and welcome to DataFramed. Renee:    Hi Hugo. Great to be here. Hugo:    It's great to have you on the show and I'm really excited to talk about all the things we're gonna talk about today, the podcast that you worked on for so long, the idea of becoming a data scientist and your journey and process there, but before that I'd like to find out a bit about you. Maybe you can tell us a bit about what you're known for in the data community. Renee:    Sure. Well I think I'm known for the podcast that you mentioned. It's called Becoming a Data Scientist. I interview people about how they got to where they are in their data science journeys and whether they consider themselves to be a data scientist. I plan to start that back up soon. I think that's what I originally kind of got known for but a lot of people also follow me on Twitter that may or may not have been an original podcast listener. I have a Twitter account called BecomingDataSci and my name on there is Data Science Renee. I try to help people that are transitioning into a data science career to find learning resources and inspiration. I've built a site called DataSciGuide.com, which collects learning resources and people can go on there and rate them. I hope to eventually make that into learning paths and things like that. I have a Twitter account called NewDataSciJobs where I share jobs that require less than three years of experience and I try to share articles about learning data science and getting into this field to help people transition in. Renee:    On top of that, I share my own data science challenges and achievements and try to encourage and inspire others so they can kind of watch what I do. I'm really happy, especially in the last year I feel, to see a wide variety of people with different educational backgrounds that want to enter this field, so I intend to help them become data scientists too because I think the broader the background of people in this field, the better it's gonna get. I guess that's what I'm known for, the podcast and Twitter account for the most part. Hugo:    Sure. I think a wonderful through line there that of course we're very aligned with at Data Camp is lowering the barrier to entry for people who want to engage with analytics and data science. One of your wonderful approaches I think, you know you stated that on the podcast you'll even ask people you have on about their journey but whether they consider themselves to be data scientists, kind of what this term means, and how their practices apply to it. It kind of demystifies data science as a whole, which can be a very I think unapproachable term with a lot of gatekeepers around as well. I think the work you do is very similar to how we think about our approach at Data Camp so that's really cool. Renee:    Great. I definitely aim for that. Hugo:    How did you get into data science initially? Renee:    This is my favorite question because this is what we talk about the whole time on my podcast, so hopefully I don't run too long but I will give a detailed answer. I've worked with data my whole career. You might call me a data generalist. Right out of college, I went to James Madison University in Harrisonburg, Virginia, where I still live, and I majored in something called integrated science and technology. It was a very broad major. It gave more breadth than depth in a lot of topics. We covered everything from biotech to manufacturing and engineering to programming, but you kind of get a taste of everything and find out what you like and don't like. It had a lot of hands-on real-world projects and one thing we learned in the programming courses in the ISAT program was relational database design. This is something I had never done before then but when I was in the class I realized hey I'm pretty good at this. I get this. It makes sense to me. Right out of college, I started doing that type of work. I was designing databases, building data-driven websites, and designing forms and reports to interact with the data. I did a lot of SQL and helped design a reporting data warehouse and building interactive reports where people can interact with the data and I did some analysis on that. Renee:    I wanted to take my career to the next level beyond that. At the time, I thought that a masters in systems engineering would fill in a lot of the gaps in my knowledge so in my undergrad program I didn't have a lot of depth in math, for instance, or coding. I just had some introductory classes. This program had, it was at the University of Virginia, and it had simulation and modeling courses, optimization, statistics, and at the time I was kind of afraid of the math. I had to take linear algebra at the community college in a summer course to even qualify to apply for this masters program. This is eight years after undergrad. I should have known that it was gonna be more math intensive than I originally thought but I found out that the title of each of these courses in the systems engineering program is kind of like a code for another type of math. It was very math intensive but I needed that. That's something that I wouldn't have learned as much on my own if I did all self directed learning. Hugo:    I have a question around that which of course I get a lot as an educator, which is to be an effective data analyst or data scientist, how much linear algebra do people need to know? Renee:    I think it's good to understand the basics. It gives you a sense of what's going on behind the scenes of those algorithms, to understand how data is being transformed and processed, however if you're really going to be an applied data scientist and not so much like a machine learning researcher, you don't have to really know all those intricacies. I'm glad I got a background in it so I understand how these things work, but I don't use those skills on my day-to-day work. They're like packages that abstract all that away so I don't have to be doing those type of calculations on a daily basis as a data scientist. I would say it's good to get a grasp of it and feel like you understand the concepts but you don't need to like have a mastery of the actual computations yourself. I mean that's what computers are for. They can do a lot of that for you. Hugo:    Yeah. I agree completely and I do think there is a lot of anxiety around learning these types of things, linear algebra and I suppose multivariate calculus in particular. I do also encourage people to push through a bit and persevere a bit because a big part of the challenge is the language and notation. A lot of the concepts aren't necessarily that tough but when you're writing a whole bunch of matrices and that type of stuff, you get pretty gnarly pretty quickly. Renee:    Yeah. I still like shudder when I see certain depictions of the ... Like you said with multi variable calculus and calculus that's done in a matrix. It just looks so overwhelming and the notation still gets me so I feel that. Hugo:    Yeah. Renee:    But I'm glad I understand the concepts behind it, even if I still shudder every time I see those. Hugo:    Yeah and you can have some crazy notation that really what it is referring to is the directional flow along a surface or something like that, like something that intuitively is quite easy to grasp but we've got this heavy archaic notation around it. Renee:    Yeah and it's not even consistent. I was in a program that had like professors from different departments at different universities and my husband is a physicist and there was a course where I was just really struggling with this particular type of computation and the notation and he looked at it and he was like you just learned this last semester. I was like I've never seen this before. He said no it's the same concept, it's just different notation. That's when I really started to understand like mathematicians and engineers for instance might use different notation for the same thing. It gets complicated. I do think if you're gonna become like a machine learning researcher or go into like a PhD program or you're developing things around the cutting edge of data science and really pushing forward the field and building algorithms that other people will use, then you need to really understand that stuff but if you're mostly applying algorithms that are already built, you don't have to get as in depth. For statistics I do think you really need a solid statistical foundation. I would kind of say the opposite. Everybody that does data science really needs to understand basic statistics well. Hugo:    Great. So what then happened in your journey while or after you did this program? Renee:    Yeah. While I was in the program, the data science institute got started at UVA. I had been hearing about data science everywhere and I kind of wanted to switch into that program but I couldn't without completely starting over. They kind of moved as a cohort through their program so I found out that I could take a machine learning course as an elective and so I started taking that just because I wanted to know what it's about and how close is it to what I'm already doing. It felt like my whole career up to that point was kind of leading towards data science and I had never heard of it. In this machine learning class, it started with a lot of the math and it moved really fast and I'll be honest I bombed that mid term. I really thought I was gonna fail out of the course but I decided to keep going because the first half of the course was the math and the second half of the course was the coding and applied part of it which was what I was looking forward to, so I thought well even if I get a bad grade I want to learn what I'm supposed to learn in this course so let me stick with it. Renee:    Like you said, with the abstract symbols and things I was having a hard time even understanding the textbook but then the last part of the course we had been building these machine learning algorithms from scratch. Oh and by the way all the examples were in C++ but the professor let us use whatever coding language we wanted to, so I started picking up Python at that point. I didn't have a very good grasp of C++. I had mostly done visual basic .NET up until that point and SQL and I didn't know Python at all but I figured that was my chance to learn it so I kind of learned Python as I went as well, which is probably part of the reason I struggled in the class. By the end we had this project. By then I kind of got Python and I kind of got what was going on with machine learning. I was going to school part time while I worked, so I asked my manager can I use this data that we use at work to apply it to this project that I'm doing in school. He said yes that was fine. Renee:    So what I did, I was working in the advancement division at JMU which is basically the fundraising arm of the university. For my project, I predicted which alumni were most likely to become donors in the next fiscal year. The professor loved it and maybe even mentioned this is something I could publish in the future. I guess that project outweighed my performance in the math portion of the course because I ended up getting an A in that class, which just blew my mind. Hugo:    That's incredible. Renee:    I was like okay now that's kind of confirmation that this is something I should be doing. Hugo:    Absolutely. I just wanna flag that before you go on, that you've actually made an incredible point there which is that you didn't do a project kind of in a vacuum essentially. You were working on data that was meaningful for you, meaningful to your employer, and actually gave some insight into something important to a bunch of stakeholders. Renee:    Yeah and it took what like in class we have pre-prepared data sets and they were all just lists of numbers. They weren't even like kind of related to the real world at all. The professor chose those data sets because the answer would come out a certain way and so diving into something that was unknown that no one had really looked at before at least in our university and finding some insights that I could share and actually make a real-world difference, that tied it all together for me. Hugo:    In a learning experience as well, working on something that means something to you and interests you is so important. Renee:    Oh absolutely. I always encourage people to find datasets that are interesting to them and use them throughout their learning journey because it keeps you interested when things get tough and also you'll understand the output better if it's something that you've had a background in or even interested in. If you're into sports, use a sports data set because you'll have a better sense of whether the output of your model even makes sense in context of sports. Hugo:    I always say if you, a lot of people wear fitness trackers these days and they can get their own data with respect to exercise and sleeping patterns and that type of stuff. They can quickly do a brief analysis or visualization of stuff that's happening physiologically with them. Renee:    Yeah. That's an awesome idea and definitely something I would encourage. Hugo:    Awesome. So what happened next in your journey? Renee:    For my last class, so most of my program that I did in grad school was online. It was synchronous so I was actually watching lectures over the internet that were live and there was a class there but for the last semester I commuted to campus which was an hour for me. I started listening to a lot of data science podcasts because I knew at that point I'm interested in this thing. Back then I was listening to Partially Derivative and Talking Machines and the O'Reilly Data Show, Linear Digressions, Data Skeptic, so I was just absorbing all of this data science information and I knew that this was what I wanted to do. As soon as I graduated, I started diving into books about data science and teaching myself what I needed to know to get a job in this field and move on from, at the time I was a data analyst and I wanted to move into being a data scientist. That's what I did next. Renee:    Then I applied to a bunch of different jobs that like at the time I was just getting comfortable with data science so I didn't want necessarily a data scientist job but I wanted to make sure it was a job that was moving in that direction because the job I was in wasn't giving me a lot of opportunities to really exercise these new skills and do machine learning on the job. I knew I was good with designing analytical reports. I knew I was good with SQL. I had this new masters degree in systems engineering but I wanted to grow into a data science role. I started applying to a bunch of different jobs that partially involved data science but they had components that I knew I already had the skills to provided value in. I didn't get any of the first several I applied to, but I was starting to learn by doing those interviews what they were gonna ask and what gaps I had in my knowledge so I can go back and learn more. Renee:    At the time, there were two different startups, one on each side of the country, that apparently needed that type of generalist that could do both the backend data engineering and SQL stuff and move into the predictive modeling side. I got two offers at the same time. They were both for remote roles that were like a combination of data analytics and entry level data science. I didn't have to do whiteboard interviews or coding interviews for either of them which was nice because that part, I don't think I was as good at the time, but they needed somebody with my background and my experience with databases and someone that was good at communicating with the stakeholders. I think that helped me stand out and I think we're gonna talk a little bit more about that later. Hugo:    Absolutely. Renee:    But one of those two job offers was with people I had worked with before. I worked at Rosetta Stone as a data analyst and a lot of the people at this startup had come from Rosetta Stone. I was more comfortable with that one and took that one and have been able to build my data science and machine learning skills on the job. That company is called HelioCampus. We work with university data and I can tell you more about that if we're interested, but I've been in that role for about two years now as a data scientist. Hugo:    Fantastic. That's telling that the project you did did involve alumni data initially, when you were first learning. Renee:    Yeah. At HelioCampus we've kind of ... It's extended me into a new domain. It's still university data but we work a lot with the student success data and admissions and things like that. I guess I'll give a little brief overview of the company. At universities they have databases that are like all kinds of data that you might not even think of when you're applying and enrolling at this university. There would be a system for admissions and applications. There's usually a separate system for enrollment and courses and faculty and then there's another system that they have for payroll and financials and then they'll have another system for the fundraising and alumni information. They have all these databases across campus and the leaders want kind of a big picture look at the students' trajectory through this whole experience of applying and then going to college and becoming alumni. Renee:    To get metrics on that whole system, you have to combine that data. We combine it into a data warehouse and we have reports in Tableau that point at that data. We have some canned reports and then my job is to then work with the end users to do analysis that's not already built to answer questions they have about the students and to do some predictive modeling. One example is for the admissions team, we have ... We'll take a look at all the students that have been admitted to a university and try to predict how many of them will enroll or which ones might be on the borderline of the type of students that sometimes enroll and sometimes don't. They might need some extra outreach in order for the school to get their attention or students that need additional financial aid for instance. We've helped them get some insight by doing predictive modeling into what their student body looks like and what type of students they can except to come to their university and what trend we expect in the future for their enrollment. That's just one example of many different aspects of what we do with the universities at HelioCampus but that's the kind of work I'm doing now. Hugo:    That sounds like very interesting and fulfilling work, particularly with your kind of deep interest and mission as an educator and investing in learners. Renee:    Yeah definitely. Hugo:    It was fantastic to find out once again about your journey to becoming a data scientist and something that of course you do is insist through your podcast, through a lot of different media that this is only one journey, that everyone's journey particularly to becoming a data scientist, there are a lot of different paths and there isn't a one-size-fits-all approach to becoming a data scientist, and that before actually deciding on a path, people need to figure out both where they are and where they need to go and connect those points somehow. So: what I'd like to know is what questions do aspiring data scientists need to think about when figuring out where they're starting from on their journey? Renee:    Yeah definitely. That's actually why I started my podcast because I was listening to all these other podcasts showing what cool stuff data scientists were doing, but none of them had focused on how did they get there? What did they do? I started asking questions and one of the things I realized that you have to asses no matter which different educational background or career background you have is your starting point. The kind of questions you need to ask to map out your data science learning path is like have you coded before? What language have you coded in before? Data scientists typically learn R or Python, often need to know SQL. How comfortable are you with the mathematics and statistics and do you need to brush up on those things and get some refreshers? Maybe you need to take it to the next level from where you're at? Have you ever presented a report based on data? Have you done an analysis in a professional setting before? Have you ever answered questions with data? These are like the basics that you need. Renee:    Then, you're gonna probably be working in a particular domain so within that field do you know the lingo? Do you know what kind of data related career paths there are in that domain? How you might focus in your data science learning to target one of those career paths. You might want to talk to a data scientist in that domain or analyst in that field and get a sense of the common questions and state of the art of what problems are they working on and what are they asking so you get that language. It's kind of this baseline of all the different parts of those common data science Venn diagrams that you see of how many of those pieces do you still need to work on to fill in. You're just assessing your starting point and then next you'll look at where you wanna go so that you know how to map out that learning path. Hugo:    Yeah. So to recap, essentially we have coding chops, whether you can program, what languages, comfort with maths and stats, then communication skills and actually presenting I was gonna say data-based reports but I really mean reports based on data and then domain knowledge. I think these are definitely very important aspects of your own practice to analyze when figuring out where you're starting from and then of course, as we both said, you need to have an idea of where you wanna end up. This may be a relatively amorphous, changing, vague notion but what are the typical data science profiles that we've seen emerge that people can end up as? Renee:    Yeah. As you mentioned, data science can mean a whole lot of things. I've noticed that there seems to be these groupings of specialties within data science. There's like an analyst type of data science: these are people that are usually working with end users or leaders or other people in the business. They're understanding the kind of questions that can be asked and figuring out how to convert those questions into data questions and determine ""do you have the data available to answer those questions?"" and doing the analysis and then presenting the results and proudly developing data visualizations for those kind of things. There are the engineer types of data scientists that are doing a lot of the backend work, the coding, working with databases and data warehouses, probably doing some of the feature engineering, working with big data systems and technologies that can handle massive data sets, building those data pipelines that support the analysis. Renee:    Then there's what I mentioned earlier, the researcher type of data scientist: they're improving those cutting edge algorithms and developing new tools and techniques, so that's a different focus of data science. I'll say that most people end up doing some combination of these things but you end up specializing either in like the analysis part or the engineering part or the research part. In my current role, I do a lot of the back-end engineering stuff because I have that background but also mostly focusing on the analysis tasks and communicating with people at the universities, the institutional researchers and decision makers that are gonna use the results of what it is that I'm doing. Hugo:    Yeah great. We've identified the three archetypes, the analyst, engineer, and researcher as end points or at least career paths. Knowing kind of the ways we need to think about where we are and knowing where we can end up, what are paths that you would recommend? What do recommended paths look like essentially? Renee:    Yeah I'm hoping to formalize this more in the future with the information I'm gathering at Data Sci Guide but it really depends on the individual. That starting point that you assessed, the ending point of where you want to end up at, and what are you comfortable teaching yourself or taking courses in, learning online, deciding if you need to go back to school. I do think it's a myth that you need a PhD to be a data scientist. I don't have one. A lot of data scientists I know don't have one. I would say go back to school if there's something like there was for math for me that you would be uncomfortable teaching yourself and you really need someone else to help you understand like the fundamental concepts there. Talk to someone that has a similar background as you and has become a data scientist or find people on Twitter that seem to be following paths that you like and you want to follow that. Renee:    Then do that project based learning like you talked about. Finding the data set that has the information that you're interested in, whether that's sports, statistics, or political data, or geospatial imagery or medical data or entertainment data. There's so many different types of data out there that you can find something that's really interesting to you. Ask a question that you can answer with the data and then learn whatever techniques you need to learn in order to answer that question. I think project directed learning is really valuable but that exact path and what resources you use, I have a really hard time recommending any one thing because different things work for different people, though I would recommend keep trying different things until you find out what works for you. Don't get discouraged if you pick up a book that a lot of people say is popular and great and you don't really get it and it's not sinking in for you. Just try something else. Don't give up and say oh I'm not cut out for this because this popular book doesn't make sense to me. Hugo:    Yeah. There's a lot of great advice in there. Something I haven't thought about a lot beforehand is talking to someone that has a similar background, essentially finding people like you. I think this is really cool because after you've done the work of identifying where you are and where you wanna go or where you'd like to be in whatever time frame you're thinking, I think it's easy to forget or to think that there aren't people like you out there and that you're alone in this journey, particularly in a field that's moving so quickly so to find people at different points in their career who are like you, that type of community to advise or be a mentor or a mentee later on, these types of things, is an incredible idea. Renee:    Yeah. I think another thing that I just thought of that ends up being difficult is just even orienting to the terminology. Even when you're out there looking for someone like you, like there's a lot of weird words that are used in data science that can be confusing at first and you don't really know is that person doing what I think I wanna do. I have an article on my blog about how I used Twitter to do this. Podcasts like yours are great for that, just hearing people talk about data science and learning like what kind of things data scientists have to think about. When I was ready to move into this career path I got this book. It was called Doing Data Science by Kathy O'Neill and Rachel Shut. That was great for me in terms of getting an overview of the big picture of what is this stuff and what do I need to learn and what are some of the basic terms and it pointed you at other resources to learn. Renee:    Yeah just orienting to like how people even talk and what ... What matters in data science and maybe there are things that you actually know already but it's called something else by data scientists. Data science is kind of a combination of fields that have already existed for a while. Yeah just learning that terminology and listening to data scientists and watching them on Twitter and reading articles to figure out what you don't know yet is important first step. Hugo:    In terms of this journey of becoming a data scientist, can you suggest any learning tasks for beginners? Renee:    Yeah. I would say build a report. Like you were saying, maybe use your own data from a Fit Bit or something like that. Just explore a dataset and do some basic statistical summaries and then practice communicating those results. As you learn, you're gonna be using different tools and techniques but you wanna make sure that the outcome is always understandable and so see if you can bridge that gap as you go. Actually I think when you're learning it's a great time to do this because that's when it's fresh and new to you as well so you can bridge that gap between the technical analysis and using that information to make decisions and talk to people that are less technical to get the point across. Constantly blogging is a great way to do this. Talking to friends or people in your field is a good way to do this and just explaining the analysis you did but in a way that just makes people comfortable that you know what you're talking about and then makes that information usable without getting into too much of the nitty gritty of statistics behind it. Hugo:    For sure. I do think working on datasets that are relevant to you is so important. The titanic and iris data sets don't count even if you think they're relevant to you. Hugo:    We need to move away. I think you dispelled very importantly the myth that you need a PhD to do this type of stuff. I'm wondering what other potential pitfalls or warnings you have for people along the way on their journey. Renee:    I think there's some misconceptions about how much you need to learn. A pitfall is that it's really easy to get discouraged when you're learning. There's so many topics under this umbrella of data science that you can easily get really overwhelmed and not know where to go, especially with self-directed learning. You have to kind of balance learning enough to qualify for the type of job you want but then not over planning it or overdoing it to the point where you're starting to feel totally off track and psyching yourself out and feeling like you're never gonna make it. Renee:    In a talk I gave, I talked about it like you're planning a trip. You could plan it out turn by turn and print out the directions and know exactly where you're gonna turn and what it's gonna look like at each of those turns, but you still wanna have your GPS handy because if you run into unexpected traffic or road closings you gotta route around that. At some point you're gonna feel lost in your learning or like you've totally hit a roadblock but instead of giving up you might just need to go back and find other resources to get you more comfortable with the topic before you move forward again or decide do I really even need to learn this? Maybe you can skip that part and come back later when you have a better understanding. Instead of just getting stuck and waiting for things to kind of clear up in front of you just be prepared to reroute. There's a whole lot of different paths to a data science career and just be prepared to change course. Renee:    Also I think a lot of people look at those terrible job postings that are like a wish list of everything that company could ever want a data scientist to be able to do and they're basically describing a whole data science team in one job posting. People think that they need to learn all of those things in order to get that job so I would say no. Learn a few key things really well. Practice applying that knowledge you have to real world problems so you have experience like overcoming challenges that you're gonna encounter in a real job and that will also help you have a story to tell in your interviews of how you overcame trouble and ended up having usable results in the end. I guess what I'm trying to say is don't derail yourself and don't feel like you have to learn everything you've ever heard of in data science in order to be a data scientist. None of us know how to do everything. You just have to know enough of the basics that you feel solid in that understanding and confident that you could pick up other tools and techniques as you need them. I would say learn the basics and then learn a couple specialty items that might set you apart or are particular to the field that you're trying to get into. Also those communication skills are really important too, not just the tools and techniques. Hugo:    Absolutely. To build on that, something that you hinted at earlier is get out there and do some job interviews as well to find out what the market is like and what interviewers want and ask them questions to figure out what gaps you may have as opposed to learning in the abstract what you think may be needed out in the job market. Renee:    Yeah. It can be discouraging not to get a job but I remember once I did get a data science job looking back and saying all those ones that I didn't get, they weren't right for me any way so why should I feel bad about not getting them? I wasn't right for the job or the company wasn't right for me and so once I found one and it was the right fit and I feel good about it and I like my job so looking back I realize there's just times when it really gets frustrating or depressing if you keep getting turned down, but there's just so many different kinds of data science jobs out there. I think everybody can find one that matches their skills even though it might take a while. Hugo:    Yeah and I do think it's incredible discouraging and horrifying to not get a bunch of jobs in a row. Advice I give which I definitely don't necessarily I find it difficult to take myself though is that you only need one hit. You're looking for one hit out of a bunch of opportunities and the ones that don't work out can be really incredible learning experiences as well. That doesn't make it any less brutal to be turned down. Renee:    Yeah. It's not until after the fact that you look back and realize like how much you learned and how valuable those rejections were. Hugo:    Yeah. Exactly. Talking about what employers are looking for, I think one thing that we can forget about when thinking about data science in the abstract is that a lot of the time it's used to solve business questions. You have a great slide that demonstrates how data analysis and science can be used essentially as an intermediary step to get from a business question to a business answer, so this movement from a business question to a business answer is factored through data science. I'm wondering how keen this concept is to your understanding of data science as a whole. Renee:    Yeah I created that for one of my first data science talks in order to illustrate what I think the data analysis process is. I got such good feedback on it and people really like it so I go back to it a lot now. If for anyone that hasn't seen it, it has four little phrases with arrows between them. It starts with business question and goes to data question and then to data answer and then to business answer. I'll go through each of those. Renee:    For the business question, I don't necessarily mean like a sales and marketing kind of business but like a domain question, something that a decision maker in your particular field or business might ask. You're job as an analyst is to convert that into a data question. What data is required in order to answer it? Do we have it available? What related questions might we have to answer first to get to that one? What type of analysis needs to be done to get us to a usable answer? Then you have to do the analysis so that's the data answer piece. This type of analysis will depend on like what kind of field you're in, what's your role and your skills, what data is available so the type of analysis differs but basically to turn that data question into a data answer you're doing analysis. Renee:    Then you have to take the results of that and turn that into a business answer. There's very few people out there that will want to hear your data answer. You have to be able to communicate that in terms that a non data scientist can understand so that they know what the data is telling them and can use that information to make a business decision. You have to be able to convey statistical results and uncertainty in business terms and explain what your analysis means and does not mean so it's not misused. A report when we talk about building a report, in the real world the end result is usually not some sort of statistical readout with model evaluation metrics. It's like a presentation of the results that are clear and usable by people that are not data scientists. Hugo:    Absolutely and I do think to keep in mind that we're always attempting to answer business questions or develop business insights in this context is incredibly important. I wanna shift slightly. We have a lot of aspiring data scientists and learners out there. I'm wondering what's your take on where people can learn, particular places people can learn the skills and knowledge necessary to become a data scientist. Renee:    Well like I said I have a hard time giving specific recommendations because it's so personal but I've heard great things about DataCamp of course. It's actually the highest rated course system on DataSciGuide, so people that use DataCamp seem to really love it. Hugo:    That's great. I'm personally I'm a huge fan of Data Camp as well. I don't know whether there's any bias involved here. Renee:    I'm not saying that just to suck up. It's really... people love it. Also there's Data Quest. There's Khan Academy for some of those basic skills. There are lots of books out there. People tend to really like the O'Reilly books and there's some other favorites. Again, I hesitate to give specific recommendation just because they vary so much. People can tweet me if you're looking for a certain resource that will get you started from where you're at and usually I retweet that and lots of people that follow me will help answer. It's really kind of a personalized answer but I'll just say there are a ton of resources and it's easy to get overwhelmed by the resources so don't be afraid to ask to find what might be best for you and then if someone recommends something and you really don't like it don't feel bad about that either. Just move on to the next thing. Renee:    So yeah I mean my site, Data Sci Guide, I'm trying to collect those reviews from data science learners so we can get a sense of what did you need to know before you used this resources because that tripped me up a lot when I was learning is there weren't clear pre-requisites for certain resources and I would start out real excited like yeah I'm getting it and then five lessons in be totally overwhelmed and wanting to give up. I think that's dangerous. Yeah talk to people that are just ahead of you on the learning path maybe and find out what helped them get to over that first step from where you are to where they are and maybe not reach out to people that are already working as data scientists but other data science learners. Hugo:    So something we've been talking around, Renee, is Twitter which can be an incredible resource for aspiring data scientists so maybe you can tell me a bit more about that. Renee:    Yeah so in addition to all like the books and courses and tutorials, I really use Twitter a lot to get the lingo of data science. There are these great communities on Twitter and you can usually use them by searching for certain hashtags. I'll give you a few of them. For Python people, there's pydata, pyladies, p4ds. For people learning R, there's Rstats and Rladies, R4ds. These are all hashtags you can search. A lot of those have slack channels too. There's a data science learning club slack channel that some followers of mine started a while back based on my podcast learning activities. There's a slack called data for democracy for people who want to get into political data. There's a hashtag for data ethics, so I'm sure there's similar groups like these on other social media like Facebook and LinkedIn but I'm mostly on Twitter so I have a whole blog post about using Twitter to learn data science and if you start searching for hashtags related to what you're learning, you'll usually start finding the leaders or the hubs in these communities and you can learn a whole lot just by following them. Then if you ask a question and use that hashtag you'll usually get an answer. It's pretty cool. Hugo:    That's awesome. We'll link to your article on how to use Twitter to learn data science in the show notes as well. So for learners, how will they know when they're ready to actually be a data science or start interviewing? Renee:    Yeah. I think people are ready to start applying for jobs before they feel fully ready to make that jump. Don't wait too long to start looking. Like we talked about, like doing those interviews is really instructional as well but I'd say that you're ready when you're confident enough with those basics so you know how to do exploratory data analysis and do some statistical summaries. You know that basic feature engineering, how to get a dataset into shape that you can use for machine learning. You know how to do some of that pre-processing and clean up. You can build a good report and a data visualization and communicate the results. Maybe you've used a few basic commonly used machine learning algorithms like logistic regression and random forest, so you're confident enough with these basics that you know that you're not gonna be totally struggling on the job. Renee:    Once you feel that you have that solid understanding of like how machine learning works and you can apply it, you probably want to also add in a few specific techniques that will make you stand out, either something you feel like you're good at. Maybe you're really awesome at building pretty visualizations that are easy to read. Maybe you're really good at that back-end data engineering stuff. Something that you can say is your specialty when you're applying for the jobs but you don't need to check off the entire list of every algorithm and every tool and technique out there. Renee:    I've interviewed for jobs that included skills that I already had throughout my career and I was confident with, plus some skills that I was still picking up. If I knew that I could understand what people wanted and I was confident enough that I could pick up those new tools and techniques along the way, then I realized like I got a job before I thought I was ready and at least I hope and I've been told that I've done really well there. A lot of stuff you can pick up as you go if you have the basics down. Don't feel like you have to be an expert in every area. Nobody is. Start applying and you'll get a sense for what it is that you still need to learn in order to get a certain type of job but yeah don't wait too long. Hugo:    I think the field is so vast and there are so many techniques and new techniques emerging all the time that if you try to be as comprehensive as possible you'll always feel there's more stuff to learn and you'll never get out there. Renee:    Yeah you're going to be learning on the job no matter how advanced you are when you apply. There's a huge demand out there right now for people with data skills, so even if you get kind of a transitional data analyst type of role you might not have the title of data scientist right away, but if it's a role that offers you the possibility of doing some machine learning yeah you can grow into that as you work. Hugo:    I wanna shift slightly. Recently you gave a talk called Can a Machine be Racist or Sexist? Using this question you posed as a jumping board, can you speak to what you consider the biggest ethical challenges facing data science and data scientists as a community? Renee:    Yeah so we could do a whole episode just about this. I'll connect you with some people that I think would be great interviews that could talk extensively on this topic but the main purpose of me doing that talk was to get people to understand that even though you're using these mathematical algorithms and computers to get a result, that doesn't mean that things produced by data science are unbiased. There's so many ways that bias, maybe you'd say racism or sexism and I'm talking about a system at kind, so not somebody yelling a word at somebody on the street, but historical racism that's baked into systems. I have that masters in systems engineering and I think I've always been kind of a systems thinker so I picked up on this quickly and I was trying to share it with other people. You can link to my whole talk for all the slides. I really struggle to cram in all the examples I wanted to give because there's really so much to learn here. With machine learning, you're really doing pattern matching. That's what those algorithms are doing, finding patterns in the data which is a lot like stereotyping. You have to be aware of what data is going into making those decisions and make sure you understand the model outputs and it's not completely a black box where you don't understand why a particular decision was made by the model when people's lives are being affected. Biases can be introduced at every step along the way in this development process. The data could have been incorrectly recorded in the first place. It might not be representative of the full population. It might be a limited sample and you're training your model assuming it's gonna generalize and it might not. Renee:    Your data could contain historic biases. For instance, crime databases are only gonna contain records for crimes in areas that are policed. If a crime at a certain location isn't observed or isn't recorded into the system by the police, an algorithm you train on that is gonna think there was no crime there and make predictions accordingly so it's just you're encoding not what's happening in the real world necessarily but you're capturing what people are capturing about the system that you're looking at. There's certain techniques that can amplify bias when you're doing your pre-processing and model training. Renee:    There's the question of what are you even optimizing for? For instance, YouTube has this problem where they're optimizing for viewing time. They want your eyeballs on their ads. If something is like particularly crazy or creepy or exciting, people are gonna watch it a little longer and so those videos that are really extreme will bubble up to the top and be recommended to more people because when you watch them you might be fascinated by them and watch longer. It can kind of radicalize people. People might get to the point, especially kids I think, where you can't necessarily separate the truth from this fiction that's constantly in front of you because that fiction is exciting and interesting and makes you watch longer. Renee:    What you're optimizing for and what kind of effects that could have is important. How do you even decide when to stop optimizing or if the results of your model are good? That's a decision that requires a human input. How do you know if the results of your model are being used properly and it's not being misused or misinterpreted? There's people and people making decisions at every step along the molded development process so you can't say that oh it's automated and computerized. There's no bias involved. There can be bias introduced at every single step. Hugo:    A lot of these issues are cultural as well, that as a community of data scientists we're only now really starting, well there's been work done on it previously, I don't wanna dismiss that but we're really only starting to think collectively about how to approach these problems now. Renee:    Yeah definitely. Yeah and it's a culture of how the company is run and it really takes us data scientists making decisions about what we're willing to do as well. So much of this like the models are being built under pressure for deadlines and being rolled out and you might not even know how it's being used in the end, but just being aware of the impact of these things that we're building is important. I love this quote from Susan Etlinger in a TED Talk that she gave. She said we have the potential to make bad decisions far more quickly, efficiently, and with far greater impact than we did in the past. We're really just speeding up these decisions. We're not necessarily making them better unless we make an effort to do that, so we have to make sure that as data scientists that we're not causing harm and we're in high demand right now so we're lucky we have some choice in what kind of businesses we're willing to work for and what kind of products we're willing to contribute to. We can make a difference in our future and hopefully make it a little less dystopian than the entertainment world imagines or that we can imagine just by being aware of this and making conscious decisions of what we're willing to build. Hugo:    I couldn't agree more. Renee, do you have a final call to action for our listeners out there? Renee:    Yeah so I know there's a lot of people that listen to these podcasts that are just getting into data science but some people have been lurking on Twitter for a long time, listening to podcasts for a long time, reading books, and so my call to action for them is like dig in. Find a data set. Start working with it. Tweet me at becomingdatasci if you need help. I'll connect you with an online community that can help get you started. Don't delay actually working with real data. Renee:    My call to action for people that aren't new to data science is I would encourage you to read up on the data ethics so that you understand how the work that you do in this field can affect real people's lives. There's lots of great books out there now so someone remind me when this episode comes out and I will tweet a list and share a bunch of books that I've collected that I've either read already or they're in my kindle waiting to be read because I'm really interested in this topic and it's important to me and I think it's vital for people in our industry to be well aware of, so that would be my call to action for people that are already data scientists. Hugo:    Fantastic. Renee, it's been such a pleasure having you on the show. Renee:    Great thanks for having me, Hugo. I've been listening for a long time and it's exciting to actually be on here. Hugo:    It's great to have you on particularly because I was listening to your podcast for so long, so it was a really fun experience. Renee:    Great.","Keyword(freq): scientist(19), question(14), skill(13), result(10), technique(10), path(9), resource(9), algorithm(8), job(8), basic(7)"
"5","mastery",2018-09-21,"How to Develop 1D Convolutional Neural Network Models for Human Activity Recognition","https://machinelearningmastery.com/cnn-models-for-human-activity-recognition-time-series-classification/","Human activity recognition is the problem of classifying sequences of accelerometer data recorded by specialized harnesses or smart phones into known well-defined movements. Classical approaches to the problem involve hand crafting features from the time series data based on fixed-sized windows and training machine learning models, such as ensembles of decision trees. The difficulty is that this feature engineering requires deep expertise in the field. Recently, deep learning methods such as recurrent neural networks and one-dimensional convolutional neural networks, or CNNs, have been shown to provide state-of-the-art results on challenging activity recognition tasks with little or no data feature engineering, instead using feature learning on raw data. In this tutorial, you will discover how to develop one-dimensional convolutional neural networks for time series classification on the problem of human activity recognition. After completing this tutorial, you will know: Let¡¯s get started. How to Develop 1D Convolutional Neural Network Models for Human Activity RecognitionPhoto by Wolfgang Staudt, some rights reserved. This tutorial is divided into four parts; they are: Human Activity Recognition, or HAR for short, is the problem of predicting what a person is doing based on a trace of their movement using sensors. A standard human activity recognition dataset is the ¡®Activity Recognition Using Smart Phones Dataset¡¯ made available in 2012. It was prepared and made available by Davide Anguita, et al. from the University of Genova, Italy and is described in full in their 2013 paper ¡°A Public Domain Dataset for Human Activity Recognition Using Smartphones.¡± The dataset was modeled with machine learning algorithms in their 2012 paper titled ¡°Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine.¡± The dataset was made available and can be downloaded for free from the UCI Machine Learning Repository: The data was collected from 30 subjects aged between 19 and 48 years old performing one of six standard activities while wearing a waist-mounted smartphone that recorded the movement data. Video was recorded of each subject performing the activities and the movement data was labeled manually from these videos. Below is an example video of a subject performing the activities while their movement data is being recorded. The six activities performed were as follows: The movement data recorded was the x, y, and z accelerometer data (linear acceleration) and gyroscopic data (angular velocity) from the smart phone, specifically a Samsung Galaxy S II. Observations were recorded at 50 Hz (i.e. 50 data points per second). Each subject performed the sequence of activities twice, once with the device on their left-hand-side and once with the device on their right-hand side. The raw data is not available. Instead, a pre-processed version of the dataset was made available. The pre-processing steps included: Feature engineering was applied to the window data, and a copy of the data with these engineered features was made available. A number of time and frequency features commonly used in the field of human activity recognition were extracted from each window. The result was a 561 element vector of features. The dataset was split into train (70%) and test (30%) sets based on data for subjects, e.g. 21 subjects for train and nine for test. Experiment results with a support vector machine intended for use on a smartphone (e.g. fixed-point arithmetic) resulted in a predictive accuracy of 89% on the test dataset, achieving similar results as an unmodified SVM implementation. The dataset is freely available and can be downloaded from the UCI Machine Learning repository. The data is provided as a single zip file that is about 58 megabytes in size. The direct link for this download is below: Download the dataset and unzip all files into a new directory in your current working directory named ¡°HARDataset¡±. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this section, we will develop a one-dimensional convolutional neural network model (1D CNN) for the human activity recognition dataset. Convolutional neural network models were developed for image classification problems, where the model learns an internal representation of a two-dimensional input, in a process referred to as feature learning. This same process can be harnessed on one-dimensional sequences of data, such as in the case of acceleration and gyroscopic data for human activity recognition. The model learns to extract features from sequences of observations and how to map the internal features to different activity types. The benefit of using CNNs for sequence classification is that they can learn from the raw time series data directly, and in turn do not require domain expertise to manually engineer input features. The model can learn an internal representation of the time series data and ideally achieve comparable performance to models fit on a version of the dataset with engineered features. This section is divided into 4 parts; they are: The first step is to load the raw dataset into memory. There are three main signal types in the raw data: total acceleration, body acceleration, and body gyroscope. Each has three axes of data. This means that there are a total of nine variables for each time step. Further, each series of data has been partitioned into overlapping windows of 2.65 seconds of data, or 128 time steps. These windows of data correspond to the windows of engineered features (rows) in the previous section. This means that one row of data has (128 * 9), or 1,152, elements. This is a little less than double the size of the 561 element vectors in the previous section and it is likely that there is some redundant data. The signals are stored in the /Inertial Signals/ directory under the train and test subdirectories. Each axis of each signal is stored in a separate file, meaning that each of the train and test datasets have nine input files to load and one output file to load. We can batch the loading of these files into groups given the consistent directory structures and file naming conventions. The input data is in CSV format where columns are separated by whitespace. Each of these files can be loaded as a NumPy array. The load_file() function below loads a dataset given the file path to the file and returns the loaded data as a NumPy array. We can then load all data for a given group (train or test) into a single three-dimensional NumPy array, where the dimensions of the array are [samples, time steps, features]. To make this clearer, there are 128 time steps and nine features, where the number of samples is the number of rows in any given raw signal data file. The load_group() function below implements this behavior. The dstack() NumPy function allows us to stack each of the loaded 3D arrays into a single 3D array where the variables are separated on the third dimension (features). We can use this function to load all input signal data for a given group, such as train or test. The load_dataset_group() function below loads all input signal data and the output data for a single group using the consistent naming conventions between the train and test directories. Finally, we can load each of the train and test datasets. The output data is defined as an integer for the class number. We must one hot encode these class integers so that the data is suitable for fitting a neural network multi-class classification model. We can do this by calling the to_categorical() Keras function. The load_dataset() function below implements this behavior and returns the train and test X and y elements ready for fitting and evaluating the defined models. Now that we have the data loaded into memory ready for modeling, we can define, fit, and evaluate a 1D CNN model. We can define a function named evaluate_model() that takes the train and test dataset, fits a model on the training dataset, evaluates it on the test dataset, and returns an estimate of the models performance. First, we must define the CNN model using the Keras deep learning library. The model requires a three-dimensional input with [samples, time steps, features]. This is exactly how we have loaded the data, where one sample is one window of the time series data, each window has 128 time steps, and a time step has nine variables or features. The output for the model will be a six-element vector containing the probability of a given window belonging to each of the six activity types. These input and output dimensions are required when fitting the model, and we can extract them from the provided training dataset. The model is defined as a Sequential Keras model, for simplicity. We will define the model as having two 1D CNN layers, followed by a dropout layer for regularization, then a pooling layer. It is common to define CNN layers in groups of two in order to give the model a good chance of learning features from the input data. CNNs learn very quickly, so the dropout layer is intended to help slow down the learning process and hopefully result in a better final model. The pooling layer reduces the learned features to 1/4 their size, consolidating them to only the most essential elements. After the CNN and pooling, the learned features are flattened to one long vector and pass through a fully connected layer before the output layer used to make a prediction. The fully connected layer ideally provides a buffer between the learned features and the output with the intent of interpreting the learned features before making a prediction. For this model, we will use a standard configuration of 64 parallel feature maps and a kernel size of 3. The feature maps are the number of times the input is processed or interpreted, whereas the kernel size is the number of input time steps considered as the input sequence is read or processed onto the feature maps. The efficient Adam version of stochastic gradient descent will be used to optimize the network, and the categorical cross entropy loss function will be used given that we are learning a multi-class classification problem. The definition of the model is listed below. The model is fit for a fixed number of epochs, in this case 10, and a batch size of 32 samples will be used, where 32 windows of data will be exposed to the model before the weights of the model are updated. Once the model is fit, it is evaluated on the test dataset and the accuracy of the fit model on the test dataset is returned. The complete evaluate_model() function is listed below. There is nothing special about the network structure or chosen hyperparameters; they are just a starting point for this problem. We cannot judge the skill of the model from a single evaluation. The reason for this is that neural networks are stochastic, meaning that a different specific model will result when training the same model configuration on the same data. This is a feature of the network in that it gives the model its adaptive ability, but requires a slightly more complicated evaluation of the model. We will repeat the evaluation of the model multiple times, then summarize the performance of the model across each of those runs. For example, we can call evaluate_model() a total of 10 times. This will result in a population of model evaluation scores that must be summarized. We can summarize the sample of scores by calculating and reporting the mean and standard deviation of the performance. The mean gives the average accuracy of the model on the dataset, whereas the standard deviation gives the average variance of the accuracy from the mean. The function summarize_results() below summarizes the results of a run. We can bundle up the repeated evaluation, gathering of results, and summarization of results into a main function for the experiment, called run_experiment(), listed below. By default, the model is evaluated 10 times before the performance of the model is reported. Now that we have all of the pieces, we can tie them together into a worked example. The complete code listing is provided below. Running the example first prints the shape of the loaded dataset, then the shape of the train and test sets and the input and output elements. This confirms the number of samples, time steps, and variables, as well as the number of classes. Next, models are created and evaluated and a debug message is printed for each. Finally, the sample of scores is printed followed by the mean and standard deviation. We can see that the model performed well achieving a classification accuracy of about 90.9% trained on the raw dataset, with a standard deviation of about 1.3. This is a good result, considering that the original paper published a result of 89%, trained on the dataset with heavy domain-specific feature engineering, not the raw dataset. Note: Given the stochastic nature of the algorithm, your specific results may vary. Now that we have seen how to load the data and fit a 1D CNN model, we can investigate whether we can further lift the skill of the model with some hyperparameter tuning. In this section, we will tune the model in an effort to further improve performance on the problem. We will look at three main areas: In the previous section, we did not perform any data preparation. We used the data as-is. Each of the main sets of data (body acceleration, body gyroscopic, and total acceleration) have been scaled to the range -1, 1. It is not clear if the data was scaled per-subject or across all subjects. One possible transform that may result in an improvement is to standardize the observations prior to fitting a model. Standardization refers to shifting the distribution of each variable such that it has a mean of zero and a standard deviation of 1. It really only makes sense if the distribution of each variable is Gaussian. We can quickly check the distribution of each variable by plotting a histogram of each variable in the training dataset. A minor difficulty in this is that the data has been split into windows of 128 time steps, with a 50% overlap. Therefore, in order to get a fair idea of the data distribution, we must first remove the duplicated observations (the overlap), then remove the windowing of the data. We can do this using NumPy, first slicing the array and only keeping the second half of each window, then flattening the windows into a long vector for each variable. This is quick and dirty and does mean that we lose the data in the first half of the first window. The complete example of loading the data, flattening it, and plotting a histogram for each of the nine variables is listed below. Running the example creates a figure with nine histogram plots, one for each variable in the training dataset. The order of the plots matches the order in which the data was loaded, specifically: We can see that each variable has a Gaussian-like distribution, except perhaps the first variable (Total Acceleration x). The distributions of total acceleration data is flatter than the body data, which is more pointed. We could explore using a power transform on the data to make the distributions more Gaussian, although this is left as an exercise. Histograms of each variable in the training data set The data is sufficiently Gaussian-like to explore whether a standardization transform will help the model extract salient signal from the raw observations. The function below named scale_data() can be used to standardize the data prior to fitting and evaluating the model. The StandardScaler scikit-learn class will be used to perform the transform. It is first fit on the training data (e.g. to find the mean and standard deviation for each variable), then applied to the train and test sets. The standardization is optional, so we can apply the process and compare the results to the same code path without the standardization in a controlled experiment. We can update the evaluate_model() function to take a parameter, then use this parameter to decide whether or not to perform the standardization. We can also update the run_experiment() to repeat the experiment 10 times for each parameter; in this case, only two parameters will be evaluated [False, True] for no standardization and standardization respectively. This will result in two samples of results that can be compared. We will update the summarize_results() function to summarize the sample of results for each configuration parameter and to create a boxplot to compare each sample of results. These updates will allow us to directly compare the results of a model fit as before and a model fit on the dataset after it has been standardized. It is also a generic change that will allow us to evaluate and compare the results of other sets of parameters in the following sections. The complete code listing is provided below. Running the example may take a few minutes, depending on your hardware. The performance is printed for each evaluated model. At the end of the run, the performance of each of the tested configurations is summarized showing the mean and the standard deviation. We can see that it does look like standardizing the dataset prior to modeling does result in a small lift in performance from about 90.4% accuracy (close to what we saw in the previous section) to about 91.5% accuracy. Note: Given the stochastic nature of the algorithm, your specific results may vary. A box and whisker plot of the results is also created. This allows the two samples of results to be compared in a nonparametric way, showing the median and the middle 50% of each sample. We can see that the distribution of results with standardization is quite different from the distribution of results without standardization. This is likely a real effect. Box and whisker plot of 1D CNN with and without standardization Now that we have an experimental framework, we can explore varying other hyperparameters of the model. An important hyperparameter for the CNN is the number of filter maps. We can experiment with a range of different values, from less to many more than the 64 used in the first model that we developed. Specifically, we will try the following numbers of feature maps: We can use the same code from the previous section and update the evaluate_model() function to use the provided parameter as the number of filters in the Conv1D layers. We can also update the summarize_results() function to save the boxplot as exp_cnn_filters.png. The complete code example is listed below. Running the example repeats the experiment for each of the specified number of filters. At the end of the run, a summary of the results with each number of filters is presented. We can see perhaps a trend of increasing average performance with the increase in the number of filter maps. The variance stays pretty constant, and perhaps 128 feature maps might be a good configuration for the network. A box and whisker plot of the results is also created, allowing the distribution of results with each number of filters to be compared. From the plot, we can see the trend upward in terms of median classification accuracy (orange line on the box) with the increase in the number of feature maps. We do see a dip at 64 feature maps (the default or baseline in our experiments), which is surprising, and perhaps a plateau in accuracy across 32, 128, and 256 filter maps. Perhaps 32 would be a more stable configuration. Box and whisker plot of 1D CNN with different numbers of filter maps The size of the kernel is another important hyperparameter of the 1D CNN to tune. The kernel size controls the number of time steps consider in each ¡°read¡± of the input sequence, that is then projected onto the feature map (via the convolutional process). A large kernel size means a less rigorous reading of the data, but may result in a more generalized snapshot of the input. We can use the same experimental set-up and test a suite of different kernel sizes in addition to the default of three time steps. The full list of values is as follows: The complete code listing is provided below: Running the example tests each kernel size in turn. The results are summarized at the end of the run. We can see a general increase in model performance with the increase in kernel size. The results suggest a kernel size of 5 might be good with a mean skill of about 91.8%, but perhaps a size of 7 or 11 may also be just as good with a smaller standard deviation. A box and whisker plot of the results is also created. The results suggest that a larger kernel size does appear to result in better accuracy and that perhaps a kernel size of 7 provides a good balance between good performance and low variance. Box and whisker plot of 1D CNN with different numbers of kernel sizes This is just the beginning of tuning the model, although we have focused on perhaps the more important elements. It might be interesting to explore combinations of some of the above findings to see if performance can be lifted even further. It may also be interesting to increase the number of repeats from 10 to 30 or more to see if it results in more stable findings. Another popular approach with CNNs is to have a multi-headed model, where each head of the model reads the input time steps using a different sized kernel. For example, a three-headed model may have three different kernel sizes of 3, 5, 11, allowing the model to read and interpret the sequence data at three different resolutions. The interpretations from all three heads are then concatenated within the model and interpreted by a fully-connected layer before a prediction is made. We can implement a multi-headed 1D CNN using the Keras functional API. For a gentle introduction to this API, see the post: The updated version of the evaluate_model() function is listed below that creates a three-headed CNN model. We can see that each head of the model is the same structure, although the kernel size is varied. The three heads then feed into a single merge layer before being interpreted prior to making a prediction. When the model is created, a plot of the network architecture is created; provided below, it gives a clear idea of how the constructed model fits together. Plot of the Multi-Headed 1D Convolutional Neural Network Other aspects of the model could be varied across the heads, such as the number of filters or even the preparation of the data itself. The complete code example with the multi-headed 1D CNN is listed below. Running the example prints the performance of the model each repeat of the experiment and then summarizes the estimated score as the mean and standard deviation, as we did in the first case with the simple 1D CNN. We can see that the average performance of the model is about 91.6% classification accuracy with a standard deviation of about 0.8. This example may be used as the basis for exploring a variety of other models that vary different model hyperparameters and even different data preparation schemes across the input heads. It would not be an apples-to-apples comparison to compare this result with a single-headed CNN given the relative tripling of the resources in this model. Perhaps an apples-to-apples comparison would be a model with the same architecture and the same number of filters across each input head of the model. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop one-dimensional convolutional neural networks for time series classification on the problem of human activity recognition. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning.","Keyword(freq): result(26), feature(19), map(11), step(10), model(8), sample(7), window(7), filter(6), activity(5), cnn(5)"
"6","mastery",2018-09-19,"How to Evaluate Machine Learning Algorithms for Human Activity Recognition","https://machinelearningmastery.com/evaluate-machine-learning-algorithms-for-human-activity-recognition/","Human activity recognition is the problem of classifying sequences of accelerometer data recorded by specialized harnesses or smart phones into known well-defined movements. Classical approaches to the problem involve hand crafting features from the time series data based on fixed-sized windows and training machine learning models, such as ensembles of decision trees. The difficulty is that this feature engineering requires deep expertise in the field. Recently, deep learning methods such as recurrent neural networks and one-dimensional convolutional neural networks, or CNNs, have been shown to provide state-of-the-art results on challenging activity recognition tasks with little or no data feature engineering, instead using feature learning on raw data. In this tutorial, you will discover how to evaluate a diverse suite of machine learning algorithms on the ¡®Activity Recognition Using Smartphones¡® dataset. After completing this tutorial, you will know: Let¡¯s get started. How to Evaluate Machine Learning Algorithms for Human Activity RecognitionPhoto by Murray Foubister, some rights reserved. This tutorial is divided into three parts; they are: Human Activity Recognition, or HAR for short, is the problem of predicting what a person is doing based on a trace of their movement using sensors. A standard human activity recognition dataset is the ¡®Activity Recognition Using Smart Phones¡¯ dataset made available in 2012. It was prepared and made available by Davide Anguita, et al. from the University of Genova, Italy and is described in full in their 2013 paper ¡°A Public Domain Dataset for Human Activity Recognition Using Smartphones.¡± The dataset was modeled with machine learning algorithms in their 2012 paper titled ¡°Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine.¡± The dataset was made available and can be downloaded for free from the UCI Machine Learning Repository: The data was collected from 30 subjects aged between 19 and 48 years old performing one of six standard activities while wearing a waist-mounted smartphone that recorded the movement data. Video was recorded of each subject performing the activities and the movement data was labeled manually from these videos. Below is an example video of a subject performing the activities while their movement data is being recorded. The six activities performed were as follows: The movement data recorded was the x, y, and z accelerometer data (linear acceleration) and gyroscopic data (angular velocity) from the smart phone, specifically a Samsung Galaxy S II. Observations were recorded at 50 Hz (i.e. 50 data points per second). Each subject performed the sequence of activities twice, once with the device on their left-hand-side and once with the device on their right-hand side. The raw data is not available. Instead, a pre-processed version of the dataset was made available. The pre-processing steps included: Feature engineering was applied to the window data, and a copy of the data with these engineered features was made available. A number of time and frequency features commonly used in the field of human activity recognition were extracted from each window. The result was a 561 element vector of features. The dataset was split into train (70%) and test (30%) sets based on data for subjects, e.g. 21 subjects for train and nine for test. Experiment results with a support vector machine intended for use on a smartphone (e.g. fixed-point arithmetic) resulted in a predictive accuracy of 89% on the test dataset, achieving similar results as an unmodified SVM implementation. The dataset is freely available and can be downloaded from the UCI Machine Learning repository. The data is provided as a single zip file that is about 58 megabytes in size. The direct link for this download is below: Download the dataset and unzip all files into a new directory in your current working directory named ¡°HARDataset¡±. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this section, we will develop code to load the feature-engineered version of the dataset and evaluate a suite of nonlinear machine learning algorithms, including SVM used in the original paper. The goal is to achieve at least 89% accuracy on the test dataset. The results of methods using the feature-engineered version of the dataset provide a baseline for any methods developed for the raw data version. This section is divided into five parts; they are: The first step is to load the train and test input (X) and output (y) data. Specifically, the following files: The input data is in CSV format where columns are separated via whitespace. Each of these files can be loaded as a NumPy array. The load_file() function below loads a dataset given the file path to the file and returns the loaded data as a NumPy array. We can call this function to load the X and y files for a given train or test set group, given the similarity in directory layout and filenames.<U+00A0> The load_dataset_group() function below will load both of these files for a group and return the X and y elements as NumPy arrays. This function can then be used to load the X and y elements for both the train and test groups. Finally, we can load both the train and test dataset and return them as NumPy arrays ready for fitting and evaluating machine learning models. We can call this function to load all of the required data; for example: Next, we can define a list of machine learning models to evaluate on this problem. We will evaluate the models using default configurations. We are not looking for optimal configurations of these models at this point, just a general idea of how well sophisticated models with default configurations perform on this problem. We will evaluate a diverse set of nonlinear and ensemble machine learning algorithms, specifically: Nonlinear Algorithms: Ensemble Algorithms: We will define the models and store them in a dictionary that maps the model object to a short name that will help in analyzing the results. The define_models() function below defines the eight models that we will evaluate. This function is quite extensible and you can easily update to define any machine learning models or model configurations you wish. The next step is to evaluate the defined models in the loaded dataset. This step is divided into the evaluation of a single model and the evaluation of all of the models. We will evaluate a single model by first fitting it on the training dataset, making a prediction on the test dataset, and then evaluating the prediction using a metric. In this case we will use classification accuracy that will capture the performance (or error) of a model given the balance observations across the six activities (or classes). The evaluate_model() function below implements this behavior, evaluating a given model and returning the classification accuracy as a percentage. We can now call the evaluate_model() function repeatedly for each of the defined model. The evaluate_models() function below implements this behavior, taking the dictionary of defined models, and returns a dictionary of model names mapped to their classification accuracy. Because the evaluation of the models may take a few minutes, the function prints the performance of each model after it is evaluated as some verbose feedback. The final step is to summarize the findings. We can sort all of the results by the classification accuracy in descending order because we are interested in maximizing accuracy. The results of the evaluated models can then be printed, clearly showing the relative rank of each of the evaluated models. The summarize_results() function below implements this behavior. We know that we have all of the pieces in place. The complete example of evaluating a suite of eight machine learning models on the feature-engineered version of the dataset is listed below. Running the example first loads the train and test datasets, showing the shape of each of the input and output components. The eight models are then evaluated in turn, printing the performance for each. Finally, a rank of the models by their performance on the test set is displayed. We can see that both the ExtraTrees ensemble method and the Support Vector Machines nonlinear methods achieve a performance of about 94% accuracy on the test set. This is a great result, exceeding the reported 89% by SVM in the original paper. The specific results may vary each time the code is run, given the stochastic nature of the algorithms. Nevertheless, given the size of the dataset, the relative relationships between the algorithm¡¯s performance should be reasonably stable. These results show what is possible given domain expertise in the preparation of the data and the engineering of domain-specific features. As such, these results can be taken as a performance upper-bound of what could be pursued through more advanced methods that may be able to automatically learn features as part of fitting the model, such as deep learning methods. Any such advanced methods would be fit and evaluated on the raw data from which the engineered features were derived. And as such, the performance of machine learning algorithms evaluated on that data directly may provide an expected lower bound on the performance of any more advanced methods. We will explore this in the next section. We can use the same framework for evaluating machine learning models on the raw data. The raw data does require some more work to load. There are three main signal types in the raw data: total acceleration, body acceleration, and body gyroscope. Each has three axes of data. This means that there are a total of nine variables for each time step. Further, each series of data has been partitioned into overlapping windows of 2.65 seconds of data, or 128 time steps. These windows of data correspond to the windows of engineered features (rows) in the previous section. This means that one row of data has 128 * 9 or 1,152 elements. This is a little less than double the size of the 561 element vectors in the previous section and it is likely that there is some redundant data. The signals are stored in the /Inertial Signals/ directory under the train and test subdirectories. Each axis of each signal is stored in a separate file, meaning that each of the train and test datasets have nine input files to load and one output file to load. We can batch the loading of these files into groups given the consistent directory structures and file naming conventions. First, we can load all data for a given group into a single three-dimensional NumPy array, where the dimensions of the array are [samples, time steps, features]. To make this clearer, there are 128 time steps and nine features, where the number of samples is the number of rows in any given raw signal data file. The load_group() function below implements this behavior. The dstack() NumPy function allows us to stack each of the loaded 3D arrays into a single 3D array where the variables are separated on the third dimension (features). We can use this function to load all input signal data for a given group, such as train or test. The load_dataset_group() function below loads all input signal data and the output data for a single group using the consistent naming conventions between the directories. Finally, we can load each of the train and test datasets. As part of preparing the loaded data, we must flatten the windows and features into one long vector. We can do this with the NumPy reshape function and convert the three dimensions of [samples, timesteps, features] into the two dimensions of [samples, timesteps * features]. The load_dataset() function below implements this behavior and returns the train and test X and y elements ready for fitting and evaluating the defined models. Putting this all together, the complete example is listed below. Running the example first loads the dataset. We can see that the raw train and test sets have the same number of samples as the engineered features (7352 and 2947 respectively) and that the three-dimensional data was loaded correctly. We can also see the flattened data and the 1152 input vectors that will be provided to the models. Next the eight defined models are evaluated in turn. The final results suggest that ensembles of decision trees perform the best on the raw data. Gradient Boosting and Extra Trees perform the best with about 87% and 86% accuracy, about seven points below the best performing models on the feature-engineered version of the dataset. It is encouraging that the Extra Trees ensemble method performed well on both datasets; it suggests it and similar tree ensemble methods may be suited to the problem, at least in this simplified framing. We can also see the drop of SVM to about 72% accuracy. The good performance of ensembles of decision trees may suggest the need for feature selection and the ensemble methods ability to select those features that are most relevant to predicting the associated activity. As noted in the previous section, these results provide a lower-bound on accuracy for any more sophisticated methods that may attempt to learn higher order features automatically (e.g. via feature learning in deep learning methods) from the raw data. In summary, the bounds for such methods extend on this dataset from about 87% accuracy with GBM on the raw data to about 94% with Extra Trees and SVM on the highly processed dataset, [87% to 94%]. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to evaluate a diverse suite of machine learning algorithms on the ¡®Activity Recognition Using Smartphones¡® dataset. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning.","Keyword(freq): model(23), feature(17), method(13), result(13), algorithm(11), file(7), activity(6), tree(6), implement(5), sample(5)"
"7","mastery",2018-09-17,"How to Model Human Activity From Smartphone Data","https://machinelearningmastery.com/how-to-model-human-activity-from-smartphone-data/","Human activity recognition is the problem of classifying sequences of accelerometer data recorded by specialized harnesses or smart phones into known well-defined movements. It is a challenging problem given the large number of observations produced each second, the temporal nature of the observations, and the lack of a clear way to relate accelerometer data to known movements. Classical approaches to the problem involve hand crafting features from the time series data based on fixed-sized windows and training machine learning models, such as ensembles of decision trees. The difficulty is that this feature engineering requires deep expertise in the field. Recently, deep learning methods such as recurrent neural networks and one-dimensional convolutional neural networks, or CNNs, have been shown to provide state-of-the-art results on challenging activity recognition tasks with little or no data feature engineering. In this tutorial, you will discover the ¡®Activity Recognition Using Smartphones¡® dataset for time series classification and how to load and explore the dataset in order to make it ready for predictive modeling. After completing this tutorial, you will know: Let¡¯s get started. How to Model Human Activity From Smartphone DataPhoto by photographer, some rights reserved. This tutorial is divided into 10 parts; they are: Human Activity Recognition, or HAR for short, is the problem of predicting what a person is doing based on a trace of their movement using sensors. Movements are often normal indoor activities such as standing, sitting, jumping, and going up stairs. Sensors are often located on the subject, such as a smartphone or vest, and often record accelerometer data in three dimensions (x, y, z). Human Activity Recognition (HAR) aims to identify the actions carried out by a person given a set of observations of him/herself and the surrounding environment. Recognition can be accomplished by exploiting the information retrieved from various sources such as environmental or body-worn sensors. <U+2014> A Public Domain Dataset for Human Activity Recognition Using Smartphones, 2013. The idea is that once the subject¡¯s activity is recognized and known, an intelligent computer system can then offer assistance. It is a challenging problem because there is no clear analytical way to relate the sensor data to specific actions in a general way. It is technically challenging because of the large volume of sensor data collected (e.g. tens or hundreds of observations per second) and the classical use of hand crafted features and heuristics from this data in developing predictive models. More recently, deep learning methods have been demonstrated successfully on HAR problems given their ability to automatically learn higher-order features. Sensor-based activity recognition seeks the profound high-level knowledge about human activities from multitudes of low-level sensor readings. Conventional pattern recognition approaches have made tremendous progress in the past years. However, those methods often heavily rely on heuristic hand-crafted feature extraction, which could hinder their generalization performance. [¡¦] Recently, the recent advancement of deep learning makes it possible to perform automatic high-level feature extraction thus achieves promising performance in many areas. <U+2014> Deep Learning for Sensor-based Activity Recognition: A Survey Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course A standard human activity recognition dataset is the ¡®Activity Recognition Using Smartphones¡® dataset made available in 2012. It was prepared and made available by Davide Anguita, et al. from the University of Genova, Italy and is described in full in their 2013 paper ¡°A Public Domain Dataset for Human Activity Recognition Using Smartphones.¡± The dataset was modeled with machine learning algorithms in their 2012 paper titled ¡°Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine.¡± The dataset was made available and can be downloaded for free from the UCI Machine Learning Repository: The data was collected from 30 subjects aged between 19 and 48 years old performing one of 6 standard activities while wearing a waist-mounted smartphone that recorded the movement data. Video was recorded of each subject performing the activities and the movement data was labeled manually from these videos. Below is an example video of a subject performing the activities while their movement data is being recorded. The six activities performed were as follows: The movement data recorded was the x, y, and z accelerometer data (linear acceleration) and gyroscopic data (angular velocity) from the smart phone, specifically a Samsung Galaxy S II. Observations were recorded at 50 Hz (i.e. 50 data points per second). Each subject performed the sequence of activities twice, once with the device on their left-hand-side and once with the device on their right-hand side. A group of 30 volunteers with ages ranging from 19 to 48 years were selected for this task. Each person was instructed to follow a protocol of activities while wearing a waist-mounted Samsung Galaxy S II smartphone. The six selected ADL were standing, sitting, laying down, walking, walking downstairs and upstairs. Each subject performed the protocol twice: on the first trial the smartphone was fixed on the left side of the belt and on the second it was placed by the user himself as preferred <U+2014> A Public Domain Dataset for Human Activity Recognition Using Smartphones, 2013. The raw data is not available. Instead, a pre-processed version of the dataset was made available. The pre-processing steps included: These signals were preprocessed for noise reduction with a median filter and a 3rd order low-pass Butterworth filter with a 20 Hz cutoff frequency. [¡¦] The acceleration signal, which has gravitational and body motion components, was separated using another Butterworth low-pass filter into body acceleration and gravity. <U+2014> A Public Domain Dataset for Human Activity Recognition Using Smartphones, 2013. Feature engineering was applied to the window data, and a copy of the data with these engineered features was made available. A number of time and frequency features commonly used in the field of human activity recognition were extracted from each window. The result was a 561 element vector of features. The dataset was split into train (70%) and test (30%) sets based on data for subjects, e.g. 21 subjects for train and nine for test. This suggests a framing of the problem where a sequence of movement activity is used as input to predict the portion (2.56 seconds) of the current activity being performed, where a model trained on known subjects is used to predict the activity from movement on new subjects. Early experiment results with a support vector machine intended for use on a smartphone (e.g. fixed-point arithmetic) resulted in a predictive accuracy of 89% on the test dataset, achieving similar results as an unmodified SVM implementation. This method adapts the standard Support Vector Machine (SVM) and exploits fixed-point arithmetic for computational cost reduction. A comparison with the traditional SVM shows a significant improvement in terms of computational costs while maintaining similar accuracy [¡¦] <U+2014> Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine, 2012. Now that we are familiar with the prediction problem, we will look at loading and exploring this dataset. The dataset is freely available and can be downloaded from the UCI Machine Learning repository. The data is provided as a single zip file that is about 58 megabytes in size. The direct link for this download is below: Download the dataset and unzip all files into a new directory in your current working directory named ¡°HARDataset¡°. Inspecting the decompressed contents, you will notice a few things: The contents of the ¡°train¡± and ¡°test¡± folders are similar (e.g. folders and file names), although with differences in the specific data they contain. Inspecting the ¡°train¡± folder shows a few important elements: The number of lines in each file match, indicating that one row is one record in each data file. The ¡°Inertial Signals¡± directory contains 9 files. The structure is mirrored in the ¡°test¡± directory. We will focus our attention on the data in the ¡°Inertial Signals¡± as this is most interesting in developing machine learning models that can learn a suitable representation, instead of using the domain-specific feature engineering. Inspecting a datafile shows that columns are separated by whitespace and values appear to be scaled to the range -1, 1. This scaling can be confirmed by a note in the README.txt file provided with the dataset. Now that we know what data we have, we can figure out how to load it into memory. In this section, we will develop some code to load the dataset into memory. First, we need to load a single file. We can use the read_csv() Pandas function to load a single data file and specify that the file has no header and to separate columns using white space. We can wrap this in a function named load_file(). The complete example of this function is listed below. Running the example loads the file ¡®total_acc_y_train.txt¡®, returns a NumPy array, and prints the shape of the array. We can see that the training data is comprised of 7,352 rows or windows of data, where each window has 128 observations. Next, it would be useful to load a group of files, such as all of the body acceleration data files as a single group. Ideally, when working with multivariate time series data, it is useful to have the data structured in the format: This is helpful for analysis and is the expectation of deep learning models such as convolutional neural networks and recurrent neural networks. We can achieve this by calling the above load_file() function multiple times, once for each file in a group. Once we have loaded each file as a NumPy array, we can combine or stack all three arrays together. We can use the dstack() NumPy function to ensure that each array is stacked in such a way that the features are separated in the third dimension, as we would prefer. The function load_group() implements this behavior for a list of file names and is listed below. We can demonstrate this function by loading all of the total acceleration files. The complete example is listed below. Running the example prints the shape of the returned NumPy array, showing the expected number of samples and time steps with the three features, x, y, and z for the dataset. Finally, we can use the two functions developed so far to load all data for the train and the test dataset. Given the parallel structure in the train and test folders, we can develop a new function that loads all input and output data for a given folder. The function can build a list of all 9 data files to load, load them as one NumPy array with 9 features, then load the data file containing the output class. The load_dataset() function below implements this behaviour. It can be called for either the ¡°train¡± group or the ¡°test¡± group, passed as a string argument. The complete example is listed below. Running the example loads the train and test datasets. We can see that the test dataset has 2,947 rows of window data. As expected, we can see that the size of windows in the train and test sets matches and the size of the output (y) in each the train and test case matches the number of samples. Now that we know how to load the data, we can start to explore it. A good first check of the data is to investigate the balance of each activity. We believe that each of the 30 subjects performed each of the six activities. Confirming this expectation will both check that the data is indeed balanced, making it easier to model, and confirm that we are correctly loading and interpreting the dataset. We can develop a function that summarizes the breakdown of the output variables, e.g. the y variable. The function class_breakdown() below implements this behavior, first wrapping the provided NumPy array in a DataFrame, grouping the rows by the class value, and calculating the size of each group (number of rows). The results are then summarized, including the count and the percentage. It may be useful to summarize the breakdown of the classes in the train and test datasets to ensure they have a similar breakdown, then compare the result to the breakdown on the combined dataset. The complete example is listed below. Running the example first summarizes the breakdown for the training set. We can see a pretty similar distribution of each class hovering between 13% and 19% of the dataset. The result on the test set and on both datasets together look very similar. It is likely safe to work with the dataset assuming the distribution of classes is balanced per train and test set and perhaps per subject. We are working with time series data, therefore an import check is to create a line plot of the raw data. The raw data is comprised of windows of time series data per variable, and the windows do have a 50% overlap. This suggests we may see some repetition in the observations as a line plot unless the overlap is removed. We can start off by loading the training dataset using the functions developed above. Next, we can load the ¡®subject_train.txt¡® in the ¡®train¡® directory that provides a mapping of rows to the subject to which it belongs. We can load this file using the load_file() function. Once loaded, we can also use the unique() NumPy function to retrieve a list of the unique subjects in the training dataset. Next, we need a way to retrieve all of the rows for a single subject, e.g. subject number 1. We can do this by finding all of the row numbers that belong to a given subject and use those row numbers to select the samples from the loaded X and y data from the training dataset. The data_for_subject() function below implements this behavior. It will take the loaded training data, the loaded mapping of row number to subjects, and the subject identification number for the subject that we are interested in, and will return the X and y data for only that subject. Now that we have data for one subject, we can plot it. The data is comprised of windows with overlap. We can write a function to remove this overlap and squash the windows down for a given variable into one long sequence that can be plotted directly as a line plot. The to_series() function below implements this behavior for a given variable, e.g. array of windows. Finally, we have enough to plot the data. We can plot each of the nine variables for the subject in turn and a final plot for the activity level. Each series will have the same number of time steps (length of x-axis), therefore, it may be useful to create a subplot for each variable and align all plots vertically so we can compare the movement on each variable. The plot_subject() function below implements this behavior for the X and y data for a single subject. The function assumes the same order of the variables (3rd axis) as was loaded in the load_dataset()<U+00A0>function. A crude title is also added to each plot so we don¡¯t get easily confused about what we are looking at. The complete example is listed below. Running the example prints the unique subjects in the training dataset, the sample of the data for the first subject, and creates one figure with 10 plots, one for each of the nine input variables and the output class. In the plot, we can see periods of large movement corresponding with activities 1, 2, and 3: the walking activities. We can also see much less activity (i.e. a relatively straight line) for higher numbered activities, 4, 5, and 6 (sitting, standing, and laying). This is good confirmation that we have correctly loaded interpreted the raw dataset. We can see that this subject has performed the same general sequence of activities twice, and some activities are performed more than two times. This suggests that for a given subject, we should not make assumptions about what activities may have been performed or their order. We can also see some relatively large movement for some stationary activities, such as laying. It is possible that these are outliers or related to activity transitions. It may be possible to smooth or remove these observations as outliers. Finally, we see a lot of commonality across the nine variables. It is very likely that only a subset of these traces are required to develop a predictive model. Line plot for all variables for a single subject We can re-run the example for another subject by making one small change, e.g. choose the identifier of the second subject in the training dataset. The plot for the second subject shows similar behavior with no surprises. The double sequence of activities does appear more regular than the first subject. Line plot for all variables for a second single subject As the problem is framed, we are interested in using the movement data from some subjects to predict activities from the movement of other subjects. This suggests that there must be regularity in the movement data across subjects. We know that the data has been scaled between -1 and 1, presumably per subject, suggesting that the amplitude of the detected movements will be similar. We would also expect that the distribution of movement data would be similar across subjects, given that they performed the same actions. We can check for this by plotting and comparing the histograms of the movement data across subjects. A useful approach would be to create one plot per subject and plot all three axis of a given data (e.g. total acceleration), then repeat this for multiple subjects. The plots can be modified to use the same axis and aligned horizontally so that the distributions for each variable across subjects can be compared. The plot_subject_histograms() function below implements this behavior. The function takes the loaded dataset and mapping of rows to subjects as well as a maximum number of subjects to plot, fixed at 10 by default. A plot is created for each subject and the three variables for one data type are plotted as histograms with 100 bins, to help to make the distribution obvious. Each plot shares the same axis, which is fixed at the bounds of -1 and 1. The complete example is listed below. Running the example creates a single figure with 10 plots with histograms for the three axis of the total acceleration data. Each of the three axes on a given plot have a different color, specifically x, y, and z are blue, orange, and green respectively. We can see that the distribution for a given axis does appear Gaussian with large separate groups of data. We can see some of the distributions align (e.g. main groups in the middle around 0.0), suggesting there may be some continuity of the movement data across subjects, at least for this data. Histograms of the total acceleration data for 10 subjects We can update the plot_subject_histograms() function to next plot the distributions of the body acceleration. The updated function is listed below. Running the updated example creates the same plot with very different results. Here we can see all data clustered around 0.0 across axis within a subject and across subjects. This suggests that perhaps the data was centered (zero mean). This strong consistency across subjects may aid in modeling, and may suggest that the differences across subjects in the total acceleration data may not be as helpful. Histograms of the body acceleration data for 10 subjects Finally, we can generate one final plot for the gyroscopic data. The updated function is listed below. Running the example shows very similar results to the body acceleration data. We see a high likelihood of a Gaussian distribution for each axis across each subject centered on 0.0. The distributions are a little wider and show fatter tails, but this is an encouraging finding for modeling movement data across subjects. Histograms of the body gyroscope data for 10 subjects We are interested in discriminating between activities based on activity data. The simplest case for this would be to discriminate between activities for a single subject. One way to investigate this would be to review the distribution of movement data for a subject by activity. We would expect to see some difference in the distribution between the movement data for different activities by a single subject. We can review this by creating a histogram plot per activity, with the three axis of a given data type on each plot. Again, the plots can be arranged horizontally to compare the distribution of each data axis by activity. We would expect to see differences in the distributions across activities down the plots. First, we must group the traces for a subject by activity. The data_by_activity() function below implements this behaviour. We can now create plots per activity for a given subject. The plot_activity_histograms() function below implements this function for the traces data for a given subject. First, the data is grouped by activity, then one subplot is created for each activity and each axis of the data type is added as a histogram. The function only enumerates the first three features of the data, which are the total acceleration variables. The complete example is listed below. Running the example creates the plot with six subplots, one for each activity for the first subject in the train dataset. Each of the x, y, and z axes for the total acceleration data have a blue, orange, and green histogram respectively. We can see that each activity has a different data distribution, with a marked difference between the large movement (first three activities) with the stationary activities (last three activities). Data distributions for the first three activities look Gaussian with perhaps differing means and standard deviations. Distributions for the latter activities look multi-modal (i.e. multiple peaks). Histograms of the total acceleration data by activity We can re-run the same example with an updated version of the plot_activity_histograms() that plots the body acceleration data instead. The updated function is listed below. Running the updated example creates a new plot. Here, we can see more similar distributions across the activities amongst the in-motion vs. stationary activities. The data looks bimodal in the case of the in-motion activities and perhaps Gaussian or exponential in the case of the stationary activities. The pattern we see with the total vs. body acceleration distributions by activity mirrors what we see with the same data types across subjects in the previous section. Perhaps the total acceleration data is the key to discriminating the activities. Histograms of the body acceleration data by activity Finally, we can update the example one more time to plot the histograms per activity for the gyroscopic data. The updated function is listed below. Running the example creates plots with the similar pattern as the body acceleration data, although showing perhaps fat-tailed Gaussian-like distributions instead of bimodal distributions for the in-motion activities. Histograms of the body gyroscope data by activity All of these plots were created for the first subject, and we would expect to see similar distributions and relationships for the movement data across activities for other subjects. A final area to consider is how long a subject spends on each activity. This is closely related to the balance of classes. If the activities (classes) are generally balanced within a dataset, then we expect the balance of activities for a given subject over the course of their trace would also be reasonably well balanced. We can confirm this by calculating how long (in samples or rows) each subject spends on each activity and look at the distribution of durations for each activity. A handy way to review this data is to summarize the distributions as boxplots showing the median (line), the middle 50% (box), the general extent of the data as the interquartile range (the whiskers), and outliers (as dots). The function plot_activity_durations_by_subject() below implements this behavior by first splitting the dataset by subject, then the subjects data by activity and counting the rows spent on each activity, before finally creating a boxplot per activity of the duration measurements. The complete example is listed below. Running the example creates six box plots, one for each activity. Each boxplot summarizes how long (in rows or the number of windows) subjects in the training dataset spent on each activity. We can see that the subjects spent more time on stationary activities (4, 5 and 6) and less time on the in motion activities (1, 2 and 3), with the distribution for 3 being the smallest, or where time was spent least. The spread across the activities is not large, suggesting little need to trim the longer duration activities or oversampling of the in-motion activities. Although, these approaches remain available if skill of a predictive model on the in-motion activities is generally worse. Boxplot of activity durations per subject on train set We can create a similar boxplot for the training data with the following additional lines. Running the updated example shows a similar relationship between activities. This is encouraging, suggesting that indeed the test and training dataset are reasonably representative of the whole dataset. Boxplot of activity durations per subject on test set Now that we have explored the dataset, we can suggest some ideas for how it may be modeled. In this section, we summarize some approaches to modeling the activity recognition dataset. These ideas are divided into the main themes of a project. The first important consideration is the framing of the prediction problem. The framing of the problem as described in the original work is the prediction of activity for a new subject given their movement data, based on the movement data and activities of known subjects. We can summarize this as: This is a reasonable and useful framing of the problem. Some other possible ways to frame the provided data as a prediction problem include the following: Some of these framings may be too challenging or too easy. Nevertheless, these framings provide additional ways to explore and understand the dataset. Some data preparation may be required prior to using the raw data to train a model. The data already appears to have been scaled to the range [-1,1]. Some additional data transforms that could be performed prior to modeling include: Generally, the problem is a time series multi-class classification problem. As we have seen, it may also be framed as a binary classification problem and a multi-step time series classification problem. The original paper explored the use of a classical machine learning algorithm on a version of the dataset where features were engineered from each window of data. Specifically, a modified support vector machine. The results of an SVM on the feature-engineered version of the dataset may provide a baseline in performance on the problem. Expanding from this point, the evaluation of multiple linear, non-linear, and ensemble machine learning algorithms on this version of the dataset may provide an improved benchmark. The focus of the problem may be on the un-engineered or raw version of the dataset. Here, a progression in model complexity may be explored in order to determine the most suitable model for the problem; some candidate models to explore include: The evaluation of the model in the original paper involved using a train/test split of the data by subject with a 70% and 30% ratio. Exploration of this pre-defined split of the data suggests that both sets are reasonably representative of the whole dataset. Another alternative methodology may be to use leave-one-out cross-validation, or LOOCV, per subject. In addition to giving the data for each subject the opportunity for being used as the withheld test set, the approach would provide a population of 30 scores that can be averaged and summarized, which may offer a more robust result. Model performance was presented using classification accuracy and a confusion matrix, both of which are suitable for the multi-class nature of the prediction problem. Specifically, the confusion matrix will aid in determining whether some classes are easier or more challenging to predict than others, such as those for stationary activities versus those activities that involve motion. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered the Activity Recognition Using Smartphones Dataset for time series classification and how to load and explore the dataset in order to make it ready for predictive modeling. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Jason, nice peace of work like always. I only wish that you have had the code for communicating to the phone (android for instance) instead of loading sampled data. Thanks. Good suggestion! There are several open sources  demoing how to collect data from the phone (cf github).
In my app (under development) I am collecting accel data (using a sliding window of 10 sec), and upon certain trigger, save to temporary file and upload to google cloud. Not trivial task due to all the pesky security.
If this is on interest to anyone, please let me know and I will try to derive a sample app. Sounds like a challenging problem! Thank you very much , please if you have any article about activity recognition or anomaly detection from activity let me know thank you again. I have a number of posts coming on activity recognition. I also cover the topic in detail in this book:https://machinelearningmastery.com/deep-learning-for-time-series-forecasting/ Thanks for sharing it. You¡¯re welcome. Thank you Jason. You¡¯re welcome. Is there a similar dataset for falls of sorts? I found a lot of references regarding fall detection of elderly people but not of younger (20-60 year old). Specifically I want to identify fall during riding (which is NOT easier as one expects since I don¡¯t have velocity data). Thanks for the blog posts! I expect there is, I don¡¯t know off hand, perhaps try a search? I have implemented a model using Keras to model a LSTM for this same problem, you can take a look at https://github.com/servomac/Human-Activity-Recognition. Nice work! Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning.","Keyword(freq): activity(46), subject(33), distribution(13), feature(11), plot(11), histogram(10), row(10), implement(9), smartphone(9), variable(9)"
"8","vidhya",2018-09-20,"Let¡¯s Think in Graphs: Introduction to Graph Theory and its Applications using Python","https://www.analyticsvidhya.com/blog/2018/09/introduction-graph-theory-applications-python/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 ¡°There is a magic in graphs. The pro<U+FB01>le of a curve reveals in a <U+FB02>ash a whole situation <U+2014> the life history of an era of prosperity. The curve informs the mind, awakens the imagination, convinces.¡± <U+2013> Henry D. Hubbard Visualizations are a powerful way to simplify and interpret the underlying patterns in data. The first thing I do, whenever I work on a new dataset is to explore it through visualization. And this approach has worked well for me. Sadly, I don¡¯t see many people using visualizations as much. That is why I thought I will share some of my ¡°secret sauce¡± with the world! Use of graphs is one such visualization technique. It is incredibly useful and helps businesses make better data-driven decisions. But to understand the concepts of graphs in detail, we must first understand it¡¯s base <U+2013> Graph Theory. In this article, we will be learning the concepts of graphs and graph theory. We will also look at the fundamentals and basic properties of graphs, along with different types of graphs. We will then work on a case study to solve a commonly seen problem in the aviation industry by applying the concepts of Graph Theory using Python. Let¡¯s get started! Consider the plot shown below: It¡¯s a nice visualization of the store-wise sales of a particular item. But this isn¡¯t a graph, it¡¯s a chart. Now you might be wondering why is this a chart and not a graph, right? Well, a chart represents the graph of a function. Let me explain this by expanding on the above example. Out of the total units of a particular item, 15.1% are sold from store A, 15.4% from store B, and so on. We can represent it using a table: Corresponding to each store is their contribution (in %) to the overall sales. In the above chart, we mapped store A with 15.1% contribution, store B with 15.4%, so on and so forth. Finally, we visualized it using a pie chart. But then what¡¯s the difference between this chart and a graph? To answer this, consider the visual shown below: The points in the above visual represent the characters of Game of Thrones, while the lines joining these points represent the connection between them. Jon Snow has connections with multiple characters, and the same goes for Tyrion, Cersei, Jamie, etc. And this is what a graph looks like. A single point might have connections with multiple points, or even a single point. Typically, a graph is a combination of vertices (nodes) and edges. In the above GOT visual, all the characters are vertices and the connections between them are edges. We now have an idea of what graphs are, but why do we need graphs in the first place? We¡¯ll look at this pertinent question in the next section. Suppose you booked an Uber cab. One of the most important things that is critical to Uber¡¯s functioning is its ability to match drivers with riders in an efficient way. Consider there are 6 possible rides that you can be matched with. So, how does Uber allocate a ride to you? We can make use of graphs to visualize how the process of allotting a ride might be: As you can interpret, there are 6 possible rides (Ride 1, Ride 2, ¡¦. Ride 6) which the rider can be matched with. Representing this in graph form makes it easier to visualize and finally fulfill our aim, i.e., to match the closest ride to the user. The numbers in the above graph represent the distance (in kilometers) between the rider and his/her corresponding ride. We (and of course Uber) can clearly visualize that Ride 3 is the closest option. Note: For simplicity, I have taken only the distance metric to decide which ride will be allotted to the rider. Whearas in a real life scenario, there are multiple metrics through which the allotment of a ride is decided, such as rating of the rider and driver, traffic between different routes, time for which the rider is idle, etc. Similarly, online food delivery aggregators like Zomato can select a rider who will pick up our orders from the corresponding restaurant and deliver it to us. This is one of the many use cases of graphs through which we can solve a lot of challenges. Graphs make visualizations easier and more interpretable. To understand the concept of graphs in detail, we must first understand graph theory. We¡¯ll first discuss the origins of graph theory to get an intuitive understanding of graphs. There is an interesting story behind its origin, and I aim to make it even more intriguing using plots and visualizations. It all started with the Seven Bridges of Konigsberg. The challenge (or just a brain teaser) with Konigsberg¡¯s bridges, was to be able to walk through the city by crossing all the seven bridges only once. Let us visualize it first to have a clear understanding of the problem: Give it a try and see if you can walk through the city with this restraint. You have to keep two things in mind while trying to solve the above problem (or should i say riddle?): You can try any number of combinations, but it remains an impossible challenge to crack. There is no way in which one can walk through the city by crossing each bridge only once.<U+00A0>Leonhard Euler delved deep into this puzzle to come up with the reason why this is such an impossible task. Let¡¯s analyze how he did this: There are four distinct places in the above image: two islands (B and D), and two parts of the mainland (A and C) and a total of seven bridges. Let us first look at each land separately and try to find patterns (if any exist at all) : One inference from the above image is that each land is connected with an<U+00A0>odd number of bridges. If you wish to cross each bridge only once, then you can enter and leave a land only if it is connected to an even number of bridges. In other words, we can generalize that if there are even number of bridges, it¡¯s possible to leave the land, while it¡¯s impossible to do so with an odd number. Let¡¯s try to add one more bridge to the current problem and see whether it can crack open this problem: Now we have 2 lands connected with an even number of bridges, and 2 lands connected with an odd number of bridges. Let¡¯s draw a new route after the addition of the new bridge: The addition of a single bridge solved the problem! You might be wondering if the number of bridges played a significant part in solving this problem? Should it be even all the time? Well, that¡¯s not always the case. Euler explained that along with the number of bridges, the number of pieces of land with an odd number of connected bridges matters as well. Euler converted this problem from land and bridges to graphs, where he represented the land as vertices and the bridges as edges: Here, the visualization is simple and crystal clear. Before we move further and delve deeper into this problem, let us first understand the fundamentals and basic properties of a graph. There are many key points and key words that we should keep in mind when we are dealing with graphs. In this section, we will discuss all those keywords in detail. These are some of the fundamentals which you must keep in mind when dealing with graphs. Now onto understanding the basic properties of a graph. Till this point, we have seen what a graph looks like and it¡¯s different components. Now we will turn our focus to some basic properties and terminologies related to a graph. We will be using the below given graph (referred to as G) and understand each terminology using the same: Take a moment and think about possible solutions to the following questions: I will try to answer all these questions using basic graph terminologies: These are some of the terminologies related to graphs. Next we will discuss the different types of graphs. There are vairous and diverse types of graphs. In this section, we will discuss some of the most commonly used ones. Now that we have an understanding of the different types of graphs, their components, and some of the basic graph-related terminologies, let¡¯s get back to the problem which we were trying to solve, i.e. the Seven Bridges of Konigsberg. We shall explore in even more detail how Leonhard Euler approached and explained his reasoning. We saw earlier that Euler transformed this problem using graphs: Here, A, B, C, and D represent the land, and the lines joining them are the bridges. We can calculate the degree of each vertex. deg(B) = 5 deg(A) = deg(C) = deg(D) = 3 Euler showed that the possibility of walking through a graph (city) using each edge (bridge) only once, strictly depends on the degree of vertices (land). And such a path, which contains each edge of a graph only once, is called Euler¡¯s path. Can you figure out Euler¡¯s path for our problem? Let¡¯s try! And this is how the classic Seven Bridges of Konigsberg challenge can be solved using graphs and Euler¡¯s path. And this is basically the origin of Graph Theory. Thanks to Leonhard Euler! Trees are one of the most powerful and effective ways of representing a graph. In this section, we will learn what binary search trees are, how they work, and how they make visualizations more interpretable. But before all that, take a moment to understand what trees actually are in this context. Trees are graphs which do not contain even a single cycle: In the above example, the first graph has no cycle (aka a tree), while the second graph has a cycle (A-B-E-C-E, hence it¡¯s not a tree). The elements of a tree are called nodes. (A, B, C, D, and E) are the nodes in the above tree. The first node (or the topmost node) of a tree is known as the root node, while the last node (node C, D and E in the above example) is known as the leaf node. All the remaining nodes are known as child nodes (node B in our case). It¡¯s time to move on to one of the most important topics in Graph Theory, i.e., Graph Traversal. Suppose we want to identify the location of a particular node in a graph. What might me the possible solution to identify nodes of a graph? How to start? What should be the starting point? Once we know the starting point, how to proceed further? I will try to answer all these questions in this section by explaining the concepts of Graph Traversal. Graph Traversal refers to visiting every vertex and edge of a graph exactly once in a well-defined order. As the aim of traversing is to visit each vertex only once, we keep a track of vertices covered so that we do not cover same vertex twice. There are various methods for graph traversal and we will discuss some of the famous methods: We start from the source node (root node) and traverse the graph, layer wise. Steps for Breadth First Search are: Let me explain it with a visualization: So in Breadth First Search, we start from the Source Node (A in our case) and move down to the first layer, i.e. Layer 1. We cover all the nodes in that layer by moving horizontally (B -> C). Then we go to the next layer, i.e. Layer 2 and repeat the same step (we move from D -> E -> F). We continue this step until all the layers and vertices are covered. Key advantage of this approach is that we will always find the shortest path to the goal. This is appropriate for small graphs and trees but for more complex and larger graphs, its performance is very slow and it also takes a lot of memory. We will look at another traversing approach which takes less memory space as compared to BFS. Let us first look at the steps involved in this approach: The sequence for Depth First Search for the above example will be: A -> B -> D -> E -> C -> F Once a path has been fully explored it can be removed from memory, so DFS only needs to store the root node, all the children of the root node and where it currently is. Hence, it overcomes the memory problem of BFS. In this approach all the nodes of a tree are arranged in a sorted order. Let¡¯s have a look at an example of Binary Search Tree: As mentioned earlier, all the nodes in the above tree are arranged based on a condition. Suppose we want to access the node with value 45. If we would have followed BFS or DFS, we would have required a lot of computational time to reach to it. Now let¡¯s look at how a Binary Search Tree will help us to reach to the required node using least number of steps. Steps to reach to the node with value 45 using Binary Search Tree: This approach is very fast and takes very less memory as well. Till now we have seen most of the concepts of Graph Theory. Next we will try to implement these concepts to solve a real life problem using Python. And finally, we get to work with data in Python! In this dataset, we have records of over 7 million flights from the USA. The below variables have been provided: It is a gigantic dataset and I have taken only a sample from it for this article. The idea is to give you an understanding of the concepts using this sample dataset, and you can then apply them to the entire dataset. Download the dataset which we will be using for the case study from here. We will first import the usual libraries, and read the dataset, which is provided in a .csv format: Let¡¯s have a look at the first few rows of the dataset using the head() function: Here, CRSDepTime, CRSArrTime, DepTime, and ArrTime represent the scheduled time of departure, the scheduled time of arrival, the actual time of departure, and the actual time of arrival respectively. Origin and Dest are the Origin and Destination of the journey. There can often be multiple paths from one airport to another, and the aim is to find the shortest possible path between all the airports. There are two ways in which we can define a path as the shortest: We can solve such problems using the concepts of graph theory which we have learned so far. Can you recall what we need to do to make a graph? The answer is identifying the vertices and edges! We can convert the problem to a graph by representing all the airports as vertices, and the route between them as edges. We will be using NetworkX for creating and visualizing graphs. NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. You can refer to the documentation of NetworkX here. After installing NetworkX, we will create the edges and vertices for our graph using the dataset: It will store the vertices and edges automatically. Take a quick look at the edges and vertices of the graph which we have created: Let us plot and visualize the graph using the<U+00A0>matplotlib and draw_networkx() functions of networkx. The above amazing visualization represents the different flight routes. Suppose a passenger wants to take the shortest route from AMA to PBI. Graph theory comes to the rescue once again! Let¡¯s try to calculate the shortest path based on the airtime between the airports AMA and PBI. We will be using Dijkstra¡¯s shortest path algorithm. This algorithm finds the shortest path from a source vertex to all the vertices of the given graph. Let me give you a brief run through of the steps this algorithm follows: Let us take an example to understand this algorithm in a better way: Here the source vertex is A. The numbers represent the distance between the vertices. Initially, the sptSet is empty so we will assign distances to all the vertices. The distances are: {0, INF, INF, INF, INF, INF}, where INF represents INFINITE. Now, we will pick the vertex with the minimum distance, i.e., A and it will be included in the sptSet. So, the new sptSet is {A}. The next step is to pick a vertex which is not in the sptSet and is closest to the source vertex. This, in our case, is B with a distance value of 2. So this will be added to the sptSet. sptSet = {A,B} Now we will update the distances of vertices adjacent to vertex B: The distance value of the vertex F becomes 6. We will again pick the vertex with the minimum distance value which is not already included in SPT (C with a distance value of 4). sptSet = {A,B,C} We will follow similar steps until all the vertices are included in the sptSet. Let¡¯s implement this algorithm and try to calculate the shortest distance between the airports. We will use the<U+00A0>dijkstra_path() function of networkx to do so: This is the shortest possible path between the two airports based on the distance between them. We can also calculate the shortest path based on the airtime just by changing the hyperparameter weight=¡¯AirTime¡¯: This is the shortest path based on the airtime. Intuitive and easy to understand, this was all about graph theory! This is just one of the many applications of Graph Theory. We can apply it to almost any kind of problem and get solutions and visualizations. Some of the application of Graph Theory which I can think of are: These are some of the applications. You can come up with many more. Feel free to share them in the comment section below. I hope you have enjoyed the article. Looking forward to your responses. Awesome Article.
Written in an intuitive and lucid manner. Thank you Ankur! Thanks Pulkit for the post. Did you find any way of drawing edge thickness? Glad you liked the article!
There is a concept called weighted graphs in which you can give weights to edges. For more details on this, refer here. Pulkit,
Well written! It is a good introduction to the computer science students. Python integration is excellent.
However, I think the quote of Henry D. Hubbard is not on graphs of Graph Theory bu t is on the graph created out of a function. Hi Joseph, Thanks for the feedback.
I just added that quote to get an intuition of how graphs can be useful for visualization. Great article! Will there be others? It would be great if you could point some books and courses about it too in the end of the article. Hi, Glad you liked the article!!
You can refer ¡°Introduction to Graph Theory¡± course of coursera to learn more about graph theory. Thanks for this nice article. I have some question as below.
1) Can we implemented travelling sales man problem using graph theory in python?
2) Need some more example of Real life project case study. Hi Ashish, 1. To solve the traveling salesman problem, you can consider the cities as vertices and the distance between them as edges. Then you can solve it using Graph Theory. For more details, refer here.","Keyword(freq): graph(29), bridge(17), vertex(17), edge(10), node(9), concept(8), step(6), visualization(6), airport(5), tree(5)"
"9","vidhya",2018-09-18,"Nuts & Bolts of Reinforcement Learning: Model Based Planning using Dynamic Programming","https://www.analyticsvidhya.com/blog/2018/09/reinforcement-learning-model-based-planning-dynamic-programming/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 Deep Reinforcement learning is responsible for the two biggest AI wins over human professionals <U+2013> Alpha Go and OpenAI Five. Championed by Google and Elon Musk, interest in this field has gradually increased in recent years to the point where it¡¯s a thriving area of research nowadays. In this article, however, we will not talk about a typical RL setup but explore Dynamic Programming (DP). DP is a collection of algorithms that<U+00A0> can solve a problem where we have the perfect model of the environment (i.e. probability distributions of any change happening in the problem setup are known) and where an agent can only take discrete actions. DP essentially solves a planning problem rather than a more general RL problem. The main difference, as mentioned, is that for an RL problem the environment can be very complex and its specifics are not known at all initially. But before we dive into all that, let¡¯s understand why you should learn dynamic programming in the first place using an intuitive example. Apart from being a good starting point for grasping reinforcement learning, dynamic programming can help find optimal solutions to planning problems faced in the industry, with an important assumption that the specifics of the environment are known.<U+00A0>DP presents a good starting point to understand RL algorithms that can solve more complex problems. Sunny manages a motorbike rental company in Ladakh. Being near the highest motorable road in the world, there is a lot of demand for motorbikes on rent from tourists. Within the town he has 2 locations where tourists can come and get a bike on rent. If he is out of bikes at one location, then he loses business. The problem that Sunny is trying to solve is to find out how many bikes he should move each day from 1 location to another so that he can maximise his earnings. Here, we exactly know the environment (g(n) & h(n)) and this is the kind of problem in which dynamic programming can come in handy.<U+00A0>Similarly, if you can properly model the environment of your problem where you can take discrete actions, then DP can help you find the optimal solution.<U+00A0>In this article, we will use DP to train an agent using Python to traverse a simple environment, while touching upon key concepts in RL such as policy, reward, value function and more. Most of you must have played the tic-tac-toe game in your childhood. If not, you can grasp the rules of this simple game from its wiki page. Suppose tic-tac-toe is your favourite game, but you have nobody to play it with. So you decide to design a bot that can play this game with you. Some key questions are: Can you define a rule-based framework to design an efficient bot? You sure can, but you will have to hardcode a lot of rules for each of the possible situations that might arise in a game. However, an even more interesting question to answer is: Can you train the bot to learn by playing against you several times? And that too without being explicitly programmed to play tic-tac-toe efficiently? A few considerations for this are: For more clarity on the aforementioned reward, let us consider a match between bots O and X: Consider the following situation encountered in tic-tac-toe: If bot X puts X in the bottom right position for example, it results in the following situation: Bot O would be rejoicing (Yes! They are programmed to show emotions) as it can win the match with just one move. Now, we need to teach X not to do this again. So we give a negative reward or punishment to reinforce the correct behaviour in the next trial. We say that this action in the given state would correspond to a negative reward and should not be considered as an optimal action in this situation. Similarly, a positive reward would be conferred to X if it stops O from winning in the next move: Now that we understand the basic terminology, let¡¯s talk about formalising this whole process using a concept called a Markov Decision Process or MDP. A Markov Decision Process (MDP) model contains: Now, let us understand the markov or ¡®memoryless¡¯ property. Any random process in which the probability of being in a given state depends only on the previous state, is a markov process. In other words, in the markov decision process setup, the environment¡¯s response at time t+1 depends only on the state and action representations at time t, and is independent of whatever happened in the past. The above diagram clearly illustrates the iteration at each time step wherein the agent receives a reward Rt+1 and ends up in state St+1<U+00A0>based on its action At at a particular state St. The overall goal for the agent is to maximise the cumulative reward it receives in the long run. Total reward at any time instant t is given by: where T is the final time step of the episode. In the above equation, we see that all future rewards have equal weight which might not be desirable. That¡¯s where an additional concept of discounting comes into the picture. Basically, we define ¥ã as a discounting factor and each reward after the immediate reward is discounted by this factor as follows: For discount factor < 1, the rewards further in the future are getting diminished. This can be understood as a tuning parameter which can be changed based on how much one wants to consider the long term (¥ã close to 1) or short term (¥ã close to 0). Can we use the reward function defined at each time step to define how good it is, to be in a given state for a given policy? The value function denoted as v(s) under a policy ¥ð represents how good a state is for an agent to be in. In other words, what is the average reward that the agent will get starting from the current state under policy ¥ð? E in the above equation represents the expected reward at each state if the agent follows policy ¥ð and S represents the set of all possible states. Policy, as discussed earlier, is the mapping of probabilities of taking each possible action at each state (¥ð(a/s)). The policy might also be deterministic when it tells you exactly what to do at each state and does not give probabilities. Now, it¡¯s only intuitive that ¡®the optimum policy¡¯ can be reached if the value function is maximised for each state. This optimal policy is then given by: The above value function only characterizes a state. Can we also know how good an action is at a particular state? A state-action value function, which is also called the q-value, does exactly that. We define the value of action a, in state s, under a policy ¥ð, as: This is the expected return the agent will get if it takes action At at time t, given state St, and thereafter follows policy ¥ð. Bellman was an applied mathematician who derived equations that help to solve an Markov Decision Process. Let¡¯s go back to the state value function v and state-action value function q. Unroll the value function equation to get: In this equation, we have the value function for a given policy ¥ð represented in terms of the value function of the next state. Choose an action a, with probability ¥ð(a/s) at the state s, which leads to state s¡¯ with prob p(s¡¯/s,a). This gives a reward [r + ¥ã*v¥ð(s)] as given in the square bracket above. This is called the Bellman Expectation Equation. The value information from successor states is being transferred back to the current state, and this can be represented efficiently by something called a backup diagram as shown below. The Bellman expectation equation averages over all the possibilities, weighting each by its probability of occurring. It states that the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way. We have n (number of states) linear equations with unique solution to solve for each state s. The goal here is to find the optimal policy, which when followed by the agent gets the maximum cumulative reward. In other words, find a policy ¥ð, such that for no other ¥ð can the agent get a better expected return. We want to find a policy which achieves maximum value for each state. Note that we might not get a unique policy, as under any situation there can be 2 or more paths that have the same return and are still optimal. Optimal value function can be obtained by finding the action a which will lead to the maximum of q*. This is called the bellman optimality equation for v*. Intuitively, the Bellman optimality equation says that the value of each state under an optimal policy must be the return the agent gets when it follows the best action as given by the optimal policy. For optimal policy ¥ð*, the optimal value function is given by: Given a value function q*, we can recover an optimum policy as follows: The value function for optimal policy can be solved through a non-linear system of equations. We can can solve these efficiently using iterative methods that fall under the umbrella of dynamic programming. Dynamic programming algorithms solve a category of problems called planning problems. Herein given the complete model and specifications of the environment (MDP), we can successfully find an optimal policy for the agent to follow. It contains two main steps: To solve a given MDP, the solution must have the components to: Policy evaluation answers the question of how good a policy is. Given an MDP and an arbitrary policy ¥ð, we will compute the state-value function. This is called policy evaluation in the DP literature. The idea is to turn bellman expectation equation discussed earlier to an update. To produce each successive approximation vk+1 from vk, iterative policy evaluation applies the same operation to each state s. It replaces the old value of s with a new value obtained from the old values of the successor states of s, and the expected immediate rewards, along all the one-step transitions possible under the policy being evaluated, until it converges to the true value function of a given policy ¥ð. Let us understand policy evaluation using the very popular example of Gridworld. A bot is required to traverse a grid of 4¡¿4 dimensions to reach its goal (1 or 16). Each step is associated with a reward of -1. There are 2 terminal states here: 1 and 16 and 14 non-terminal states given by [2,3,¡¦.,15]. Consider a random policy for which, at every state, the probability of every action {up, down, left, right} is equal to 0.25. We will start with initialising v0 for the random policy to all 0s. This is definitely not very useful. Let¡¯s calculate v2 for all the states of 6: Similarly, for all non-terminal states, v1(s) = -1. For terminal states p(s¡¯/s,a) = 0 and hence vk(1) = vk(16) = 0 for all k. So v1 for the random policy is given by: Now, for v2(s) we are assuming ¥ã or the discounting factor to be 1: As you can see, all the states marked in red in the above diagram are identical to 6 for the purpose of calculating the value function. Hence, for all these states, v2(s) = -2. For all the remaining states, i.e., 2, 5, 12 and 15, v2 can be calculated as follows: If we repeat this step several times, we get v¥ð: Using policy evaluation we have determined the value function v for an arbitrary policy ¥ð. We know how good our current policy is. Now for some state s, we want to understand what is the impact of taking an action a that does not pertain to policy ¥ð.<U+00A0> Let¡¯s say we select a in s, and after that we follow the original policy ¥ð. The value of this way of behaving is represented as: If this happens to be greater than the value function v¥ð(s), it implies that the new policy ¥ð¡¯ would be better to take. We do this iteratively for all states to find the best policy. Note that in this case, the agent would be following a greedy policy in the sense that it is looking only one step ahead. Let¡¯s get back to our example of gridworld. Using v¥ð, the value function obtained for random policy ¥ð, we can improve upon ¥ð by following the path of highest value (as shown in the figure below). We start with an arbitrary policy, and for each state one step look-ahead is done to find the action leading to the state with the highest value. This is done successively for each state. As shown below for state 2, the optimal action is left which leads to the terminal state having a value . This is the highest among all the next states (0,-18,-20). This is repeated for all states to find the new policy. Overall, after the policy improvement step using v¥ð, we get the new policy ¥ð¡¯: Looking at the new policy, it is clear that it¡¯s much better than the random policy. However, we should calculate v¥ð¡¯ using the policy evaluation technique we discussed earlier to verify this point and for better understanding. Once the policy has been improved using v¥ð to yield a better policy ¥ð¡¯, we can then compute v¥ð¡¯ to improve it further to ¥ð¡¯¡¯. Repeated iterations are done to converge approximately to the true value function for a given policy ¥ð (policy evaluation). Improving the policy as described in the policy improvement section is called policy iteration. In this way, the new policy is sure to be an improvement over the previous one and given enough iterations, it will return the optimal policy. This sounds amazing but there is a drawback <U+2013> each iteration in policy iteration itself includes another iteration of policy evaluation that may require multiple sweeps through all the states. Value iteration technique discussed in the next section provides a possible solution to this. We saw in the gridworld example that at around k = 10, we were already in a position to find the optimal policy. So, instead of waiting for the policy evaluation step to converge exactly to the value function v¥ð, we could stop earlier. We can also get the optimal policy with just 1 step of policy evaluation followed by updating the value function repeatedly (but this time with the updates derived from bellman optimality equation). Let¡¯s see how this is done as a simple backup operation: This is identical to the bellman update in policy evaluation, with the difference being that we are taking the maximum over all actions. Once the updates are small enough, we can take the value function obtained as final and estimate the optimal policy corresponding to that. Some important points related to DP: It is of utmost importance to first have a defined environment in order to test any kind of policy for solving an MDP efficiently. Thankfully, OpenAI, a non profit research organization provides a large number of environments to test and play with various reinforcement learning algorithms. To illustrate dynamic programming here, we will use it to navigate the Frozen Lake environment. The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile. The surface is described using a grid like the following: (S: starting point, safe),<U+00A0><U+00A0>(F: frozen surface, safe),<U+00A0>(H: hole, fall to your doom),<U+00A0>(G: goal) The idea is to reach the goal from the starting point by walking only on frozen surface and avoiding all the holes. Installation details and documentation is available at this link. Once gym library is installed, you can just open a jupyter notebook to get started. Now, the env variable contains all the information regarding the frozen lake environment. Before we move on, we need to understand what an episode is. An episode represents a trial by the agent in its pursuit to reach the goal. An episode ends once the agent reaches a terminal state which in this case is either a hole or the goal. Description of parameters for policy iteration function policy: 2D array of a size n(S) x n(A), each cell represents a probability of taking action a in state s. environment: Initialized OpenAI gym environment object discount_factor: MDP discount factor theta: A threshold of a value function change. Once the update to value function is below this number max_iterations: Maximum number of iterations to avoid letting the program run indefinitely This function will return a vector of size nS, which represent a value function for each state. Let¡¯s start with the policy evaluation step. The objective is to converge to the true value function for a given policy ¥ð. We will define a function that returns the required value function. Now coming to the policy improvement part of the policy iteration algorithm. We need a helper function that does one step lookahead to calculate the state-value function. This will return an array of length nA containing expected value of each action Now, the overall policy iteration would be as described below. This will return a tuple (policy,V) which is the optimal policy matrix and value function for each state. The parameters are defined in the same manner for value iteration.<U+00A0>The value iteration algorithm can be similarly coded: Finally, let¡¯s compare both methods to look at which of them works better in a practical setting. To do this, we will try to learn the optimal policy for the frozen lake environment using both techniques described above. Later, we will check which technique performed better based on the average return after 10,000 episodes. We observe that value iteration has a better average reward and higher number of wins when it is run for 10,000 episodes. In this article, we became familiar with model based planning using dynamic programming, which given all specifications of an environment, can find the best policy to take. I want to particularly mention the brilliant book on RL by Sutton and Barto which is a bible for this technique and encourage people to refer it. More importantly, you have taken the first step towards mastering reinforcement learning. Stay tuned for more articles covering different algorithms within this exciting domain.","Keyword(freq): state(16), algorithm(5), problem(4), action(3), equation(3), iteration(3), reward(3), bike(2), environment(2), episode(2)"
"10","vidhya",2018-09-17,"DataHack Radio #10: The Role of Computer Science in the Data Science World with Dr. Jeannette M. Wing","https://www.analyticsvidhya.com/blog/2018/09/datahack-radio-data-science-podcast-jeanette-wing/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 Have you noticed that the recent surge of data scientists have a background in computer science? It¡¯s not a coincidence. These two domains are important in their own right but when merged together, they produce powerful results. We are thrilled to announce the release of episode 10 of our DataHack Radio podcast with none other than Professor Jeannette M. Wing! She has over 4 decades of experience in academia and the industry, and there is no one better to give a perspective on how computer science has evolved, and how it meshes with the data science world. I have briefly summarized the key takeaways from this episode below. I recommend listening to the podcast to truly get a feel for how computer science and data science are a powerful combination when used together. Enjoy this episode! Subscribe to DataHack Radio NOW and listen to this, as well as all previous episodes, on any of the below platforms: Professor Wing has always been fascinated by mathematics and engineering since her childhood. She went to graduation school at MIT and started majoring in electrical engineering there. During her initial days at the university, she was introduced to the world of computer science and that prompted her to change majors. And there was no looking back from that point on. Post her days at MIT (where she also successfully completed her Ph.D in computer science), she worked at the University of Southern California for a couple of years before joining Carnegie Mellon University. She was the computer science department head twice at Carnegie Mellon. In between those two stints, she worked at the National Science Foundation (NSF). During her second time as the department head at Carnegie Mellon, Microsoft approached her and she took up a role there in 2013. Within a year of joining, she was put in charge of all the basic research labs, including in Silicon Valley, New York, Bangalore, and Beijing, among others. And then last year came Columbia University and a chance to work in academia again. At Columbia, she is the Avanessians Director of the Data Science Institute and Professor of Computer Science. She reports directly to the President of the University. Although there has been decades of research done in computer science to formally show how one can prove how a program is correct, this is all with respect to mathematical logic. What data science is now bringing is the complexity for proving how a property is correct with respect to inherently probabilistic and statistical methods. Professor Wing firmly believes that a lot of the new data science methods should be revisited by the formal methods techniques. Its a challenge for the formal methods community to help data science grow using these concepts, something which hasn¡¯t yet happened. In case you are not aware, formal methods are mathematics based techniques especially used in computer science. You can read more about them here. Professor Wing, in her current role at Columbia University, is working with the AI community to understand what methods and logic are required to specify the relevant properties that these machine learned models should have. She feels this will help build safe and trustworthy AI systems for the future, a topic Professor Wing is a strong advocate of. At Microsoft, she was overlooking several research projects in multiple locations as I mentioned above. The Bangalore lab, in particular, had a couple of big strengths: ¡°I¡¯m really just an academic at heart.¡± <U+2013> Professor Wing A very common question from folks new to data science is <U+2013> ¡°what¡¯s the difference between working in academia versus getting industry experience¡±? And Professor Wing was kind enough to cover this topic. She echoes the wide-held belief that being a scholar has it¡¯s own distinct advantages. You have more freedom to explore questions like why something works, rather than just focusing on how it works (which is what happens in most industry roles). The science part of both computer and data science comes from research and academia far more than the industry. It was a privilege hosting Professor Wing on our podcast. Her explanation of formal methods and the important part they are playing in the software industry was a true delight to listen to. Fans of mathematics will surely love this episode. Happy listening!","Keyword(freq): method(7), mathematic(3), decade(2), technique(2), advantage(1), avanessian(1), concept(1), domain(1), episode(1), fan(1)"
