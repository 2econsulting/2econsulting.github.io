"site","date","headline","url_address","text"
"mastery",2018-10-19,"How to Develop Machine Learning Models for Multivariate Multi-Step Air Pollution Time Series Forecasting","https://machinelearningmastery.com/how-to-develop-machine-learning-models-for-multivariate-multi-step-air-pollution-time-series-forecasting/","Real-world time series forecasting is challenging for a whole host of reasons not limited to problem features such as having multiple input variables, the requirement to predict multiple time steps, and the need to perform the same type of prediction for multiple physical sites. The EMC Data Science Global Hackathon dataset, or the ¡®Air Quality Prediction¡¯ dataset for short, describes weather conditions at multiple sites and requires a prediction of air quality measurements over the subsequent three days. Machine learning algorithms can be applied to time series forecasting problems and offer benefits such as the ability to handle multiple input variables with noisy complex dependencies. In this tutorial, you will discover how to develop machine learning models for multi-step time series forecasting of air pollution data. After completing this tutorial, you will know: Let¡¯s get started. How to Develop Machine Learning Models for Multivariate Multi-Step Air Pollution Time Series ForecastingPhoto by Eric Schmuttenmaer, some rights reserved. This tutorial is divided into nine parts; they are: The Air Quality Prediction dataset describes weather conditions at multiple sites and requires a prediction of air quality measurements over the subsequent three days. Specifically, weather observations such as temperature, pressure, wind speed, and wind direction are provided hourly for eight days for multiple sites. The objective is to predict air quality measurements for the next 3 days at multiple sites. The forecast lead times are not contiguous; instead, specific lead times must be forecast over the 72 hour forecast period. They are: Further, the dataset is divided into disjoint but contiguous chunks of data, with eight days of data followed by three days that require a forecast. Not all observations are available at all sites or chunks and not all output variables are available at all sites and chunks. There are large portions of missing data that must be addressed. The dataset was used as the basis for a short duration machine learning competition (or hackathon) on the Kaggle website in 2012. Submissions for the competition were evaluated against the true observations that were withheld from participants and scored using Mean Absolute Error (MAE). Submissions required the value of -1,000,000 to be specified in those cases where a forecast was not possible due to missing data. In fact, a template of where to insert missing values was provided and required to be adopted for all submissions (what a pain). A winning entrant achieved a MAE of 0.21058 on the withheld test set (private leaderboard) using random forest on lagged observations. A writeup of this solution is available in the post: In this tutorial, we will explore how to develop naive forecasts for the problem that can be used as a baseline to determine whether a model has skill on the problem or not. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course Before we can evaluate naive forecasting methods, we must develop a test harness. This includes at least how the data will be prepared and how forecasts will be evaluated. The first step is to download the dataset and load it into memory. The dataset can be downloaded for free from the Kaggle website. You may have to create an account and log in, in order to be able to download the dataset. Download the entire dataset, e.g. ¡°Download All¡± to your workstation and unzip the archive in your current working directory with the folder named ¡®AirQualityPrediction¡®. Our focus will be the ¡®TrainingData.csv¡® file that contains the training dataset, specifically data in chunks where each chunk is eight contiguous days of observations and target variables. We can load the data file into memory using the Pandas read_csv() function and specify the header row on line 0. We can group data by the ¡®chunkID¡¯ variable (column index 1). First, let¡¯s get a list of the unique chunk identifiers. We can then collect all rows for each chunk identifier and store them in a dictionary for easy access. Below defines a function named to_chunks() that takes a NumPy array of the loaded data and returns a dictionary of chunk_id to rows for the chunk. The complete example that loads the dataset and splits it into chunks is listed below. Running the example prints the number of chunks in the dataset. Now that we know how to load the data and split it into chunks, we can separate into train and test datasets. Each chunk covers an interval of eight days of hourly observations, although the number of actual observations within each chunk may vary widely. We can split each chunk into the first five days of observations for training and the last three for test. Each observation has a row called ¡®position_within_chunk¡® that varies from 1 to 192 (8 days * 24 hours). We can therefore take all rows with a value in this column that is less than or equal to 120 (5 * 24) as training data and any values more than 120 as test data. Further, any chunks that don¡¯t have any observations in the train or test split can be dropped as not viable. When working with the naive models, we are only interested in the target variables, and none of the input meteorological variables. Therefore, we can remove the input data and have the train and test data only comprised of the 39 target variables for each chunk, as well as the position within chunk and hour of observation. The split_train_test() function below implements this behavior; given a dictionary of chunks, it will split each into a list of train and test chunk data. We do not require the entire test dataset; instead, we only require the observations at specific lead times over the three day period, specifically the lead times: Where, each lead time is relative to the end of the training period. First, we can put these lead times into a function for easy reference: Next, we can reduce the test dataset down to just the data at the preferred lead times. We can do that by looking at the ¡®position_within_chunk¡® column and using the lead time as an offset from the end of the training dataset, e.g. 120 + 1, 120 +2, etc. If we find a matching row in the test set, it is saved, otherwise a row of NaN observations is generated. The function to_forecasts() below implements this and returns a NumPy array with one row for each forecast lead time for each chunk. We can tie all of this together and split the dataset into train and test sets and save the results to new files. The complete code example is listed below. Running the example first comments that chunk 69 is removed from the dataset for having insufficient data. We can then see that we have 42 columns in each of the train and test sets, one for the chunk id, position within chunk, hour of day, and the 39 training variables. We can also see the dramatically smaller version of the test dataset with rows only at the forecast lead times. The new train and test datasets are saved in the ¡®naive_train.csv¡® and ¡®naive_test.csv¡® files respectively. Once forecasts have been made, they need to be evaluated. It is helpful to have a simpler format when evaluating forecasts. For example, we will use the three-dimensional structure of [chunks][variables][time], where variable is the target variable number from 0 to 38 and time is the lead time index from 0 to 9. Models are expected to make predictions in this format. We can also restructure the test dataset to have this dataset for comparison. The prepare_test_forecasts() function below implements this. We will evaluate a model using the mean absolute error, or MAE. This is the metric that was used in the competition and is a sensible choice given the non-Gaussian distribution of the target variables. If a lead time contains no data in the test set (e.g. NaN), then no error will be calculated for that forecast. If the lead time does have data in the test set but no data in the forecast, then the full magnitude of the observation will be taken as error. Finally, if the test set has an observation and a forecast was made, then the absolute difference will be recorded as the error. The calculate_error() function implements these rules and returns the error for a given forecast. Errors are summed across all chunks and all lead times, then averaged. The overall MAE will be calculated, but we will also calculate a MAE for each forecast lead time. This can help with model selection generally as some models may perform differently at different lead times. The evaluate_forecasts() function below implements this, calculating the MAE and per-lead time MAE for the provided predictions and expected values in [chunk][variable][time] format. Once we have the evaluation of a model, we can present it. The summarize_error() function below first prints a one-line summary of a model¡¯s performance then creates a plot of MAE per forecast lead time. We are now ready to start exploring the performance of naive forecasting methods. Machine Learning Modeling The problem can be modeled with machine learning. Most machine learning models do not directly support the notion of observations over time. Instead, the lag observations must be treated as input features in order to make predictions. This is a benefit of machine learning algorithms for time series forecasting. Specifically, that they are able to support large numbers of input features. These could be lag observations for one or multiple input time series. Other general benefits of machine learning algorithms for time series forecasting over classical methods include: A challenge with this dataset is the need to make multi-step forecasts. There are two main approaches that machine learning methods can be used to make multi-step forecasts; they are: The recursive approach can make sense when forecasting a short contiguous block of lead times, whereas the direct approach may make more sense when forecasting discontiguous lead times. The direct approach may be more appropriate for the air pollution forecast problem given that we are interested in forecasting a mixture of 10 contiguous and discontiguous lead times over a three-day period. The dataset has 39 target variables, and we develop one model per target variable, per forecast lead time. That means that we require (39 * 10) 390 machine learning models. Key to the use of machine learning algorithms for time series forecasting is the choice of input data. We can think about three main sources of data that can be used as input and mapped to each forecast lead time for a target variable; they are: Data can be drawn from across all chunks, providing a rich dataset for learning a mapping from inputs to the target forecast lead time. The 39 target variables are actually comprised of 12 variables across 14 sites. Because of the way the data is provided, the default approach to modeling is to treat each variable-site as independent. It may be possible to collapse data by variable and use the same models for a variable across multiple sites. Some variables have been purposely mislabeled (e.g different data used variables with the same identifier). Nevertheless, perhaps these mislabeled variables can be identified and excluded from multi-site models. Before we can explore machine learning models of this dataset, we must prepare the data in such a way that we can fit models. This requires two data preparation steps: For now, we will focus on the 39 target variables and ignore the meteorological and metadata. Chunks are comprised of five days or less of hourly observations for 39 target variables. Many of the chunks do not have all five days of data, and none of the chunks have data for all 39 target variables. In those cases where a chunk has no data for a target variable, a forecast is not required. In those cases where a chunk does have some data for a target variable, but not all five days worth, there will be gaps in the series. These gaps may be a few hours to over a day of observations in length, sometimes even longer. Three candidate strategies for dealing with these gaps are as follows: We could ignore the gaps. A problem with this would be that that data would not be contiguous when splitting data into inputs and outputs. When training a model, the inputs will not be consistent, but could mean the last n hours of data, or data spread across the last n days. This inconsistency will make learning a mapping from inputs to outputs very noisy and perhaps more difficult for the model than it needs to be. We could use only the data without gaps. This is a good option. A risk is that we may not have much or enough data with which to fit a model. Finally, we could fill the gaps. This is called data imputation and there are many strategies that could be used to fill the gaps. Three methods that may perform well include: In this tutorial, we will use the latter approach and fill the gaps by using the median for the time of day across chunks. This method seems to result in more training samples and better model performance after a little testing. For a given variable, there may be missing observations defined by missing rows. Specifically, each observation has a ¡®position_within_chunk¡®. We expect each chunk in the training dataset to have 120 observations, with ¡®positions_within_chunk¡® from 1 to 120 inclusively. Therefore, we can create an array of 120 NaN values for each variable, mark all observations in the chunk using the ¡®positions_within_chunk¡® values, and anything left will be marked<U+00A0>NaN. We can then plot each variable and look for gaps. The variable_to_series() function below will take the rows for a chunk and a given column index for the target variable and will return a series of 120 time steps for the variable with all available data marked with the value from the chunk. We need to calculate a parallel series of the hour of day for each chunk that we can use for imputing hour specific data for each variable in the chunk. Given a series of partially filled hours of day, the interpolate_hours() function below will fill in the missing hours of day. It does this by finding the first marked hour, then counting forward, filling in the hour of day, then performing the same operation backwards. We can call the same variable_to_series() function (above) to create the series of hours with missing values (column index 2), then call interpolate_hours() to fill in the gaps. We can then pass the hours to any impute function that may make use of it. We can now try filling in missing values in a chunk with values within the same series with the same hour. Specifically, we will find all rows with the same hour on the series and calculate the median value. The impute_missing() below takes all of the rows in a chunk, the prepared sequence of hours of the day for the chunk, and the series with missing values for a variable and the column index for a variable. It first checks to see if the series is all missing data and returns immediately if this is the case as no impute can be performed. It then enumerates over the time steps of the series and when it detects a time step with no data, it collects all rows in the series with data for the same hour and calculates the median value. We need to transform the series for each target variable into rows with inputs and outputs so that we can fit supervised machine learning algorithms. Specifically, we have a series, like: When forecasting the lead time of +1 using 2 lag variables, we would split the series into input (X) and output (y) patterns as follows: This first requires that we choose a number of lag observations to use as input. There is no right answer; instead, it is a good idea to test different numbers and see what works. We then must perform the splitting of the series into the supervised learning format for each of the 10 forecast lead times. For example, forecasting +24 with 2 lag observations might look like: This process is then repeated for each of the 39 target variables. The patterns prepared for each lead time for each target variable can then be aggregated across chunks to provide a training dataset for a model. We must also prepare a test dataset. That is, input data (X) for each target variable for each chunk so that we can use it as input to forecast the lead times in the test dataset. If we chose a lag of 2, then the test dataset would be comprised of the last two observations for each target variable for each chunk. Pretty straightforward. We can start off by defining a function that will create input-output patterns for a given complete (imputed) series. The supervised_for_lead_time() function below will take a series, a number of lag observations to use as input, and a forecast lead time to predict, then will return a list of input/out rows drawn from the series. It is important to understand this piece. We can test this function and explore different numbers of lag variables and forecast lead times on a small test dataset. Below is a complete example that generates a series of 20 integers and creates a series with two input lags and forecasts the +6 lead time. Running the example prints the resulting patterns showing lag observations and their associated forecast lead time. Experiment with this example to get comfortable with this data transform as it is key to modeling time series using machine learning algorithms. We can now call supervised_for_lead_time() for each forecast lead time for a given target variable series. The target_to_supervised() function below implements this. First the target variable is converted into a series and imputed using the functions developed in the previous section. Then training samples are created for each target lead time. A test sample for the target variable is also created. Both the training data for each forecast lead time and the test input data are then returned for this target variable. We have the pieces; we now need to define the function to drive the data preparation process. This function builds up the train and test datasets. The approach is to enumerate each target variable and gather the training data for each lead time from across all of the chunks. At the same time, we collect the samples required as input when making a prediction for the test dataset. The result is a training dataset that has the dimensions [var][lead time][sample] where the final dimension are the rows of training samples for a forecast lead time for a target variable. The function also returns the test dataset with the dimensions [chunk][var][sample] where the final dimension is the input data for making a prediction for a target variable for a chunk. The data_prep() function below implements this behavior and takes the data in chunk format and a specified number of lag observations to use as input. We can tie everything together and prepare a train and test dataset with a supervised learning format for machine learning algorithms. We will use the prior 12 hours of lag observations as input when predicting each forecast lead time. The resulting train and test datasets are then saved as binary NumPy arrays. The complete example is listed below. Running the example may take a minute. The result are two binary files containing the train and test datasets that we can load in the following sections for training and evaluating machine learning algorithms on the problem. Before we can start evaluating algorithms, we need some more elements of the test harness. First, we need to be able to fit a scikit-learn model on training data. The fit_model() function below will make a clone of the model configuration and fit it on the provided training data. We will need to fit many (360) versions of each configured model, so this function will be called a lot. Next, we need to fit a model for each variable and forecast lead time combination. We can do this by enumerating the training dataset first by the variables and then by the lead times. We can then fit a model and store it in a list of lists with the same structure, specifically: [var][time][model]. The fit_models() function below implements this. Fitting models is the slow part and could benefit from being parallelized, such as with the Joblib library. This is left as an extension. Once the models are fit, they can be used to make predictions for the test dataset. The prepared test dataset is organized first by chunk, and then by target variable. Making predictions is fast and involves first checking that a prediction can be made (we have input data) and if so, using the appropriate models for the target variable. Each of the 10 forecast lead times for the variable will then be predicted with each of the direct models for those lead times. The make_predictions() function below implements this, taking the list of lists of models and the loaded test dataset as arguments and returning an array of forecasts with the structure [chunks][var][time]. We need a list of models to evaluate. We can define a generic get_models() function that is responsible for defining a dictionary of model-names mapped to configured scikit-learn model objects. Finally, we need a function to drive the model evaluation process. Given the dictionary of models, enumerate the models, first fitting the matrix of models on the training data, making predictions of the test dataset, evaluating the predictions, and summarizing the results. The evaluate_models() function below implements this. We now have everything we need to evaluate machine learning models. In this section, we will spot check a suite of linear machine learning algorithms. Linear algorithms are those that assume that the output is a linear function of the input variables. This is much like the assumptions of classical time series forecasting models like ARIMA. Spot checking means evaluating a suite of models in order to get a rough idea of what works. We are interested in any models that outperform a simple autoregression model AR(2) that achieves a MAE error of about 0.487. We will test eight linear machine learning algorithms with their default configuration; specifically: We can define these models in the get_models() function. The complete code example is listed below. Running the example prints the MAE for each of the evaluated algorithms. We can see that many of the algorithms show skill compared to a simple AR model, achieving a MAE below 0.487. Huber regression seems to perform the best (with default configuration), achieving a MAE of 0.434. This is interesting as Huber regression, or robust regression with Huber loss, is a method that is designed to be robust to outliers in the training dataset. It may suggest that the other methods may perform better with a little more data preparation, such as standardization and/or outlier removal. We can use the same framework to evaluate the performance of a suite of nonlinear and ensemble machine learning algorithms. Specifically: Nonlinear Algorithms Ensemble Algorithms The get_models() function below defines these nine models. The complete code listing is provided below. Running the example, we can see that many algorithms performed well compared to the baseline of an autoregression algorithm, although none performed as well as Huber regression in the previous section. Both support vector regression and perhaps gradient boosting machines may be worth further investigation of achieving MAEs of 0.437 and 0.450 respectively. In the previous spot check experiments, the number of lag observations was arbitrarily fixed at 12. We can vary the number of lag observations and evaluate the effect on MAE. Some algorithms may require more or fewer prior observations, but general trends may hold across algorithms. Prepare the supervised learning dataset with a range of different numbers of lag observations and fit and evaluate the HuberRegressor on each. I experimented with the following number of lag observations: The results were as follows: A plot of these results is provided below. Line Plot of Number of Lag Observations vs MAE for Huber Regression We can see a general trend of decreasing overall MAE with the increase in the number of lag observations, at least to a point after which error begins to rise again. The results suggest, at least for the HuberRegressor algorithm, that 36 lag observations may be a good configuration achieving a MAE of 0.422. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop machine learning models for multi-step time series forecasting of air pollution data. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Wow, that¡¯s a complete tutorial! Tnks Thanks! i d like to apply something similar to the lottery viewed as time series with multiple time steps You cannot predict the lottery:https://machinelearningmastery.com/faq/single-faq/can-i-use-machine-learning-to-predict-the-lottery Comment  Name (required)  Email (will not be published) (required)  Website"
"mastery",2018-10-17,"How to Develop Autoregressive Forecasting Models for Multi-Step Air Pollution Time Series Forecasting","https://machinelearningmastery.com/how-to-develop-autoregressive-forecasting-models-for-multi-step-air-pollution-time-series-forecasting/","Real-world time series forecasting is challenging for a whole host of reasons not limited to problem features such as having multiple input variables, the requirement to predict multiple time steps, and the need to perform the same type of prediction for multiple physical sites. The EMC Data Science Global Hackathon dataset, or the ¡®Air Quality Prediction¡¯ dataset for short, describes weather conditions at multiple sites and requires a prediction of air quality measurements over the subsequent three days. Before diving into sophisticated machine learning and deep learning methods for time series forecasting, it is important to find the limits of classical methods, such as developing autoregressive models using the AR or ARIMA method. In this tutorial, you will discover how to develop autoregressive models for multi-step time series forecasting for a multivariate air pollution time series. After completing this tutorial, you will know: Let¡¯s get started. Impact of Dataset Size on Deep Learning Model Skill And Performance EstimatesPhoto by Eneas De Troya, some rights reserved. This tutorial is divided into six parts; they are: The Air Quality Prediction dataset describes weather conditions at multiple sites and requires a prediction of air quality measurements over the subsequent three days. Specifically, weather observations such as temperature, pressure, wind speed, and wind direction are provided hourly for eight days for multiple sites. The objective is to predict air quality measurements for the next 3 days at multiple sites. The forecast lead times are not contiguous; instead, specific lead times must be forecast over the 72 hour forecast period. They are: Further, the dataset is divided into disjoint but contiguous chunks of data, with eight days of data followed by three days that require a forecast. Not all observations are available at all sites or chunks and not all output variables are available at all sites and chunks. There are large portions of missing data that must be addressed. The dataset was used as the basis for a short duration machine learning competition (or hackathon) on the Kaggle website in 2012. Submissions for the competition were evaluated against the true observations that were withheld from participants and scored using Mean Absolute Error (MAE). Submissions required the value of -1,000,000 to be specified in those cases where a forecast was not possible due to missing data. In fact, a template of where to insert missing values was provided and required to be adopted for all submissions (what a pain). A winning entrant achieved a MAE of 0.21058 on the withheld test set (private leaderboard) using random forest on lagged observations. A writeup of this solution is available in the post: In this tutorial, we will explore how to develop naive forecasts for the problem that can be used as a baseline to determine whether a model has skill on the problem or not. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course Before we can evaluate naive forecasting methods, we must develop a test harness. This includes at least how the data will be prepared and how forecasts will be evaluated. The first step is to download the dataset and load it into memory. The dataset can be downloaded for free from the Kaggle website. You may have to create an account and log in, in order to be able to download the dataset. Download the entire dataset, e.g. ¡°Download All¡± to your workstation and unzip the archive in your current working directory with the folder named ¡®AirQualityPrediction¡®. Our focus will be the ¡®TrainingData.csv¡® file that contains the training dataset, specifically data in chunks where each chunk is eight contiguous days of observations and target variables. We can load the data file into memory using the Pandas read_csv() function and specify the header row on line 0. We can group data by the ¡®chunkID¡¯ variable (column index 1). First, let¡¯s get a list of the unique chunk identifiers. We can then collect all rows for each chunk identifier and store them in a dictionary for easy access. Below defines a function named to_chunks() that takes a NumPy array of the loaded data and returns a dictionary of chunk_id to rows for the chunk. The complete example that loads the dataset and splits it into chunks is listed below. Running the example prints the number of chunks in the dataset. Now that we know how to load the data and split it into chunks, we can separate into train and test datasets. Each chunk covers an interval of eight days of hourly observations, although the number of actual observations within each chunk may vary widely. We can split each chunk into the first five days of observations for training and the last three for test. Each observation has a row called ¡®position_within_chunk¡® that varies from 1 to 192 (8 days * 24 hours). We can therefore take all rows with a value in this column that is less than or equal to 120 (5 * 24) as training data and any values more than 120 as test data. Further, any chunks that don¡¯t have any observations in the train or test split can be dropped as not viable. When working with the naive models, we are only interested in the target variables, and none of the input meteorological variables. Therefore, we can remove the input data and have the train and test data only comprised of the 39 target variables for each chunk, as well as the position within chunk and hour of observation. The split_train_test() function below implements this behavior; given a dictionary of chunks, it will split each into a list of train and test chunk data. We do not require the entire test dataset; instead, we only require the observations at specific lead times over the three day period, specifically the lead times: Where, each lead time is relative to the end of the training period. First, we can put these lead times into a function for easy reference: Next, we can reduce the test dataset down to just the data at the preferred lead times. We can do that by looking at the ¡®position_within_chunk¡® column and using the lead time as an offset from the end of the training dataset, e.g. 120 + 1, 120 +2, etc. If we find a matching row in the test set, it is saved, otherwise a row of NaN observations is generated. The function to_forecasts() below implements this and returns a NumPy array with one row for each forecast lead time for each chunk. We can tie all of this together and split the dataset into train and test sets and save the results to new files. The complete code example is listed below. Running the example first comments that chunk 69 is removed from the dataset for having insufficient data. We can then see that we have 42 columns in each of the train and test sets, one for the chunk id, position within chunk, hour of day, and the 39 training variables. We can also see the dramatically smaller version of the test dataset with rows only at the forecast lead times. The new train and test datasets are saved in the ¡®naive_train.csv¡® and ¡®naive_test.csv¡® files respectively. Once forecasts have been made, they need to be evaluated. It is helpful to have a simpler format when evaluating forecasts. For example, we will use the three-dimensional structure of [chunks][variables][time], where variable is the target variable number from 0 to 38 and time is the lead time index from 0 to 9. Models are expected to make predictions in this format. We can also restructure the test dataset to have this dataset for comparison. The prepare_test_forecasts() function below implements this. We will evaluate a model using the mean absolute error, or MAE. This is the metric that was used in the competition and is a sensible choice given the non-Gaussian distribution of the target variables. If a lead time contains no data in the test set (e.g. NaN), then no error will be calculated for that forecast. If the lead time does have data in the test set but no data in the forecast, then the full magnitude of the observation will be taken as error. Finally, if the test set has an observation and a forecast was made, then the absolute difference will be recorded as the error. The calculate_error() function implements these rules and returns the error for a given forecast. Errors are summed across all chunks and all lead times, then averaged. The overall MAE will be calculated, but we will also calculate a MAE for each forecast lead time. This can help with model selection generally as some models may perform differently at different lead times. The evaluate_forecasts() function below implements this, calculating the MAE and per-lead time MAE for the provided predictions and expected values in [chunk][variable][time] format. Once we have the evaluation of a model, we can present it. The summarize_error() function below first prints a one-line summary of a model¡¯s performance then creates a plot of MAE per forecast lead time. We are now ready to start exploring the performance of naive forecasting methods. The first step in fitting classical time series models to this data is to take a closer look at the data. There are 208 (actually 207) usable chunks of data, and each chunk has 39 time series to fit; that is a total of 8,073 separate models that would need to be fit on the data. That is a lot of models, but the models are trained on a relatively small amount of data, at most (5 * 24) or 120 observations and the model is linear so it will find a fit quickly. We have choices about how to configure models to the data; for example: We will investigate the simplest approach of one model configuration for all series, but you may want to explore one or more of the other approaches. This section is divided into three parts; they are: Classical time series methods require that the time series to be complete, e.g. that there are no missing values. Therefore the first step is to investigate how complete or incomplete the target variables are. For a given variable, there may be missing observations defined by missing rows. Specifically, each observation has a ¡®position_within_chunk¡®. We expect each chunk in the training dataset to have 120 observations, with ¡®positions_within_chunk¡® from 1 to 120 inclusively. Therefore, we can create an array of 120 nan values for each variable, mark all observations in the chunk using the ¡®positions_within_chunk¡® values, and anything left will be marked NaN. We can then plot each variable and look for gaps. The variable_to_series() function below will take the rows for a chunk and a given column index for the target variable and will return a series of 120 time steps for the variable with all available data marked with the value from the chunk. We can then call this function for each target variable in one chunk and create a line plot. The function below named plot_variables() will implement this and create a figure with 39 line plots stacked horizontally. Tying this together, the complete example is listed below. A plot of all variables in the first chunk is created. Running the example creates a figure with 39 line plots, one for each target variable in the first chunk. We can see a seasonal structure in many of the variables. This suggests it may be beneficial to perform a 24-hour seasonal differencing of each series prior to modeling. The plots are small, and you may need to increase the size of the figure to clearly see the data. We can see that there are variables for which we have no data. These can be detected and ignored as we cannot model or forecast them. We can see gaps in many of the series, but the gaps are short, lasting for a few hours at most. These could be imputed either with persistence of previous values or values at the same hours within the same series. Line Plots for All Targets in Chunk 1 With Missing Values Marked Looking at a few other chunks randomly, many result in plots with much the same observations. This is not always the case though. Update the example to plot the 4th chunk in the dataset (index 3). The result is a figure that tells a very different story. We see gaps in the data that last for many hours, perhaps up to a day or more. These series will require dramatic repair before they can be used to fit a classical model. Imputing the missing data using persistence or observations within the series with the same hour will likely not be sufficient. They may have to be filled with average values taken across the entire training dataset. Line Plots for All Targets in Chunk 4 With Missing Values Marked There are many ways to impute the missing data, and we cannot know which is best a priori. One approach would be to prepare the data using multiple different imputation methods and use the skill of the models fit on the data to help guide the best approach. Some imputation approaches already suggested include: It may also be useful to use combinations, e.g. persist or fill from the series for small gaps and draw from the whole dataset for large gaps. We can also investigate the effect of imputing methods by filling in the missing data and looking at plots to see if the series looks reasonable. It¡¯s crude, effective, and fast. First, we need to calculate a parallel series of the hour of day for each chunk that we can use for imputing hour-specific data for each variable in the chunk. Given a series of partially filled hours of day, the interpolate_hours() function below will fill in the missing hours of day. It does this by finding the first marked hour, then counting forward, filling in the hour of day, then performing the same operation backwards. I¡¯m sure there is a more Pythonic way to write this function, but I wanted to lay it all out to make it obvious what was going on. We can test this out on a mock list of hours with missing data. The complete example is listed below. Running the example first prints the hour data with missing values, then the same sequence with all of the hours filled in correctly. We can use this function to prepare a series of hours for a chunk that can be used to fill in missing values for a chunk using hour-specific information. We can call the same variable_to_series() function from the previous section to create the series of hours with missing values (column index 2), then call interpolate_hours() to fill in the gaps. We can then pass the hours to any impute function that may make use of it. Let¡¯s try filling in missing values in a chunk with values within the same series with the same hour. Specifically, we will find all rows with the same hour on the series and calculate the median value. The impute_missing() below takes all of the rows in a chunk, the prepared sequence of hours of the day for the chunk, and the series with missing values for a variable and the column index for a variable. It first checks to see if the series is all missing data and returns immediately if this is the case as no impute can be performed. It then enumerates over the time steps of the series and when it detects a time step with no data, it collects all rows in the series with data for the same hour and calculates the median value. To see the impact of this impute strategy, we can update the plot_variables() function from the previous section to first plot the imputed series then plot the original series with missing values. This will allow the imputed values to shine through in the gaps of the original series and we can see if the results look reasonable. The updated version of the plot_variables() function is listed below with this change, calling the impute_missing() function to create the imputed version of the series and taking the hours series as an argument. Tying all of this together, the complete example is listed below. Running the example creates a single figure with 39 line plots: one for each target variable in the first chunk in the training dataset. We can see that the series is orange, showing the original data and the gaps have been imputed and are marked in blue. The blue segments seem reasonable. Line Plots for All Targets in Chunk 1 With Imputed Missing Values We can try the same approach on the 4th chunk in the dataset that has a lot more missing data. Running the example creates the same kind of figure, but here we can see the large missing segments filled in with imputed values. Again, the sequences seem reasonable, even showing daily seasonal cycle structure where appropriate. Line Plots for All Targets in Chunk 4 With Imputed Missing Values This looks like a good start; you can explore other imputation strategies and see how they compare either in terms of line plots or on the resulting model skill. Now that we know how to fill in the missing values, we can take a look at autocorrelation plots for the series data. Autocorrelation plots summarize the relationship of each observation with observations at prior time steps. Together with partial autocorrelation plots, they can be used to determine the configuration for an ARMA model. The statsmodels library provides the plot_acf() and plot_pacf() functions that can be used to plot ACF and PACF plots respectively. We can update the plot_variables() to create these plots, one of each type for each of the 39 series. That is a lot of plots. We will stack all ACF plots on the left vertically and all PACF plots on the right vertically. That is two columns of 39 plots. We will limit the lags considered by the plot to 24 time steps (hours) and ignore the correlation of each variable with itself as it is redundant. The updated plot_variables() function for plotting ACF and PACF plots is listed below. The complete example is listed below. Running the example creates a figure with a lot of plots for the target variables in the first chunk of the training dataset. You may need to increase the size of the plot window to better see the details of each plot. We can see on the left that most ACF plots show significant correlations (dots above the significance region) at lags 1-2 steps, maybe lags 1-3 steps in some cases, with a slow, steady decrease over the lags Similarly, on the right, we can see significant lags in the PACF plot at 1-2 time steps with steep fall-off. This strongly suggests an autocorrelation process with an order of perhaps 1, 2, or 3, e.g. AR(3). In the ACF plots on the left we can also see a daily cycle in the correlations. This may suggest some benefit in a seasonal differencing of the data prior to modeling or the use of an AR model capable of seasonal differencing. ACF and PACF Plots for Target Variables in Chunk 1 We can repeat this analysis of the target variables for other chunks and we see much the same picture. It suggests we may be able to get away with a general AR model configuration for all series across all chunks. In this section, we will develop an autoregressive model for the imputed target series data. The first step is to implement a general function for making a forecast for each chunk. The function tasks the training dataset and the input columns (chunk id, position in chunk, and hour) for the test set and returns forecasts for all chunks with the expected 3D format of [chunk][variable][time]. The function enumerates the chunks in the forecast, then enumerates the 39 target columns, calling another new function named forecast_variable() in order to make a prediction for each lead time for a given target variable. The complete function is listed below. We can now implement a version of the forecast_variable(). For each variable, we first check if there is no data (e.g. all NaNs) and if so, we return a forecast that is a NaN for each forecast lead time. We then create a series from the variable using the variable_to_series() and then impute the missing values using the median within the series by calling impute_missing(), both of which were developed in the previous section. Finally, we call a new function named fit_and_forecast() that fits a model and predicts the 10 forecast lead times. We will fit an AR model to a given imputed series. To do this, we will use the statsmodels ARIMA class. We will use ARIMA instead of AR to offer some flexibility if you would like to explore any of the family of ARIMA models. First, we must define the model, including the order of the autoregressive process, such as AR(1). Next, the model is fit on the imputed series. We turn off the verbose information during the fit by setting disp to False. The fit model is then used to forecast the next 72 hours beyond the end of the series. We are only interested in specific lead times, so we prepare an array of those lead times, subtract 1 to turn them into array indices, then use them to select the values at the 10 forecast lead times in which we are interested. The statsmodels ARIMA models use linear algebra libraries to fit the model under the covers, and sometimes the fit process can be unstable on some data. As such, it can throw an exception or report a lot of warnings. We will trap exceptions and return a NaN forecast, and ignore all warnings during the fit and evaluation. The fit_and_forecast() function below ties all of this together. We are now ready to evaluate an autoregressive process for each of the 39 series in each of the 207 training chunks. We will start off by testing an AR(1) process. The complete code example is listed below. Running the example first reports the overall MAE for the test set, followed by the MAE for each forecast lead time. We can see that the model achieves a MAE of about 0.492, which is less than a MAE 0.520 achieved by a naive persistence model. This shows that indeed the approach has some skill. A line plot of MAE per forecast lead time is created, showing the linear increase in forecast error with the increase in forecast lead time. MAE vs Forecast Lead Time for AR(1) We can change the code to test other AR models. Specifically the order of the ARIMA model in the fit_and_forecast() function. An AR(2) model can be defined as: Running the code with this update shows a further drop in error to an overall MAE of about 0.490. We can also try an AR(3): Re-running the example with the update shows an increase in the overall MAE compared to an AR(2). An AR(2) might be a good global level configuration to use, although it is expected that models tailored to each variable or each series may perform better overall. We can evaluate the AR(2) model with an alternate imputation strategy. Instead of calculating the median value for the same hour across the series in the chunk, we can calculate the same value across the variable in all chunks. We can update the impute_missing() to take all training chunks as an argument, then collect rows from all chunks for a given hour in order to calculate the median value used to impute. The updated version of the function is listed below. In order to pass the train_chunks to the impute_missing() function, we must update the forecast_variable() function to also take train_chunks as an argument and pass it along, and in turn update the forecast_chunks() function to pass train_chunks. The complete example using a global imputation strategy is listed below. Running the example shows a further drop in the overall MAE to about 0.487. It may be interesting to explore imputation strategies that alternate the method used to fill in missing values based on how much missing data a series has or the gap being filled. A line plot of MAE vs. forecast lead time is also created. MAE vs Forecast Lead Time for AR(2) Impute With Global Strategy This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop autoregressive models for multi-step time series forecasting for a multivariate air pollution time series. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hi Dr. Brownlee! Awesome job done as always! Thank you so much for your contribution.
I wonder is it possible to implement some kind of Machine Learning methods to fill missing data by neighboring stations. I mean if there are several stations with correlated observed timeseries and I want to use them to fill missing timeseries data in the station that I use for modeling. Usually pairwise correlation method used for such tasks and it is statistically based approach, maybe it would be better to use Neural Networks. What do you think? Thanks. Sure, try it and let me know how you go. Comment  Name (required)  Email (will not be published) (required)  Website"
"mastery",2018-10-15,"How to Develop Baseline Forecasts for Multi-Site Multivariate Air Pollution Time Series Forecasting","https://machinelearningmastery.com/how-to-develop-baseline-forecasts-for-multi-site-multivariate-air-pollution-time-series-forecasting/","Real-world time series forecasting is challenging for a whole host of reasons not limited to problem features such as having multiple input variables, the requirement to predict multiple time steps, and the need to perform the same type of prediction for multiple physical sites. The EMC Data Science Global Hackathon dataset, or the ¡®Air Quality Prediction¡® dataset for short, describes weather conditions at multiple sites and requires a prediction of air quality measurements over the subsequent three days. An important first step when working with a new time series forecasting dataset is to develop a baseline in model performance by which the skill of all other more sophisticated strategies can be compared. Baseline forecasting strategies are simple and fast. They are referred to as ¡®naive¡¯ strategies because they assume very little or nothing about the specific forecasting problem. In this tutorial, you will discover how to develop naive forecasting methods for the multistep multivariate air pollution time series forecasting problem. After completing this tutorial, you will know: Let¡¯s get started. How to Develop Baseline Forecasts for Multi-Site Multivariate Air Pollution Time Series ForecastingPhoto by DAVID HOLT, some rights reserved. This tutorial is divided into six parts; they are: The Air Quality Prediction dataset describes weather conditions at multiple sites and requires a prediction of air quality measurements over the subsequent three days. Specifically, weather observations such as temperature, pressure, wind speed, and wind direction are provided hourly for eight days for multiple sites. The objective is to predict air quality measurements for the next 3 days at multiple sites. The forecast lead times are not contiguous; instead, specific lead times must be forecast over the 72 hour forecast period. They are: Further, the dataset is divided into disjoint but contiguous chunks of data, with eight days of data followed by three days that require a forecast. Not all observations are available at all sites or chunks and not all output variables are available at all sites and chunks. There are large portions of missing data that must be addressed. The dataset was used as the basis for a short duration machine learning competition (or hackathon) on the Kaggle website in 2012. Submissions for the competition were evaluated against the true observations that were withheld from participants and scored using Mean Absolute Error (MAE). Submissions required the value of -1,000,000 to be specified in those cases where a forecast was not possible due to missing data. In fact, a template of where to insert missing values was provided and required to be adopted for all submissions (what a pain). A winning entrant achieved a MAE of 0.21058 on the withheld test set (private leaderboard) using random forest on lagged observations. A writeup of this solution is available in the post: In this tutorial, we will explore how to develop naive forecasts for the problem that can be used as a baseline to determine whether a model has skill on the problem or not. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course A baseline in forecast performance provides a point of comparison. It is a point of reference for all other modeling techniques on your problem. If a model achieves performance at or below the baseline, the technique should be fixed or abandoned. The technique used to generate a forecast to calculate the baseline performance must be easy to implement and naive of problem-specific details. The principle is that if a sophisticated forecast method cannot outperform a model that uses little or no problem-specific information, then it does not have skill. There are problem-agnostic forecast methods that can and should be used first, followed by naive methods that use a modicum of problem-specific information. Two examples of problem agnostic naive forecast methods that could be used include: The data is divided into chunks, or intervals, of time. Each chunk of time has multiple variables at multiple sites to forecast. The persistence forecast method makes sense at this chunk-level of organization of the data. Other persistence methods could be explored; for example: These are desirable baseline methods to explore, but the large amount of missing data and discontiguous structure of most of the data chunks make them challenging to implement without non-trivial data preparation. Forecasting the average observations for each series can be elaborated further; for example: A three-day forecast is required for each series with different start-times, e.g. times of day. As such, the forecast lead times for each chunk will fall on different hours of the day. A further elaboration of forecasting the average value is to incorporate the hour of day that is being forecasted; for example: Many variables are measured at multiple sites; as such, it may be possible to use information across series, such as in the calculation of averages or averages per hour of day for forecast lead times. These are interesting, but may exceed the mandate of naive. This is a good starting point, although there may be further elaborations of the naive methods that you may want to consider and explore as an exercise. Remember, the goal is to use very little problem specific information in order to develop a forecast baseline. In summary, we will investigate five different naive forecasting methods for this problem, the best of which will provide a lower-bound on performance by which other models can be compared. They are: Before we can evaluate naive forecasting methods, we must develop a test harness. This includes at least how the data will be prepared and how forecasts will be evaluated. The first step is to download the dataset and load it into memory. The dataset can be downloaded for free from the Kaggle website. You may have to create an account and log in, in order to be able to download the dataset. Download the entire dataset, e.g. ¡°Download All¡± to your workstation and unzip the archive in your current working directory with the folder named ¡®AirQualityPrediction¡®. Our focus will be the ¡®TrainingData.csv¡® file that contains the training dataset, specifically data in chunks where each chunk is eight contiguous days of observations and target variables. We can load the data file into memory using the Pandas read_csv() function and specify the header row on line 0. We can group data by the ¡®chunkID¡¯ variable (column index 1). First, let¡¯s get a list of the unique chunk identifiers. We can then collect all rows for each chunk identifier and store them in a dictionary for easy access. Below defines a function named to_chunks() that takes a NumPy array of the loaded data and returns a dictionary of chunk_id to rows for the chunk. The complete example that loads the dataset and splits it into chunks is listed below. Running the example prints the number of chunks in the dataset. Now that we know how to load the data and split it into chunks, we can separate into train and test datasets. Each chunk covers an interval of eight days of hourly observations, although the number of actual observations within each chunk may vary widely. We can split each chunk into the first five days of observations for training and the last three for test. Each observation has a row called ¡®position_within_chunk¡® that varies from 1 to 192 (8 days * 24 hours). We can therefore take all rows with a value in this column that is less than or equal to 120 (5 * 24) as training data and any values more than 120 as test data. Further, any chunks that don¡¯t have any observations in the train or test split can be dropped as not viable. When working with the naive models, we are only interested in the target variables, and none of the input meteorological variables. Therefore, we can remove the input data and have the train and test data only comprised of the 39 target variables for each chunk, as well as the position within chunk and hour of observation. The split_train_test() function below implements this behavior; given a dictionary of chunks, it will split each into a list of train and test chunk data. We do not require the entire test dataset; instead, we only require the observations at specific lead times over the three day period, specifically the lead times: Where, each lead time is relative to the end of the training period. First, we can put these lead times into a function for easy reference: Next, we can reduce the test dataset down to just the data at the preferred lead times. We can do that by looking at the ¡®position_within_chunk¡® column and using the lead time as an offset from the end of the training dataset, e.g. 120 + 1, 120 +2, etc. If we find a matching row in the test set, it is saved, otherwise a row of NaN observations is generated. The function to_forecasts() below implements this and returns a NumPy array with one row for each forecast lead time for each chunk. We can tie all of this together and split the dataset into train and test sets and save the results to new files. The complete code example is listed below. Running the example first comments that chunk 69 is removed from the dataset for having insufficient data. We can then see that we have 42 columns in each of the train and test sets, one for the chunk id, position within chunk, hour of day, and the 39 training variables. We can also see the dramatically smaller version of the test dataset with rows only at the forecast lead times. The new train and test datasets are saved in the ¡®naive_train.csv¡® and ¡®naive_test.csv¡® files respectively. Once forecasts have been made, they need to be evaluated. It is helpful to have a simpler format when evaluating forecasts. For example, we will use the three-dimensional structure of [chunks][variables][time], where variable is the target variable number from 0 to 38 and time is the lead time index from 0 to 9. Models are expected to make predictions in this format. We can also restructure the test dataset to have this dataset for comparison. The prepare_test_forecasts() function below implements this. We will evaluate a model using the mean absolute error, or MAE. This is the metric that was used in the competition and is a sensible choice given the non-Gaussian distribution of the target variables. If a lead time contains no data in the test set (e.g. NaN), then no error will be calculated for that forecast. If the lead time does have data in the test set but no data in the forecast, then the full magnitude of the observation will be taken as error. Finally, if the test set has an observation and a forecast was made, then the absolute difference will be recorded as the error. The calculate_error() function implements these rules and returns the error for a given forecast. Errors are summed across all chunks and all lead times, then averaged. The overall MAE will be calculated, but we will also calculate a MAE for each forecast lead time. This can help with model selection generally as some models may perform differently at different lead times. The evaluate_forecasts() function below implements this, calculating the MAE and per-lead time MAE for the provided predictions and expected values in [chunk][variable][time] format. Once we have the evaluation of a model, we can present it. The summarize_error() function below first prints a one-line summary of a model¡¯s performance then creates a plot of MAE per forecast lead time. We are now ready to start exploring the performance of naive forecasting methods. In this section, we will explore naive forecast methods that use all data in the training dataset, not constrained to the chunk for which we are making a prediction. We will look at two approaches: The first step is to implement a general function for making a forecast for each chunk. The function takes the training dataset and the input columns (chunk id, position in chunk, and hour) for the test set and returns forecasts for all chunks with the expected 3D format of [chunk][variable][time]. The function enumerates the chunks in the forecast, then enumerates the 39 target columns, calling another new function named forecast_variable() in order to make a prediction for each lead time for a given target variable. The complete function is listed below. We can now implement a version of the forecast_variable() that calculates the mean for a given series and forecasts that mean for each lead time. First, we must collect all observations in the target column across all chunks, then calculate the average of the observations while also ignoring the NaN values. The nanmean() NumPy function will calculate the mean of an array and ignore NaN values. The forecast_variable() function below implements this behavior. We now have everything we need. The complete example of forecasting the global mean for each series across all forecast lead times is listed below. Running the example first prints the overall MAE of 0.634, followed by the MAE scores for each forecast lead time. A line plot is created showing the MAE scores for each forecast lead time from +1 hour to +72 hours. We cannot see any obvious relationship in forecast lead time to forecast error as we might expect with a more skillful model. MAE by Forecast Lead Time With Global Mean We can update the example to forecast the global median instead of the mean. The median may make more sense to use as a central tendency than the mean for this data given the non-Gaussian like distribution the data seems to show. NumPy provides the nanmedian() function that we can use in place of nanmean() in the forecast_variable() function. The complete updated example is listed below. Running the example shows a drop in MAE to about 0.59, suggesting that indeed using the median as the central tendency may be a better baseline strategy. A line plot of MAE per lead time is also created. MAE by Forecast Lead Time With Global Median We can update the naive model for calculating a central tendency by series to only include rows that have the same hour of day as the forecast lead time. For example, if the +1 lead time has the hour 6 (e.g. 0600 or 6AM), then we can find all other rows in the training dataset across all chunks for that hour and calculate the median value for a given target variable from those rows. We record the hour of day on the test dataset and make it available to the model when making forecasts. One wrinkle is that in some cases the test dataset did not have a record for a given lead time and one had to be invented with NaN values, including a NaN value for the hour. In these cases, no forecast is required so we will skip them and forecast a NaN value. The forecast_variable() function below implements this behavior, returning forecasts for each lead time for a given variable. It is not very efficient, and it might be a lot more efficient to pre-calculate the median values for each hour for each variable first and then forecast using a lookup table. Efficiency is not a concern at this point as we are looking for a baseline of model performance. The complete example of forecasting the global median value by hour of the day across is listed below. Running the example summarizes the performance of the model with a MAE of 0.567, which is an improvement over the global median for each series. A line plot of the MAE by forecast lead time is also created showing that +72 had the lowest overall forecast error. This is interesting, and may suggest that hour-based information may be useful in more sophisticated models. MAE by Forecast Lead Time With Global Median By Hour of Day It is possible that using information specific to the chunk may have more predictive power than using global information from the entire training dataset. We can explore this with three local or chunk-specific naive forecasting methods; they are: The last two of which are the chunk-specific version of the global strategies that were evaluated in the previous section. Forecasting the last non-NaN observation for a chunk is perhaps the simplest model, classically called the persistence model or the naive model. The forecast_variable() function below implements this forecast strategy. The complete example for evaluating the persistence forecast strategy on the test set is listed below. Running the example prints the overall MAE and the MAE per forecast lead time. We can see that the persistence forecast appears to out-perform all of the global strategies evaluated in the previous section. This adds some support that the reasonable assumption that chunk-specific information is important in modeling this problem. A line plot of MAE per forecast lead time is created. Importantly, this plot shows the expected behavior of increasing error with the increase in forecast lead time. Namely, the further one predicts into the future, the more challenging it is, and in turn, the more error one would be expected to make. MAE by Forecast Lead Time via Persistence Instead of persisting the last observation for the series, we can persist the average value for the series using only the data in the chunk. Specifically, we can calculate the median of the series, which as we found in the previous section seems to lead to better performance. The forecast_variable() implements this local strategy. The complete example is listed below. Running the example summarizes the performance of this naive strategy, showing a MAE of about 0.568, which is worse than the above persistence strategy. A line plot of MAE per forecast lead time is also created showing the familiar increasing curve of error per lead time. MAE by Forecast Lead Time via Local Median Finally, we can dial in the persistence strategy by using the average value per series for the specific hour of day at each forecast lead time. This approach was found to be effective at the global strategy. It may be effective using only the data from the chunk, although at the risk of using a much smaller data sample. The forecast_variable() function below implements this strategy, first finding all rows with the hour of the forecast lead time, then calculating the median of those rows for the given target variable. The complete example is listed below. Running the example prints the overall MAE of about 0.574, which is worse than the global variation of the same strategy. As suspected, this is likely due to the small sample size, that is at most five rows of training data contributing to each forecast. A line plot of MAE per forecast lead time is also created showing the familiar increasing curve of error per lead time. MAE by Forecast Lead Time via Local Median By Hour of Day We can summarize the performance of all of the naive forecast methods reviewed in this tutorial. The example below lists each method using a shorthand of ¡®g¡® and ¡®l¡® for global and local and ¡®h¡® for the hour-of-day variations. The example creates a bar chart so that we can compare the naive strategies based on their relative performance. Running the example creates a bar chart comparing the MAE for each of the six strategies. We can see that the persistence strategy was better than all of the other methods and that the second best strategy was the global median for each series that used the hour of day. Models evaluated on this train/test separation of the dataset must achieve an overall MAE lower than 0.520 in order to be considered skillful. Bar Chart with Summary of Naive Forecast Methods This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop naive forecasting methods for the multistep multivariate air pollution time series forecasting problem. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Exactly how does this:
     chunk_ids = unique(values[:, 1]) develop a list of unique identifiers?   When I see ¡®unique¡¯ I think set().  I am not following the above line of code. Yes, a set, in this case a list of unique values in column 1. Hi,
Thanks for a great tutorial. Just a small remark <U+2013> to me, it would be much easier/quicker to understand steps if use pandas dataframes.
All the best! Thanks. Comment  Name (required)  Email (will not be published) (required)  Website"
