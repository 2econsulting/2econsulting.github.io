"site","date","headline","url_address","text"
"vidhya",2018-09-14,"Heroes of Deep Learning: Top Takeaways for Aspiring Data Scientists from Andrew Ng¡¯s Interview Series","https://www.analyticsvidhya.com/blog/2018/09/heroes-deep-learning-top-takeaways-andrew-ng-interview-series/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 Andrew Ng is the most recognizable personality of the modern deep learning world. His machine learning course is cited as the starting point for anyone looking to understand the math behind algorithms. But even the great Andrew Ng looks up to and takes inspiration from other experts. In this amazing and in-depth video series, he has interviewed some of the most eminent personalities in the world of deep learning (eight heroes, to be precise). The interviews span the length and breadth of deep learning, including topics like backpropogation, GANs, transfer learning, etc. Even artificial intelligence crops up in between conversations. But don¡¯t worry if these terms sound overwhelming, we have listed down the key takeaways from each interview just for you. Source: Forbes The ¡°heroes¡± Andrew Ng has interviewed are: What a stellar cast of experts! Now it¡¯s time to dive in and look at the top takeaways from each video. Geoffrey Hinton is best known for his work on artificial neural networks (ANNs). His contributions in the field of deep learning are the main reason behind the success of the field and he is often called the ¡°Godfather of Deep Learning¡± (with good reason). His research on the backpropagation algorithm brought about a drastic change in the performance of deep learning models. Ian Goodfellow is a rockstar in the deep learning space and is currently working as a research scientist at Google Brain. He is best known for his invention of generative adversarial networks (GANs). His book on ¡°Deep Learning¡± covers a broad range of topics like mathematical and conceptual backgrounds and deep learning techniques used in the industry, which can be a good starting point for any deep learning enthusiast. We strongly recommend reading that book, it¡¯s free! Yoshua Bengio is a computer scientist, well known for his work on ANN and deep learning. He is the co-founder of Element AI, a Montreal-based business incubator that seeks to transform AI research into real-world business applications. Pieter Abbeel is the Director of the UC Berkeley Robot Learning Lab. His work in reinforcement learning is often cited by scholars as the best in the modern era. He has previously worked in a senior role at OpenAI. Yuanquing Lin is the Director at the Institute of Deep Learning at Baidu. He has a background in mathematics and physics, and holds a Ph.D in machine learning. A word of caution <U+2013> the English might be a little hard to understand in the video as it¡¯s not Mr.<U+00A0>Yuanquing¡¯s first language. Andrej Karpathy is the director of artificial intelligence and Autopilot Vision at Tesla. Like Pieter Abbeel, Andrej previously worked at OpenAI, but as a research scientist. He is a widely considered and cited as a leading expert in the field of computer vision, especially image recognition (though of course he¡¯s an expert in quite a lot of deep learning areas). This is one the most intriguing videos in the series! Ruslan Salakhutdinov is the director of AI Research at Apple and is known as the developer of Bayesian Program Learning. His areas of specialization are many, but are listed as probabilistic graphical models, large-scale optimization, and of course, deep learning. Here¡¯s a fun fact <U+2013> his doctoral adviser? None other than Geoffrey Hinton! Yann LeCun is the founding father of convolutional nets. He is currently the Chief AI Scientist and VP at Facebook. He is a professor, researcher, and R&D manager with academic and industry experience in AI, machine learning,<U+00A0>deep learning, computer vision, intelligent data analysis, data mining, data compression, digital library<U+00A0>systems, and robotics. And that¡¯s just scraping the surface of what this expert is capable of. This is easily the most fascinating interview series on YouTube concerning deep learning. There is SO MUCH to learn from each of these seven heroes. If you haven¡¯t seen these videos before, we¡¯re glad you stopped by because this will feel like hitting the jackpot. Andrew Ng is a wonderful interviewer and him conversing with other experts feels like a dream. Grab your pen and notebook because there¡¯s a whole host of things for you to learn."
"vidhya",2018-09-13,"How Machine Learning Algorithms & Hardware Power Apple¡¯s Latest Watch and iPhones","https://www.analyticsvidhya.com/blog/2018/09/how-machine-learning-hardware-and-algorithms-power-apples-latest-watch-and-iphones/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 This is a great time to be a data scientist <U+2013> all the top tech giants are integrating machine learning into their flagship products and the demand for such professionals is at an all-time high. And it¡¯s only going to get better! Apple has been a major advocate of machine learning, and has packed it¡¯s products with features like FaceID, Augmented Reality, Animoji, Healthcare sensors, etc. While watching Apple¡¯s keynote event yesterday, I couldn¡¯t help but wonder at the new chip technology they have developed that uses the power of machine learning algorithms. In this article, we¡¯ll check out some of the ways Apple has used machine learning to enrich the user experience. And believe me, some of the numbers you¡¯ll see will blow your mind. And if you¡¯re already itching to get started with building your first ML models on an iPhone using Apple¡¯s CoreML, check out this excellent article. Source: The Verge Designed in-house by Apple¡¯s developers, the A12 chip features an even more advanced neural engine than last year (when the neural engine made it¡¯s official debut inside the A11 chip). The A11 chip powers the iPhone X, 8, and 8 Plus so you can imagine why the A12 has created quite a stir in the machine learning community. The A12 is using features as small as 7 nanometers as compared to 10 in the A11, which explains the acceleration in speed. And did you really think Apple would let the event slide without mentioning battery life? The A12 chip has a smart compute system that automatically recognizes which tasks should run on the primary part of the chip, which ones should be sent to the GPU, and which ones should be delegated to the neural engine. Source: Apple Insider The neural engine¡¯s key functions are two-fold: This year¡¯s engine has eight cores which is how the chip can perform 5 trillion operations per second. Last year¡¯s version had two cores and could go up to 600 billion operations per second. It¡¯s a nice microcosm of how rapidly technology is evolving in front of our eyes. And the neural engine can do even more.. It will help iPhone users take better pictures (how much better can you get every year?!). When you press the shutter button, the neural network identifies the kind of scene in the lens, and makes a clear distinction between any object in the image and the background. So next time you take a photograph, just remember how quick the neural network must be, to do all this in a matter of milliseconds. You can learn all about object detection and computer vision algorithms in our ¡®Computer Vision using Deep Learning¡® course! It¡¯s a comprehensive offering and an invaluable addition to your machine learning skillset. The Apple Watch Series 4 feels like a health monitoring device more than at any point since it¡¯s debut four years back. Of course all the excitement is around the watch¡¯s design and how it¡¯s 35% bigger than last year¡¯s product. But let¡¯s step out of that limelight and look at one of the more intriguing features <U+2013> new health sensors. The Watch comes with an electrocardiogram (ECG) sensor. Why is this important, you ask? Well for starters, it¡¯s the first smartwatch to pack in this feature. But more importantly, the sensor measure not just your heart¡¯s rate, but also it¡¯s rhythm. This helps monitor any irregular rhythm and the Watch immediately alerts you in case of any impending danger. These sensors have been approved by the FDA and the American Heart Association. Further, these the Series 4 watches are integrated with an improved accelerometer and gyroscope. This will help the sensors in detecting if the wearer has fallen over. Once a person has fallen over and shown no sign of movement for 60 seconds, the device sends out an emergency call to up to five (pre-defined) emergency contacts simultaneously. I¡¯m sure you must have guessed by now what¡¯s behind all these updates? Yes, it¡¯s machine learning. Healthcare, as I mentioned in this article, is ripe for taking in machine learning terms. There are billions of data points at play, and combining ML with domain expertise is where the jackpot lies. I¡¯m glad to see companies like Apple utilizing it, albeit in their own products. The competition between the likes of Apple, Google, and others is heating up and artificial intelligence and machine learning could be the key to winning the battle. Hardware is critical here <U+2013> as it gets significant upgrades each year, more and more complex algorithms can be built in. Fascinated by all this and looking for a way to get started with data science? Try out our ¡®Introduction to Data Science¡® course today! We will help you take your first steps into this awesome new world. You couldn¡¯t have picked a better time to get into data science, honestly. A quick glance at Apple¡¯s official job postings shows more than 400 openings for machine learning related positions. The question then remains whether there are enough experienced people to fulfill that demand."
"vidhya",2018-09-13,"A Gentle Introduction to Handling a Non-Stationary Time Series in Python","https://www.analyticsvidhya.com/blog/2018/09/non-stationary-time-series-python/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 What do these applications have in common: predicting the electricity consumption of a household for the next three months, estimating traffic on roads at certain periods, and predicting the price at which a stock will trade on the New York Stock Exchange? They all fall under the concept of time series data! You cannot accurately predict any of these results without the ¡®time¡¯ component. And as more and more data is generated in the world around us, time series forecasting keeps becoming an ever more critical technique for a data scientist to master. But time series is a complex topic with multiple facets at play simultaneously. For starters, making the time series stationary is critical if we want the forecasting model to work well. Why? Because most of the data you collect will have non-stationary trends. And if the spikes are erratic how can you be sure the model will work properly? The focus of this article is on the methods for checking stationarity in time series data. This article assumes that the reader is familiar with time series, ARIMA, and the concept of stationarity. Below are some references to brush up on the basics: ¡®Stationarity¡¯ is one of the most important concepts you will come across when working with time series data. A stationary series is one in which the properties <U+2013> mean, variance and covariance, do not vary with time. Let us understand this using an intuitive example. Consider the three plots shown below: The three examples shown above represent non-stationary time series. Now look at a fourth plot: In this case, the mean, variance and covariance are constant with time. This is what a stationary time series looks like. Think about this for a second <U+2013> predicting future values using which of the above plots would be easier? The fourth plot, right? Most statistical models require the series to be stationary to make effective and precise predictions. So to summarize, a stationary time series is the one for which the properties (namely mean, variance and covariance) do not depend on time. In the next section we will cover various methods to check if the given series is stationary or not. In this and the next few sections, I will be introducing methods to check the stationarity of time series data and the techniques required to deal with any non-stationary series. I have also provided the python code for applying each technique.<U+00A0>You can download the dataset we¡¯ll be using from this link: AirPassengers. Before we go ahead and analyze our dataset, let¡¯s load and preprocess<U+00A0>the data first. Looks like we are good to go! The next step is to determine whether a given series is stationary or not and deal with it accordingly. This section looks at some common methods which we can use to perform this check. Consider the plots we used in the previous section. We were able to identify the series in which mean and variance were changing with time, simply by looking at each plot. Similarly, we can plot the data and determine if the properties of the series are changing with time or not. Although its very clear that we have a trend (varying mean) in the above series, this visual approach might not always give accurate results. It is better to confirm the observations using some statistical tests. Instead of going for the visual test, we can use statistical tests like the unit root stationary tests. Unit root indicates that the statistical properties of a given series are not constant with time, which is the condition for stationary time series. Here is the mathematics explanation of the same : Suppose we have a time series : yt = a*yt-1 + ¥å t where yt is the value at the time instant t and ¥å t is the error term. In order to calculate yt we need the value of yt-1, which is : yt-1 = a*yt-2 + ¥å t-1 If we do that for all observations, the value of yt will come out to be: yt = an*yt-n + ¥Ò¥åt-i*ai If the value of a is 1 (unit) in the above equation, then the predictions will be equal to the yt-n and sum of all errors from t-n to t, which means that the variance will increase with time. This is knows as unit root in a time series. We know that for a stationary time series, the variance must not be a function of time. The unit root tests check the presence of unit root in the series by checking if value of a=1. Below are the two of the most commonly used unit root stationary tests: The Dickey Fuller test is one of the most popular statistical tests. It can be used to determine the presence of unit root in the series, and hence help us understand if the series is stationary or not. The null and alternate hypothesis of this test are: Null Hypothesis: The series has a unit root (value of a =1) Alternate Hypothesis: The series has no unit root. If we fail to reject the null hypothesis, we can say that the series is non-stationary. This means that the series can be linear or difference stationary (we will understand more about difference stationary in the next section). Python code: Results of ADF test: The ADF tests gives the following results <U+2013> test statistic, p value and the critical value at 1%, 5% , and 10% confidence intervals. The results of our test for this particular series are: Test for stationarity: If the test statistic is less than the critical value, we can reject the null hypothesis (aka the series is stationary). When the test statistic is greater than the critical value, we fail to reject the null hypothesis (which means the series is not stationary). In our above example, the test statistic > critical value, which implies that the series is not stationary. This confirms our original observation which we initially saw in the visual test. KPSS is another test for checking the stationarity of a time series (slightly less popular than the Dickey Fuller test). The null and alternate hypothesis for the KPSS test are opposite that of the ADF test, which often creates confusion. The authors of the KPSS test have defined the null hypothesis as the process is trend stationary, to an alternate hypothesis of a unit root series. We will understand the trend stationarity in detail in the next section. For now, let¡¯s focus on the implementation and see the results of the KPSS test. Null Hypothesis: The process is trend stationary. Alternate Hypothesis: The series has a unit root (series is not stationary). Python code: Results of KPSS test: Following are the results of the KPSS test <U+2013> Test statistic, p-value, and the critical value at 1%, 2.5%, <U+00A0>5%, and 10% confidence intervals. For the air passengers dataset, here are the results: Test for stationarity: If the test statistic is greater than the critical value, we reject the null hypothesis (series is not stationary). If the test statistic is less than the critical value, if fail to reject the null hypothesis (series is stationary). For the air passenger data, the value of the test statistic is greater than the critical value at all confidence intervals, and hence we can say that the series is not stationary. I usually perform both the statistical tests before I prepare a model for my time series data. It once happened that both the tests showed contradictory results. One of the tests showed that the series is stationary while the other showed that the series is not! I got stuck at this part for hours, trying to figure out how is this possible. As it turns out, there are more than one type of stationarity. So in summary, the ADF test has an alternate hypothesis of linear or difference stationary, while the KPSS test identifies trend-stationarity in a series. Let us understand the different types of stationarities and how to interpret the results of the above tests. It¡¯s always better to apply both the tests, so that we are sure that the series is truly stationary. Let us look at the possible outcomes of applying these stationary tests. Now that we are familiar with the concept of stationarity and its different types, we can finally move on to actually making our series stationary. Always keep in mind that in order to use time series forecasting models, it is necessary to convert any non-stationary series to a stationary series first. In this method, we compute the difference of consecutive terms in the series. Differencing is typically performed to get rid of the varying mean. Mathematically, differencing can be written as: yt¡® = yt <U+2013> y(t-1) where yt is the value at a time t Applying differencing on our series and plotting the results: In seasonal differencing, instead of calculating the difference between consecutive values, we calculate the difference between an observation and a previous observation from the same season. For example, an observation taken on a Monday will be subtracted from an observation taken on the previous Monday. Mathematically it can be written as: yt¡® = yt <U+2013> y(t-n) Transformations are used to stabilize the non-constant variance of a series. Common transformation methods include power transform, square root, and log transform. Let¡¯s do a quick log transform and differencing on our air passenger dataset: As you can see, this plot is a significant improvement over the previous plots. You can use square root or power transformation on the series and see if they come up with better results. Feel free to share your findings in the comments section below! In this article we covered different methods that can be used to check the stationarity of a time series. But the buck doesn¡¯t stop here. The next step is to apply a forecasting model on the series we obtained. You can refer to the following article to build such a model: Beginner¡¯s Guide to Time Series Forecast. You can connect with me in the comments section below if you have any questions or feedback on this article. Great post. For differencing, you can also use the diff method on pandas Series and DataFrame objects."
"vidhya",2018-09-11,"Artificial Intelligence, Machine Learning and Big Data <U+2013> A Comprehensive Report","https://www.analyticsvidhya.com/blog/2018/09/artificial-intelligence-machine-learning-and-big-data-a-comprehensive-report/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 Artificial Intelligence and Machine Learning are the hottest jobs in the industry right now. 2018 has seen an even bigger leap in interest in these fields and it is expected to grow exponentially in the next five years! For instance, did you know that more than 50,000 positions related to Data and Analytics are currently vacant in India? We are excited to release a comprehensive report together with Great Learning on how AI, ML and Big Data are changing and evolving the world around us. Additionally, this report aims to provide an overview of the kind of career opportunities available in these fields right now, and the different roles we might see in the future. The aim behind creating this report is to provide our Data Science community with the context of changes happening at a macro level, and how they can best prepare for these upcoming changes. So, if you are already a Data Science professional or want to get into Data Science, we expect this report to be useful in providing you a context and preparing you for the future. <U+2013> Kunal Jain, Founder and CEO, Analytics Vidhya Wondering what¡¯s in the report and if you should download it? Check out what all is included below: There are a whole host of amazing statistics and insights in the report that will blow your mind. For example<U+00A0><U+2013> there are over 10 lakh registered companies in India. A survey by Gartner shows that around 75% of these companies are either already investing or are planning to invest in the field of Big Data. Let¡¯s go through some of the most intriguing patterns and insights from this report. An oft-asked question I¡¯ve seen is <U+2013> which industries have the most job opportunities? Here¡¯s your answer: Data science isn¡¯t confined to one narrow field. Its reach spans across domains and applications. The banking and finance sector is clearly the biggest market for data science professionals. 44% of all jobs were created in this domain in 2017. Yes, 44%! E-commerce and healthcare have also emerged as promising areas for data science professionals. Python or R? Ah, that question again. But now we have a concrete answer! The below graph, using the job postings from Indeed.com, shows a neat analysis of the data science skills in demand these days: The most number of jobs listed contain SQL as a requirement. But Python has truly become a universally popular tool and is eating up that ground with incredible speed. These languages are followed by Java, Hadoop (especially for data engineers, a very necessary role), and R. SAS is a bit further behind Tableau round-up our top 10. This gives you a really good idea of what the industry demand is in today¡¯s market. It might be time to buckle up and upskill your existing skillset! What¡¯s in store for all the aspiring data scientists, engineers, and analysts? The below chart depicts the expected number of jobs by 2020 in various industries: Data science jobs in healthcare are expected to soar. Agriculture, transportation and aviation are also expected to integrate a lot of data science tasks soon (the transformation is well under way). Cyber security, lagging a touch behind at the moment, should see significant investment. It¡¯s a field ripe for data science and we expect to see professionals moving in that direction in the next couple of years. There is a whole lot more in this report. We have an entire section dedicated to understanding the different roles in data science, their expected skills, etc. So what are you waiting for?<U+00A0>Get your hands on the report right now!"
"vidhya",2018-09-11,"Deep Learning Tutorial to Calculate the Screen Time of Actors in any Video (with Python codes)","https://www.analyticsvidhya.com/blog/2018/09/deep-learning-video-classification-python/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 When I started my deep learning journey, one of the first things I learned was image classification. It¡¯s such a fascinating part of the computer vision fraternity and I was completely immersed in it! But I have a curious mind and once I had a handle on image classification, I wondered if I could transfer that learning to videos. Was there a way to build a model that automatically identified specific people in a given video at a particular time interval? Turns out, there was and I¡¯m excited to share my approach with you! Source: Coastline Automation Now to give you some context on the problem we¡¯ll be solving, keep in mind that screen time is extremely important for an actor. It is directly related to the<U+00A0>money he/she gets. Just to give you a sense of this commission, did you know that Robert Downey Jr. Downey picked up $10 million for just 15 minutes of screen time in ¡°Spider-Man Homecoming¡±? Incredible. How cool would it be if we could take any video and calculate the screen time of any actor present in it? In this article, I will help you understand how to use deep learning on video data. To do this, we will be working with videos from the popular TOM and JERRY cartoon series. The aim is to calculate the screen time of both TOM and JERRY in any given video. Sounds interesting? Read on then! Note: This article assumes you have a prior knowledge of image classification using deep learning. If not, I recommend going through this article which will help you get a grasp of the basics of deep learning and image classification. Ever heard of a flip book? If you haven¡¯t, you¡¯re missing out! Check out the one below: (Source: giphy.com) We have a different image on each page of the book, and as we flip these pages, we get an animation of a shark dancing. You could even call it a kind of video. The visualization gets better the<U+00A0> faster we flip the pages. In other words, this visual is a collection of different images arranged in a particular order. Similarly, videos are nothing but a collection of a set of images. These images are called frames and can be combined to get the original video. So, a problem related to video data is not that different from an image classification or an object detection problem. There is just one extra step of extracting frames from the video. Remember, our challenge here is to calculate the screen time of both Tom and Jerry from a given video. Let me first summarize the steps we will follow in this article to crack this problem: Believe me, just following these steps will help you in solving many such video related problems in deep learning. Time to get our Python hats on now, and dig into this challenge. Let us start with importing all the necessary libraries. Go ahead and install the below libraries in case you haven¡¯t already: Now we will load the video and convert it into frames. You can download the video used for this example from this link. We will first capture the video from the given directory using the VideoCapture() function, and then we¡¯ll extract frames from the video and save them as an image using the imwrite() function. Let¡¯s code it: Done! Once this process is complete, ¡®Done!¡¯ will be printed on the screen as confirmation that the frames have been created. Let us try to visualize an image (frame). We will first read the image using the imread() function of matplotlib, and then plot it using the imshow() function. Getting excited, yet? This is the first frame from the video. We have extracted one frame for each second, from the entire duration of the video. Since the duration of the video is 4:58 minutes (298 seconds), we now have 298 images in total. Our task is to identify which image has TOM, and which image has JERRY. If our extracted images would have been similar to the ones present in the popular Imagenet dataset, this challenge could have been a breeze. How? We could simply have used models pre-trained on that Imagenet data and achieved a high accuracy score! But then where¡¯s the fun in that? We have cartoon images so it¡¯ll be very difficult (if not impossible) for any pre-trained model to identify TOM and JERRY in a given video. So how do we go about handling this? A possible solution is to manually give labels to a few of the images and train the model on them. Once the model has learned the patterns, we can use it to make predictions on a previously unseen set of images. Keep in mind that there could be frames when neither TOM nor JERRY are present. So, we will treat it as a multi-class classification problem. The classes which I have defined are: Don¡¯t worry, I have labelled all the images so you don¡¯t have to! Go ahead and download the mapping.csv file which contains each image name and their corresponding class (0 or 1 or 2). The mapping file contains two columns: Our next step is to read the images which we will do based on their names, aka, the Image_ID column. Tada! We now have the images with us. Remember, we need two things to train our model: Since there are three classes, we will one hot encode them using the to_categorical() function of keras.utils. We will be using a VGG16 pretrained model which takes an input image of shape (224 X 224 X 3). Since our images are in a different size, we need to reshape all of them. We will use the resize() function of skimage.transform to do this. All the images have been reshaped to 224 X 224 X 3. But before passing any input to the model, we must preprocess it as per the model¡¯s requirement. Otherwise, the model will not perform well enough. Use the preprocess_input() function of<U+00A0>keras.applications.vgg16 to perform this step. We also need a validation set to check the performance of the model on unseen images. We will make use of the train_test_split() function of the sklearn.model_selection module to randomly divide images into training and validation set. The next step is to build our model. As mentioned, we shall be using the VGG16 pretrained model for this task. Let us first import the required libraries to build the model: We will now load the VGG16 pretrained model and store it as base_model: We will make predictions using this model for X_train and X_valid, get the features, and then use those features to retrain the model. The shape of X_train and X_valid is (208, 7, 7, 512), (90, 7, 7, 512) respectively. In order to pass it to our neural network, we have to reshape it to 1-D. We will now preprocess the images and make them zero-centered which helps the model to converge faster. Finally, we will build our model. This step can be divided into 3 sub-steps: Let¡¯s check the summary of the model using the summary() function: We have a hidden layer with 1,024 neurons and an output layer with 3 neurons (since we have 3 classes to predict). Now we will compile our model: In the final step, we will fit the model and simultaneously also check its performance on the unseen images, i.e., validation images: We can see it is performing really well on the training as well as the validation images. We got an accuracy of around 85% on unseen images. And this is how we train a model on video data to get predictions for each frame. In the next section, we will try to calculate the screen time of TOM and JERRY in a new video. First, download the video we¡¯ll be using in this section from here. Once done, go ahead and load the video and extract frames from it. We will follow the same steps as we did above: Done! After extracting the frames from the new video, we will now load the test.csv file which contains the names of each extracted frame. Download the test.csv file and load it: Next, we will import the images for testing and then reshape them as per the requirements of the aforementioned pretrained model: We need to make changes to these images similar to the ones we did for the training images. We will preprocess the images, use the<U+00A0>base_model.predict() function to extract features from these images using the VGG16 pretrained model, reshape these images to 1-D form, and make them zero-centered: Since we have trained the model previously, we will make use of that model to make prediction for these images. Recall that Class ¡®1¡¯ represents the presence of JERRY, while Class ¡®2¡¯ represents the presence of TOM. We shall make use of the above predictions to calculate the screen time of both these legendary characters: And there you go! We have the total screen time of both TOM and JERRY in the given video. I tried and tested many things for this challenge <U+2013> some worked exceedingly well, while some ended up flat. In this section, I will elaborate a bit on some of the difficulties I faced, and then how I tackled them. After that, I have provided the entire code for the final model which gave me the best accuracy. First, I tried using the pretrained model without removing the top layer. The results were not satisfactory. The possible reason could be that these are the cartoon images and our pretrained model was trained on actual images and hence it was not able to classify these cartoon images. To tackle this problem, i retrained the pretrain model using few labelled images and the results were better from the previous results. Even after training on the labelled images, the accuracy was not satisfactory. The model was not able to perform well on the training images itself. So, i tried to increase the number of layers. Increasing the number of layers proved to be a good solution to increase the training accuracy but there was no sync between training and validation accuracy. The model was overfitting and its performance on the unseen data was not satisfactory. So I added a Dropout layer after every Dense layer and then there was good sync between training and validation accuracy. I noticed that the classes are imbalanced. TOM had more screen time so the predictions were dominated by it and most of the frames were predicted as TOM. To overcome this and make the classes balanced, i used compute_class_weight() function of sklearn.utils.class_weight module. It assigned higher weights to the classes with lower value counts as compared to the classes with higher value counts. I also used Model Checkpointing to save the best model, i.e. the model which produced lowest validation loss and then used that model to make the final predictions. I will summarize all the above mentioned steps and will give the final code now. The actual classes for the testing images can be found in testing.csv file. Done! Done! We got an accuracy of around 88% on the validation data and 64% on the test data using this model. One possible reason for getting a low accuracy on test data could be a lack of training data. As the model does not have much knowledge of cartoon images like TOM and JERRY, we must feed it more images during the training process. My advice would be to extract more frames from different TOM and JERRY videos, label them accordingly, and use them for training the model. Once the model has seen a plethora of images of these two characters, there¡¯s a good chance it will lead to a better classification result. Such models can help us in various fields: These are just a few examples where this technique can be used. You can come up with many more such applications on your own! Feel free to share your thoughts and feedback in the comments section below. Very interesting Hi Abdul, Glad you found it useful! Good job Pulkit!
I think it is a useful project too.
But the limit is the fact that we have generate each time images from a movie and label them. We have so to build a new model for each actor. Perhaps it is good to think now on automatic models, which are generalizable on any movie (autolabelled). Hi, As per my knowledge, I don¡¯t think there are pretrained models trained on the faces of actors (correct me if I am wrong). So, we need to give labels for training the model. As seen in this project, labeling only few images can produce good results. Will look forward and try to automate these labeling part. If you find some insights related to this, please share it here. It would be helpful to take this forward. I really enjoyed this project. Thanks for your work and sharing it! I am not sure, but it looks as if a fourth category of both Tom and Jerry being in a frame is overlooked? Hi,"
