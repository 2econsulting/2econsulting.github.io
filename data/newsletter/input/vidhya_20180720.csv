"site","date","headline","url_address","text"
"vidhya",2018-07-19,"FLASH SALE! Get Flat 55% off on our Popular ‘Introduction to Data Science’ Course!","https://www.analyticsvidhya.com/blog/2018/07/flash-sale-flat-55-off-introduction-to-data-science-course/","“Without data, you’re just another person with an opinion.” <U+2013> W. Edwards Deming The role of a data scientist is the most sought after in the industry these days. If you are one of those aspiring data scientists, but don’t know where to start, you’ve come to the right place! We are excited to announce an exclusive 48-hours flash sale, starting today at 12.48pm till 21st July 12.48pm, on our most popular and best selling-course <U+2013> The course will be available for Rs. 5,310 ($77.48) on our trainings platform for the entirety of this flash sale.<U+00A0>Apply your exclusive coupon code:<U+00A0>FLASH55<U+00A0>and enroll at a mind-boggling 55% discount (the list price for this course is Rs. 11,800). This is the ultimate unmissable opportunity to take your first step towards becoming a data scientist. This course covers the basics of Python, before diving into the world of statistics and probability and finally a detailed go-through of the various data science modeling techniques commonly used in the industry. The course has been curated and taught by experienced instructors from Analytics Vidhya and requires no prior knowledge of data science or Python. Below is a summary of the learning modules covered in this course: One of the most unique aspects of this course is the real-life projects we provide. We firmly believe in practicing what you learn, and our core belief is reflected in how we have structured the projects. Check out the projects below: Your client is a Financial Distribution company. Over the last 10 years, they have created an offline distribution channel across country. They sell Financial products to consumers by hiring agents in their network. These agents are freelancers and get commission when they make a product sale. The data scientists at BigMart have collected sales data for 1559 products across 10 stores in different cities for an entire year. Also, certain attributes of each product and store have been defined. You will build a predictive model to forecast the sales of each product at a particular store. You will analyse what kind of people were likely to survive in Titanic tragedy. You will apply machine learning tools to predict which passengers survived the tragedy. Check out a sample video from the course, which covers the difference between forecasting, predictive modeling and machine learning: Enroll NOW! Nice course"
"vidhya",2018-07-19,"Ultimate Guide: Building a Mask R-CNN Model for Detecting Car Damage (with Python codes)","https://www.analyticsvidhya.com/blog/2018/07/building-mask-r-cnn-model-detecting-damage-cars-python/","The applications of computer vision continue to amaze. From detecting objects in a video, to counting the number of people in a crowd, there is no challenge that computer vision seemingly cannot overcome. One of the more intriguing applications of computer vision is identifying pixels in a scene and using them for diverse and remarkably useful purposes. We will be taking up one such application in this article, and trying to understand how it works using Python! The aim of this post is to build a custom Mask R-CNN model that can detect the area of damage on a car (see the image example above). The rationale for such a model is that it can be used by insurance companies for faster processing of claims if users can upload pics and they can assess damage from them. This model can also be used by lenders if they are underwriting a car loan especially for a used car. Sponsored: Check out our Computer Vision using Deep Learning Course for creating Computer Vision applications Mask R-CNN is an instance segmentation model that allows us to identify pixel wise location for our class. “Instance segmentation” means segmenting individual objects within a scene, regardless of whether they are of the same type<U+200A><U+2014><U+200A>i.e, identifying individual cars, persons, etc. Check out the below GIF of a Mask-RCNN model trained on the<U+00A0>COCO<U+00A0>dataset. As you can see, we can identify pixel locations for cars, persons, fruits, etc. Mask R-CNN is different from classical object detection models like Faster R-CNN where, in addition to identifying the class and its bounding box location, it can also color pixels in the bounding box that correspond to that class. When do you think we would be need this additional detail? Some examples I can think of are: The easiest way to try a Mask R-CNN model built on COCO classes is to use the<U+00A0>Tensorflow Object Detection API. You can refer to this article (written by me) that has information on how to use the API and run the model on YouTube videos. Before we build a Mask R-CNN model, let’s first understand how it actually works. A good way to think about Mask R-CNN is that it is a combination of a Faster R-CNN that does object detection (class + bounding box) and FCN (Fully Convolutional Network) that does pixel wise boundary. See figure below: Mask R-CNN is conceptually simple: Faster R-CNN has two outputs for each candidate object, a class label and a bounding-box offset; to this we add a third branch that outputs the object mask<U+200A><U+2014><U+200A>which is a binary mask that indicates the pixels where the object is in the bounding box. But the additional mask output is distinct from the class and box outputs, requiring extraction of much finer spatial layout of an object. To do this Mask R-CNN uses the<U+00A0>Fully Convolution Network (FCN) described below. FCN is a popular algorithm for doing semantic segmentation. This model uses various blocks of convolution and max pool layers to first decompress an image to 1/32th of its original size. It then makes a class prediction at this level of granularity. Finally it uses up sampling and deconvolution layers to resize the image to its original dimensions. So, in short, we can say that Mask R-CNN combines the two networks<U+200A><U+2014><U+200A>Faster R-CNN and FCN in one mega architecture. The loss function for the model is the total loss in doing classification, generating bounding box and generating the mask. Mask RCNN has a couple of additional improvements that make it much more accurate than FCN. You can read more about them in their<U+00A0>paper. For building a custom Mask R-CNN, we will leverage the<U+00A0>Matterport Github repository. The latest TensorFlow Object Detection repository also provides the option to build Mask R-CNN. However I would only recommend this for the strong-hearted! The versions of TensorFlow, object detection, format for mask, etc. can demand debugging of errors. I was able to successfully train a Mask R-CNN using it. But I have seen many people struggle with all kinds of errors. So I now highly recommend the<U+00A0>Matterport Mask R-CNN repository to anyone venturing into this domain. For this exercise, I collected 66 images (50 train and 16 validation) of damaged cars from Google. Check out some examples below. A Mask R-CNN model requires the user to annotate the images and identify the region of damage. The annotation tool I used is the VGG Image Annotator<U+200A><U+2014><U+200A>v 1.0.6. You can use the html version available at this<U+00A0>link. Using this tool you can create a polygon mask as shown below: Once you have created all the annotations, you can download the annotation and save it in a json format. You can look at my images and annotations on my repository here. Now we start the interesting work of actually training the model! Start by cloning the ‘Matterport Mask R-CNN’ repository<U+2014><U+200A>https://github.com/matterport/Mask_RCNN. Next we will load our images and annotations. I have used the balloon.py file shared by Matterport and modified it to create a custom code that loads images and annotations and adds them to a CustomDataset class. Check out the entire code here. Follow the same code block and update it for any specifics for your class. Please note that this code only works for one class. Further, you can use this notebook<U+00A0>to visualize the mask on the given images. See an example of this below: To train the model, we use the COCO trained model as the checkpoint to perform transfer learning. You can download this model from the Matterport repository as well. To train the model, run the below code block: I am using a GPU and trained the model for 10 epochs in 20<U+2013>30 minutes. You can inspect the model weights using the notebook<U+200A><U+2014><U+200A>Inspect Custom Weights. Please link your last checkpoint in this notebook. This notebook can help you perform a sanity check if your weights and biases are properly distributed. See a sample output below: Use the notebook<U+00A0>inspect_custom_model<U+00A0>to run model on images from test/val set and see model predictions. See a sample result below: And there you have it! You just built a Mask R-CNN model to detect damage on a car. What an awesome way to learn deep learning. Mask-RCNN is the next evolution of object detection models which allow detection with better precision. A big thanks to Matterport for making their repository public and allowing us to leverage it to build custom models. This is just a small example of what we can accomplish with this wonderful model. If you have any questions, or feedback for me on this article, please share it using the comments section below. that’s a cool application Priya.
Thanks for sharing that, I’ll need some time to digest all of that. Helllo Priya,
The post is really cool. I am trying to just draw a bounding box over the damaged area and possibly try to crop it out from the image and also name the part of the car it is. Is this possible ? I am actually trying to build a system that can quantitatively tell out how much a car is damaged from the provided image and also the parts that are damaged. Please do suggest me a good way to do it. Possibly any reference links available too.
Thank you"
"vidhya",2018-07-19,"MyStory: Step by Step process of How I Became a Machine Learning Expert in 10 Months","https://www.analyticsvidhya.com/blog/2018/07/mystory-became-a-machine-learning-expert-10-months/","Not so long ago, using the pivot tables option in Excel was the upper limit of my skills with numbers and the word python was more likely to make me think about a dense jungle or a nature program on TV than a tool to generate business insights and create complex solutions. It took me ten months to leave that life behind and start feeling like I belonged to the exclusive world of people who can tell their medians from their means, their x-bars from the neighborhood pub, and who know how to teach machines what they need to learn. The transformation process was not easy and demanded hard work, lots of time, dedication and required plenty of help along the way. It also involved well over hundreds of hours of “studying” in different forms and an equal amount of time practicing and applying all that was being learnt. In short, it wasn’t easy to transform from being data dumb to a data nerd, but I managed to do so while going through a terribly busy work schedule as well as being a dad to a one-year old. The point of this article is to help you if you are looking to make a similar transformation but do not know where to start and how to proceed from one step to the next. If you are interested in finding out, read on to get an idea about the topics you need to cover and also develop an understanding of the level of expertise you need to build at each stage of the learning process.  There are plenty of great online and offline resources to help you master each of these steps, but very often, the trouble for the uninitiated can be in figuring out where to start and where to finish. I hope spending the next ten to fifteen minutes going through this article will help solve that problem for you.  And finally, before proceeding any further, I would like to point out that I had a lot of help in making this transformation. Right at the end of the article, I will reveal how I managed to squeeze in so much learning and work in a matter of ten months. But that’s for later. For now, I want to give you more details about the nine steps that I had to go through in my transformation process. Spend a couple of weeks enhancing your “general knowledge” about the field of data science and machine learning. You may already have ideas and some sort of understanding about what the field is, but if you want to become an expert, you need to understand the finer details to a point where you can explain it in simple terms to just about anyone. Suggested topics: <U+00A0><U+00A0> Exercise to show that you know: I have a confession to make. Even though I feel like a machine learning expert, I do not feel that I have any level of expertise in statistics. Which should be good news for people who struggle with concepts in statistics as much as I do, as it proves that you can be a data scientist without being a statistician. Having said that, you cannot ignore statistical concepts <U+2013> not in machine learning and data science!  So what you need to do is to understand certain concepts and know when they may be applied or used. If you can also completely understand the theory behind these concepts, give yourself a few good pats on your back.  Suggested topics: <U+00A0><U+00A0> Suggested exercise to mark completion of this step: Programming turned out to be easier to learn, more fun and more rewarding in terms of the things it made possible, than I had ever imagined. While mastering a programming language could be an eternal quest, at this stage, you need to get familiar with the process of learning a language and that is not too difficult. Both Python and R are very popular and mastering one can make it quite easy to learn the other. I started with R and have slowly started using Python for doing similar tasks as well.  Suggested topics: Know that you are set for the next step: In the first cricket test match ever played (see scorecard), Australian Charles Bannerman scored 67.35% (165 out of 245) of his team’s total score, in the very first innings of cricket’s history. This remains a record in cricket at the time of writing, for the highest share of the total score by a batsman in an innings of a test match. What makes the innings even more remarkable is that the other 43 innings in that test match had an average of only 10.8 runs an innings, with only about 40% of all batsmen registering a score of ten or more runs. In fact, the second highest score by an Australian in the match was 20 runs. Given that Australia won the match by 45 runs, we can say with conviction that Bannerman’s innings was the most important contributor to Australia’s win. Just like we were able to build this story from the scorecard of the test match, exploratory data analysis is about studying data to understand the story that is hidden beneath it, and then sharing the story with everyone. Personally, I find this phase of a data project the most interesting, which is a good thing as quite a lot of the time in a typical project could be expected to be taken up by exploratory data analysis.  Topics to cover: Project output:<U+00A0> Let’s say we had data for all the countries in the world across many parameters ranging from population, to income, to health, to major industries and more. Now suppose we wanted to find out which countries are similar to each other across all these parameters. How do we go about doing this, when we have to compare each country with all the others, across over 50 different parameters? That is where unsupervised machine learning algorithms come in. This is not the time to bore you with details about what these are all about, but the good news is that once you reach this stage, you have moved on into the world of machine learning and are already in elite company. Topics to cover: Milestone exercise: If you had data about millions of loan applicants and their repayment history from the past, could you identify an applicant who is likely to default on payments, even before the loan is approved? Given enough prior data, could you predict which users are more likely to respond to a digital advertising campaign? Could you identify if someone is more likely to develop a certain disease later in their life based on their current lifestyle and habits? Supervised learning algorithms help solve all these problems and a lot more. While there are a plethora of algorithms to understand and master, just getting started with some of the most popular ones will open up a world of new possibilities for you and the ways in which you can make data useful for an organization.  Topics to cover: You have not really started with creating models till you have done this: Many of the machine learning models in use today have been around for decades. The reason why these algorithms are only finding applications now, is that we finally have access to sufficiently large amounts of data, that can be supplied to these algorithms for them to be able to come up with useful outputs. Data engineering and architecture is a field of specialization in itself, but every machine learning expert must know how to deal with big data systems, irrespective of their specialization within the industry. Understanding how large amounts of data can be stored, accessed and processed efficiently is important to being able to create solutions that can be implemented in practice and are not just theoretical exercises.  I had approached this step with a real lack of conviction, but as I soon found out, it was driven more by the fear of the unknown in the form of Linux interfaces than any real complexity in finding my way around a Hadoop system. <U+00A0><U+00A0><U+00A0> Topics to cover: Do this to know that you have understood the basics: Deep learning models are helping companies like Apple and Google create solutions like Siri or the Google Assistant. They are helping global giants test driverless cars and suggesting best courses of treatment to doctors. Machines are able to see, listen, read, write and speak thanks to deep learning models that are going to transform the world in many ways, including significantly changing the skills required for people to be useful to organizations. Getting started with creating a model that can tell the image of a flower from a fruit may not immediately help you start building your own driverless car, but it will certainly help you start seeing the path to getting there. Topics to cover: Milestone exercise: By now you are almost ready to unleash yourself to the world as a machine learning pro, but you need to showcase all that you have learnt before anyone else will be willing to agree with you. <U+00A0> The internet presents glorious opportunities to find such projects. If you have been diligent about the previous eight steps, chances are that you would already know how to find a project that will excite you, be useful to someone, as well as help demonstrate your knowledge and skills. Topics to cover: Milestone exercise: Machine learning and artificial intelligence is a set of skills for the present and future. It is also a field where learning will never cease and very often you may have to keep running to stay in the same place, as far as being equipped with the most in-demand skills is concerned. However, if you start the journey well, you will be able to understand how to go about taking the next step in your learning path. As you must have gathered by now, starting the journey well is a pretty challenging exercise in itself. If you choose to start upon it, I hope this article will have been of some help to you and I wish you the very best. Finally, I will confess that I got a lot of help with my ten-month transition. The reason I was able to cover so much ground in this amount of time, along with a busy schedule at work and home, was that I enrolled for the Post Graduate Program in Data Science and Machine Learning offered by Jigsaw Academy and Graham School, University of Chicago. Investing in the course helped in keeping my learning hours focused, created external pressure that ensured that I was finding time for it irrespective of whatever else was going on in life, and gave me access to experts in the form of faculty and a great peer group through other students.  Transforming from being non-technical to someone who is comfortable with the machine learning world has already opened up many new doors for me. Whatever path you choose to make this transformation, you can do so with the assurance that going through the rigor will reap rewards for a long time and will banish any fears of becoming irrelevant in tomorrow’s economy. Madhukar Jha, Founder <U+2013> Blue Footed Ideas Madhukar Jha<U+00A0>believes that great digital experiences are created by concocting a perfect mix of data driven insights, understanding of behavioural drivers, a design thinking approach, and cutting edge technology. He applies this philosophy to help businesses make world class products, run campaigns that rock and tell compelling stories. These r future ready courses , having strength to face the challenges from ever growing technological world I want to prepare myself for the same. Awe inspiring & excellent article Madhukar Jha! My 2 cents:  1. Interchanging steps 5 & 6 ( I found it easier to begin with Supervised & migrate to unsupervised)
2. To supplement topics to cover you may have included some tips around “Learning Materials” You have done an awesome job by posting this which could motivate many to take up this profession seriously. Kudos & keep inspiring!"
"vidhya",2018-07-16,"An Introductory Guide to Maximum Likelihood Estimation (with a case study in R)","https://www.analyticsvidhya.com/blog/2018/07/introductory-guide-maximum-likelihood-estimation-case-study-r/","Interpreting how a model works is one of the most basic yet critical aspects of data science. You build a model which is giving you pretty impressive results, but what was the process behind it? As a data scientist, you need to have an answer to this oft-asked question. For example, let’s say you built a model to predict the stock price of a company. You observed that the stock price increased rapidly over night. There could be multiple reasons behind it. Finding the likelihood of the most probable reason is what Maximum Likelihood Estimation is all about. This concept is used in economics, MRIs, satellite imaging, among other things. Source: YouTube In this post we will look into how Maximum Likelihood Estimation (referred as MLE hereafter) works and how it can be used to determine coefficients of a model with any kind of distribution. Understanding MLE would involve probability and mathematics, but I will try to make it easier with examples. Note: As mentioned, this article assumes that you know the basics of maths and probability. You can refresh your concepts by going through this article first <U+2013><U+00A0>6 Common Probability Distributions every data science professional should know. Let us say we want to predict the sale of tickets for an event. The data has the following histogram and density. How would you model such a variable? The variable is not normally distributed and is asymmetric and hence it violates the assumptions of linear regression. A popular way is to transform the variable with log, sqrt, reciprocal, etc. so that the transformed variable is normally distributed and can be modelled with linear regression. <U+00A0><U+00A0><U+00A0> Let’s try these transformations and see how the results are: With Log transformation:  With Square Root Transformation: With Reciprocal:  None of these are close to a normal distribution. How should we model such data so that the basic assumptions of the model are not violated? How about modelling this data with a different distribution rather than a normal one? If we do use a different distribution, how will we estimate the coefficients?  This is where Maximum Likelihood Estimation (MLE) has such a major advantage. While studying stats and probability, you must have come across problems like <U+2013> What is the probability of x > 100, given that x follows a normal distribution with mean 50 and standard deviation (sd) 10. In such problems, we already know the distribution (normal in this case) and its parameters (mean and sd) but in real life problems these quantities are unknown and must be estimated from the data. MLE is the technique which helps us in determining the parameters of the distribution that best describe the given data. Let’s understand this with an example: Suppose we have data points representing the weight (in kgs) of students in a class. The data points are shown in the figure below (the R code that was used to generate the image is provided as well): Figure 1 This appears to follow a normal distribution. But how do we get the mean and standard deviation (sd) for this distribution? One way is to directly compute the mean and sd of the given data, which comes out to be 49.8 Kg and 11.37 respectively. These values are a good representation of the given data but may not best describe the population. We can use MLE in order to get more robust parameter estimates. Thus, MLE can be defined as a method for estimating population parameters (such as the mean and variance for Normal, rate (lambda) for Poisson, etc.) from sample data such that the probability (likelihood) of obtaining the observed data is maximized.  In order to get an intuition of MLE, try to guess which of the following would maximize the probability of observing the data in the above figure? Clearly, it is not very likely we’ll observe the above data shape if the population mean is 100. Now that you got an intuition of what MLE can do, we can get into the details of what actually likelihood is and how it can be maximized. But first, let’s start with a quick review of distribution parameters. Let us first understand distribution parameters. Wikipedia’s definition of this term is as follows: “It is a quantity that indexes a family of probability distributions”. It can be regarded as a numerical characteristic of a population or a statistical model. We can understand it by the following diagram: Figure 2, Source: Wikipedia The width and height of the bell curve is governed by two parameters <U+2013> mean and variance. These are known as distribution parameters for normal distribution. Similarly, Poisson distribution is governed by one parameter <U+2013> lambda, which is the<U+00A0>number of times an event occurs in an interval of time or space. Figure 3, Source: Wikipedia Most of the distributions have one or two parameters, but some distributions can have up to 4 parameters, like a 4 parameter beta distribution.  From Fig. 2 and 3 we can see that given a set of distribution parameters, some data values are more probable than other data. From Fig. 1, we have seen that the given data is more likely to occur when the<U+00A0>mean is 50, rather than<U+00A0>100. In reality however, we have already observed the data. Accordingly, we are faced with an inverse problem: Given the observed data and a model of interest, we need to find the one Probability Density Function/Probability Mass Function (f(x|θ)), among all the probability densities that are most likely to have produced the data. To solve this inverse problem, we define the likelihood function by reversing the roles of the data vector x and the (distribution) parameter vector θ in f(x| θ),<U+00A0>i.e., L(θ;x) = f(x| θ) In MLE, we can assume that we have a likelihood function L(θ;x), where θ<U+00A0>is the distribution parameter vector and x is the set of observations. We are interested in finding the value of θ that maximizes the likelihood with given observations (values of x).  The mathematical problem at hand becomes simpler if we assume that the observations (xi) are independent and identically distributed random variables drawn from a Probability Distribution, f0 (where f0<U+00A0>= Normal Distribution for example in Fig.1). This reduces the Likelihood function to:<U+00A0><U+00A0> To find the maxima/minima of this function, we can take the derivative of this function w.r.t θ<U+00A0>and equate it to 0 (as zero slope indicates maxima or minima). Since we have terms in product here, we need to apply the chain rule which is quite cumbersome with products. A clever trick would be to take log of the likelihood function and maximize the same. This will convert the product to sum and since log is a strictly increasing function, it would not impact the resulting value of θ. So we have: To find the maxima of the log likelihood function LL(θ; x), we can: There are many situations where calculus is of no direct help in maximizing a likelihood, but a maximum can still be readily identified. There’s nothing that gives setting the first derivative equal to zero any kind of ‘primacy’ or special place in finding the parameter value(s) that maximize log-likelihood. It’s simply a convenient tool when a few parameters need to be estimated.  As a general principle, pretty much any valid approach for identifying the argmax of a function may be suitable to find maxima of the log likelihood function. This is an unconstrained non-linear optimization problem. We seek an optimization algorithm that behaves in the following manner: It’s very common to use optimization techniques to maximize likelihood; there are a large variety of methods (Newton’s method, Fisher scoring, various conjugate gradient-based approaches, steepest descent, Nelder-Mead type (simplex) approaches, BFGS and a wide variety of other techniques).  It turns out that when the model is assumed to be Gaussian as in the examples above, the MLE estimates are equivalent to the ordinary least squares method. You can refer to the proof here. Let us now look at how MLE can be used to determine the coefficients of a predictive model.<U+00A0><U+00A0> Suppose that we have a sample of n observations y1, y2, . . . , yn which can be treated as realizations of independent Poisson random variables, with Yi ∼ P(μi). Also, suppose that we want to let the mean μi (and therefore the variance!) depend on a vector of explanatory variables xi . We could form a simple linear model as follows <U+2013>  where<U+00A0>θ<U+00A0>is the vector of model coefficients. This model has the disadvantage that the linear predictor on the right-hand side can assume any real value, whereas the Poisson mean on the left-hand side, which represents an expected count, has to be non-negative. A straightforward solution to this problem is to model the logarithm of the mean using a linear model. Thus, we consider a generalized linear model with log link log, which can be written as follows <U+2013>  Our aim is to find θ by using MLE.  Now, Poisson distribution is given by: We can apply the log likelihood concept that we learnt in the previous section to find the θ. Taking logs of the above equation and ignoring a constant involving log(y!), we find that the log-likelihood function is <U+2013> where μi depends on the covariates xi and a vector of<U+00A0>θ coefficients.<U+00A0>We can substitute μi = exp(xi’θ) and solve the equation to get<U+00A0>θ<U+00A0>that maximizes the likelihood.<U+00A0>Once we have the<U+00A0>θ<U+00A0>vector, we can then predict the expected value of the mean by multiplying the xi and θ vector. In this section, we will use a real-life dataset to solve a problem using the concepts learnt earlier.<U+00A0>You can download the dataset from this link.<U+00A0><U+00A0>A sample from the dataset is as follows:  <U+00A0><U+00A0><U+00A0>Datetime  Count of tickets sold 25-08-2012 00:00   <U+00A0><U+00A0><U+00A0> 8 25-08-2012 01:00 <U+00A0><U+00A0><U+00A0><U+00A0> 2 25-08-2012 02:00 <U+00A0><U+00A0><U+00A0><U+00A0> 6 25-08-2012 03:00 <U+00A0><U+00A0><U+00A0><U+00A0> 2 25-08-2012 04:00 <U+00A0><U+00A0><U+00A0><U+00A0> 2 25-08-2012 05:00 <U+00A0><U+00A0><U+00A0><U+00A0> 2 It has the count of tickets sold in each hour from 25th Aug 2012 to 25th Sep 2014<U+00A0> (about 18K records). Our aim is to predict the number of tickets sold in each hour. This is the same dataset which was discussed in the first section of this article.  The problem can be solved using techniques like regression, time series, etc. Here we will use the statistical modeling technique that we have learnt above using R. Let’s first analyze the data. In statistical modelling, we are concerned more with how the target variable is distributed. Let’s have a look at the distribution of counts: <U+00A0> This could be treated as a Poisson distribution or we could even try fitting an exponential distribution.  Since the variable at hand is count of tickets, Poisson is a more suitable model for this. Exponential distribution is generally used to model time interval between events. Let’s plot the count of tickets sold over these 2 years: Looks like there is a significant increase in sale of tickets over time.<U+00A0>In order to keep things simple, let’s model the outcome by only using age as a factor, where age is the defined no. of weeks elapsed since 25th Aug 2012. We can write this as: where, μ (Count of tickets sold) is assumed to follow the mean of Poisson distribution and θ0<U+00A0>and θ1 are the coefficients that we need to estimate.  Combining Eq. 1 and 2, we get the log likelihood function as follows: We can use the mle() function in R stats4 package to estimate the coefficients θ0<U+00A0>and θ1. It needs the following primary parameters:  For our example, the negative log likelihood function can be coded as follows: I have divided the data into train and test set so that we can objectively evaluate the performance of the model. idx is the indices of the rows which are in test set. Next let’s call the mle function to get the parameters: This gives us the estimate of the coefficients. Let’s use RMSE as the evaluation metric for getting results on the test set: Now let’s see how our model fairs against the standard linear model (with errors normally distributed), modelled with log of count.  As you can see, RMSE for the standard linear model is higher than our model with Poisson distribution.<U+00A0>Let’s compare the residual plots for these 2 models on a held out sample to see how the models perform in different regions: We see that the errors using Poisson regression are much closer to zero when compared to Normal linear regression.  Similar thing can be achieved in Python by using the<U+00A0>scipy.optimize.minimize() function which accepts objective function to minimize, initial guess for the parameters and methods like BFGS, L-BFGS, etc. Its further simpler to model popular distributions in R using the glm function from the<U+00A0>stats package. It supports Poisson, Gamma, Binomial, Quasi, Inverse Gaussian, Quasi Binomial, Quasi Poisson distributions out of the box. For the example shown above, you can get the coefficients directly using the below command: Same can be done in Python using pymc.glm() and setting the family as pm.glm.families.Poisson().  One way to think of the above example is that there exist better coefficients in the parameter space than those estimated by a standard linear model. Normal distribution is the default and most widely used form of distribution, but we can obtain better results if the correct distribution is used instead. Maximum likelihood estimation is a technique which can be used to estimate the distribution parameters irrespective of the distribution used. So next time you have a modelling problem at hand, first look at the distribution of data and see if something other than normal makes more sense! The detailed code and data is present on my Github repository.<U+00A0>Refer to the “Modelling single variables.R” file for an example that covers data reading, formatting and modelling using only age variables. I have also modelled using multiple variables, which is present in the<U+00A0>“Modelling multiple variables.R” file."
