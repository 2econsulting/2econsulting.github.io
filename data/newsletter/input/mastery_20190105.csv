"site","date","headline","url_address","text"
"mastery",2019-01-04,"How to Develop a Snapshot Ensemble Deep Learning Neural Network in Python With Keras","https://machinelearningmastery.com/snapshot-ensemble-deep-learning-neural-network/","Model ensembles can achieve lower generalization error than single models but are challenging to develop with deep learning neural networks given the computational cost of training each single model. An alternative is to train multiple model snapshots during a single training run and combine their predictions to make an ensemble prediction. A limitation of this approach is that the saved models will be similar, resulting in similar predictions and predictions errors and not offering much benefit from combining their predictions. Effective ensembles require a diverse set of skillful ensemble members that have differing distributions of prediction errors. One approach to promoting a diversity of models saved during a single training run is to use an aggressive learning rate schedule that forces large changes in the model weights and, in turn, the nature of the model saved at each snapshot. In this tutorial, you will discover how to develop snapshot ensembles of models saved using an aggressive learning rate schedule over a single training run. After completing this tutorial, you will know: Let¡¯s get started. How to Develop a Snapshot Ensemble Deep Learning Neural Network in Python With KerasPhoto by Jason Jacobs, some rights reserved. This tutorial is divided into five parts; they are: A problem with ensemble learning with deep learning methods is the large computational cost of training multiple models. This is because of the use of very deep models and very large datasets that can result in model training times that may extend to days, weeks, or even months. Despite its obvious advantages, the use of ensembling for deep networks is not nearly as widespread as it is for other algorithms. One likely reason for this lack of adaptation may be the cost of learning multiple neural networks. Training deep networks can last for weeks, even on high performance hardware with GPU acceleration. <U+2014> Snapshot Ensembles: Train 1, get M for free, 2017. One approach to ensemble learning for deep learning neural networks is to collect multiple models from a single training run. This addresses the computational cost of training multiple deep learning models as models can be selected and saved during training, then used to make an ensemble prediction. A key benefit of ensemble learning is in improved performance compared to the predictions from single models. This can be achieved through the selection of members that have good skill, but in different ways, providing a diverse set of predictions to be combined. A limitation of collecting multiple models during a single training run is that the models may be good, but too similar. This can be addressed by changing the learning algorithm for the deep neural network to force the exploration of different network weights during a single training run that will result, in turn, with models that have differing performance. One way that this can be achieved is by aggressively changing the learning rate used during training. An approach to systematically and aggressively changing the learning rate during training to result in very different network weights is referred to as ¡°Stochastic Gradient Descent with Warm Restarts¡± or SGDR for short, described by Ilya Loshchilov and Frank Hutter in their 2017 paper ¡°SGDR: Stochastic Gradient Descent with Warm Restarts.¡± Their approach involves systematically changing the learning rate over training epochs, called cosine annealing. This approach requires the specification of two hyperparameters: the initial learning rate and the total number of training epochs. The ¡°cosine annealing¡± method has the effect of starting with a large learning rate that is relatively rapidly decreased to a minimum value before being dramatically increased again. The model weights are subjected to the dramatic changes during training, having the effect of using ¡°good weights¡± as the starting point for the subsequent learning rate cycle, but allowing the learning algorithm to converge to a different solution. The resetting of the learning rate acts like a simulated restart of the learning process and the re-use of good weights as the starting point of the restart is referred to as a ¡°warm restart,¡± in contrast to a ¡°cold restart¡± where a new set of small random numbers may be used as a starting point. The ¡°good weights¡± at the bottom of each cycle can be saved to file, providing a snapshot of the model. These snapshots can be collected together at the end of the run and used in a model averaging ensemble. The saving and use of these models during an aggressive learning rate schedule is referred to as a ¡°Snapshot Ensemble¡± and was described by Gao Huang, et al. in their 2017 paper titled ¡°Snapshot Ensembles: Train 1, get M for free¡± and subsequently also used in an updated version of the Loshchilov and Hutter paper. ¡¦ we let SGD converge M times to local minima along its optimization path. Each time the model converges, we save the weights and add the corresponding network to our ensemble. We then restart the optimization with a large learning rate to escape the current local minimum. <U+2014> Snapshot Ensembles: Train 1, get M for free, 2017. The ensemble of models is created during the course of training a single model, therefore, the authors claim that the ensemble forecast is provided at no additional cost. [the approach allows] learning an ensemble of multiple neural networks without incurring any additional training costs. <U+2014> Snapshot Ensembles: Train 1, get M for free, 2017. Although a cosine annealing schedule is used for the learning rate, other aggressive learning rate schedules could be used, such as the simpler cyclical learning rate schedule described by Leslie Smith in the 2017 paper titled ¡°Cyclical Learning Rates for Training Neural Networks.¡± Now that we are familiar with the snapshot ensemble technique, we can look at how to implement it in Python with Keras. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course We will use a small multi-class classification problem as the basis to demonstrate the snapshot ensemble. The scikit-learn class provides the make_blobs() function that can be used to create a multi-class classification problem with the prescribed number of samples, input variables, classes, and variance of samples within a class. The problem has two input variables (to represent the x and y coordinates of the points) and a standard deviation of 2.0 for points within each group. We will use the same random state (seed for the pseudorandom number generator) to ensure that we always get the same data points. The result is the input and output elements of a dataset that we can model. In order to get a feeling for the complexity of the problem, we can plot each point on a two-dimensional scatter plot and color each point by class value. The complete example is listed below. Running the example creates a scatter plot of the entire dataset. We can see that the standard deviation of 2.0 means that the classes are not linearly separable (separable by a line) causing many ambiguous points. This is desirable as it means that the problem is non-trivial and will allow a neural network model to find many different ¡°good enough¡± candidate solutions resulting in a high variance. Scatter Plot of Blobs Dataset With Three Classes and Points Colored by Class Value Before we define a model, we need to contrive a problem that is appropriate for the ensemble. In our problem, the training dataset is relatively small. Specifically, there is a 10:1 ratio of examples in the training dataset to the holdout dataset. This mimics a situation where we may have a vast number of unlabeled examples and a small number of labeled examples with which to train a model. We will create 1,100 data points from the blobs problem. The model will be trained on the first 100 points and the remaining 1,000 will be held back in a test dataset, unavailable to the model. The problem is a multi-class classification problem, and we will model it using a softmax activation function on the output layer. This means that the model will predict a vector with three elements with the probability that the sample belongs to each of the three classes. Therefore, we must one hot encode the class values before we split the rows into the train and test datasets. We can do this using the Keras to_categorical() function. Next, we can define and compile the model. The model will expect samples with two input variables. The model then has a single hidden layer with 25 nodes and a rectified linear activation function, then an output layer with three nodes to predict the probability of each of the three classes and a softmax activation function. Because the problem is multi-class, we will use the categorical cross entropy loss function to optimize the model and stochastic gradient descent with a small learning rate and momentum. The model is fit for 200 training epochs and we will evaluate the model each epoch on the test set, using the test set as a validation set. At the end of the run, we will evaluate the performance of the model on the train and test sets. Then finally, we will plot learning curves of the model accuracy over each training epoch on both the training and validation datasets. Tying all of this together, the complete example is listed below. Running the example prints the performance of the final model on the train and test datasets. Your specific results will vary (by design!) given the high variance nature of the model. In this case, we can see that the model achieved about 84% accuracy on the training dataset, which we know is optimistic, and about 79% on the test dataset, which we would expect to be more realistic. A line plot is also created showing the learning curves for the model accuracy on the train and test sets over each training epoch. We can see that training accuracy is more optimistic over most of the run as we also noted with the final scores. Line Plot Learning Curves of Model Accuracy on Train and Test Dataset over Each Training Epoch Next, we can look at how to implement an aggressive learning rate schedule. An effective snapshot ensemble requires training a neural network with an aggressive learning rate schedule. The cosine annealing schedule is an example of an aggressive learning rate schedule where learning rate starts high and is dropped relatively rapidly to a minimum value near zero before being increased again to the maximum. We can implement the schedule as described in the 2017 paper ¡°Snapshot Ensembles: Train 1, get M for free.¡± The equation requires the total training epochs, maximum learning rate, and number of cycles as arguments as well as the current epoch number. The function then returns the learning rate for the given epoch. Equation for the Cosine Annealing Learning Rate ScheduleWhere a(t) is the learning rate at epoch t, a0 is the maximum learning rate, T is the total epochs, M is the number of cycles, mod is the modulo operation, and square brackets indicate a floor operation.Taken from ¡°Snapshot Ensembles: Train 1, get M for free¡±. The function cosine_annealing() below implements the equation. We can test this implementation by plotting the learning rate over 100 epochs with five cycles (e.g. 20 epochs long) and a maximum learning rate of 0.01. The complete example is listed below. Running the example creates a line plot of the learning rate schedule over 100 epochs. We can see that the learning rate starts at the maximum value at epoch 0 and decreases rapidly to epoch 19, before being reset at epoch 20, the start of the next cycle. The cycle is repeated five times as specified in the argument. Line Plot of Cosine Annealing Learning Rate Schedule We can implement this schedule as a custom callback in Keras. This allows the parameters of the schedule to be specified and for the learning rate to be logged so we can ensure it had the desired effect. A custom callback can be defined as a Python class that extends the Keras Callback class. In the class constructor, we can take the required configuration as arguments and save them for use, specifically the total number of training epochs, the number of cycles for the learning rate schedule, and the maximum learning rate. We can use our cosine_annealing() defined above to calculate the learning rate for a given training epoch. The Callback class allows an on_epoch_begin() function to be overridden that will be called prior to each training epoch. We can override this function to calculate the learning rate for the current epoch and set it in the optimizer. We can also keep track of the learning rate in an internal list. The complete custom callback is defined below. We can create an instance of the callback and set the arguments. We will train the model for 400 epochs and set the number of cycles to be 50 epochs long, or 500 / 50, a suggestion made and configuration used throughout the snapshot ensembles paper. We lower the learning rate at a very fast pace, encouraging the model to converge towards its first local minimum after as few as 50 epochs. <U+2014> Snapshot Ensembles: Train 1, get M for free, 2017. The paper also suggests that the learning rate can be set each sample or each mini-batch instead of prior to each epoch to give more nuance to the updates, but we will leave this as a future exercise. ¡¦ we update the learning rate at each iteration rather than at every epoch. This improves the convergence of short cycles, even when a large initial learning rate is used. <U+2014> Snapshot Ensembles: Train 1, get M for free, 2017. Once the callback is instantiated and configured, we can specify it as part of the list of callbacks to the call to the fit() function to train the model. At the end of the run, we can confirm that the learning rate schedule was performed by plotting the contents of the lrates list. Tying these elements together, the complete example of training an MLP on the blobs problem with a cosine annealing learning rate schedule is listed below. Running the example first reports the accuracy of the model on the training and test sets. Your results will vary given the stochastic nature of the training algorithm. In this case, we do not see much difference in the performance of the final model as compared to the previous section. A line plot of the learning rate schedule is created, showing eight cycles of 50 epochs each. Cosine Annealing Learning Rate Schedule While Fitting an MLP on the Blobs Problem Finally, a line plot of model accuracy on the train and test sets is created over each training epoch. We can see that although the learning rate was changed dramatically, there was not a dramatic effect on model accuracy, likely because the chosen classification problem is not very difficult. Line Plot of Train and Test Set Accuracy on the Blobs Dataset With a Cosine Annealing Learning Rate Schedule Now that we know how to implement the cosine annealing learning schedule, we can use it to prepare a snapshot ensemble. We can develop a snapshot ensemble in two parts. The first part involves creating a custom callback to save the model at the bottom of each learning rate schedule. The second part involves loading the saved models and using them to make an ensemble prediction. The CosineAnnealingLearningRateSchedule can be updated to override the on_epoch_end() function called at the end of each training epoch. In this function, we can check if the current epoch that just ended was the end of a cycle. If so, we can save the model to file. Below is the updated callback, named the SnapshotEnsemble class. A debug message is printed each time a model is saved as confirmation that models are being saved at the right time. For example, with 50-epoch long cycles, we would expect a model to be saved on epoch 49, 99, etc. and the learning rate reset at epoch 50, 100, etc. We will train the model for 500 epochs, to give 10 models to choose from later when making an ensemble prediction. The complete example of using this new snapshot ensemble to save models to file is listed below. Running the example reports that 10 models were saved for the 10-ends of the cosine annealing learning rate schedule. Once the snapshot models have been saved to file, they can be loaded and used to make an ensemble prediction. The first step is to load the models into memory. For large models, this could be done one model at a time, make a prediction, and move on to the next model before combining predictions. In this case, the models are relatively small and we can load all 10 from file as a list. We would expect that models saved towards the end of the run may have better performance than models saved earlier in the run. As such, we can reverse the list of loaded models so that the older models are first. We don¡¯t know how many snapshots are required to make a good prediction for this problem. We can explore the effect of the number of ensemble members on test set accuracy by creating ensembles of increasing size starting with the final model at epoch 499, then adding the model saved at epoch 449, and so on until all 10 models are included. First, we require a function to make a prediction given a list of models. Given that each model predicts the probabilities of each of the output classes, we can sum the predicted probabilities across the models and select the class with the most support via the argmax() function. The ensemble_predictions()<U+00A0>function below implements this functionality. We can then evaluate an ensemble of a given size by selecting the first n members from the list of models, making a prediction by calling the<U+00A0>ensemble_predictions() function, and then calculating and returning the accuracy of the prediction. The evaluate_n_members() function below implements this behavior. The performance of each ensemble can also be contrasted with the performance of each standalone model and the average performance of all standalone models. Finally, we can plot the performance of each individual snapshot model (blue dots) compared to the performance of an ensemble that includes all models up to and including each individual model (orange line). The complete example of making snapshot ensemble predictions with different sized ensembles is listed below. Running the example first loads all 10 models into memory. Next, each snapshot model is evaluated on the test dataset and the accuracy is reported. This is contrasted with the accuracy of a snapshot ensemble that includes all snapshot models working backward from the end of the run including the single model. The results show that as we work backward from the end of the run, the performance of the snapshot models gets worse, as we might expect. Combining snapshot models into an ensemble shows that performance increases up to and including the last 3-to-5 models, reaching about 82%. This can be compared to the average performance of a snapshot model of about 80% test set accuracy. Note, your specific results may vary given the stochastic nature of the learning algorithm. Try re-running the example a few times. Finally, a line plot is created plotting the same test set accuracy scores. We can set the performance of each individual snapshot model as a blue dot and the snapshot ensemble of increasing size (number of members) from 1 to 10 members as an orange line. At least on this run, we can see that the snapshot ensemble quickly out-performs the final model at 82.2% with 3 members and all other saved models before performance degrades back down to about the same as the final model at 81.3%. Line Plot of Single Snapshot Models (blue dots) vs Snapshot Ensembles of Varied Sized (orange line) This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop snapshot ensembles of models saved using an aggressive learning rate schedule over a single training run. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Better Deep Learning It provides self-study tutorials on topics like: weight decay, batch normalization, dropout, model stacking and much more¡¦ Skip the Academics. Just Results. Click to learn more. Comment  Name (required)  Email (will not be published) (required)  Website"
"mastery",2019-01-02,"Impact of Dataset Size on Deep Learning Model Skill And Performance Estimates","https://machinelearningmastery.com/impact-of-dataset-size-on-deep-learning-model-skill-and-performance-estimates/","Supervised learning is challenging, although the depths of this challenge are often learned then forgotten or willfully ignored. This must be the case, because dwelling too long on this challenge may result in a pessimistic outlook. In spite of the challenge, we continue to wield supervised learning algorithms and they perform well in practice. Fundamental to the challenge of supervised learning, are the concerns: Generally, it is common knowledge that too little training data results in a poor approximation. An over-constrained model will underfit the small training dataset, whereas an under-constrained model, in turn, will likely overfit the training data, both resulting in poor performance. Too little test data will result in an optimistic and high variance estimation of model performance. It is critical to make this ¡°common knowledge¡± concrete with worked examples. In this post, we will work through a detailed case study for developing a Multilayer Perceptron neural network on a simple two-class classification problem. You will discover that, in practice, we don¡¯t have enough data to learn the mapping function or to evaluate models, yet supervised learning algorithms like neural networks remain remarkably effective. Let¡¯s get started. Impact of Dataset Size on Deep Learning Model Skill And Performance EstimatesPhoto by Eneas De Troya, some rights reserved. This tutorial is divided into five parts; they are: As the basis for our exploration, we will use a very simple two-class or binary classification problem. The scikit-learn library provides the make_circles() function that can be used to create a binary classification problem with the prescribed number of samples and statistical noise. Each example has two input variables that define the x and y coordinates of the point on a two-dimensional plane. The points are arranged in two concentric circles (they have the same center) for the two classes. The number of points in the dataset is specified by a parameter, half of which will be drawn from each circle. Gaussian noise can be added when sampling the points via the ¡°noise¡± argument that defines the standard deviation of the noise, where 0.0 indicates no noise or points drawn exactly from the circles. The seed for the pseudorandom number generator can be specified via the ¡°random_state¡± argument that allows the exact same points to be sampled each time the function is called. The example below generates 100 examples from the two circles with no noise and a value of 1 to seed the pseudorandom number generator. Running the example generates the points and prints the shape of the input (X) and output (y) components of the samples. We can see that there are 100 examples of inputs with two features per example for the x and y coordinates and a matching 100 examples of the output variable or class value with 1 variable. The first five examples from the dataset are shown. We can see that the x and y components of the input variables are centered on 0.0 and have the bounds [-1, 1]. We can also see that the class values are integers for either 0 or 1 and that examples are shuffled between the classes. We can re-run the example and always get the same ¡°randomly generated¡± points given the same pseudorandom number generator seed. The example below generates the same points and plots the input variables of the samples using a scatter plot. We can use the scatter() matplotlib function to create the plot and pass in all rows of the X array with the first variable for the x coordinates and the second variable for the y coordinates on the plot. Running the example creates a scatter plot clearly showing the concentric circles of the dataset. Scatter Plot of the Input Variables of the Circles Dataset We can re-create the scatter plot, but instead plot all input samples for class 0 blue and all points for class 1 red. We can select the indices of samples in the y array that have a given value using the where() NumPy function and then use those indices to select rows in the X array. The complete example is below. Running the example, we can see that the samples for class 0 are the inner circle in blue and samples for class belong to the outer circle in red. Scatter Plot of the Input Variables of the Circles Dataset Colored By Class Value All real data has statistical noise. More statistical noise means that the problem is more challenging for the learning algorithm to map the input variables to the output or target variables. The circles dataset allows us to simulate the addition of noise to the samples via the ¡°noise¡± argument. We can create a new function called scatter_plot_circles_problem() that creates a dataset with the given amount of noise and creates a scatter plot with points colored by their class value. We can call this function multiple times with differing amounts of noise to see the effect on the complexity of the problem. We will create four scatter plots as subplots via the subplot() matplotlib function in a 4-by-4 matrix with the noise values [0.0, 0.1, 0.2, 0.3]. The complete example is listed below. Running the example, a plot is created with four subplots, one for each of the four different noise values [0.0, 0.1, 0.2, 0.3] in the top left, top right, bottom left, and bottom right respectively. We can see that a small amount of noise 0.1 makes the problem challenging, but still distinguishable. A noise value of 0.0 is not realistic and a dataset so perfect would not require machine learning. A noise value of 0.2 makes the problem very challenging and a value of 0.3 may the problem too challenging to learn. Four Scatter Plots of the Circles Dataset Varied by the Amount of Statistical Noise We can create a similar plot of the problem with a varied number of samples. More samples give a learning algorithm more opportunity to understand the underlying mapping of inputs to outputs, and, in turn, a better performing model. We can update the scatter_plot_circles_problem() function to take the number of samples to generate as an argument as well as the amount of noise, and set a default for the noise of 0.1, which makes the problem noisy, but not too noisy. We can call this function to create multiple scatter plots with different numbers of points, spread evenly between the two circles or classes. We will try versions of the problem with the following sized samples [50, 100, 500, 1000]. The complete example is listed below. Running the example creates a plot with four subplots, one for each of the different sized samples [50, 100, 500, 1000] in the top left, top right, bottom left, and bottom right respectively. We can see that 50 examples are probably too few and even 100 points do not look sufficient to really learn the problem. The plots suggest that 500 and 1,000 examples may be significantly easier to learn, although hide the fact that many ¡°outlier¡± points result in overlaps between the two circles. Four Scatter Plots of the Circles Dataset Varied by the Amount of Samples Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course We can model the circles problem with a neural network. Specifically, we can train a Multilayer Perceptron model, or MLP for short, providing input and output examples generated from the circles problem to the model. Once learned, we can evaluate how well the model has learned the problem by using it to make predictions on new examples and evaluate the accuracy. We can develop a small MLP for the problem using the Keras deep learning library with two inputs, 25 nodes in the hidden layer, and one output. A rectified linear activation function can be used for the nodes in the hidden layer. Because the problem is a binary classification problem, the model can use the sigmoid activation function on the output layer to predict the probability of a sample belonging to class 0 or 1. The model can be trained using an efficient version of mini-batch stochastic gradient descent called Adam, where each weight in the model has its own adaptive learning rate. The binary cross entropy loss function can be used as the basis for optimization, where smaller loss values indicate a better model fit. We require samples generated from the circles problem to train the model and a separate test set not used to train the model that can be used estimate how well the model might perform on average when making predictions on new data. The create_dataset() function below will create these train and test sets of a given size and uses a default noise of 0.1. We can call this function to prepare the train and test sets split into input and output components. Once we have defined the datasets and the model, we can train the model. We will train the model for 500 epochs on the training dataset. Once fit, we use the model to make predictions for the input examples of the test set and compare them to the true output values of the test set, then calculate an accuracy score. The evaluate() function performs this operation returning both the loss and accuracy of the model on the test dataset. We can ignore the loss and display the accuracy to get an idea of how well the model has learned to map random examples from the circles domain to a class 0 or class 1. Tying all of this together, the complete example is listed below. Running the example generates the dataset, fits the model on the training dataset, and evaluates the model on the test dataset. In this case, the model achieved an estimated accuracy of about 84.4%. A problem with this example is that your results may vary. You may see different estimated accuracy each time the example is run. Running the example again, I see an estimated accuracy of about 84.8%. Again, your specific results are expected to vary. The examples from the dataset are the same each time the example is run. This is because we fixed the pseudorandom number generator when creating the samples. The samples do have noise, but we are getting the same noise each time. Neural networks are a nonlinear learning algorithm, meaning that they can learn complex nonlinear relationships between the input variables and the output variables. They can approximate challenging nonlinear functions. As such, we refer to neural network models as having a low bias and a high variance. They have a low bias because the approach makes few assumptions about the mathematical functional form of the mapping function. They have a high variance because they are sensitive to the specific examples used to train the model. Differences in the training examples can mean a very different resulting model with, in turn, differing skill. Although neural networks are a high-variance low-bias method, this is not the cause of the difference in estimated performance when the same algorithm is run on the same generated data points. Instead, we are seeing a difference in performance across multiple runs because the learning algorithm is stochastic. The learning algorithm uses elements of randomness that aid the model in doing a good job on average of learning how to map input variables to output variables in the training dataset. Examples of randomness include the small random values used to initialize the model weights and the random sorting of examples in the training dataset before each training epoch. This is useful randomness as it allows the model to automatically ¡°discover¡± good solutions to the mapping function. It can be frustrating because it often finds different solutions each time the learning algorithm is run, and sometimes the differences between the solutions is large, resulting in differences in the estimated performance of the model when making predictions on new data. We can counter the variance in the solution found by a specific neural network by summarizing the performance of the approach over multiple runs. This involves fitting the same algorithm on the same dataset multiple times but allowing the randomness used in the learning algorithm to vary each time the algorithm is run. The model is evaluated on the same test set each run and the score is recorded. At the end of all repeats, the distribution of the scores is summarized using a mean and standard deviation. The mean of the performance of a model over multiple runs gives an idea of the average performance of the specific model on the specific dataset. The spread or standard deviation of the scores gives an idea of the variance introduced by the learning algorithm. We can move the evaluation of a single MLP to a function that takes the dataset and returns the accuracy on the test set. The evaluate_model() function below implements this behavior. We can then call this function multiple times such as 10, 30, or more. Given the law of large numbers, more runs will mean a more accurate estimate. To keep running time modest, we will repeat the run 10 times. Then, at the end of these runs, the mean and standard deviation of the scores will be reported. We can also summarize the distribution using a box and whisker plot via the boxplot() matplotlib function. Tying all of these elements together, an example of the repeated evaluation of an MLP model on the circles dataset is listed below. Running the example first reports the score of the model on each repeated evaluation. Your specific scores will vary, but on this run, we see accuracy scores spread between 83% and 85%. At the end of the repeats, the average score is reported as about 84.7% with a standard deviation of about 0.5%. That means for this specific model trained on the specific training set and evaluated on the specific test set, that 99% of runs will result in a test accuracy between 83.2% and 86.2%, given three standard deviations from the mean. No doubt, the small sample size of 10 has resulted in some error in these estimates. A box and whisker plot of the test accuracy scores is created showing the middle 50% of scores (called the interquartile range) denoted by the box ranges from a little below 84.5% to a little below 85%. We can also see that the 83% value observed may be an outlier, given it is denoted as a dot. Box and Whisker Plot of Test Accuracy of MLP on the Circles Problem Given a fixed amount of statistical noise and a fixed but reasonably configured model, how many examples are required to learn the circles problem? We can investigate this question by evaluating the performance of MLP models fit on training datasets of different size. As a foundation, we can define a large number of examples that we believe would be far more than sufficient to learn the problem, such as 100,000. We can use this as an upper-bound on the number of training examples and use this many examples in the test set. We will define a model that performs well on this dataset as a model that has effectively learned the two circles problem. We can then experiment by fitting models with different sized training datasets and evaluate their performance on the test set. Too few examples will result in a low test accuracy, perhaps because the chosen model overfits the training set or the training set is not sufficiently representative of the problem. Too many examples will result in good, but perhaps slightly lower than ideal test accuracy, perhaps because the chosen model does not have the capacity to learn the nuance of such a large training dataset, or the dataset is over-representative of the problem. A line plot of training dataset size to model test accuracy should show an increasing trend to a point of diminishing returns and perhaps even a final slight drop in performance. We can use the create_dataset() function defined in the previous section to create train and test datasets and set a default for the size of the test set argument to be 100,000 examples while allowing the size of the training set to be specified and vary with each call. Importantly, we want to use the same test set for each different sized training dataset. We can directly use the same evaluate_model() function from the previous section to fit and evaluate an MLP model on a given train and test set. We can create a new function to perform the repeated evaluation of a given model to account for the stochastic learning algorithm. The evaluate_size()<U+00A0>function below takes the size of the training set as an argument, as well as the number of repeats, that defaults to five to keep running time down. The create_dataset() function is created to create the train and test sets, and then the evaluate_model() function is called repeatedly to evaluate the model. The function then returns a list of scores for the repeats. This function can then be called repeatedly. I would guess that somewhere between 1,000 and 10,000 examples of the problem would be sufficient to learn the problem, where sufficient means only small fractional differences in test accuracy. Therefore, we will investigate training set sizes of 100, 1,000, 5,000, and 10,000 examples. The mean test accuracy will be reported for each test size to give an idea of progress. At the end of the run, a line plot will be created to show the relationship between train set size and model test set accuracy. We would expect to see an exponential curve from poor accuracy to a point of diminishing returns. Box and whisker plots of the score distributions for each test set size are also created. We would expect to see a shrinking in the spread of test accuracy as the size of the training set is increased. The complete example is listed below. Running the example may take a few minutes on modern hardware. The mean model performance is reported for each training set size, showing a steady improvement in test accuracy as the training set is increased, as we expect. We also see a small drop in the average model performance from 5,000 to 10,000 examples, very likely highlighting that the variance in the data sample has exceeded the capacity of the chosen model configuration (number of layers and nodes). A line plot of test accuracy vs training set size is created. We can see a sharp increase in test accuracy from 100 to 1,000 examples, after which performance appears to level off. Line Plot of Training Set Size vs Test Set Accuracy for an MLP Model on the Circles Problem A box and whisker plot is created showing the distribution of test accuracy scores for each sized training dataset. As expected, we can see that the spread of test set accuracy scores shrinks dramatically as the training set size is increased, although remains small on the plot given the chosen scale. Box and Whisker Plots of Test Set Accuracy of MLPs Trained With Different Sized Training Sets on the Circles Problem The results suggest that the chosen MLP model configuration can learn the problem reasonably well with 1,000 examples, with quite modest improvements seen with 5,000 and 10,000 examples. Perhaps there is a sweet spot of 2,500 examples that results in an 84% test set accuracy with less than 5,000 examples. The performance of neural networks can continually improve as more and more data is provided to the model, but the capacity of the model must be adjusted to support the increases in data. Eventually, there will be a point of diminishing returns where more data will not provide more insight into how to best model the mapping problem. For simpler problems, like two circles, this point will be reached sooner than in more complex problems, such as classifying photos of objects. This study has highlighted a fundamental aspect of applied machine learning, specifically that you need enough examples of the problem to learn a useful approximation of the unknown underlying mapping function. We almost never have an abundance of or too much training data. Therefore, our focus is often on how to most economically use available data and how to avoid overfitting the statistical noise present in the training dataset. Given a fixed model and a fixed training dataset, how much test data is required to achieve an accurate estimate of the model performance? We can investigate this question by fitting an MLP with a fixed sized training set and evaluating the model with different sized test sets. We can use much the same strategy as the study in the previous section. We will fix the training set size at 1,000 examples as it resulted in a reasonably effective model with an estimated accuracy of about 83.7% when evaluated on 100,000 examples. We would expect that there is a smaller test set size that can reasonably approximate this value. The create_dataset() function can be updated to specify the test set size and to use a default of 1,000 examples for the training set size. Importantly, the same 1,000 examples are used for the training set each time the size of the test set is varied. We can use the same fit_model() function to fit the model. Because we¡¯re using the same training dataset and varying the test dataset, we can create and fit the models once and re-use them for each differently sized test set. In this case, we will fit 10 models on the same training dataset to simulate 10 repeats. Once fit, we can evaluate each of the models using a given sized test dataset. The evaluate_test_set_size() function below implements this behavior, returning a list of test set accuracy scores for the list of fit models and a given test set size. We will evaluate four different sized test sets with 100, 1,000, 5,000, and 10,000 examples. We can then report the mean scores for each sized test set and create the same line and box and whisker plots. Tying these elements together, the complete example is listed below. Running the example reports the test set accuracy for each differently sized test set, averaged across the 10 models trained on the same dataset. If we take the result from the previous section of 83.7% when evaluated on 100,000 examples as an estimate of ground truth, then we can see that the smaller sized test sets vary above and below this estimate. A line plot of test set size vs test set accuracy is created. The plot shows that smaller test set sizes of 100 and 1,000 examples overestimate and underestimate the accuracy of the model respectively. The results for a test set of 5,000 and 10,000 are a closer fit, the latter perhaps showing the best match. This is surprising as a naive expectation might be that a test set of the same size as the training set would be a reasonable approximation of model accuracy, e.g. as though we performed a 50%/50% split of a 2,000 example dataset. Line Plot of Test Set Size vs Test Set Accuracy for an MLP on the Circles Problem A box and whisker plots show that the smaller test set demonstrates a large spread of scores, all of which are optimistic. Box and Whisker Plot of the Distribution of Test set Accuracy for Different Test Set Sizes on the Circles Problem It should be pointed out that we are not reporting on the average performance of the chosen model on random examples drawn from the domain. The model and the examples are fixed and the only source of variation is from the learning algorithm. The study demonstrates how sensitive the estimated accuracy of the model is to the size of the test dataset. This is an important consideration as often little thought is given to the test set size, using a familiar split of 70%/30% or 80%/20% for train/test splits. As pointed out in the previous section, we often rarely have enough data, and spending a precious portion of the data on a test or even validation dataset is often rarely considered in any great depth. The results highlight how important cross-validation methods such as leave-one-out cross-validation are, allowing the model to make a test prediction for each example in the dataset, yet come at such a great computational cost, requiring almost one model to be trained for each example in the dataset. In practice, we must struggle with a training dataset that is too small to learn the unknown underlying mapping function and a test dataset that is too small to reasonably approximate the performance of the model on the problem. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered that in practice, we don¡¯t have enough data to learn the mapping function or to evaluate models, yet supervised learning algorithms like neural networks remain remarkably effective. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Better Deep Learning It provides self-study tutorials on topics like: weight decay, batch normalization, dropout, model stacking and much more¡¦ Skip the Academics. Just Results. Click to learn more. Comment  Name (required)  Email (will not be published) (required)  Website"
"mastery",2018-12-31,"How to Develop a Stacking Ensemble for Deep Learning Neural Networks in Python With Keras","https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/","Model averaging is an ensemble technique where multiple sub-models contribute equally to a combined prediction. Model averaging can be improved by weighting the contributions of each sub-model to the combined prediction by the expected performance of the submodel. This can be extended further by training an entirely new model to learn how to best combine the contributions from each submodel. This approach is called stacked generalization, or stacking for short, and can result in better predictive performance than any single contributing model. In this tutorial, you will discover how to develop a stacked generalization ensemble for deep learning neural networks. After completing this tutorial, you will know: Let¡¯s get started. How to Develop a Stacking Ensemble for Deep Learning Neural Networks in Python With KerasPhoto by David Law, some rights reserved. This tutorial is divided into six parts; they are: A model averaging ensemble combines the predictions from multiple trained models. A limitation of this approach is that each model contributes the same amount to the ensemble prediction, regardless of how well the model performed. A variation of this approach, called a weighted average ensemble, weighs the contribution of each ensemble member by the trust or expected performance of the model on a holdout dataset. This allows well-performing models to contribute more and less-well-performing models to contribute less. The weighted average ensemble provides an improvement over the model average ensemble. A further generalization of this approach is replacing the linear weighted sum (e.g. linear regression) model used to combine the predictions of the sub-models with any learning algorithm. This approach is called stacked generalization, or stacking for short. In stacking, an algorithm takes the outputs of sub-models as input and attempts to learn how to best combine the input predictions to make a better output prediction. It may be helpful to think of the stacking procedure as having two levels: level 0 and level 1. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. <U+2014> Stacked generalization, 1992. Unlike a weighted average ensemble, a stacked generalization ensemble can use the set of predictions as a context and conditionally decide to weigh the input predictions differently, potentially resulting in better performance. Interestingly, although stacking is described as an ensemble learning method with two or more level 0 models, it can be used in the case where there is only a single level 0 model. In this case, the level 1, or meta-learner, model learns to correct the predictions from the level 0 model. ¡¦ although it can also be used when one has only a single generalizer, as a technique to improve that single generalizer <U+2014> Stacked generalization, 1992. It is important that the meta-learner is trained on a separate dataset to the examples used to train the level 0 models to avoid overfitting. A simple way that this can be achieved is by splitting the training dataset into a train and validation set. The level 0 models are then trained on the train set. The level 1 model is then trained using the validation set, where the raw inputs are first fed through the level 0 models to get predictions that are used as inputs to the level 1 model. A limitation of the hold-out validation set approach to training a stacking model is that level 0 and level 1 models are not trained on the full dataset. A more sophisticated approach to training a stacked model involves using k-fold cross-validation to develop the training dataset for the meta-learner model. Each level 0 model is trained using k-fold cross-validation (or even leave-one-out cross-validation for maximum effect); the models are then discarded, but the predictions are retained. This means for each model, there are predictions made by a version of the model that was not trained on those examples, e.g. like having holdout examples, but in this case for the entire training dataset. The predictions are then used as inputs to train the meta-learner. Level 0 models are then trained on the entire training dataset and together with the meta-learner, the stacked model can be used to make predictions on new data. In practice, it is common to use different algorithms to prepare each of the level 0 models, to provide a diverse set of predictions. ¡¦ stacking is not normally used to combine models of the same type [¡¦] it is applied to models built by different learning algorithms. <U+2014> Practical Machine Learning Tools and Techniques, Second Edition, 2005. It is also common to use a simple linear model to combine the predictions. Because use of a linear model is common, stacking is more recently referred to as ¡°model blending¡± or simply ¡°blending,¡± especially in machine learning competitions. ¡¦ the multi-response least squares linear regression technique should be employed as the high-level generalizer. This technique provides a method of combining level-0 models¡¯ confidence <U+2014> Issues in Stacked Generalization, 1999. A stacked generalization ensemble can be developed for regression and classification problems. In the case of classification problems, better results have been seen when using the prediction of class probabilities as input to the meta-learner instead of class labels. ¡¦ class probabilities should be used instead of the single predicted class as input attributes for higher-level learning. The class probabilities serve as the confidence measure for the prediction made. <U+2014> Issues in Stacked Generalization, 1999. Now that we are familiar with stacked generalization, we can work through a case study of developing a stacked deep learning model. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course We will use a small multi-class classification problem as the basis to demonstrate the stacking ensemble. The scikit-learn class provides the make_blobs() function that can be used to create a multi-class classification problem with the prescribed number of samples, input variables, classes, and variance of samples within a class. The problem has two input variables (to represent the x and y coordinates of the points) and a standard deviation of 2.0 for points within each group. We will use the same random state (seed for the pseudorandom number generator) to ensure that we always get the same data points. The results are the input and output elements of a dataset that we can model. In order to get a feeling for the complexity of the problem, we can graph each point on a two-dimensional scatter plot and color each point by class value. The complete example is listed below. Running the example creates a scatter plot of the entire dataset. We can see that the standard deviation of 2.0 means that the classes are not linearly separable (separable by a line) causing many ambiguous points. This is desirable as it means that the problem is non-trivial and will allow a neural network model to find many different ¡°good enough¡± candidate solutions, resulting in a high variance. Scatter Plot of Blobs Dataset With Three Classes and Points Colored by Class Value Before we define a model, we need to contrive a problem that is appropriate for the stacking ensemble. In our problem, the training dataset is relatively small. Specifically, there is a 10:1 ratio of examples in the training dataset to the holdout dataset. This mimics a situation where we may have a vast number of unlabeled examples and a small number of labeled examples with which to train a model. We will create 1,100 data points from the blobs problem. The model will be trained on the first 100 points and the remaining 1,000 will be held back in a test dataset, unavailable to the model. The problem is a multi-class classification problem, and we will model it using a softmax activation function on the output layer. This means that the model will predict a vector with three elements with the probability that the sample belongs to each of the three classes. Therefore, we must one hot encode the class values before we split the rows into the train and test datasets. We can do this using the Keras to_categorical() function. Next, we can define and combine the model. The model will expect samples with two input variables. The model then has a single hidden layer with 25 nodes and a rectified linear activation function, then an output layer with three nodes to predict the probability of each of the three classes and a softmax activation function. Because the problem is multi-class, we will use the categorical cross entropy loss function to optimize the model and the efficient Adam flavor of stochastic gradient descent. The model is fit for 500 training epochs and we will evaluate the model each epoch on the test set, using the test set as a validation set. At the end of the run, we will evaluate the performance of the model on the train and test sets. Then finally, we will plot learning curves of the model accuracy over each training epoch on both the training and validation datasets. Tying all of this together, the complete example is listed below. Running the example first prints the shape of each dataset for confirmation, then the performance of the final model on the train and test datasets. Your specific results will vary (by design!) given the high variance nature of the model. In this case, we can see that the model achieved about 85% accuracy on the training dataset, which we know is optimistic, and about 80% on the test dataset, which we would expect to be more realistic. A line plot is also created showing the learning curves for the model accuracy on the train and test sets over each training epoch. We can see that training accuracy is more optimistic over most of the run as we also noted with the final scores. Line Plot Learning Curves of Model Accuracy on Train and Test Dataset Over Each Training Epoch We can now look at using instances of this model as part of a stacking ensemble. To keep this example simple, we will use multiple instances of the same model as level-0 or sub-models in the stacking ensemble. We will also use a holdout validation dataset to train the level-1 or meta-learner in the ensemble. A more advanced example may use different types of MLP models (deeper, wider, etc.) as sub-models and train the meta-learner using k-fold cross-validation. In this section, we will train multiple sub-models and save them to file for later use in our stacking ensembles. The first step is to create a function that will define and fit an MLP model on the training dataset. Next, we can create a sub-directory to store the models. Note, if the directory already exists, you may have to delete it when re-running this code. Finally, we can create multiple instances of the MLP and save each to the ¡°models/¡± subdirectory with a unique filename. In this case, we will create five sub-models, but you can experiment with a different number of models and see how it impacts model performance. We can tie all of these elements together; the complete example of training the sub-models and saving them to file is listed below. Running the example creates the ¡°models/¡± subfolder and saves five trained models with unique filenames. Next, we can look at training a meta-learner to make best use of the predictions from these submodels. We can now train a meta-learner that will best combine the predictions from the sub-models and ideally perform better than any single sub-model. The first step is to load the saved models. We can use the load_model() Keras function and create a Python list of loaded models. We can call this function to load our five saved models from the ¡°models/¡± sub-directory. It would be useful to know how well the single models perform on the test dataset as we would expect a stacking model to perform better. We can easily evaluate each single model on the training dataset and establish a baseline of performance. Next, we can train our meta-learner. This requires two steps: We will prepare a training dataset for the meta-learner by providing examples from the test set to each of the submodels and collecting the predictions. In this case, each model will output three predictions for each example for the probabilities that a given example belongs to each of the three classes. Therefore, the 1,000 examples in the test set will result in five arrays with the shape [1000, 3]. We can combine these arrays into a three-dimensional array with the shape [1000, 5, 3] by using the dstack() NumPy function that will stack each new set of predictions. As input for a new model, we will require 1,000 examples with some number of features. Given that we have five models and each model makes three predictions per example, then we would have 15 (3 x 5) features for each example provided to the submodels. We can transform the [1000, 5, 3] shaped predictions from the sub-models into a [1000, 15] shaped array to be used to train a meta-learner using the reshape() NumPy function and flattening the final two dimensions. The stacked_dataset() function implements this step. Once prepared, we can use this input dataset along with the output, or y part, of the test set to train a new meta-learner. In this case, we will train a simple logistic regression algorithm from the scikit-learn library. Logistic regression only supports binary classification, although the implementation of logistic regression in scikit-learn in the LogisticRegression class supports multi-class classification (more than two classes) using a one-vs-rest scheme. The function fit_stacked_model() below will prepare the training dataset for the meta-learner by calling the stacked_dataset() function, then fit a logistic regression model that is then returned. We can call this function and pass in the list of loaded models and the training dataset. Once fit, we can use the stacked model, including the members and the meta-learner, to make predictions on new data. This can be achieved by first using the sub-models to make an input dataset for the meta-learner, e.g. by calling the stacked_dataset() function, then making a prediction with the meta-learner. The stacked_prediction() function below implements this. We can use this function to make a prediction on new data; in this case, we can demonstrate it by making predictions on the test set. Tying all of these elements together, the complete example of fitting a linear meta-learner for the stacking ensemble of MLP sub-models is listed below. Running the example first loads the sub-models into a list and evaluates the performance of each. We can see that the best performing model is the final model with an accuracy of about 81.3%. Your specific results may vary given the stochastic nature of the neural network learning algorithm. Next, a logistic regression meta-learner is trained on the predicted probabilities from each sub-model on the test set, then the entire stacking model is evaluated on the test set. We can see that in this case, the meta-learner out-performed each of the sub-models on the test set, achieving an accuracy of about 82.4%. When using neural networks as sub-models, it may be desirable to use a neural network as a meta-learner. Specifically, the sub-networks can be embedded in a larger multi-headed neural network that then learns how to best combine the predictions from each input sub-model. It allows the stacking ensemble to be treated as a single large model. The benefit of this approach is that the outputs of the submodels are provided directly to the meta-learner. Further, it is also possible to update the weights of the submodels in conjunction with the meta-learner model, if this is desirable. This can be achieved using the Keras functional interface for developing models. After the models are loaded as a list, a larger stacking ensemble model can be defined where each of the loaded models is used as a separate input-head to the model. This requires that all of the layers in each of the loaded models be marked as not trainable so the weights cannot be updated when the new larger model is being trained. Keras also requires that each layer has a unique name, therefore the names of each layer in each of the loaded models will have to be updated to indicate to which ensemble member they belong. Once the sub-models have been prepared, we can define the stacking ensemble model. The input layer for each of the sub-models will be used as a separate input head to this new model. This means that k copies of any input data will have to be provided to the model, where k is the number of input models, in this case, 5. The outputs of each of the models can then be merged. In this case, we will use a simple concatenation merge, where a single 15-element vector will be created from the three class-probabilities predicted by each of the 5 models. We will then define a hidden layer to interpret this ¡°input¡± to the meta-learner and an output layer that will make its own probabilistic prediction. The define_stacked_model() function below implements this and will return a stacked generalization neural network model given a list of trained sub-models. A plot of the network graph is created when this function is called to give an idea of how the ensemble model fits together. Creating the plot requires that pygraphviz is installed. If this is a challenge on your workstation, you can comment out the call to the<U+00A0>plot_model() function. Visualization of Stacked Generalization Ensemble of Neural Network Models Once the model is defined, it can be fit. We can fit it directly on the holdout test dataset. Because the sub-models are not trainable, their weights will not be updated during training and only the weights of the new hidden and output layer will be updated. The fit_stacked_model()<U+00A0>function below will fit the stacking neural network model on for 300 epochs. We can call this function providing the defined stacking model and the test dataset. Once fit, we can use the new stacked model to make a prediction on new data. This is as simple as calling the predict() function on the model. One minor change is that we require k copies of the input data in a list to be provided to the model for each of the k sub-models. The predict_stacked_model() function below simplifies this process of making a prediction with the stacking model. We can call this function to make a prediction for the test dataset and report the accuracy. We would expect the performance of the neural network learner to be better than any individual submodel and perhaps competitive with the linear meta-learner used in the previous section. Tying all of these elements together, the complete example is listed below. Running the example first loads the five sub-models. A larger stacking ensemble neural network is defined and fit on the test dataset, then the new model is used to make a prediction on the test dataset. We can see that, in this case, the model achieved an accuracy of about 83.3%, out-performing the linear model from the previous section. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop a stacked generalization ensemble for deep learning neural networks. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Better Deep Learning It provides self-study tutorials on topics like: weight decay, batch normalization, dropout, model stacking and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hi Jason,
Awesome post. I tried to perform a stacking ensemble for a binary classification task. However, I have an issue with the stacked_dataset definition. It gives a value error saying ¡°bad_shape (200, 2)¡±. I have 200 test samples with 2 classes. Kindly suggest the modification needed for the stacked_dataset definition. Many thanks. You may have to adapt the example to your specific models and dataset. Thank you No problem. Tableau What about it? Since you uses multiple same height models, is it possible to use different sub models like different layers or different type, VGG, Inception etc. as the sub models?
Another question is, if we shuffle the data sets between the sub models, how possible the stacking ensemble model over fitting? Yes, you can use models of varying depth. Fitting each model on separate data would no longer be a stacking model, but instead a bagging type model. Thank you Jason. Although I need to digest all you have written as I am a newbie this this field, I appreciate your effort in sharing your knowledge. Thanks. Stick with it! Comment  Name (required)  Email (will not be published) (required)  Website"
