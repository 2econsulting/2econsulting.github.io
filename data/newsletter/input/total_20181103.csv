"","site","date","headline","url_address","text","keyword"
"1","kaggle",2018-11-02,"Pulse of the Competition: November Edition","http://blog.kaggle.com/2018/11/02/pulse-of-the-competition-november-edition/","Submit! If you haven't made a Kaggle competition submission, this is a great moment to jump in. Here's a rundown of current competitions, trending Kernels, and tips from winners.<U+200B> <U+200B>Things are really heating up in the last 2 weeks of the Airbus Ship Detection Challenge. 20 unique leaderboard scores, 8 different Kagglers, and 10 leader changes in only 16 days? Can you find an even better solution and topple the Top 10? Join the excitement in the final few days! <U+200B><U+200B>Each color represents a different team at the top of the leaderboard<U+200B> <U+200B>Nobody likes starting a competition with just a blinking cursor¡¦ that's what the community is here for! Fork a starter Kernel from the Kaggle Team to submit right away: <U+0001F441> Two Sigma Challenge: Official Getting Started KernelUsing the twosigmanews module to make predictions <U+0001F441> The PLAsTiCC Astronomy Starter Kit KernelAll you need to know about astrophysics to get started Or discover interesting explorations by top Kagglers: <U+0001F4C8> Two Sigma: Use news to predict stock prices
<U+200B>By Kernels Expert Andrew Lukyanenko <U+0001F3EC> RStudio: Predict GStore customer spendingBy Kernels Master kxx <U+0001F52C> Protein Atlas: Classify protein patterns in human cells
By Kernels Expert Allunia<U+200B> <U+200B>The TGS Salt Identification Competition just wrapped up! How did the winners do it? They talk about their solutions in the discussion below. Remember, you can continue to make submissions to closed competitions and still receive a score on your prediction. Even though you won¡¯t be on the leaderboard, this is a great way to practice winning techniques. Will these winning tips and techniques help improve your model? <U+0001F3C5> 1st Place by b.e.s.""We created five common folds stratified by depth¡¦"" <U+0001F3C5> 4th Place by SeuTao""Deep supervision structure with Lovasz softmax (a great idea from Heng)..."" <U+0001F3C5> 8th Place by Igor Krashenyi""Best performing backbones: SeNet154, SeResNext101¡¦"" <U+0001F3C5> 9th Place by tugstugi""We started with a se_resnext50 model¡¦""<U+200B> The PLAsTiCC Astronomical Classification Challenge involves categorizing astronomical sources that vary with time. Participants are struggling with predicting class 99 from the training set due to the unique nature of the problem. Follow along to see how leading ML researchers talk through this problem¡¦ or add your own insights. <U+200B>Join the Discussion<U+200B><U+200B> <U+200B>Why not ¡°drop¡± into a new challenge and win some Kaggle Swag? Here's the latest playground competition from the Kaggle Team, where you can work to predict the finish placement of players in PUBG. <U+200B><U+200B>","Keyword(freq): kernel(4), competition(2), kaggler(2), technique(2), tip(2), winner(2), astrophysic(1), atlas(1), backbone(1), cell(1)"
"2","kaggle",2018-11-01,"Data Notes: Chinese Tourism's Impact on Taiwan","http://blog.kaggle.com/2018/11/01/data-notes-chinese-tourisms-impact-on-taiwan/","Chinese tourism, US elections, and PyTorch: Enjoy these new, intriguing, and overlooked datasets and kernels 1.<U+0001F1F9><U+0001F1FC> Impact of Chinese Tourism Ban to Taiwan (Link)
2. <U+0001F41D> Honey Bee Subspecies Classification (Link)
3. <U+0001F925> <U+00A0>LSTM's with Attention for Emotion Recognition (Link)
4. <U+2797> Mathematics of Linear Regression (Link)
5. <U+0001F3A5> Towards a Movie Recommendation Engine (Link)
6. <U+0001F93A> From Zero to Hero in PyTorch!!! (Link)
7. <U+0001F327> Best Values in Deep Learning Cloud Providers (Link)
8. <U+0001F911> Dataset: Economic Freedom of <U+00A0>World Countries (Link)
9. <U+0001F1FA><U+0001F1F8> Dataset: US Senate Forecasts (Link #1,<U+00A0>Link #2)
10. <U+0001F1FA><U+0001F1F8> Dataset: US House Forecasts (Link #1,<U+00A0>Link #2) <U+200B><U+200B> Want to compare multiple ML models? <U+00A0>Try this<U+00A0>fun tutorial<U+00A0>and classify some flowers!
<U+200B> Copyright ¨Ï 2018 Kaggle, All rights reserved.","Keyword(freq): forecast(2), dataset(1), election(1), flower(1), kernel(1), mathematic(1), model(1), provider(1), right(1), subspecy(1)"
"3","datacamp",2018-10-29,"Arnaub Chatterjee discusses artificial intelligence (AI) and machine learning (ML) in healthcare.","https://www.datacamp.com/community/blog/ai-healthcare","Hugo Bowne-Anderson, the host of DataFramed, the DataCamp podcast, recently interviewed Arnaub Chatterjee, Senior Expert and Associate Partner in the Pharmaceutical and Medical Products group at McKinsey & Company. Here is the podcast link. Hugo:    Arnaub, I'm really excited to have you here today to talk about the role of AI, data science, and machine learning in health care, what has and hasn't worked, but before we get there, I'd love for you to set the scene of your own journey and let us know how you got into data science originally. Arnaub:    Thank, Hugo, and thanks to DataCamp for having me today. The notion of how I got into data science is quite serendipitous, and as is a lot of things in life, a bit of the right time in the right place. I think there's also a concurrent movement within health care where data science is really taken off within the last 10 years, so as a lot of perfect storms work, all these factors aligned. I think my career's been a little bit of a zig-zag in terms of the fact that I've held roles in consulting, worked for the previous administration in both of the technology and the policy worlds, and then in pharma, and now back in consulting. The central theme or ethos around how I've worked has always been around data science and the common thread and some link to data science. Arnaub:    Just to give you the quick background, I started my career after grad school as a consultant, originally focusing on pharma MNA, but then helping AIDS filled health data infrastructure in a pre-ACA time, Affordable Care Act time. That actually led me to go work for the Obama administration. I transitioned over to initially work on some data science efforts around health care fraud and abuse, and thinking not only about the policy, but also around how do we think about the utilization of that data, and to predict who may be more likely to commit fraud and fraud the government. Arnaub:    Things transitioned into more of a technology bench from that perspective. I then had the opportunity to work with a few technology officers within HHS, and at the time Todd Park and Bryan Sivak were creating a new movement around open data and building APIs and platforms that access this tremendous amount of data the government sat on, so very, very fortunate to have run into two Silicon Valley guys who brought their DNA over to government and start a number of initiatives like the Health Data Initiative. We're building platforms around fda.gov, clinicaltrials.gov, open APIs. A lot of that was also very right time and the right place, and being able to learn from folks who'd done it in the private sector with a technology mindset. Then the pharma aspect of it, I actually ended up transitioning over to pharma because of government, and followed some folks up who had spent some time working in government and started a team that was focused on data science at Merck. What we were doing at Merck was very much around how do we utilize and identify novel data sets, which could include rogue data-like claims, but it could also include clinical and genomic and social media and censors. It really utilized and think about how we demonstrate the clinical and economic value of Merck products. This was a whole new way of thinking about positioning the drug in a different way and thinking about the advent of new methodologies within data science to support and bolster the value of the drug. Arnaub:    I did that for a number of years and worked with a variety of academic institutions on different machine learning methodologies, different data science methods. All that eventually led me to McKinsey, where I am right now. In that capacity, I do a lot of work with not only pharma clients but technology clients and how they're entering the health care sector. I feel in that capacity I've been fortunate to be at the front lines of how different companies are deploying machine learning, data science in a variety of different settings. Hopefully we'll cover a lot of that today. Hugo:    Absolutely. And as you say, it took a lot of moving parts, or it's almost a perfect storm of your interests and serendipity of technology and the emerging data science stack and all the data available that made this career path for you what it's become. We'll see that that happened in this space as well. That is actually took a lot of moving parts, availability, and generation of large scale data, computational power, statistical insights, that allowed data science, in health and otherwise, to emerge as it has. Hugo:    I'm also interested in the fact that you started thinking about this type of stuff at grad school, and that's when you started working with data. The skills you needed then and need now to work with data, are these skills that you learned on the job, or were you trained specifically to do this type of work? Arnaub:    It's an interesting question. I think in grad school I spent some time in biostatistics and epidemiology. My background's in health care on the business side and then also health policy bent. The funny thing about data science is if you ask a lot of folks in healthcare they'll tell you that a data scientist is a statistician in California, and they'll basically say that that notion and term has changed a lot as methods and the advent of machine learning has really warped what that definition means. Arnaub:    In some capacity I think people who have worked with traditional claims data and have skill sets like epidemiology, like biostatistics, are in their own right data scientists. I think what's changed now is, like you mentioned, greater volumes and different types of data. There are different ways of processing and understanding how we use it. My training started there, and then now I think, like a lot of people, I'm having to evolve and learn just based on the fact that a lot of disciplines are converging. Computer scientists and the fact that there are actually data science degrees now, programs that are teaching different ways of using data, those are converging with a lot of old school ways that people have used health care data. That's where I find myself right now. Hugo:    Great, and I love that you mentioned different types of data because I suppose heterogeneity of data is something which is so abundant, and actually, when people ask me where does this occur, the clinical setting is one that I mention initially that you can have tabular data from experiments and controls in addition to imaging data from scans, which we'll get to, in addition to natural language from doctors' notes on patients files and that types of stuff. Hugo:    I think that's a great time to move into this conversation about the use of AI in health care. As we know, there's a great deal of hype around the use of machine learning in AI and health care. I'm wondering from your point of view, what has actually worked in this space? Arnaub:    Yeah, it's a really important question, and I think when we talk about what's actually worked, I think it's important to know that this is a very much evolving space, so in some cases the jury is still out. In other cases, we're starting to see very promising signs. Just to acknowledge the hype, I guess in my personal opinion, we're in a golden era of funding for AI in health care. I think the statistic that I saw more recently was that since 2016, there have been 300 startups in AI in health care alone that have emerged. These companies span the gamut from reducing administrative scut work within insurance and billing companies, to actually creating new drugs and empowering compound development. I think what's important about that is we're starting to now see where a lot of the attention for AI is being driven towards and where the media attention is going towards. Venture dollars are flowing to companies that are also streamlining a lot of operational and efficiency tasks, administrative efficiency tasks in health care. They're also flowing towards companies that have these bold aspirations of upending processes that have been happening for decades now. Arnaub:    What we're trying to discern is what a successful outcome looks like, and how do we think about what the bigger aspiration is, where do we see tangible improvements in health care, how do we think about the advancement of patient outcomes? This conversation, it's not meant to splash cold water or put a wet blanket on the great work that's being done. It's just meant to try and understand, like you mentioned at the very beginning of the conversation, a lot of talk about this has been taken place to where we've seen the promise. Arnaub:    Let me go ahead and take on a few different examples in spaces where I think this has worked, and we can have a deeper conversation about this. The very first things you mentioned was around imaging, and particularly within diagnostic imaging and health care, that's a bedrock. I think it's really important to remember that pioneering use cases of AI in other industries actually started with the ability to read back pictures and look at faces and patterns and objects in photographs. That is very much similar in health care where a lot of our lighthouse use cases where we've seen great success are starting to take place. Arnaub:    To give you a little bit of a context on the growth of this market, AI facilitated diagnostic imaging is supposed to be a two billion dollar industry by 2023. That is just a fraction of the whole medical imaging market, which is 57 billion dollars. 57 billions dollars includes equipment, it includes software, services, so it's a massive, massive marketplace that has been in health care for quite some time. I think what we're seeing now is a consensus from a number of parties, whether it's hospitals and technology companies, is that AI is going to transform the diagnostic imaging industry, whether it's enhanced productivity, whether it's improved accuracy, personalized treatment planning, all of these functions are up for grabs. Arnaub:    Just to build a little bit more on this, why imaging as sort of the first place we're seeing improvement, for one thing, hospitals are producing around 50 petabytes of data per year, and 90% of that data is from medical imaging. We're talking about MRI scans, PET scans, CT scans, and all of these are also embedded within EHRs. I think that's one reason, is the availability and the ubiquity of this data. Arnaub:    I think the second reason is that there are a number of compelling use cases that are actually there now within health care. To pick on some great work that Google has done, Google Brain has published their really powerful paper within JAMA, where they worked with a 130 thousand patients from an eye institute, and they looked at retinal fundus images. What they were able to do was come up with a more sophisticated, convolutional neural network that was able to predict diabetic retinopathy, which is one of the leading causes of blindness globally. Around 400 million people have this disorder. Arnaub:    What they effectively did was come up with a more refined version using a subset of those 130,000 images, and they outperformed a panel of eight ophthalmologists in terms of understanding where the retinopathy took place and how do you actually characterize what are the contextual clues. Their F-score was .95. The fact that they had an adjusted AUC, the fact that it was in JAMA. There's quite a strong, clinical argument to be made that if we get access to more of this type of data and we're able to build it into different tools and processes and how ophthalmologists see their patients, that's just the beginning, I think. Deep Mind has a very similar study within the retinal space. Just those two examples alone, I think, are pretty compelling. Not only are you seeing this in ophthalmology, but you're also starting to see it within dermatology and pathology as your next set of lighthouse use cases. Hugo:    It strikes me as interesting, and I'm wondering if it's interesting to you, that it's companies such as Google in this case, that aren't traditionally known in the health care space that are making such advancements. Arnaub:    Yeah, I think you're starting to see a lot of interesting collaborations between companies that have world class machine learning and health care institutions. On the other side of Silicon Valley you have Facebook. Facebook just announced their collaboration with the NYU School of Medicine, where they're utilizing their AI to speed up recognition of MRI scans. The projects for those of you who are interested is called Fast MRI. It's looking initially at around three million images of the knee and the brain and the liver, and looking at around 10 thousand different patient cases. That was just recently announced. We'll see what the fruits of that labor are. Arnaub:    I don't think it's all that surprising. I think the computational power, it's now incumbent upon Google to figure out how do they think about where the applications are, where the use cases are, and I think this is where you're starting to see imaging as that first initial lighthouse for them because they can, compellingly ... they have done this in other industries, and now they're compellingly able to do it with health care data as well. Hugo:    So you were about to go and tell us a bit more about use cases in ophthalmology and dermatology, for example. Arnaub:    Yeah. I think we're starting to see similar ... ophthalmology's obviously the retinal disease example. We've started to see different cases with breast cancer. There is a great example of a partnership between Kaggle and Intel and a company called MobileODT, where they developed an algorithm that accurately identified a woman's cervix and how do we better screen and treat woman for cervical cancer. The data was consisting of around 10 thousand label cervix images, and it had type one, two and three cervical cancer. This was a 50 layer convolutional, neural network, deep learning model that accurately segmented different parts of cervix type identification. Just another example where this algorithm, just by leveraging the power of the crowd, it wasn't even academically trained or clinically trained folks, they are able to capture and accurately identify cervix type 75% of the time. Arnaub:    I think something to notice that's really important is that these CNNs are actually reproducible. You don't have to rebuild and reinvent to wheel every single time. I think that's where you're going to start to see great refinement, and you're going to start to see a lot of enhancement in terms of how we do imaging recognition and reproducing these algorithms. Arnaub:    I think the second thing is these major partnerships where you're starting to see tech companies partner with eye institutes and large corporations that have imaging data. That's going to be very compelling and powerful. Hugo:    And so when you say reproducible, do you also mean reusable in the transfer learning sense? Arnaub:    Yeah, I think we'll get to this, I think, later on, hopefully, but one of the big challenges with AI is just making it reproducible within health care. The big hurdle there is that data is different in many different parts of the health care system. The patient that you're seeing California will be very different than the one that you're seeing in Texas or South Carolina or Boston. I think what we're trying to better understand is how do you create a generalize-ability on an algorithm that may have been used within a certain subsection of the US population or the global population. Then being able to consistently come up with those algorithms is what is a challenge because there are also different ways of characterizing this, and I'll spend some time talking about this later. Arnaub:    For radiology specifically, the outcomes that you're looking for are different. You might look at the probability of a lesion, and you might look at the feature of a tumor. You might look at the location of a tumor. You have to consistently do that exercise with different types of imaging data over and over again in order to day the algorithm is reproducible. I think that's where we're starting to see that we have to continuously be able to prove that this algorithm is accurate and identifiable with other data settings. Hugo:    Look, in all honesty, having this conversation makes me realize even more how important it is to de-mystify these things, particularly because there are so many touch points, right, of where AI can have an impact in health, as you've mentioned, everything from administrative tasks to the scut work, the insurance industry, to all of these diagnostics. Before we move on, I'm wondering if there are any other examples or super interesting use cases to your mind? Arnaub:    Yeah. Absolutely. The second, I think, bedrock, lighthouse that we're seeing a lot of is around diagnosis prediction. How do you think about new variables that you haven't unearthed within data that might contribute to the progression of treatment. Arnaub:    We're actually working with several clients in this space right now on coming up with novel prognostic variables that could lead to the progression of a disease, perhaps predicting earlier onset of the disease. I think what makes this compelling is there's still a great amount of misunderstanding, there's still a great amount of unmet needs that we're not characterizing within our patient population. If we're able to better understand using machine learning methods on who those patients might be, we might be able to do incredible things in how we're getting them in and out of the hospital, seeing a provider quicker. Arnaub:    A great example of this is: Emory just released a study on sepsis where they looked at 42 thousand patients, and they looked at 65 different measure that might be predictive on the onset of sepsis. They looked at within different time intervals, so within four, six, eight, and 12 hours. What was cool about this is that they were able to come up with the same model and the same level of accuracy as a physician in predicting sepsis with a validation cohort between the doctor and the tool that the algorithm was basically indistinguishable. This is an example of not machine versus physician. It was more that we have the ability to not only confirm and corroborate what physicians are finding. If we continuously refine this, we might find more measures that are more predictive on sepsis. Arnaub:    The one other example I want to share with you actually just came out last week, and this was in the Journal of the American Medical Association, a pretty top tier publication. This was looking at a randomized trial of 500 and some patients with staph infections. They looked at patients over a six year period, and they found that an algorithm did just as well as doctors in suggesting how to treat them with antibiotics. What makes this really compelling is that they were able to say that patients who were on certain antibiotic protocols were on a drug for a certain number of days. They might have been on a certain number of drugs for a fewer number of days. You basically are looking at how we can think about antibiotic protocols and what is the best practice for keeping patients in and out of the hospital. I think that's where you're starting to see a lot of compelling evidence, and given that this is now appearing in top tier medical journals, it's not something that's in the future. These are things that are happening right now. Hugo:    And something you've hinted at several times already is that we're not necessarily ... there's a false dichotomy between humans and machines, essentially, right? Arnaub:    Yeah. Hugo:    And what is more interesting, I think, is the idea of human algorithmic interaction. The idea of having artificial intelligence and machine learning models with humans in the loop. Do you see this being part of the future of AI in health care? Arnaub:    I guess this is your robots versus physicians conversation? Hugo:    Sure. Arnaub:    Yeah, I think there are a few ... I'll give you two funny anecdotes that are perpetuating this theory of whether we ... We hear a lot of statements around whether physicians are going to be replaced by doctors. One example is medical students are actually reportedly not specializing in radiology because they fear that the job market won't even exist anymore within 10 years. Arnaub:    The other examples is there's a really interesting company in China called IFLYTEK, that is a pretty substantial Chinese AI company. It was the first machine to pass a medical exam, and it scored substantially higher than the student body population. When you hear these types of statements, and then you see all the JAMA evidence or New England Journal evidence of doctors being at the same level as a machine, there's going to be a lot of conversation. It also demonstrates how far machine learning has actually come. Arnaub:    I think there's a few thing that make me believe that we're not at a place where physicians are anywhere close to being replaced. One is that a lot of these AI systems, like if you take the radiology example, they perform what's called narrow AI. These are single tasks, and they're being programmed, and the deep learning models are being set for specific image recognition tasks, so detecting a nodule or looking at a chest CT and looking for a hemorrhage. These are N of one tasks, and they're binary and either yes or no. I think if we keep tasks in this narrow detection focus, we're going to find a lot of interesting things, but what that means is that these are going to be augmenting tools. They're going to help physicians improve diagnostic accuracy, but as we all know, physicians do quite a variety of tasks. There is a substantial amount of mental labor that goes into how physicians diagnose patients. Arnaub:    In the short term, I think we're looking for AI to power a lot of solutions that can reduce costs and improve accuracy and augment physician's decision making. I don't see it replacing the tremendous work that docs do or our providers do any time soon. Hugo:    Yeah, and I love that you mention narrow AI which, as you said, is algorithms, AI models, to solve specific tasks. I think in the cultural consciousness when people hear AI, they don't think of narrow, weak AI. They think of a strong AI, which reflects human cognition in some sense, and that isn't even necessarily what we want in most places and what we're working towards. Right? Arnaub:    Mm-hmm (affirmative). Yeah. That's right. Exactly. It has to be more expansive. I think the other things that's worth mentioning is that there has to be ... we talked about this already, but the consistency and the portability in the models has to happen. We're still a long way from integrating this into physician decision making. I think different vendors are focused on different deep learning algorithms and a variety of different use cases. Even certain things, we'll get to this, they're being approved by the FDA, but they have completely different focal points. Until we can start to standardize a lot of that, it's going to take some time. To your point of narrow versus much more expansive thinking of AI, that's also part of the equation, and then how do we actually make this reproducible. Hugo:    Something that you've mentioned several times is that a lot of the power we see now is from deep learning. You mentioned the use of convolutional neural networks. I'll just step back a bit, and for those people who want a bit of de-mystification of deep learning, deep learning is ... and correct me if I'm wrong at any point ... is a subclass of machine learning and mostly in supervised learning where you're trying to predict something. This particular type of supervised learning model, which is inspired loosely form neural networks in our physiological systems, in our brains.
Hugo:    Convolutional neural networks are ones that are quite good at picking out patterns in images, essentially. It uses a convolutional technique to do so. Of course, AI pre-dates convolutional neural networks, and although they're very strong at the moment, I'm sure you've seen trends emerge and dissolve. I'm just wondering if you could speak to how the moving parts of data science, ML and AI, have evolved in the health care space since you've been working in it? Arnaub:    Yeah, and I think if we're going to generalize it to health care, there's quite a bit ... there's a whole variety of different models and sophistication of those modes that are being thrown at different problems. I think at it's very basic level, a lot of early applications of AI in health care were focused on the relationships between diagnoses and medications. Some of the more basic techniques, such as association rule mining or supervised learning, those were meant to come up and find and extract important associations. There are a lot of limitations to those methods, so I think if you look at our methods, they were only looking at item level co-occurrences. They weren't really to abstractions at a higher level. There wasn't a lot of usefulness for data exploration or clinical decision support. Arnaub:    And I think if you look at supervised learning techniques, they are tackling these problems from a prediction perspective. If we have the right level of data, we can come up with more non-predictive applications. The things like disease group identifications or patient stratification. There are things that can happen as data becomes much more usable, I guess, for lack of a better word. I think that's where we'll actually be able to see supervised learning become much more applicable, going from small data sets with few observations into much more massive examples. There's great work, for example being done at Stanford and UCFS where they've looked at 100s of thousands of patients over 10 years of longitudinality, billions of observations, and come up with sophisticated deep learning neural networks. I think that's where you're starting to see the far reaching applications of AI. Arnaub:    In other cases, we're still working on the data problem, which is that we're getting enough data to make this interesting, but the sophistication of certain models or methods may not be there because the data's, quite frankly, not that good. Hugo:    All that being said, what does the future of data science, ML and AI, in health care looks like to you? Arnaub:    Yeah, so I think there's a lot of applications that we haven't talked about yet. I think we've picked on the two easy ones in terms of what's happened and what's been working - disease diagnosis prediction and then on the imaging. I think there's quite a bit of work in terms of what's happening in drug development. The fact that we are looking at companies now ... there are exciting startups that are doing this that are focused on things like drug repurposing where they're using real world data and machine learning algorithms to explore the relationships between drug molecules and disease. That is extremely compelling. That's where you're starting to see a lot of funding go into, especially from biotech and pharma, there are companies like BenevolentAI and Numerate and others that are using deep learning to mine a quite vast amount of data to look at everything from scientific papers, clinical trials, they're effectively just trying to just understand which compounds can be more effective at targeting disease. Arnaub:    These are the types of things that I think are getting quite a bit of investment, but we haven't seen the fruits of the labor yet. I mentioned Benevolent. They started identifying hypothesis around ALS treatments, and you know this is just a start, but it's starting to narrow down which drug targets or which compounds to go after. It not only saves a tremendous amount of time for biotech and pharma, it also expedites the drug development process. I think that's one example. Arnaub:    There are really interesting and powerful examples of genomic data that we haven't talked about yet, so DeepVariant, if I go back to Google for one sec, DeepVariant is an open source tool that was about two years of work between Google Brain and Verily, which is Google's life science arm. What they effectively are able to do is come up with a more sophisticated statistical approach to spot mutations and filter out errors. What DeepVariant does, it changes to whole task of variant calling, which is trying to figure out which base pairs are part of you and they're not part of some kind of processing artifact. It turns it into an image classification problem. Deep variant is starting to replace and outperform these basic biology tools, like GATK and SAM tools, and reducing the amount of error by up to 10 times. Arnaub:    This is just, I think, in the beginning stages. Even companies like Google will tell you that their genomics work is a couple years out, considering this one took two years to build. But I'm extremely excited about that type of potential. There are other examples around physician burnout and the advent of voice technology within health care, where we're starting understand that doctors spend an enormous amounts of time on EHR, electronic health record data entry, and if we're able to use machine learning and natural language processing and voice technology in the future, then we start to auto populate structure fields within records, make the doctor's job less burdensome, reduce physician documentation burden. Those are three use cases that I think are on the frontier. They're areas that there's a lot of hype and interest and really amazing work happening, but that's a short list of where I see the future going. Hugo:    Great. Before I go on, there are actually a few interesting questions from the crowd, and Gamal has asked a question, what about liability? I actually want to frame that in terms of thinking about the future of data science and ML and AI in healthcare, and in particular the fact that a lot of the algorithms we've discussed are essentially black box algorithms in terms of it's difficult to see why they make the predictions that they do. So in terms of interpretability versus black box, maybe you can discuss that with respect to, I suppose, liability for the models that we as data scientists build. Arnaub:    Yeah, I think that's an incredibly important question. One thing I want to talk about it is the policy space in terms of where the future is. This notion of FDA approved algorithms is actually starting to happen. What we're seeing right now is this lack of consistency, transferability in the current models because they focus on different end points, they are done in a black box setting where it's data in, we're not really sure what comes out. I think what that means is that regulatory bodies are going to intervene, albeit in a positive way. Arnaub:    As an example, the American College of Radiology is actually helping to provide methodologies for vendors to verify the effectiveness of the algorithm before they're taken to market. I think that's one example. The other example: about accepting algorithms and approving them is part of diagnosis. They gave a positive vote to a decision support tool that uses an algorithm for neurovascular disease. They did the same thing for diabetic retinopathy in April, and then they did something for a computer aided tool that helps with wrist fractures in adult patients. These are all the FDA's permitting the marketplace to be open. They are allowing algorithms to actually help providers in a regulated way. Arnaub:    There's actually really cool stuff happening within the White House and the House Oversight Information Technology Committee. If you guys are extremely bored, you should read this really ominous report called ""Rise of the Machines"" that the House Oversight Committee just put out. It's basically how is the NIH going to ensure that there's standardization within algorithms. The same thing with the White House. They put out a really interesting plan from the government to build up AI in an ethical way. I think the black box problem is going to continue to happen. We've already seen it be problematic for some large companies. We need to be able to address that, and although we don't love government intervention, I think this is one instance where we're actually seeing a lot of positive things come out of it. Hugo:    Following up on this, we've actually got a great question from someone in the audience called Daniel about ethical questions in general: Do ethical questions in data science have a much bigger impact on AI in health care, and are there moves towards developing ethical guidelines for researchers in this space? You've spoken to that we respective to top down. I'm also wondering about practice from within the data science community. What type of stakeholders will hold data scientists accountable? And also, the fact that in marketing, for example, or advertising perhaps... if you show someone the wrong ad, that doesn't have such an important impact as giving someone the wrong diagnosis, right? Are there things that are particularly valuable in the health space? Arnaub:    Yeah, so I think what we're seeing is how do we standardize an ontology for a disease, and that's an evolving question. There are academic consortiums that are focused on reproducing these phenotypes. So a phenotype is basically how are we characterizing a patient and their respective disease. If academic groups and organizations come together to say this is a generally accepted algorithm, this is how we can avoid erroneous cancer treatment advice, or this is how we see this is an unsafe or incorrect treatment recommendation, I think that will actually compel more folks to work within certain parameters and build algorithm that are within guidelines and practice. Otherwise, it's incredibly easy to find signal within a lot of health data noise. I think there are companies that have suffered some hard lessons that way. I think as long as we are working with organizations that are attempting to do this, that's one way of tackling this. Arnaub:    I think the other thing is health data is incredibly inconsistent, and there is a national subcommittee called HL-7, which is a health data standards committee. They are really making a strong push for something called FHIR, which is Fast Health Care Interoperability Resourcing. It's trying to create a standard where data is no longer somebody's competitive advantage, but it's something that everyone can use, and it's something that is standardized for everyone. You're not just looking at inconsistent standards. Arnaub:    The Centers for Medicare/Medicaid services are really trying to push standard ontologies. I think FHIR and these other organizations are trying to create a consistency behind all the chaos and the noise.
Hugo:    Fantastic. That actually answers the next question that we had from the audience, which is from David, which concerns research into the policy implications of AI in health care, and in particular, will FHIR have any impact on AI implementation. That's great that you were able to answer that question even before I framed it.
Hugo:    Another question, I'll just remind people who are here but listening and watching today to send through questions in the chat as soon as they come to mind. I've got a really interesting question from William, which we hinted at earlier, but William says ""What I notice is that a big part of the hype is focused on the R&D side of health care. For example, image analysis, drug discovery. What the the hypes, and more important, the promising applications on the manufacturing side?"" Arnaub:    Yeah, so that's a good question. I assume that's in relation to drug development, sorry, within pharmacos? Hugo:    Yeah, exactly. Arnaub:    Yeah, so I think we're starting to see a lot of activity in this space. It's a little bit more nuanced, but in terms of how manufacturing is trying to tackle this, I think we are now in a place where the ability to standardize and create better understanding of how drug cycling takes place, where the supply chain can be optimized. I think that's where there are companies like BERG, for example, that is using AI for not only research applications but for manufacturing. It's something that I come across less, but something that is still popular. I think there are ways to think about unsupervised learning methods in terms of how we're trying to understand drug circulation and where we can improve our supply chain efforts. Arnaub:    There is actually a cool effort from the UK Royal Society that is looking at the role of machine learning in bio-manufacturing. Can we actually help optimize the time factor here, like helping manufacturers reduce the time to produce drugs, lower costs, improve replication? Yes, still very much a popular topic. It's not something that we've avoided, but discovery is where I've seen a lot of the money and interest flow to right now. Hugo:    We've got several questions coming through about, I suppose, the ethical nature of using AI and ML and data science in health care. The first one I want to ask you is from a listener called James. James says written given the black box nature of AI and the proprietary nature of industry, how will external validation and reproducibility of algorithms be assessed? He also says, basically, where does open science fit in for the commercial AI space? Arnaub:    Yeah, that's a good question. I think what we need to do is come up with more of an interdisciplinary, multi-stakeholder way of evaluating different algorithms that are entering the marketplace. You have these large top down institutions like the FDA that are evaluating the ability for a physician to utilize an algorithm in practice. I think there are other organizations at the academic level that are more interdisciplinary. A great one is called OHDSI, which is the observational health data sciences and informatics group. What they're attempting to do, and this is actually a collaboration between pharma and academics and startups. One thing that they've done that I think is really important is they've created a common data model for health care. They've looked at disparate observational health care databases and decided that EMRs are really important for supporting clinical care. Databases like clean data are important for reimbursement, but they both serve different purposes. We need to create a common data model that accommodates both of these different types of data. Arnaub:    This CDM, through a partnership called OMOP, which stands for the Observational Medical Outcomes Partnership, it basically is trying to take all this noise from random coding systems and create one standardized ontology and vocabulary. That's one way of trying to get buy in from other people, multiple players, interdisciplinary players. That's, I think, something that helps with the ethical challenge. Arnaub:    OHDSI is an organization that actually works on reproducing and publishing all this research. All of it's open source. They've created a number of software tools like Atlas and Achilles that are standardized databases for profiling different data and data quality. This is not something that we're going to solve overnight. I think regulatory bodies are going to be very judicious about what they approve and what they don't approve. What tends to happen in health care is as soon as there's some sort of adverse event, or there's some kind of clinical error, you're going to see the entire industry get slapped. Nobody wants that to happen. I think that's what's stifling for innovation. Arnaub:    We are, hopefully, trying to ... It's weird, the world that we sit in right now where we're trying to get as much AI related work out there as possible, also being very mindful that a successful deployment of this, once it enters the physician's hands, or it starts to be part of patient care, is incredibly challenging. That's what the next evolution of all of this stuff is, it's the very careful implementation of these algorithm into a clinical decision model. Hugo:    I'm glad that you mentioned regulation there because we have a related question from Stephen, the question is from a regulatory perspective, is it or will it be enough to prove the efficacy of an algorithm or model, or is a fully, full-functional description required? Arnaub:    Yeah, I don't know if we have proper guidance around that. I think there are a lot of organizations that are trying to demystify how the AI, or how the FDA should be thinking about this. A few things that are standard in terms of how much do you have to prove something. One is that what is your benchmark data? And are you using ... are you using ground truth data? Meaning is it a trusted claims data source? Is it a standardized ontology for EHR data? I think companies grapple with choosing a whole variety of different data sources and collection methods. Then they realize that their algorithm isn't that good, or they're hoping for its approval. There are good definitions on what ground truth is. That's one way of creating a really strong model. Arnaub:    I think there are other ways of thinking about what's the intended use of the algorithm. How do we think about this interfacing with a physician? Does it have any downstream patient implications? Is there going to be algorithm bias? Meaning are you going to deny care to a certain patient population? That's something that the FDA considers in terms of how it considers it to be ethical or not ethical. Arnaub:    Then I think there's a whole regulatory approach on refitting models so that they're continuously learning. Scott Gottlieb, who runs the FDA, has talked a lot about how will patients evolve, and how do we think about when companies have to do a refit of the model and how it should be validated. What is the right refitting cadence? Is it every hour? Is it six months? Is it annually? I feel like a few organizations have taken a stab at trying to create these lists of guiding light questions that can help us come up with a good model versus a subpar model and one that's more likely to be implemented and accepted by the clinical community versus ones that have a great finding but may have some holes in them. Hugo:    In terms of the regulation, as well we've got a great question from Harsh. Harsh asks that due to health care being a regulated area, do you feel that AI and health care will be focused area for only big companies like Google and Facebook to be able to make advances? Or do you feel small companies have a space? You mentioned earlier that there have been several hundred startups in this space in particular. Maybe you can speak a bit more about it? Arnaub:    Sure. I think it's a really interesting question. I think ... I'll give you one example that I think raised a to of eyebrows. To your point about big tech companies like Google. Facebook actually announced earlier this year that they are looking at using AI to monitor suicide and understand which one of their users might be more likely to commit a suicidal event. That is a highly ethically challenging place for a tech company to play in. Arnaub:    I think that large tech companies, while they have great applications and great computer science, are treading very carefully because they realize that this isn't something that you can just wade into. I think they actually have a tremendous amount of organizational risk as they enter this market. One, because patient care is totally different than selling ads. I think the ability for them to use their science and use the computational power comes at a great risk. They have to evaluate whether it's worth it or not, but they all want to make the world a better place. That's their aspiration. Arnaub:    With other companies, there are so many powerful tech companies. I mentioned Voice Technologies in this space. Companies like Virtual Assistance and Voice Technology are going to be major players in this space. And I don't just mean Amazon, but I mean companies like Orbita and others are doing incredible work, Robin AI, they are basically trying to help reduce physician documentation burden. These are well funded, well capitalized, strongly supported startups that are doing great things. There is patient data and risk analytics. There's companies like NarrativeDx that are doing really compelling work. They're partnering directly with health care systems to do this in a safe and compliant way, so I don't think large tech companies are the only one that can play in this space. Arnaub:    I think if you are very calculated, methodical about how you enter, if you engage in the right partnerships, a startup ... I mean, there are quite a few startups that have made a lot of headway in this space, in the drug discovery world. These are startups that have raised hundreds of millions of dollars, literally, that are now functioning quite well and successfully. Pharma companies are making multi-year bets on companies like Numerate and BenevolentAI and investing quite a bit of money. This is not just a large tech company space anymore. Hugo:    There are actually an overwhelming number of fantastic questions coming through, but we're going to need to wrap up in the next 10 minutes or so. I've got two more questions. The first is from Christopher, which I'm very interested in. Christopher asks what are the main limitations on the rate of adoption on AI in health care? Arnaub:    Yeah. That's a good question. We talked a little bit about what are the policy hurdles, and we talked about how do we think about approaches. I think what is the biggest rate limiting step in health care is going to be the ubiquity of good quality data. This is the biggest challenge that I think has been plaguing health care for decades now, is that as soon as a novel data set is released into the health care world, everybody gets really excited about it. As soon as the government made standards for EHR, electronic health records, EHR became competitive domain for any organization that had the record. The challenge was getting access to that data. It's the same thing with genomics now. We're starting to see bio-banks and the ability to have genomically sequenced data, genetically sequenced data. That is the next domain. That's where people are trying to get to, but none of this matters unless the data is linkable, unless there is a standard, unless there is labeled data. We talk a lot about imaging today, but radiologists suffer from the fact that imaging data is stored within these pax warehouses, pax is the archiving system, and then they're not labeled. We don't know what we're necessarily looking at. What that all goes to show is that the biggest hurdle for AI adoption in health care is good quality data, which is why I mentioned standards like FHIR and others trying to create some kind of harmony and consistency in the data in a very chaotic world. Arnaub:    I think the other thing is that hospitals and other organizations that have the data are very much willing to work with players, but there's quite a bit of overlap in terms of what companies are promising. We're starting to see a lot of companies tread into different spaces and say that they're doing compound development or they're looking at molecule identification or target validation. They're trying to be jack of all trades. I think that is obfuscating what the company actually does. Arnaub:    My suggestion is just having a very clear, refined focus on what you think you are doing and what you're good at versus trying to then wade into a lot of other murky waters. With that said, I mean, the market is so hot right now that you will see a tremendous amount of partnership and opportunity for startups. The biggest, limiting step is access to the data, finding the right partner, being able to demonstrate a use case, and then the application of that algorithm within clinical practice. Hugo:    I've got one more question from one of our listeners, and then I'm going ask you one final question. This is from Gamal, and it's a relatively general question that I'd like you to interpret in whatever way you see fit. Do you think artificial intelligence will democratize medicine? Arnaub:    Oh, interesting. I think that we will get to a place where ... I'm going to be liberal about the use of the world democratize. I think what that means is enable access to care, perhaps, or maybe that's the definition that we'll choose to use. I think patients are increasingly interfacing with the health system in different ways, and the fact that the majority, the vast majority, of patients go online to look up health information, almost 90% now. The fact that there is still ... there's a lot of ways for patients to share their data now with physicians, with technology companies. We all know the work that Apple's doing in its work in health kit and research kit to try to get more access to data. I think there is going to be some greater role for AI, and maybe for technology to help with access to care. And hopefully I'm addressing your question, but feel free to rephrase. Arnaub:    At the same time the US is suffering from tremendous endemic health policy challenges that I don't think AI will fix. I think AI will enable and help certain things. It will maybe power diagnosis. Maybe it will improve better health outcomes over time. There's still a good portion of the population that will never be AI enabled, for lack of a better word, or have access to health care resources. I think that's the greatest hurdle in our system. Hugo:    That definitely does answer the question. My final question for you is: for all our listeners out there, do you have a final call to action? Arnaub:    Yeah. I think we've talked a lot about challenges. We've also talked a lot about promise and where the industry is going. I think this concept of fixing a lot of low-hanging fruit problems, we've picked on a lot of sexier things like drug development, but our health care system suffers from tremendous waste. These are enormous problems, and AI can fix a lot of these things, like insurance and billing claims. My old mentors used to say that a lot of the most lucrative work in health care is also the least sexy and the back office application type stuff. Arnaub:    If we're able to do things like predict better waste or fraud, or if we're able to improve billing and documentation processes, these are incredibly important problems to go after, and I think there's something there. You should use AI and your powers to go fix them. Arnaub:    I think the other thing is that these problems shouldn't be done in isolation, or fixed in isolation. You are going to see a lot of different and perhaps unique partnerships in health care take place. Hospitals and tech companies and patient groups working with startups. I think that whole model has flipped on its head. I encourage everyone to be very inventive about how they want to work with different parties. There are a lot of non-traditional folks that are getting into the health care space, so think about where the intersections lie and where the cross-functionality lies. That's where you usually find more inventive solutions as opposed to working through the same channels. Hugo:    Thanks, Arnaub. Arnaub:    Yeah, thank you, Hugo. Thanks to DataCamp for the time. I really appreciate the opportunity.
Hugo:    Absolutely. All right. Arnaub:    Thank you.","Keyword(freq): company(33), patient(16), term(15), case(14), algorithm(13), model(9), startup(9), task(9), application(8), method(8)"
"4","mastery",2018-11-02,"LSTM Model Architecture for Rare Event Time Series Forecasting","https://machinelearningmastery.com/lstm-model-architecture-for-rare-event-time-series-forecasting/","Time series forecasting with LSTMs directly has shown little success. This is surprising as neural networks are known to be able to learn complex non-linear relationships and the LSTM is perhaps the most successful type of recurrent neural network that is capable of directly supporting multivariate sequence prediction problems. A recent study performed at Uber AI Labs demonstrates how both the automatic feature learning capabilities of LSTMs and their ability to handle input sequences can be harnessed in an end-to-end model that can be used for drive demand forecasting for rare events like public holidays. In this post, you will discover an approach to developing a scalable end-to-end LSTM model for time series forecasting. After reading this post, you will know: Let¡¯s get started. In this post, we will review the 2017 paper titled ¡°Time-series Extreme Event Forecasting with Neural Networks at Uber¡± by Nikolay Laptev, et al. presented at the Time Series Workshop, ICML 2017. This post is divided into four sections; they are: The goal of the work was to develop an end-to-end forecast model for multi-step time series forecasting that can handle multivariate inputs (e.g. multiple input time series). The intent of the model was to forecast driver demand at Uber for ride sharing, specifically to forecast demand on challenging days such as holidays where the uncertainty for classical models was high. Generally, this type of demand forecasting for holidays belongs to an area of study called extreme event prediction. Extreme event prediction has become a popular topic for estimating peak electricity demand, traffic jam severity and surge pricing for ride sharing and other applications. In fact there is a branch of statistics known as extreme value theory (EVT) that deals directly with this challenge. <U+2014> Time-series Extreme Event Forecasting with Neural Networks at Uber, 2017. Two existing approaches were described: The difficulty of these existing models motivated the desire for a single end-to-end model. Further, a model was required that could generalize across locales, specifically across data collected for each city. This means a model trained on some or all cities with data available and used to make forecasts across some or all cities. We can summarize this as the general need for a model that supports multivariate inputs, makes multi-step forecasts, and generalizes across multiple sites, in this case cities. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course The model was fit in a propitiatory Uber dataset comprised of five years of anonymized ride sharing data across top cities in the US. A five year daily history of completed trips across top US cities in terms of population was used to provide forecasts across all major US holidays. <U+2014> Time-series Extreme Event Forecasting with Neural Networks at Uber, 2017. The input to each forecast consisted of both the information about each ride, as well as weather, city, and holiday variables. To circumvent the lack of data we use additional features including weather information (e.g., precipitation, wind speed, temperature) and city level information (e.g., current trips, current users, local holidays). <U+2014> Time-series Extreme Event Forecasting with Neural Networks at Uber, 2017. The figure below taken from the paper provides a sample of six variables for one year. Scaled Multivariate Input for ModelTaken from ¡°Time-series Extreme Event Forecasting with Neural Networks at Uber¡±. A training dataset was created by splitting the historical data into sliding windows of input and output variables. The specific size of the look-back and forecast horizon used in the experiments were not specified in the paper. Sliding Window Approach to Modeling Time SeriesTaken from ¡°Time-series Extreme Event Forecasting with Neural Networks at Uber¡±. Time series data was scaled by normalizing observations per batch of samples and each input series was de-trended, but not deseasonalized. Neural networks are sensitive to unscaled data, therefore we normalize every minibatch. Furthermore, we found that de-trending the data, as opposed to de-seasoning, produces better results. <U+2014> Time-series Extreme Event Forecasting with Neural Networks at Uber, 2017. LSTMs, e.g. Vanilla LSTMs, were evaluated on the problem and show relatively poor performance. This is not surprising as it mirrors findings elsewhere. Our initial LSTM implementation did not show superior performance relative to the state of the art approach. <U+2014> Time-series Extreme Event Forecasting with Neural Networks at Uber, 2017. A more elaborate architecture was used, comprised of two LSTM models: An LSTM autoencoder model was developed for use as the feature extraction model and a Stacked LSTM was used as the forecast model. We found that the vanilla LSTM model¡¯s performance is worse than our baseline. Thus, we propose a new architecture, that leverages an autoencoder for feature extraction, achieving superior performance compared to our baseline. <U+2014> Time-series Extreme Event Forecasting with Neural Networks at Uber, 2017. When making a forecast, time series data is first provided to the autoencoders, which is compressed to multiple feature vectors that are averaged and concatenated. The feature vectors are then provided as input to the forecast model in order to make a prediction. ¡¦ the model first primes the network by auto feature extraction, which is critical to capture complex time-series dynamics during special events at scale. [¡¦] Features vectors are then aggregated via an ensemble technique (e.g., averaging or other methods). The final vector is then concatenated with the new input and fed to LSTM forecaster for prediction. <U+2014> Time-series Extreme Event Forecasting with Neural Networks at Uber, 2017. It is not clear what exactly is provided to the autoencoder when making a prediction, although we may guess that it is a multivariate time series for the city being forecasted with observations prior to the interval being forecasted. A multivariate time series as input to the autoencoder will result in multiple encoded vectors (one for each series) that could be concatenated. It is not clear what role averaging may take at this point, although we may guess that it is an averaging of multiple models performing the autoencoding process. Overview of Feature Extraction Model and Forecast ModelTaken from ¡°Time-series Extreme Event Forecasting with Neural Networks at Uber.¡± The authors comment that it would be possible to make the autoencoder a part of the forecast model, and that this was evaluated, but the separate model resulted in better performance. Having a separate auto-encoder module, however, produced better results in our experience. <U+2014> Time-series Extreme Event Forecasting with Neural Networks at Uber, 2017. More details of the developed model were made available in the slides used when presenting the paper. The input for the autoencoder was 512 LSTM units and the bottleneck in the autoencoder used to create the encoded feature vectors as 32 or 64 LSTM units. Details of LSTM Autoencoder for Feature ExtractionTaken from ¡°Time-series Extreme Event Forecasting with Neural Networks at Uber.¡± The encoded feature vectors are provided to the forecast model with ¡®new input¡®, although it is not specified what this new input is; we could guess that it is a time series, perhaps a multivariate time series of the city being forecasted with observations prior to the forecast interval. Or, features extracted from this series as the blog post on the paper suggests (although I¡¯m skeptical as the paper and slides contradict this). The model was trained on a lot of data, which is a general requirement of stacked LSTMs or perhaps LSTMs in general. The described production Neural Network Model was trained on thousands of time-series with thousands of data points each. <U+2014> Time-series Extreme Event Forecasting with Neural Networks at Uber, 2017. The model is not retrained when making new forecasts. An interesting approach to estimating forecast uncertainty was also implemented that used the bootstrap. It involved estimating model uncertainty and forecast uncertainty separately, using the autoencoder and the forecast model respectively. Inputs were provided to a given model and dropout of the activations (as commented in the slides) was used. This process was repeated 100 times, and the model and forecast error terms were used in an estimate of the forecast uncertainty. Overview of Forecast Uncertainty EstimationTaken from ¡°Time-series Extreme Event Forecasting with Neural Networks at Uber.¡± This approach to forecast uncertainty may be better described in the 2017 paper ¡°Deep and Confident Prediction for Time Series at Uber.¡± The model was evaluated with a special focus on demand forecasting for U.S. holidays by U.S. city. The specifics of the model evaluation were not specified. The new generalized LSTM forecast model was found to outperform the existing model used at Uber, which may be impressive if we assume that the existing model was well tuned. The results presented show a 2%-18% forecast accuracy improvement compared to the current proprietary method comprising a univariate timeseries and machine learned model. <U+2014> Time-series Extreme Event Forecasting with Neural Networks at Uber, 2017. The model trained on the Uber dataset was then applied directly to a subset of the M3-Competition dataset comprised of about 1,500 monthly univariate time series forecasting datasets. This is a type of transfer learning, a highly-desirable goal that allows the reuse of deep learning models across problem domains. Surprisingly, the model performed well, not great compared to the top performing methods, but better than many sophisticated models. The result is suggests that perhaps with fine tuning (e.g. as is done in other transfer learning case studies) the model could be reused and be skillful. Performance of LSTM Model Trained on Uber Data and Evaluated on the M3 DatasetsTaken from ¡°Time-series Extreme Event Forecasting with Neural Networks at Uber.¡± Importantly, the authors suggest that perhaps the most beneficial application of deep LSTM models to time series forecasting are situations where: From our experience there are three criteria for picking a neural network model for time-series: (a) number of timeseries (b) length of time-series and (c) correlation among the time-series. If (a), (b) and (c) are high then the neural network might be the right choice, otherwise classical timeseries approach may work best. <U+2014> Time-series Extreme Event Forecasting with Neural Networks at Uber, 2017. This is summarized well by a slide used in the presentation of the paper. Lessons Learned Applying LSTMs for Time Series ForecastingTaken from ¡°Time-series Extreme Event Forecasting with Neural Networks at Uber¡± Slides. This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered a scalable end-to-end LSTM model for time series forecasting. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hi, Is there a way to identify and remove outliers from data sets without affecting rare events?
Or how not to mistakenly have outliers as rare events ?
Thanks Vali You must carefully define what you mean by ¡°outlier¡± and ¡°rare event¡± so that the methods that detect the former don¡¯t detect the latter. Outliers usually are anomalies which are abnormal ie. outside a normal distribution. Something like mean+/-2*std. in a time series outliers are sparks, with much higher freq than the normal signal even with rare events.
For instance a Black Friday is rare event but fits in the normal frequency whereas an outlier is much higher frequency.
So how can I bring the frequency part in the equation?
Thanks Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): series(23), network(21), model(8), lstm(7), holiday(6), vector(6), event(5), forecast(4), outlier(4), result(4)"
"5","mastery",2018-10-31,"Results From Comparing Classical and Machine Learning Methods for Time Series Forecasting","https://machinelearningmastery.com/findings-comparing-classical-and-machine-learning-methods-for-time-series-forecasting/","Machine learning and deep learning methods are often reported to be the key solution to all predictive modeling problems. An important recent study evaluated and compared the performance of many classical and modern machine learning and deep learning methods on a large and diverse set of more than 1,000 univariate time series forecasting problems. The results of this study suggest that simple classical methods, such as linear methods and exponential smoothing, outperform complex and sophisticated methods, such as decision trees, Multilayer Perceptrons (MLP), and Long Short-Term Memory (LSTM) network models. These findings highlight the requirement to both evaluate classical methods and use their results as a baseline when evaluating any machine learning and deep learning methods for time series forecasting in order demonstrate that their added complexity is adding skill to the forecast. In this post, you will discover the important findings of this recent study evaluating and comparing the performance of a classical and modern machine learning methods on a large and diverse set of time series forecasting datasets. After reading this post, you will know: Let¡¯s get started. Findings Comparing Classical and Machine Learning Methods for Time Series ForecastingPhoto by Lyndon Hatherall, some rights reserved. Spyros Makridakis, et al. published a study in 2018 titled ¡°Statistical and Machine Learning forecasting methods: Concerns and ways forward.¡± In this post, we will take a close look at the study by Makridakis, et al. that carefully evaluated and compared classical time series forecasting methods to the performance of modern machine learning methods. This post is divided into seven sections; they are: The goal of the study was to clearly demonstrate the capability of a suite of different machine learning methods as compared to classical time series forecasting methods on a very large and diverse collection of univariate time series forecasting problems. The study was a response to the increasing number of papers and claims that machine learning and deep learning methods offer superior results for time series forecasting with little objective evidence. Literally hundreds of papers propose new ML algorithms, suggesting methodological advances and accuracy improvements. Yet, limited objective evidence is available regarding their relative performance as a standard forecasting tool. <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. The authors clearly lay out three issues with the flood of claims; they are: As a response, the study includes eight classical methods and 10 machine learning methods evaluated using one-step and multiple-step forecasts across a collection of 1,045 monthly time series. Although not definitive, the results are intended to be objective and robust. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course The time series datasets used in the study were drawn from the time series datasets used in the M3-Competition. The M3-Competition was the third in a series of competitions that sought to discover exactly what algorithms perform well in practice on real time series forecasting problems. The results of the competition were published in the 2000 paper titled ¡°The M3-Competition: Results, Conclusions and Implications.¡± The datasets used in the competition were drawn from a wide range of industries and had a range of different time intervals, from hourly to annual. The 3003 series of the M3-Competition were selected on a quota basis to include various types of time series data (micro, industry, macro, etc.) and different time intervals between successive observations (yearly, quarterly, etc.). The table below, taken from the paper, provides a summary of the 3,003 datasets used in the competition. Table of Datasets, Industry and Time Interval Used in the M3-CompetitionTaken from ¡°The M3-Competition: Results, Conclusions and Implications.¡± The finding of the competition was that simpler time series forecasting methods outperform more sophisticated methods, including neural network models. This study, the previous two M-Competitions and many other empirical studies have proven, beyond the slightest doubt, that elaborate theoretical constructs or more sophisticated methods do not necessarily improve post-sample forecasting accuracy, over simple methods, although they can better fit a statistical model to the available historical data. <U+2014> The M3-Competition: Results, Conclusions and Implications, 2000. The more recent study that we are reviewing in this post that evaluate machine learning methods selected a subset of 1,045 time series with a monthly interval from those used in the M3 competition. ¡¦ evaluate such performance across multiple forecasting horizons using a large subset of 1045 monthly time series used in the M3 Competition. <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. The study evaluates the performance of eight classical (or simpler) methods and 10 machine learning methods. ¡¦ of eight traditional statistical methods and eight popular ML ones, [¡¦], plus two more that have become popular during recent years. <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. The eight classical methods evaluated were as follows: A total of eight machine learning methods were used in an effort to reproduce and compare to results presented in the 2010 paper ¡°An Empirical Comparison of Machine Learning Models for Time Series Forecasting.¡± They were: An additional two ¡®modern¡® neural network algorithms were also added to the list given the recent rise in their adoption; they were: A careful data preparation methodology was used, again, based on the methodology described in the 2010 paper ¡°An Empirical Comparison of Machine Learning Models for Time Series Forecasting.¡± In that paper, each time series was adjusted using a power transform, deseasonalized and detrended. [¡¦] before computing the 18 forecasts, they preprocessed the series in order to achieve stationarity in their mean and variance. This was done using the log transformation, then deseasonalization and finally scaling, while first differences were also considered for removing the component of trend. <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. Inspired by these operations, variations of five different data transforms were applied for an MLP for one-step forecasting and their results were compared. The five transforms were: Generally, it was found that the best approach was to apply a power transform and deseasonalize the data, and perhaps detrend the series as well. The best combination according to sMAPE is number 7 (Box-Cox transformation, deseasonalization) while the best one according to MASE is number 10 (Box-Cox transformation, deseasonalization and detrending) <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. All models were evaluated using one-step time series forecasting. Specifically, the last 18 time steps were used as a test set, and models were fit on all remaining observations. A separate one-step forecast was made for each of the 18 observations in the test set, presumably using a walk-forward validation method where true observations were used as input in order to make each forecast. The forecasting model was developed using the first n <U+2013> 18 observations, where n is the length of the series. Then, 18 forecasts were produced and their accuracy was evaluated compared to the actual values not used in developing the forecasting model. <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. Reviewing the results, the MLP and BNN were found to achieve the best performance from all of the machine learning methods. The results [¡¦] show that MLP and BNN outperform the remaining ML methods. <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. A surprising result was that RNNs and LSTMs were found to perform poorly. It should be noted that RNN is among the less accurate ML methods, demonstrating that research progress does not necessarily guarantee improvements in forecasting performance. This conclusion also applies in the performance of LSTM, another popular and more advanced ML method, which does not enhance forecasting accuracy too. <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. Comparing the performance of all methods, it was found that the machine learning methods were all out-performed by simple classical methods, where ETS and ARIMA models performed the best overall. This finding confirms the results from previous similar studies and competitions. Bar Chart Comparing Model Performance (sMAPE) for One-Step ForecastsTaken from ¡°Statistical and Machine Learning forecasting methods: Concerns and ways forward.¡± Multi-step forecasting involves predicting multiple steps ahead of the last known observation. Three approaches to multi-step forecasting were evaluated for the machine learning methods; they were: The classical methods were found to outperform the machine learning methods again. In this case, methods such as Theta, ARIMA, and a combination of exponential smoothing (Comb) were found to achieve the best performance. In brief, statistical models seem to generally outperform ML methods across all forecasting horizons, with Theta, Comb and ARIMA being the dominant ones among the competitors according to both error metrics examined. <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. The study provides important supporting evidence that classical methods may dominate univariate time series forecasting, at least on the types of forecasting problems evaluated. The study demonstrates the worse performance and the increase in computational cost of machine learning and deep learning methods for univariate time series forecasting for both one-step and multi-step forecasts. These findings strongly encourage the use of classical methods, such as ETS, ARIMA, and others as a first step before more elaborate methods are explored, and requires that the results from these simpler methods be used as a baseline in performance that more elaborate methods must clear in order to justify their usage. It also highlights the need to not just consider the careful use of data preparation methods, but to actively test multiple different combinations of data preparation schemes for a given problem in order to discover what works best, even in the case of classical methods. Machine learning and deep learning methods may still achieve better performance on specific univariate time series problems and should be evaluated. The study does not look at more complex time series problems, such as those datasets with: The study concludes with an honest puzzlement at why machine learning methods perform so poorly in practice, given their impressive performance in other areas of artificial intelligence. The most interesting question and greatest challenge is to find the reasons for their poor performance with the objective of improving their accuracy and exploiting their huge potential. AI learning algorithms have revolutionized a wide range of applications in diverse fields and there is no reason that the same cannot be achieved with the ML methods in forecasting. Thus, we must find how to be applied to improve their ability to forecast more accurately. <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. Comments are made by the authors regarding LSTMs and RNNs, that are generally believed to be the deep learning approach for sequence prediction problems in general, and in this case their clearly poor performance in practice. [¡¦] one would expect RNN and LSTM, which are more advanced types of NNs, to be far more accurate than the ARIMA and the rest of the statistical methods utilized. <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. They comment that LSTMs appear to be more suited at fitting or overfitting the training dataset rather than forecasting it. Another interesting example could be the case of LSTM that compared to simpler NNs like RNN and MLP, report better model fitting but worse forecasting accuracy <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. There is work to do and machine learning methods and deep learning methods hold the promise of better learning time series data than classical statistical methods, and even doing so directly on the raw observations via automatic feature learning. Given their ability to learn, ML methods should do better than simple benchmarks, like exponential smoothing. Accepting the problem is the first step in devising workable solutions and we hope that those in the field of AI and ML will accept the empirical findings and work to improve the forecasting accuracy of their methods. <U+2014> Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018. This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered the important findings of a recent study evaluating and comparing the performance of classical and modern machine learning methods on a large and diverse set of time series forecasting datasets. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Thanks for posting this. I recently started going through a course on Time Series forecasting and this just confirms my need to understand the more basic and standard statistical methods before I look at deep-learning techniques. You can get started with the basics here:https://machinelearningmastery.com/start-here/#timeseries Cool, like this write up. One question, it seems like it was all just univariate models am I right? Would be really curious to see something similar but using multivariate data and models. Yes, just univariate data/models.  I agree, I similar study for multivariate datasets would be fascinating! Very informatinve thanks Jason. Confirmed what I¡¯ve always assumed that more variables are probably required to give deep learning an upper hand in TS prediction.
I¡¯m surprised that you don¡¯t know of any comparative studies involving the latter? Agreed! Such studies may exist, but I have not come across them, at least not at this level of robustness. Appreciate the insightful article! Maybe this was stated in the readings, but it seems like for each time series, no work was done on feature selection / feature engineering, i.e. the inputs to both the traditional models and the ML models consisted solely of the time series¡¯ own historical values?  Could you confirm if this is true? All of the data was univariate, only one feature to select. Thanks for this. Juicy! Have you a ¡®go to¡¯ reference for survey of issues and methods for (quoting you) Complex irregular temporal structures.
Missing observations
Heavy noise.
Complex interrelationships between multiple variates. Thank you again ¡¦would gladly repay your effort with a coffee or drink any time you¡¯re in the DMV. Thanks! Not at this stage. The benchmarking has been done for various models with just time series as one of the input component and typical output as predictive component is that correct, if so whats the value plotted on the right side, and why is it mentioned as the basic statistic methods to be compartively better, can you throw some light on it ¡°Right side¡±?  I don¡¯t follow, can you elaborate please? Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): method(73), concern(15), result(15), model(14), dataset(9), problem(8), observation(7), finding(6), algorithm(4), forecast(4)"
"6","mastery",2018-10-29,"How to Develop Deep Learning Models for Univariate Time Series Forecasting","https://machinelearningmastery.com/how-to-develop-deep-learning-models-for-univariate-time-series-forecasting/","Deep learning neural networks are capable of automatically learning and extracting features from raw data. This feature of neural networks can be used for time series forecasting problems, where models can be developed directly on the raw observations without the direct need to scale the data using normalization and standardization or to make the data stationary by differencing. Impressively, simple deep learning neural network models are capable of making skillful forecasts as compared to naive models and tuned SARIMA models on univariate time series forecasting problems that have both trend and seasonal components with no pre-processing. In this tutorial, you will discover how to develop a suite of deep learning models for univariate time series forecasting. After completing this tutorial, you will know: Let¡¯s get started. How to Develop Deep Learning Models for Univariate Time Series ForecastingPhoto by Nathaniel McQueen, some rights reserved. This tutorial is divided into five parts; they are: The ¡®monthly car sales¡® dataset summarizes the monthly car sales in Quebec, Canada between 1960 and 1968. You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®monthly-car-sales.csv¡® in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). Once loaded, we can summarize the shape of the dataset in order to determine the number of observations. We can then create a line plot of the series to get an idea of the structure of the series. We can tie all of this together; the complete example is listed below. Running the example first prints the shape of the dataset. The dataset is monthly and has nine years, or 108 observations. In our testing, will use the last year, or 12 observations, as the test set. A line plot is created. The dataset has an obvious trend and seasonal component. The period of the seasonal component could be six months or 12 months. Line Plot of Monthly Car Sales From prior experiments, we know that a naive model can achieve a root mean squared error, or RMSE, of 1841.155 by taking the median of the observations at the three prior years for the month being predicted; for example: Where the negative indexes refer to observations in the series relative to the end of the historical data for the month being predicted. From prior experiments, we know that a SARIMA model can achieve an RMSE of 1551.842 with the configuration of SARIMA(0, 0, 0),(1, 1, 0),12 where no elements are specified for the trend and a seasonal difference with a period of 12 is calculated and an AR model of one season is used. The performance of the naive model provides a lower bound on a model that is considered skillful. Any model that achieves a predictive performance of lower than 1841.155 on the last 12 months has skill. The performance of the SARIMA model provides a measure of a good model on the problem. Any model that achieves a predictive performance lower than 1551.842 on the last 12 months should be adopted over a SARIMA model. Now that we have defined our problem and expectations of model skill, we can look at defining the test harness. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this section, we will develop a test harness for developing and evaluating different types of neural network models for univariate time series forecasting. This section is divided into the following parts: The first step is to split the loaded series into train and test sets. We will use the first eight years (96 observations) for training and the last 12 for the test set. The train_test_split() function below will split the series taking the raw observations and the number of observations to use in the test set as arguments. Next, we need to be able to frame the univariate series of observations as a supervised learning problem so that we can train neural network models. A supervised learning framing of a series means that the data needs to be split into multiple examples that the model learn from and generalize across. Each sample must have both an input component and an output component. The input component will be some number of prior observations, such as three years or 36 time steps. The output component will be the total sales in the next month because we are interested in developing a model to make one-step forecasts. We can implement this using the shift() function on the pandas DataFrame. It allows us to shift a column down (forward in time) or back (backward in time). We can take the series as a column of data, then create multiple copies of the column, shifted forward or backward in time in order to create the samples with the input and output elements we require. When a series is shifted down, NaN values are introduced because we don¡¯t have values beyond the start of the series. For example, the series defined as a column: Can be shifted and inserted as a column beforehand: We can see that on the second row, the value 1 is provided as input as an observation at the prior time step, and 2 is the next value in the series that can be predicted, or learned by the model to be predicted when 1 is presented as input. Rows with NaN values can be removed. The series_to_supervised() function below implements this behavior, allowing you to specify the number of lag observations to use in the input and the number to use in the output for each sample. It will also remove rows that have NaN values as they cannot be used to train or test a model. Time series forecasting models can be evaluated on a test set using walk-forward validation. Walk-forward validation is an approach where the model makes a forecast for each observation in the test dataset one at a time. After each forecast is made for a time step in the test dataset, the true observation for the forecast is added to the test dataset and made available to the model. Simpler models can be refit with the observation prior to making the subsequent prediction. More complex models, such as neural networks, are not refit given the much greater computational cost. Nevertheless, the true observation for the time step can then be used as part of the input for making the prediction on the next time step. First, the dataset is split into train and test sets. We will call the train_test_split() function to perform this split and pass in the pre-specified number of observations to use as the test data. A model will be fit once on the training dataset for a given configuration. We will define a generic model_fit() function to perform this operation that can be filled in for the given type of neural network that we may be interested in later. The function takes the training dataset and the model configuration and returns the fit model ready for making predictions. Each time step of the test dataset is enumerated. A prediction is made using the fit model. Again, we will define a generic function named model_predict() that takes the fit model, the history, and the model configuration and makes a single one-step prediction. The prediction is added to a list of predictions and the true observation from the test set is added to a list of observations that was seeded with all observations from the training dataset. This list is built up during each step in the walk-forward validation, allowing the model to make a one-step prediction using the most recent history. All of the predictions can then be compared to the true values in the test set and an error measure calculated. We will calculate the root mean squared error, or RMSE, between predictions and the true values. RMSE is calculated as the square root of the average of the squared differences between the forecasts and the actual values. The measure_rmse() implements this below using the mean_squared_error() scikit-learn function to first calculate the mean squared error, or MSE, before calculating the square root. The complete walk_forward_validation()<U+00A0>function that ties all of this together is listed below. It takes the dataset, the number of observations to use as the test set, and the configuration for the model, and returns the RMSE for the model performance on the test set. Neural network models are stochastic. This means that, given the same model configuration and the same training dataset, a different internal set of weights will result each time the model is trained that will in turn have a different performance. This is a benefit, allowing the model to be adaptive and find high performing configurations to complex problems. It is also a problem when evaluating the performance of a model and in choosing a final model to use to make predictions. To address model evaluation, we will evaluate a model configuration multiple times via walk-forward validation and report the error as the average error across each evaluation. This is not always possible for large neural networks and may only make sense for small networks that can be fit in minutes or hours. The repeat_evaluate() function below implements this and allows the number of repeats to be specified as an optional parameter that defaults to 30 and returns a list of model performance scores: in this case, RMSE values. Finally, we need to summarize the performance of a model from the multiple repeats. We will summarize the performance first using summary statistics, specifically the mean and the standard deviation. We will also plot the distribution of model performance scores using a box and whisker plot to help get an idea of the spread of performance. The summarize_scores() function below implements this, taking the name of the model that was evaluated and the list of scores from each repeated evaluation, printing the summary and showing a plot. Now that we have defined the elements of the test harness, we can tie them all together and define a simple persistence model. Specifically, we will calculate the median of a subset of prior observations relative to the time to be forecasted. We do not need to fit a model so the model_fit() function will be implemented to simply return None. We will use the config to define a list of index offsets in the prior observations relative to the time to be forecasted that will be used as the prediction. For example, 12 will use the observation 12 months ago (-12) relative to the time to be forecasted. The model_predict() function can be implemented to use this configuration to collect the observations, then return the median of those observations. The complete example of using the framework with a simple persistence model is listed below. Running the example prints the RMSE of the model evaluated using walk-forward validation on the final 12 months of data. The model is evaluated 30 times, although, because the model has no stochastic element, the score is the same each time. We can see that the RMSE of the model is 1841, providing a lower-bound of performance by which we can evaluate whether a model is skillful or not on the problem. Box and Whisker Plot of Persistence RMSE Forecasting Car Sales Now that we have a robust test harness, we can use it to evaluate a suite of neural network models. The first network that we will evaluate is a multilayer Perceptron, or MLP for short. This is a simple feed-forward neural network model that should be evaluated before more elaborate models are considered. MLPs can be used for time series forecasting by taking multiple observations at prior time steps, called lag observations, and using them as input features and predicting one or more time steps from those observations. This is exactly the framing of the problem provided by the series_to_supervised() function in the previous section. The training dataset is therefore a list of samples, where each sample has some number of observations from months prior to the time being forecasted, and the forecast is the next month in the sequence. For example: The model will attempt to generalize over these samples, such that when a new sample is provided beyond what is known by the model, it can predict something useful; for example: We will implement a simple MLP using the Keras deep learning library. The model will have an input layer with some number of prior observations. This can be specified using the input_dim argument when we define the first hidden layer. The model will have a single hidden layer with some number of nodes, then a single output layer. We will use the rectified linear activation function on the hidden layer as it performs well. We will use a linear activation function (the default) on the output layer because we are predicting a continuous value. The loss function for the network will be the mean squared error loss, or MSE, and we will use the efficient Adam flavor of stochastic gradient descent to train the network. The model will be fit for some number of training epochs (exposures to the training data) and batch size can be specified to define how often the weights are updated within each epoch. The model_fit() function for fitting an MLP model on the training dataset is listed below. The function expects the config to be a list with the following configuration hyperparameters: Making a prediction with a fit MLP model is as straightforward as calling the predict() function and passing in one sample worth of input values required to make the prediction. In order to make a prediction beyond the limit of known data, this requires that the last n known observations are taken as an array and used as input. The predict() function expects one or more samples of inputs when making a prediction, so providing a single sample requires the array to have the shape [1, n_input], where n_input is the number of time steps that the model expects as input. Similarly, the predict() function returns an array of predictions, one for each sample provided as input. In the case of one prediction, there will be an array with one value. The model_predict() function below implements this behavior, taking the model, the prior observations, and model configuration as arguments, formulating an input sample and making a one-step prediction that is then returned. We now have everything we need to evaluate an MLP model on the monthly car sales dataset. A simple grid search of model hyperparameters was performed and the configuration below was chosen. This may not be an optimal configuration, but is the best that was found. This configuration can be defined as a list: Note that when the training data is framed as a supervised learning problem, there are only 72 samples that can be used to train the model. Using a batch size of 72 or more means that the model is being trained using batch gradient descent instead of mini-batch gradient descent. This is often used for small datasets and means that weight updates and gradient calculations are performed at the end of each epoch, instead of multiple times within each epoch. The complete code example is listed below. Running the example prints the RMSE for each of the 30 repeated evaluations of the model. At the end of the run, the average and standard deviation RMSE are reported of about 1,526 sales. We can see that, on average, the chosen configuration has better performance than both the naive model (1841.155) and the SARIMA model (1551.842). This is impressive given that the model operated on the raw data directly without scaling or the data being made stationary. A box and whisker plot of the RMSE scores is created to summarize the spread of the performance for the model. This helps to understand the spread of the scores. We can see that although on average the performance of the model is impressive, the spread is large. The standard deviation is a little more than 134 sales, meaning a worse case model run that is 2 or 3 standard deviations in error from the mean error may be worse than the naive model. A challenge in using the MLP model is in harnessing the higher skill and minimizing the variance of the model across multiple runs. This problem applies generally for neural networks. There are many strategies that you could use, but perhaps the simplest is simply to train multiple final models on all of the available data and use them in an ensemble when making predictions, e.g. the prediction is the average of 10-to-30 models. Box and Whisker Plot of Multilayer Perceptron RMSE Forecasting Car Sales Convolutional Neural Networks, or CNNs, are a type of neural network developed for two-dimensional image data, although they can be used for one-dimensional data such as sequences of text and time series. When operating on one-dimensional data, the CNN reads across a sequence of lag observations and learns to extract features that are relevant for making a prediction. We will define a CNN with two convolutional layers for extracting features from the input sequences. Each will have a configurable number of filters and kernel size and will use the rectified linear activation function. The number of filters determines the number of parallel fields on which the weighted inputs are read and projected. The kernel size defines the number of time steps read within each snapshot as the network reads along the input sequence. A max pooling layer is used after convolutional layers to distill the weighted input features into those that are most salient, reducing the input size by 1/4. The pooled inputs are flattened to one long vector before being interpreted and used to make a one-step prediction. The CNN model expects input data to be in the form of multiple samples, where each sample has multiple input time steps, the same as the MLP in the previous section. One difference is that the CNN can support multiple features or types of observations at each time step, which are interpreted as channels of an image. We only have a single feature at each time step, therefore the required three-dimensional shape of the input data will be [n_samples, n_input, 1]. The model_fit() function for fitting the CNN model on the training dataset is listed below. The model takes the following five configuration parameters as a list: Making a prediction with the fit CNN model is very much like making a prediction with the fit MLP model in the previous section. The one difference is in the requirement that we specify the number of features observed at each time step, which in this case is 1. Therefore, when making a single one-step prediction, the shape of the input array must be: The model_predict() function below implements this behavior. A simple grid search of model hyperparameters was performed and the configuration below was chosen. This is not an optimal configuration, but is the best that was found. The chosen configuration is as follows: This can be specified as a list as follows: Tying all of this together, the complete example is listed below. Running the example first prints the RMSE for each repeated evaluation of the model. At the end of the run, we can see that indeed the model is skillful, achieving an average RMSE of 1,524.067, which is better than the naive model, the SARIMA model, and even the MLP model in the previous section. This is impressive given that the model operated on the raw data directly without scaling or the data being made stationary. The standard deviation of the score is large, at about 57 sales, but is 1/3 the size of the variance observed with the MLP model in the previous section. We have some confidence that in a bad-case scenario (3 standard deviations), the model RMSE will remain below (better than) the performance of the naive model. A box and whisker plot of the scores is created to help understand the spread of error across the runs. We can see that the spread does seem to be biased towards larger error values, as we would expect, although the upper whisker of the plot (in this case, the largest error that are not outliers) is still limited at an RMSE of 1,650 sales. Box and Whisker Plot of Convolutional Neural Network RMSE Forecasting Car Sales Recurrent neural networks, or RNNs, are those types of neural networks that use an output of the network from a prior step as an input in attempt to automatically learn across sequence data. The Long Short-Term Memory, or LSTM, network is a type of RNN whose implementation addresses the general difficulties in training RNNs on sequence data that results in a stable model. It achieves this by learning the weights for internal gates that control the recurrent connections within each node. Although developed for sequence data, LSTMs have not proven effective on time series forecasting problems where the output is a function of recent observations, e.g. an autoregressive type forecasting problem, such as the car sales dataset. Nevertheless, we can develop LSTM models for autoregressive problems and use them as a point of comparison with other neural network models. In this section, we will explore three variations on the LSTM model for univariate time series forecasting; they are: The LSTM neural network can be used for univariate time series forecasting. As an RNN, it will read each time step of an input sequence one step at a time. The LSTM has an internal memory allowing it to accumulate internal state as it reads across the steps of a given input sequence. At the end of the sequence, each node in a layer of hidden LSTM units will output a single value. This vector of values summarizes what the LSTM learned or extracted from the input sequence. This can be interpreted by a fully connected layer before a final prediction is made. Like the CNN, the LSTM can support multiple variables or features at each time step. As the car sales dataset only has one value at each time step, we can fix this at 1, both when defining the input to the network in the input_shape argument [n_input, 1], and in defining the shape of the input samples. Unlike the MLP and CNN that do not read the sequence data one-step at a time, the LSTM does perform better if the data is stationary. This means that difference operations are performed to remove the trend and seasonal structure. In the case of the car sales dataset, we can make the data stationery by performing a seasonal adjustment, that is subtracting the value from one year ago from each observation. This can be performed systematically for the entire training dataset. It also means that the first year of observations must be discarded as we have no prior year of data to difference them with. The difference() function below will difference a provided dataset with a provided offset, called the difference order, e.g. 12 for one year of months prior. We can make the difference order a hyperparameter to the model and only perform the operation if a value other than zero is provided. The model_fit() function for fitting an LSTM model is provided below. The model expects a list of five model hyperparameters; they are: Making a prediction with the LSTM model is the same as making a prediction with a CNN model. A single input must have the three-dimensional structure of samples, timesteps, and features, which in this case we only have 1 sample and 1 feature: [1, n_input, 1]. If the difference operation was performed, we must add back the value that was subtracted after the model has made a forecast. We must also difference the historical data prior to formulating the single input used to make a prediction. The model_predict() function below implements this behavior. A simple grid search of model hyperparameters was performed and the configuration below was chosen. This is not an optimal configuration, but is the best that was found. The chosen configuration is as follows: This can be specified as a list: Tying all of this together, the complete example is listed below. Running the example, we can see the RMSE for each repeated evaluation of the model. At the end of the run, we can see that the average RMSE is about 2,109, which is worse than the naive model. This suggests that the chosen model is not skillful, and it was the best that could be found given the same resources used to find model configurations in the previous sections. This provides further evidence (although weak evidence) that LSTMs, at least alone, are perhaps a bad fit for autoregressive-type sequence prediction problems. A box and whisker plot is also created summarizing the distribution of RMSE scores. Even the base case for the model did not achieve the performance of a naive model. Box and Whisker Plot of Long Short-Term Memory Neural Network RMSE Forecasting Car Sales We have seen that the CNN model is capable of automatically learning and extracting features from the raw sequence data without scaling or differencing. We can combine this capability with the LSTM where a CNN model is applied to sub-sequences of input data, the results of which together form a time series of extracted features that can be interpreted by an LSTM model. This combination of a CNN model used to read multiple subsequences over time by an LSTM is called a CNN-LSTM model. The model requires that each input sequence, e.g. 36 months, is divided into multiple subsequences, each read by the CNN model, e.g. 3 subsequence of 12 time steps. It may make sense to divide the sub-sequences by years, but this is just a hypothesis, and other splits could be used, such as six subsequences of six time steps. Therefore, this splitting is parameterized with the n_seq and n_steps for the number of subsequences and number of steps per subsequence parameters. The number of lag observations per sample is simply (n_seq * n_steps). This is a 4-dimensional input array now with the dimensions: The same CNN model must be applied to each input subsequence. We can achieve this by wrapping the entire CNN model in a TimeDistributed layer wrapper. The output of one application of the CNN submodel will be a vector. The output of the submodel to each input subsequence will be a time series of interpretations that can be interpreted by an LSTM model. This can be followed by a fully connected layer to interpret the outcomes of the LSTM and finally an output layer for making one-step predictions. The complete model_fit() function is listed below. The model expects a list of seven hyperparameters; they are: Making a prediction with the fit model is much the same as the LSTM or CNN, although with the addition of splitting each sample into subsequences with a given number of time steps. The updated model_predict() function is listed below. A simple grid search of model hyperparameters was performed and the configuration below was chosen. This may not be an optimal configuration, but it is the best that was found. We can define the configuration as a list; for example: The complete example of evaluating the CNN-LSTM model for forecasting the univariate monthly car sales is listed below. Running the example prints the RMSE for each repeated evaluation of the model. The final averaged RMSE is reported at the end of about 1,626, which is lower than the naive model, but still higher than a SARIMA model. The standard deviation of this score is also very large, suggesting that the chosen configuration may not be as stable as the standalone CNN model. A box and whisker plot is also created summarizing the distribution of RMSE scores. The plot shows one single outlier of very poor performance just below 3,000 sales. Box and Whisker Plot of CNN-LSTM RMSE Forecasting Car Sales It is possible to perform a convolutional operation as part of the read of the input sequence within each LSTM unit. This means, rather than reading a sequence one step at a time, the LSTM would read a block or subsequence of observations at a time using a convolutional process, like a CNN. This is different to first reading an extracting features with an LSTM and interpreting the result with an LSTM; this is performing the CNN operation at each time step as part of the LSTM. This type of model is called a Convolutional LSTM, or ConvLSTM for short. It is provided in Keras as a layer called ConvLSTM2D for 2D data. We can configure it for use with 1D sequence data by assuming that we have one row with multiple columns. As with the CNN-LSTM, the input data is split into subsequences where each subsequence has a fixed number of time steps, although we must also specify the number of rows in each subsequence, which in this case is fixed at 1. The shape is five-dimensional, with the dimensions: Like the CNN, the ConvLSTM layer allows us to specify the number of filter maps and the size of the kernel used when reading the input sequences. The output of the layer is a sequence of filter maps that must first be flattened before it can be interpreted and followed by an output layer. The model expects a list of seven hyperparameters, the same as the CNN-LSTM; they are: The model_fit() function that implements all of this is listed below. A prediction is made with the fit model in the same way as the CNN-LSTM, although with the additional rows dimension that we fix to 1. The model_predict() function for making a single one-step prediction is listed below. A simple grid search of model hyperparameters was performed and the configuration below was chosen. This may not be an optimal configuration, but is the best that was found. We can define the configuration as a list; for example: We can tie all of this together. The complete code listing for the ConvLSTM model evaluated for one-step forecasting of the monthly car sales dataset is listed below. Running the example prints the RMSE for each repeated evaluation of the model. The final averaged RMSE is reported at the end of about 1,660, which is lower than the naive model, but still higher than a SARIMA model. It is a result that is perhaps on par with the CNN-LSTM model. The standard deviation of this score is also very large, suggesting that the chosen configuration may not be as stable as the standalone CNN model. A box and whisker plot is also created, summarizing the distribution of RMSE scores. Box and Whisker Plot of ConvLSTM RMSE Forecasting Car Sales This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop a suite of deep learning models for univariate time series forecasting. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hello . Andy. Ukraine. Kiev. In your example  CNN LSTM (whole code model) I have error.   File ¡°C:\Users\User\Dropbox\DeepLearning3\realmodproj\project\educationNew\sub2\step_005.py¡±, line 89, in model_fit
    model.add(TimeDistributed(Conv1D(filters=n_filters, kernel_size=n_kernel, activation=¡¯relu¡¯, input_shape=(None,n_steps,1))))
  File ¡°C:\Users\User\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\models.py¡±, line 454, in add
    raise ValueError(¡®The first layer in a ¡®
ValueError: The first layer in a Sequential model must get an input_shape or batch_input_shape argument.  Coud you explame me what the reason ? Thank you. I believe you need to update your version of Keras to 2.2.4 or higher. Using TensorFlow backend.
1.8.0 I recommend using tensorflow 1.11.0 or higher. Andy , Kiev May be i should use ?
model.add(TimeDistributed(   Conv1D(filters=n_filters, kernel_size=n_kernel, activation=¡¯relu¡¯,  input_shape=(None,n_steps,1) ) , input_shape=(None,n_steps,1) )  ) Sure, that was required for older versions of Keras. So, in a nutshell, RNN doesn¡¯t work very well when dealing with time series forecasting? Correct, they will be outperformed by linear methods on univariate data in most cases. Also see this:https://machinelearningmastery.com/findings-comparing-classical-and-machine-learning-methods-for-time-series-forecasting/ Thanks for the article
I have a univariate time series classification problem. Data comes from the accelerometer sensor (vibration analysis). only X-axis. and the sampling rate is 2.5 KHz and I need a real-time classification.
what¡¯s your suggestion for classifying this time series? I want to differentiate between multiple labeled classes (failures).
which of your articles can help me? I read all of them but many of them are not univariate and this article which is univariate is about forecasting and not classification.
thanks indeed. Sounds like time series classification. You could look at the HAR articles, for example:https://machinelearningmastery.com/how-to-develop-rnn-models-for-human-activity-recognition-time-series-classification/ Hi Jason and thanks for all of your amazing turorials on ML!
I am new to time-series forecasting, but I have been learning a lot from your articles.  I have a issue were am trying to forecast the amount of tickets sold per day, but I would like to the forecast 60 days before the actual date. I have 3 years of historical data on the tickets sold per day.  Testing the above models on my data, the persistence model scored the best (I think) with a 4.041 RMSE, the next best was the MLP with 7.501 (+/- 0.589)  RMSE.  Do you have any recommendation on how to proceed doing the forecast for my data?  Thanks again for your great work! Nice work. Yes, what is the problem that you are having exactly? Hey, Say I have show, and 60 days before the show I want to forecast how many tickets sold during those 60days.(t+60) If I look at the the 60days as one time period, the forecast would not consider seasonal effects, which have a big effect on the tickets sold. Therfore, I think I need to use each day during those 60 days as a period for a more accurate forecast.  So, I want to forecast the tickest sold each day during a 60 day period, but I want to know the forecast before that 60 day period starts.  And the output of the model is a list of 60 elements representing the expected ticket sale for each day  [0,1,7,5,3¡¦,3] My problem is choosing the correct approach for such forecast. Do you have any tips? That sounds like a very challenging problem. We cannot know the best approach. You must discover the best approach through experimentation with a suite of different methods. This might help:https://machinelearningmastery.com/how-to-develop-a-skilful-time-series-forecasting-model/ Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): observation(33), model(21), sale(21), step(12), feature(11), value(11), hyperparameter(9), network(9), score(9), prediction(8)"
"7","vidhya",2018-11-01,"Top 5 Machine Learning GitHub Repositories & Reddit Discussions (October 2018)","https://www.analyticsvidhya.com/blog/2018/11/best-machine-learning-github-repositories-reddit-threads-october-2018/","¡°Should I use GitHub for my projects?¡± <U+2013> I¡¯m often asked this question by aspiring data scientists. There¡¯s only one answer to this <U+2013> ¡°Absolutely!¡±. GitHub is an invaluable platform for data scientists looking to stand out from the crowd. It¡¯s an online resume for displaying your code to recruiters and other fellow professionals. The fact that GitHub hosts open-source projects from the top tech behemoths like Google, Facebook, IBM, NVIDIA, etc. is what adds to the gloss of an already shining offering. If you¡¯re a beginner in data science, or even an established professional, you should have a GitHub account. And to save you the time of looking for the most interesting repositories out there (and there are plenty), I am delighted to scour the platform and bring them straight to you in this monthly series. This month¡¯s collection comes from a variety of use cases <U+2013> computer vision (object detection and segmentation), PyTorch implementation of Google AI¡¯s record-breaking BERT framework for NLP, extracting the latest research papers with their summaries, among others. Scroll down to start learning! Why do we include Reddit discussions in this series? I have personally found Reddit an incredibly rewarding platform for a number of reasons <U+2013> rich content, top machine learning/deep learning experts taking the time to propound their thoughts, a stunning variety of topics, open-source resources, etc. I could go on all day, but suffice to say I highly recommend going through these threads I have shortlisted <U+2013> they are unique and valuable in their own way. You can check out the top GitHub repositories and Reddit discussions (from April onwards) we have covered each month below: Computer vision has become so incredibly popular these days that organizations are rushing to implement and integrate the latest algorithms in their products. Sounds like a pretty compelling reason to jump on the bandwagon, right? Of course, object detection is easily the most sought-after skill to learn in this domain. So here¡¯s a really cool project from Facebook that aims to provide the building blocks for creating segmentation and detection models using the their popular PyTorch 1.0 framework. Facebook claims that this is upto two times faster than it¡¯s Detectron framework, and comes with pre-trained models. Enough resources and details to get started! I encourage you to check out a step-by-step introduction to the basic object detection algorithms if you need a quick refresher. And if you¡¯re looking to get familiar with the basics of PyTorch, check out this awesome beginner-friendly tutorial. This repository is a goldmine for all deep learning enthusiasts. Intrigued by the heading? Just wait till you check out some numbers about this dataset:<U+00A0>17,609,752 training and 88,739 validation image URLs, which are annotated with up to 11,166 categories. Incredible! This project also include a pre-trained Resnet-101 model, which has so far achieved a 80.73% accuracy on ImageNet via transfer learning. The repository contains exhaustive details and code on how and where to get started. This is a significant step towards making high quality data available to the community. Oh, and did I mentioned that these images are annotated? What are you waiting for, go ahead and download it NOW! Wait, another PyTorch entry? Just goes to show how popular this framework has become. For those who haven¡¯t heard of BERT, it¡¯s a language representation model that stands for<U+00A0>Bidirectional Encoder Representations from Transformers. It sounds like a mouthful, but it has been making waves in the machine learning community. BERT has set all sorts of new benchmarks in 11 natural language processing (NLP) tasks. A pre-trained language model being used on a wide range of NLP tasks might sound outlandish to some, but the BERT framework has transformed it into reality. It even emphatically outperformed humans on the popular SQuAD question answering test. This repository contains the PyTorch code for implementing BERT on your own machine. As Google Brain¡¯s Research Scientist Thang Luong tweeted, this could well by the beginning of a new era in NLP. In case you¡¯re interested in reading the research paper, that¡¯s also available here. And in case you¡¯re eager (like me) to see the official Google code, bookmark (or star)<U+00A0>this repository. How can we stay on top of the latest research in machine learning? It seems we see breakthroughs on an almost weekly basis and keeping up with them is a daunting, if not altogether impossible, challenge. Most top researchers post their full papers on arxiv.org so is there any way of sorting through the latest ones? Yes, there is! This repository uses Python (v3.x) to return the latest results by scraping arxiv papers and summarizing their abstracts. This is an really useful tool, as it will help us stay in touch with the latest papers and let us pick the one(s) we want to read. As mentioned in the repository, you can run the below command to search for a keyword: The script returns five results by default if you fail to specify how many instances you want. I always try to include at least one reinforcement learning repository in these lists <U+2013> primarily because I feel everyone in this field should be aware of the latest advancements in this space. And this month¡¯s entry is a fascinating one <U+2013> motion imitation with deep reinforcement learning. This repository in an implementation of the ¡°DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills¡± paper presented at SIGGRAPH 2018. Quoting from the repository, ¡°The framework uses reinforcement learning to train a simulated humanoid to imitate a variety of motion skills¡±. Check out the above project link which includes videos and code on how to implement this framework on your own. I just couldn¡¯t leave out this incredibly useful repository. AdaNet is a lightweight and scalable TensorFlow-based framework for automatically learning high-quality models. The best part about it is that you don¡¯t need to intervene too much <U+2013> the framework is smart and flexible enough for building better models. You can read more about AdaNet here. Google, as usual, does a great job of explaining complex concepts. Ah, the question on everybody¡¯s mind. Will autoML be ruling the roost? How will the hardware have advanced? Will there finally be official rules and policies around ethics? Will machine learning have integrated itself into the very fabric of society? Will reinforcement learning finally have found a place in the industry? These are just some of the many thoughts propounded in this discussion. Individuals have their own predictions about what they expect and what they want to see, and this discussion does an excellent job of combining the two. The conversation varies between technical and non-technical topics so you have the luxury of choosing which ones you prefer reading about. Interesting topic. We¡¯ve seen this trend before <U+2013> a non-ML person is assigned to lead a team of ML experts and it usually ends in frustration for both parties. Due to various reasons (time constraints being top of that list), it often feels like things are at an impasse. I implore all project managers, leaders, CxOs, etc. to take the time and go through this discussion thread. There are some really useful ideas that you can implement in your own projects as soon as possible. Getting all the technical and non-technical folks on the same page is a crucial cog in the overall project¡¯s success so it¡¯s important that the leader(s) sets the right example. Looking for a new project to experiment with? Or need ideas for your thesis? You¡¯ve landed at the right place. This is a collection of ideas graduate students are working on to hone and fine tune their machine learning skills. Some of the ones that stood out for me are: This is where Reddit becomes so useful <U+2013> you can pitch your idea in this discussion and you¡¯ll receive feedback from the community on how you can approach the challenge. This one is a fully technical discussion as you might have gathered from the heading. This is an entirely subjective question and answers vary depending on the level of experience the reader has and how well the researcher has put across his/her thoughts. I like this discussion because there very specific examples of linked research papers so you can explore them and form your own opinion. It¡¯s a well known (and accepted) fact that quite a lot of papers have math and findings all cobbled together <U+2013> not everyone has the patience, willingness or even the ability to present their study in a lucid manner. It¡¯s always a good idea to work on your presentation skills while you can. How do established professionals feel when their field starts getting tons of attention from newbies? It¡¯s an interesting question that potentially spans domains, but this thread focuses on machine learning. This is not a technical discussion per se, but it¡¯s interesting to note how top data scientists and applied machine learning professionals feel about the recent spike in interest in this field. The discussion, with over 120+ comments, is rich in thought and suggestions. Things get especially interesting when the topic of how to deal with non-technical leaders and team members comes up. There are tons of ideas to steal from here!","Keyword(freq): idea(4), model(4), project(4), skill(4), professional(3), scientist(3), thought(3), algorithm(2), detail(2), discussion(2)"
"8","vidhya",2018-11-01,"An Introduction to Text Summarization using the TextRank Algorithm (with Python implementation)","https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/","Text Summarization is one of those applications of Natural Language Processing (NLP) which is bound to have a huge impact on our lives. With growing digital media and ever growing publishing <U+2013> who has the time to go through entire articles / documents / books to decide whether they are useful or not? Thankfully <U+2013> this technology is already here. Have you come across the mobile app inshorts? It¡¯s an innovative news app that converts news articles into a 60-word summary. And that is exactly what we are going to learn in this article<U+00A0><U+2014><U+00A0>Automatic Text Summarization. Automatic Text Summarization is one of the most challenging and interesting problems in the field of Natural Language Processing (NLP). It is a process of generating a concise and meaningful summary of text from multiple text resources such as books, news articles, blog posts, research papers, emails, and tweets. The demand for automatic text summarization systems is spiking these days thanks to the availability of large amounts of textual data.  Through this article, we will explore the realms of text summarization. We will understand how the TextRank algorithm works, and will also implement it in Python. Strap in, this is going to be a fun ride! Automatic Text Summarization gained attention as early as the 1950¡¯s. A research paper, published by Hans Peter Luhn in the late 1950s, titled ¡°The automatic creation of literature abstracts¡±, used features such as word frequency and phrase frequency to extract important sentences from the text for summarization purposes. Another important research, done by Harold P Edmundson in the late 1960¡¯s, used methods like the presence of cue words, words used in the title appearing in the text, and the location of sentences, to extract significant sentences for text summarization. Since then, many important and exciting studies have been published to address the challenge of automatic text summarization. Text summarization can broadly be divided into two categories <U+2014> Extractive Summarization and Abstractive Summarization. In this article, we will be focusing on the extractive summarization technique. Before getting started with the TextRank algorithm, there¡¯s another algorithm which we should become familiar with <U+2013> the PageRank algorithm. In fact, this actually inspired TextRank! PageRank is used primarily for ranking web pages in online search results. Let¡¯s quickly understand the basics of this algorithm with the help of an example. Source: http://www.scottbot.net/HIAL/ Suppose we have 4 web pages <U+2014> w1, w2, w3, and w4. These pages contain links pointing to one another. Some pages might have no link <U+2013> these are called dangling pages. In order to rank these pages, we would have to compute a score called the PageRank score. This score is the probability of a user visiting that page.  To capture the probabilities of users navigating from one page to another, we will create a square matrix M, having n rows and n columns, where n is the number of web pages. Each element of this matrix denotes the probability of a user transitioning from one web page to another. For example, the highlighted cell below contains the probability of transition from w1 to w2. The initialization of the probabilities is explained in the steps below:<U+00A0> Hence, in our case, the matrix M will be initialized as follows: Finally, the values in this matrix will be updated in an iterative fashion to arrive at the web page rankings. Let¡¯s understand the TextRank algorithm, now that we have a grasp on PageRank. I have listed the similarities between these two algorithms below: TextRank is an extractive and unsupervised text summarization technique. Let¡¯s take a look at the flow of the TextRank algorithm that we will be following: So, without further ado, let¡¯s fire up our Jupyter Notebooks and start coding! Note: If you want to learn more about Graph Theory, then I¡¯d recommend checking out this article. Being a major tennis buff, I always try to keep myself updated with what¡¯s happening in the sport by religiously going through as many online tennis updates as possible. However, this has proven to be a rather difficult job! There are way too many resources and time is a constraint. Therefore, I decided to design a system that could prepare a bullet-point summary for me by scanning through multiple articles. How to go about doing this? That¡¯s what I¡¯ll show you in this tutorial. We will apply the TextRank algorithm on a dataset of scraped articles with the aim of creating a nice and concise summary. Please note that this is essentially a single-domain-multiple-documents summarization task, i.e., we will take multiple articles as input and generate a single bullet-point<U+00A0>summary. Multi-domain text summarization is not covered in this article, but feel free to try that out at your end. So, without any further ado, fire up your Jupyter Notebooks and let¡¯s implement what we¡¯ve learned so far. First, import the libraries we¡¯ll be leveraging for this challenge. Now let¡¯s read our dataset. I have provided the link to download the data in the previous section (in case you missed it). Let¡¯s take a quick glance at the data. 
We have 3 columns in our dataset <U+2014> ¡®article_id¡¯, ¡®article_text¡¯, and ¡®source¡¯. We are most interested in the ¡®article_text¡¯ column as it contains the text of the articles.<U+00A0>Let¡¯s print some of the values of the variable just to see what they look like. Output: Now we have 2 options <U+2013> we can either summarize each article individually, or we can generate a single summary for all the articles. For our purpose, we will go ahead with the latter. Now the next step is to break the text into individual sentences. We will use the<U+00A0>sent_tokenize( )<U+00A0>function of the<U+00A0>nltk library to do this. Let¡¯s print a few elements of the list sentences. Output: GloVe word embeddings are vector representation of words. These word embeddings will be used to create vectors for our sentences. We could have also used the Bag-of-Words or TF-IDF approaches to create features for our sentences, but these methods ignore the order of the words (and the number of features is usually pretty large). We will be using the pre-trained Wikipedia 2014 + Gigaword 5 GloVe vectors available here. Heads up <U+2013> the size of these word embeddings is 822 MB. Let¡¯s extract the words embeddings or word vectors. We now have word vectors for 400,000 different terms stored in the dictionary <U+2013> ¡®word_embeddings¡¯. It is always a good practice to make your textual data noise-free as much as possible. So, let¡¯s do some basic text cleaning. Get rid of the<U+00A0>stopwords (commonly used words of a language <U+2013> is, am, the, of, in, etc.) present in the sentences. If you have not downloaded nltk-stopwords, then execute the following line of code: Now we can import the stopwords. Let¡¯s define a function to remove these stopwords from our dataset. We will use clean_sentences to create vectors for sentences in our data with the help of the GloVe word vectors. Now, let¡¯s create vectors for our sentences. We will first fetch vectors (each of size 100 elements) for the constituent words in a sentence and then take mean/average of those vectors to arrive at a consolidated vector for the sentence. The next step is to find similarities between the sentences, and we will use the cosine similarity approach for this challenge. Let¡¯s create an empty similarity matrix for this task and populate it with cosine similarities of the sentences. Let¡¯s first define a zero matrix of dimensions (n * n).<U+00A0> We will initialize this matrix with cosine similarity scores of the sentences. Here,<U+00A0>n is the number of sentences. We will use Cosine Similarity to compute the similarity between a pair of sentences. And initialize the matrix with cosine similarity scores. Before proceeding further, let¡¯s convert the similarity matrix sim_mat into a graph. The nodes of this graph will represent the sentences and the edges will represent the similarity scores between the sentences. On this graph, we will apply the PageRank algorithm to arrive at the sentence rankings. Finally, it¡¯s time to extract the top N sentences based on their rankings for summary generation. And there we go! An awesome, neat, concise, and useful summary for our articles. Automatic Text Summarization is a<U+00A0>hot topic of research, and in this article, we have covered just the tip of the iceberg. Going forward, we will explore the abstractive text summarization technique where deep learning plays a big role. In addition, we can also look into the following summarization tasks: Problem-specific Algorithm-specific I hope this post helped you in understanding the concept of automatic text summarization. It has a variety of use cases and has spawned extremely successful applications. Whether it¡¯s for leveraging in your business, or just for your own knowledge, text summarization is an approach all NLP enthusiasts should be familiar with. I will try to cover the<U+00A0>abstractive text summarization technique using advanced techniques in a future article. Meanwhile, feel free to use the comments section below to let me know your thoughts or ask any questions you might have on this article. Hi Prateek, Thanks for sharing. Since I¡¯m an absolute beginner, hope you don¡¯t me asking. Why did I get this error & how do I fix this? Thanks. NameError Traceback (most recent call last)
in ()
1 for s in df [¡®article_text¡¯]:
<U+2014>-> 2 sentences.append (sent_tokenize(s))
3 sentences = [y for x in sentences for y in x] #flatten list NameError: name ¡®sentences¡¯ is not defined","Keyword(freq): sentence(22), article(9), vector(9), page(7), embedding(4), feature(3), score(3), similarity(3), stopword(3), thank(3)"
"9","vidhya",2018-10-29,"DataHack Radio #13: Data Science and AI in the Oil & Gas Industry with Yogendra Pandey, Ph.D.","https://www.analyticsvidhya.com/blog/2018/10/datahack-radio-podcast-oil-gas-ai/","Did you know that the oil and gas industry is currently only using close to 1% of the data it generates? A mind-boggling figure, and not one we usually think about when talking about artificial intelligence and machine learning applications. In episode #13 of the DataHack Radio podcast, we are joined by Yogendra Narayan Pandey, Ph.D, as he takes us on a knowledge-rich journey in the world of oil and gas. This is not a field that grabs a lot of headlines in the AI and ML community, but as you¡¯ll find out in this podcast, the potential applications are vast. The amount of data collected in a typical oil and gas exploration and production process is staggeringly high, and that in turn spawns multiple use cases where machine learning techniques like regression, clustering and neural networks can be applied. Yogendra has done a phenomenal job of condensing the end-to-end oil and gas life-cycle into byte-sized knowledge for you and me to capture. It¡¯s well worth spending your time listening to this podcast and broadening your horizons. Happy listening! Subscribe to DataHack Radio today and listen to this, as well as all previous episodes, on any of the below platforms: Yogendra is the Founder and Managing Consultant at PRABUDDHA, an organization that provides AI solutions for the oil and energy industry. He<U+00A0>is a chemical engineer from IIT-Varanasi and successfully completed his Ph.D. from the University of Houston in the same field (his dissertation topic was ¡°A Simulation Approach to Thermodynamics in Interfacial Phenomena¡±). In his professional career, Yogendra has worked for organizations like Halliburton, Innowatts, and W.D. Von Gonten Laboratories. His role in all these organizations has been in the capacity of a data scientist. His passion for the oil and gas industry has driven him to pursue and make a mark in this field. In the initial part of the podcast, Yogendra has described his work in this fascinating space following his Ph.D. Anyone with an interest in data science and the energy field will really appreciate this episode! Oil and gas is a high-risk industry, so this makes the validation phase longer than usual. Decision makers have to be far more cautious, and this is one of the primary reasons why AI and ML have seen a slow adoption rate in this domain. But as Yogendra mentioned, this scenario is starting to change as technological advancements gather pace. One of the most important use cases of AI in oil and gas are predictive maintenance and equipment failure analytics. Another application is around autonomous drilling rigs, which means designing an end-to-end fully automatic drilling system. This system is smart enough to understand where to drill (optimal well path), how to drill, and the optimal duration required to finish the job. Like most AI applications these days, these autonomous rigs aim to augment the manual effort workers put in, rather than replace their jobs. To give you a very high-level overview, we can broadly divide the end-to-end oil and gas life-cycle (starting from a drop of oil found thousands of feet beneath the surface) into three major segments: For drilling operations, a large setup offshore can generate up to 1-2 terabytes of data everyday. The same goes for a large downstream refinery <U+2013> it can generate up to 1 terabyte of data per day. So if you were wondering where and how much data this industry can come up, this is a pretty good place to start! Each segment mentioned above has been explained eloquently and in detail with multiple examples by Yogendra in the podcast and trust me, the entire process is incredibly enthralling. My favorite part was about how a model can tell you whether a certain region has oil in it or not with a remarkably high accuracy rate (a probabilistic model). This helps the organization(s) decide whether it¡¯s worth drilling in that region. Unsupervised learning techniques like clustering are heavily leveraged in this process. Other algorithms used by data scientists in this domain for forecasting include regression, Hidden Markov Models for time series, Recurrent Neural Networks, Gated Recurrent Units (GRUs), and Long Short Term Memory (LSTMs), among others.","Keyword(freq): application(3), case(2), network(2), organization(2), rig(2), technique(2), advancement(1), algorithm(1), analytics(1), episode(1)"
"10","vidhya",2018-10-29,"An Intuitive Guide to Interpret a Random Forest Model using fastai library (Machine Learning for Programmers <U+2013> Part 2)","https://www.analyticsvidhya.com/blog/2018/10/interpret-random-forest-model-machine-learning-programmers/","Machine Learning is a fast evolving field <U+2013> but a few things would remain as they were years ago. One such thing is ability to interpret and explain your machine learning models. If you build a model and can not explain it to your business users <U+2013> it is very unlikely that it will see the light of the day. Can you imagine integrating a model into your product without understanding how it works? Or which features are impacting your final result? In addition to backing from stakeholders, we as data scientists benefit from interpreting our work and improving upon it. It¡¯s a win-win situation all around! The first article of this fast.ai machine learning course saw an incredible response from our community. I¡¯m delighted to share part 2 of this series, which primarily deals with how you can interpret a random forest model. We will understand the theory and also implement it in Python to solidify our grasp on this critical concept. As always, I encourage you to replicate the code on your own machine while you go through the article. Experiment with the code and see how different your results are from what I have covered in this article. This will help you understand the different facets of both the random forest algorithm and the importance of interpretability. Before we dive into the next lessons of this course, let¡¯s quickly recap what we covered in the first two lessons. This will give you some context as to what to expect moving forward. We will continue working on the same dataset in this article. We will have a look at what are the different variables in the dataset and how can we build a random forest model to make valuable interpretations. Alright, it¡¯s time to fire up our Jupyter notebooks and dive right in to lesson#3! You can access the notebook for this lesson here. This notebook will be used for all the three lessons covered in this video. You can watch the entire lesson in the below video (or just scroll down and start implementing things right away): NOTE: Jeremy Howard regularly provides various tips that can be used for solving a certain problem more efficiently, as we saw in the previous article as well. A part of this video is about how to deal with very large datasets. I have included this in the last section of the article so we can focus on the topic at hand first.  Let¡¯s continue from where we left off at the end of lesson 2. We had created new features using the date column and dealt with the categorical columns as well. We will load the processed dataset which includes our newly engineered features and the<U+00A0>log of the<U+00A0>saleprice<U+00A0>variable (since the evaluation metric is RMSLE): We will define the necessary functions which we¡¯ll be frequently<U+00A0>using throughout our implementation. The next step will be to implement a random forest model and interpret the results to understand our dataset better. We have so far learned that random forest is a group of many trees, each trained on a different subset of data points and features. Each individual tree is as different as possible, capturing unique relations from the dataset. We make predictions by running each row through each tree and taking the average of the values at the leaf node. This average is taken as the final prediction for the row. While interpreting the results, it is necessary that the process is interactive and takes lesser time to run. To make this happen, we will make two changes in the code (as compared to what we implemented in the previous article): We¡¯re only using a sample as working with the entire data will take a long time to run. An important thing to note here is that the sample should not be very small. This might end up giving a different result and that¡¯ll be detrimental to our entire project. A sample size of 50,000 works well. Previously, we made predictions for each row using every single tree and then we calculated the mean of the results and the standard deviation. You might have noticed that this works in a sequential manner. Instead, we can call the predict function on multiple trees in parallel! This can be achieved using the<U+00A0>parallel_trees function in the fastai library. The time taken here is less and the results are exactly the same! We will now create a copy of the data so that any changes we make do not affect the original dataset. Once we have the predictions, we can calculate the RMSLE to determine how well the model is performing. But the overall value does not help us identify how close the predicted values are for a particular row or how confident we are that the predictions are correct. We will look at the standard deviation for the rows in this case. If a row is different from those present in the train set, each tree will give different values as predictions. This consequently means means that the standard deviation will be high. On the other hand, the trees would make almost similar predictions for a row that is quite similar to the ones present in the train set, t, i.e., the standard deviation will be low. So, based on the value of the standard deviations we can decide how confident we are about the predictions. Let¡¯s save these predictions and standard deviations: Now, let¡¯s take up a variable from the dataset and visualization it¡¯s distribution and understand what it actually represents. We¡¯ll begin with the<U+00A0>Enclosure variable. The actual sale price and the prediction values are almost similar in three categories <U+2013> ¡®EROPS¡¯, ¡®EROPS w AC¡¯, ¡®OROPS¡¯ (the remaining have null values). Since these null value columns do not add any extra information, we will drop them and visualize the plots for salesprice and prediction: Note that the small black bars represent standard deviation. In the same way, let¡¯s look at another variable <U+2013> ProductSize. We will take a ratio of the standard deviation values and the sum of predictions in order to compare which category has a higher deviation. The standard deviation is higher for the ¡®Large¡¯ and ¡®Compact¡¯ categories. Why do you that is? Take a moment to ponder the answer before reading on. Have a look at the bar plot of values for each category in ProductSize.<U+00A0>Found the reason? We have a lesser number of rows for these two categories. Thus, the model is giving a relatively poor prediction accuracy for these variables. Using this information, we can say that we are more confident about the predictions for the mini, medium and medium/large product size, and less confident about the small, compact and large ones. Feature importance is one of the key aspects of a machine learning model. Understanding which variable is contributing the most to a model is critical to interpreting the results. This is what data scientists strive for when building models that need to be explained to non-technical stakeholders. Our dataset has multiple features and it is often difficult to understand which feature is dominant. This is where the feature importance function of random forest is so helpful. Let¡¯s look at the top 10 most important features for our current model (including visualizing them by their importance): That¡¯s a pretty intuitive plot. Here¡¯s a bar plot visualization of the top 30 features: Clearly<U+00A0>YearMade is the most important feature, followed by Coupler_System.<U+00A0>The majority of the features seems to have little importance in the final model. Let¡¯s verify this statement by removing these features and checking whether this affects the model¡¯s performance. So, we will build a random forest model using only the features that have a feature importance greater than 0.005: When you think about it, removing redundant columns should not decrease the model score, right? And in this case, the model performance has slightly improved. Some of the features we dropped earlier might have been highly collinear with others, so removing them did not affect the model adversely. Let¡¯s check feature importance again to verify our hypothesis: The difference between the feature importance of the<U+00A0>YearMade and Coupler_System<U+00A0>variables is more significant. From the list of features removed, some features were highly collinear to YearMade, resulting in distribution of feature importance between them. On removing these features, we can see that the difference between the importance of YearMade and CouplerSystem has increased from the previous plot. Here is a detailed explanation of how feature importance is actually calculated: And that wraps up the implementation of lesson #3! I encourage you to try out these codes and experiment with them on your own machine to truly understand how each aspect of a random forest model works. In this lesson, Jeremy Howard gives a quick overview of lesson 3 initially before introducing a few important concepts like One Hot Encoding, Dendrogram, and Partial Dependence. Below is the YouTube video of the lecture (or you can jump straight to the implementation below): In the first article of the series, we learned that a lot of machine learning models cannot deal with categorical variables. Using proc_df, we converted the categorical variables into numeric columns. For example, we have a variable UsageBand,<U+00A0>which has three levels -¡®High¡¯, ¡®Low¡¯, and ¡®Medium¡¯. We replaced these categories with numbers (0, 1, 2) to make things easier for ourselves. Surely there must be another way of handling this that takes a significantly less effort on our end? There is! Instead of converting these categories into numbers, we can create separate columns for each category. The column UsageBand can be replaced with three columns: Each of these has 1s and 0s as the values. This is called one-hot encoding. What happens when there are far more than 3 categories? What if we have more than 10? Let¡¯s take an example to understand this. Assume we have a column ¡®zip_code¡¯ in the dataset which has a unique value for every row. Using one-hot encoding here will not be beneficial for the model, and will end up increasing the run time (a lose-lose scenario). Using proc_df in fastai, we can perform one-hot encoding by passing a parameter max_n_cat. Here, we have set the max_n_cat=7, which means that variables having levels more than 7 (such as zip code) will not be encoded, while all the other variables will be one-hot encoded. This can be helpful in determining if a particular level in a particular column is important or not. Since we have separated each level for the categorical variables, plotting feature importance will show us comparisons between them as well: Earlier,<U+00A0>YearMade was the most important feature in the dataset, but EROPS w AC has a higher feature importance in the above chart. Curious what this variable is? Don¡¯t worry, we will discuss what EROPS w AC actually represents in the following section. So far, we¡¯ve understood that having a high number of features can affect the performance of the model and also make it difficult to interpret the results. In this section, we will see how we can identify redundant features and remove them from the data. We will use cluster analysis, more specifically hierarchical clustering, to identify similar variables. In this technique, we look at every object and identify which of them are the closest in terms of features. These variables are then replaced by their midpoint. To understand this better, let us have a look at the cluster plot for our dataset: From the above dendrogram plot, we can see that the variables SaleYear and SaleElapsed are very similar to each other and tend to represent the same thing.<U+00A0>Similarly, Grouser_Tracks, Hydraulics_Flow, and Coupler_System<U+00A0>are highly correlated. The same happens with ProductGroup & ProductGroupDesc and fiBaseModel & fiModelDesc. We will remove each of these features one by one and see how it affects the model performance. First, we define a function to calculate the Out of Bag (OOB) score (to avoid repeating the same lines of code): For the sake of comparison, below is the original OOB score before dropping any feature: We will now drop one variable at a time and calculate the score: This hasn¡¯t heavily affected the OOB score. Let us now remove one variable from each pair and check the overall score: The score has changed from 0.8901 to 0.8885. We will use these selected features on the complete dataset and see how our model performs: Once these variables are removed from the original dataframe, the model¡¯s score turns out to be 0.907 on the validation set. I¡¯ll introduce another technique here that has the potential to help us understand the data better. This technique is called Partial Dependence and it¡¯s used to find out how features are related to the target variable. Let us compare YearMade and SalePrice. If you create a scatter plot for YearMade and SaleElapsed, you¡¯d notice that some vehicles were created in the year 1000, which is not practically possible. These could be the values which were initially missing and have been replaced with 1,000. To keep things practical, we will focus on values that are greater than 1930 for the YearMade variable<U+00A0>and create a plot using the popular ggplot package. This plot shows that the sale price is higher for more recently made vehicles, except for one drop between 1991 and 1997. There could be various reasons for this drop <U+2013> recession, customers preferred vehicles of lower price, or some other external factor. To understand this, we will create a plot that shows the relationship between YearMade and SalePrice, given that all other feature values are the same. This plot is obtained by fixing the YearMade for each row to 1960, then 1961, and so on. In simple words, we take a set of rows and calculate SalePrice for each row when YearMade is 1960. Then we take the whole set again and calculate SalePrice by setting YearMade to 1962. We repeat this multiple times, which results in the multiple blue lines we see in the above plot. The dark black line represents the average. This confirms our hypothesis that the sale price increases for more recently manufactured vehicles. Similarly, you can check for other features like SaleElapsed, or YearMade and SaleElpased together. Performing the same step for the categories under Enclosure (since Enclosure_EROPS w AC proved to be one of the most important features), the resulting plot looks like this: Enclosure_EROPS w AC seems to have a higher sale price as compared to the other two variables (which have almost equal values). So what in the world is EROPS? It¡¯s an enclosed rollover protective structure which can be with or without an AC. And obviously, EROPS with an AC will have a higher sale price. Tree interpreter in another interesting technique that analyzes each individual row in the dataset. We have seen so far how to interpret a model, and how each feature (and the levels in each categorical feature) affect the model predictions. So we will now use this tree interpreter concept and visualize the predictions for a particular row. Let¡¯s import the tree interpreter library and evaluate the results for the first row in the validation set. These are the original values for first row (and it¡¯s every column) in the validation set. Using tree interpreter, we will make predictions for the same using a random forest model. Tree interpreter gives three results <U+2013> prediction, bias and contribution. The value of Coupler_System < 0.5 increased the value from 10.189 to 10.345 and enclosure less than 0.2 reduced the value from 10.345 to 9.955, and so on. So the contributions will represent this change in the predicted values. To understand this in a better way, take a look at the table below: In this table, we have stored the value against each feature and the split point (verify from the image above). The change is the difference between the value before and after the split. These are plotted using a waterfall chart in Excel. The change seen here is for an individual tree. An average of change across all the trees in the random forest is given by contribution in the tree interpreter. Printing the prediction and bias for the first row in our validation set: The value of contribution of each feature in the dataset for this first row: Note: If you are watching the video simultaneously with this article, the values may differ. This is because initially the values were sorted based on index which presented incorrect information. This was corrected in the later video and also in the notebook we have been following throughout the lesson. You should have a pretty good understanding of the random forest algorithm at this stage. In lesson #5, we will focus on how to identify whether model is generalizing well or not. Jeremy Howard also talks about tree interpreters, contribution, and understanding the same using a waterfall chart (which we have already covered in the previous lesson, so will not elaborate on this further).<U+00A0> The primary focus of the video is on Extrapolation and understanding how we can build a random forest algorithm from scratch. A model might not perform well if it¡¯s built on data spanning four years and then used to predict the values for the next one year. In other words, the model does not extrapolate. We have previously seen that there is a significant difference between the training score and validation score, which might be because our validation set consists of a set of recent data points (and the model is using time dependent variables for making predictions). Also, the validation score is worse than the OOB score<U+00A0>which should not be the case, right? A detailed explanation of the OOB score has been given in part 1 of the series. One way of fixing this problem is by attacking it directly <U+2013> deal with the time dependent variables. To figure out which variables are time dependent, we will create a random forest model that tries to predict if a particular row is in the validation set or not. Then we will check which variable has the highest contribution in making a successful prediction. Defining the target variable: The model is able to separate the train and validation sets with a r-square value 0.99998, and the most important features are SaleID, SaleElapsed, MachineID.  It is evident from the tables above that the mean value of these three variables is significantly different. We will drop these variables, fit the random forest again and check the feature importance: Although these variables are obviously time dependent, they can also be important for making the predictions. Before we drop these variables, we need to check how they affect the OOB score. The initial OOB score in a sample is calculated for comparison: Dropping each feature one by one: Looking at the results, age, MachineID and SaleDayofYear actually improved the score while others did not. So, we will remove the remaining variables and fit the random forest on the complete dataset. After removing the time dependent variables, the validation score (0.915) is now better than the OOB score (0.909). We can now play around with other parameters like n_estimator on max_features. To create the final model, Jeremy increased the number of trees to 160 and here are the results: The validation score is 0.92 while the RMSE drops to 0.21. A great improvement indeed! We have learned about how a random forest model actually works, how the features are selected and how predictions are eventually made. In this section, we will create our own random forest model from absolute scratch. Here is the notebook for this section : Random Forest from scratch. We¡¯ll start with importing the basic libraries: We¡¯ll just use two variables to start with. Once we are confident that the model works well with these selected variables, we can use the complete set of features. We have loaded the dataset, split it into train and validation sets, and selected two features <U+2013><U+00A0>YearMade and MachineHoursCurrentMeter.<U+00A0>The first thing to think about while building any model from scratch is <U+2013> what information do we need? So, for a random forest, we need: Let¡¯s define a class with the inputs as mentioned above and set the random seed to 42. We have created a function create_trees that will be called as many times as the number assigned to n_trees. The function create_trees generates<U+00A0>a randomly shuffled set of rows (of size = sample_sz) and returns DecisionTree. We¡¯ll see DecisionTree in a while, but first let¡¯s figure out how predictions are created and saved. We learned earlier that in a random forest model, each single tree makes a prediction for each row and the final prediction is calculated by taking the average of all the predictions. So we will create a predict function, where .predict is used on every tree to create a list of predictions and the mean of this list is calculated as our final value. The final step is to create the DecisionTree. We first select a feature and split point that gives the least error. At present, this code is only for a single decision. We can make this recursive if the code runs successfully. self.n defines the number of rows used in each tree and self.c is the number of columns. Self.val calculates the mean of predictions for each index. This code is still incomplete and will be continued in the next lesson. Yes, part 3 is coming soon! I consider this one of the most important articles in this ongoing series. I cannot stress enough on how important model interpretability is. In real-life industry scenarios, you will quite often face the situation of having to explain the model¡¯s results to the stakeholder (who is usually a non-technical person). Your chances of getting the model approved will lie in how well you are able to explain how and why the model is behaving the way it is. Plus it¡¯s always a good idea to always explain any model¡¯s performance to yourself in a way that a layman will understand <U+2013> this is always a good practice! Use the comments section below to let me know your thoughts or ask any questions you might have on this article. And as I mentioned, part 3 is coming soon so stay tuned! The link to the dataset is not working.","Keyword(freq): feature(26), variable(23), prediction(20), value(17), result(12), category(7), column(7), model(7), row(5), tree(5)"
"11","vidhya",2018-10-29,"MADRaS: A Multi-Agent DRiving Simulator for Autonomous Driving Research","https://www.analyticsvidhya.com/blog/2018/10/madras-multi-agent-driving-simulator/","In this article, we present MADRaS: Multi-Agent DRiving Simulator. It is a multi-agent version of<U+00A0>TORCS, a racing simulator popularly used for autonomous driving research by the reinforcement learning and imitation learning communities. You can read more about TORCS in the below resources: MADRaS is a multi-agent extension of<U+00A0>Gym-TORCS<U+00A0>and is open source, lightweight, easy to install, and has the OpenAI Gym API, which makes it ideal for beginners in autonomous driving research. It enables independent control of tens of agents within the same environment, opening up a prolific direction of research in multi-agent reinforcement learning and imitation learning research aimed at acquiring human-like negotiation skills in complicated traffic situations<U+2014>a major challenge in autonomous driving that all major players are racing to solve. Most open-source autonomous driving simulators (like<U+00A0>CARLA*,<U+00A0>DeepDrive,<U+00A0>AirSim, and<U+00A0>Udacity* SDC) innately support only egocentric control; that is, single agent behavior, and have preprogrammed behaviors for the other agents. The difficulty in introducing agents with custom behaviors in these simulators restricts the diversity of real-world scenarios that can be simulated. To address this issue, we developed MADRaS, wherein each car on the racing track can be independently controlled, enabling the creation of rich, custom-made traffic scenarios, and learning the policy of control of multiple agents simultaneously. The task of negotiation in traffic can be posed as that of finding the winning strategy in a multi-agent game, wherein multiple entities (cars, buses, two-wheelers, and pedestrians) are trying to achieve their objectives of getting from one place to another fast, yet safely and reliably. Imitation learning algorithms like Behavioral Cloning, Active Learning, and Apprenticeship Learning (Inverse Reinforcement Learning followed by Reinforcement Learning) have proved to be effective for learning such sophisticated behaviors, under a multitude of simplifying assumptions and constraining conditions. A major portion of the contemporary literature makes the single-agent assumption; that is, the agent acts in an environment with a plethora of other agents<U+2014>similar or different<U+2014>but does not<U+00A0>interact<U+00A0>with any of them, robbing it of data and information that could potentially be extremely useful in decision making, at both the egocentric and collaborative levels. Driving, however, is inherently multi-agent, and the following is a partial list of things that become possible once we get rid of the single-agent assumption. Source: eDriving One of the earliest instances of multi-agent systems being deployed in vehicles (starting<U+00A0>way back in 1993!) was in the use of platooning, wherein vehicles travel at highway speeds with small inter-vehicle spacing to reduce congestion and still achieve high throughput without compromising safety. Now it seems obvious that autonomous cars in the near future will communicate, cooperate, and form platoons over intersecting lengths of their commutes. Source: phys.org Apart from transferring information about pile-ups and possible diversions ahead to all the vehicles in the geographical vicinity, this power of reliable communication can be used to pool together the<U+00A0>knowledge<U+00A0>of multiple learning agents. An intuitive motivation could be to consider a large gridworld. With a single learning agent, one could<U+00A0>solve<U+00A0>the gridworld in<U+00A0>n<U+00A0>hours of training. With multiple learning agents pooling their experiences, we could cut down the training time significantly, possibly even linearly! There¡¯s a host of untapped literature<U+00A0>on communication among multiple agents in various environments (not autonomous driving¡¦ yet.) See: Now this raises important questions about the reliability of the communication between vehicles. With the imminent advent of 5G,1<U+00A0>fast and reliable communication between vehicles can help lead to the training and deployment of completely hands-free autonomous cars. Drivers on the road constantly anticipate the potential actions of fellow drivers. As an example, for close maneuvering in car parks and intersections, eye contact is made to ensure a shared understanding. Defense Advanced Research Projects Agency (DARPA) stated that traffic vehicle drivers, unnerved by being unable to make eye contact with the robots, had resorted to watching the front wheels of the robots for an indication of their intent. Source: The Star Multi-agent learning comes with its own share of complications: But remember why we started solving fully autonomous driving (FAD) in the first place. Writing for<U+00A0>Technology Review, Will Knight<U+00A0>outlines the possibilities of our driverless car future: The list goes on.. So, today we¡¯re excited to release MADRaS for the community to kickstart research into making FAD a reality. With the ability of introducing multiple learning agents in the environment at the same time, this simulator, built on top of<U+00A0>TORCS, can be used to benchmark and try out existing and new multi-agent learning algorithms for self-driving cars such as: Multi-Agent Deep Deterministic Policy Gradient (MADDPG),<U+00A0>PSMADDPG, and the lot. And since this extends TORCS, it supports the deployment of all the single-agent learning algorithms as well. Scripts for training a DDPG agent are provided as a sample. Check out the following video for an overview of the features and the general interface. This project was developed by<U+00A0>Abhishek Naik<U+00A0>and<U+00A0>Anirban Santara<U+00A0>(an Intel¢ç Student Ambassador for AI) during their internship at the Parallel Computing Lab, Intel Labs, Bangalore, India. This project was driven by Intel¡¯s urge to address the absence of an open source multi-agent autonomous driving simulator that can be utilized by machine learning (particularly, reinforcement learning) scientists to rapidly prototype and evaluate their ideas. Although the system was developed and optimized entirely on the Intel¢ç<U+00A0>Core¢â i7 processor and Intel¢ç<U+00A0>Xeon¢ç<U+00A0>processors, we believe that it would run smoothly on other<U+00A0>x86 platforms, too. Currently, we are working on integrating MADRaS with the<U+00A0>Intel¢ç<U+00A0>Nervana¢âplatform Reinforcement Learning Coach<U+00A0>and we invite the community to participate in its development.","Keyword(freq): agent(8), car(4), algorithm(3), behavior(3), driver(3), madra(2), oftorc(2), robot(2), scenario(2), simulator(2)"
"12","vidhya",2018-10-29,"A Computer Vision Approach to Hand Gesture Recognition","https://www.analyticsvidhya.com/blog/2018/10/computer-vision-approach-hand-gesture-recognition/","Soldiers communicate with each other through gestures. But sometimes those gestures are not visible due to obstructions or poor lighting. For that purpose, an instrument is required to record the gesture and send it to the fellow soldiers. The two options for gesture recognition are through Computer Vision and through some sensors attached to the hands. The first option is not viable in this case as proper lighting is required for recognition through Computer Vision. Hence the second option of using sensors for recognitions has been used. We present a system which recognizes the gestures given in this<U+00A0>link. The given gestures include motions of fingers, wrist, and elbow. To detect any changes in them we have used flex sensors which detect the amount by which it has been bent at each of these joints. To take into account for the dynamic gestures an Inertial Measurement Unit (IMU-MPU-9250) was used. The parameters used from the IMU are acceleration, gyroscopic acceleration, and angles in all three axes. An Arduino* Mega was used to receive the signals from the sensors and send it to the processor. A flex sensor is a strip which has a resistance proportional to the amount of strain in the sensor. Thus it gives out a variable voltage value according to the strain. An IMU (MPU-6050) gives out linear acceleration and gyroscopic acceleration in all three axes (x, y, z). The gestures can be classified into two sub-classes: The number of features primarily used for both the sub classes differ First of all the angles have to be calculated from the acceleration values using these formulae. The angle values have some noise in them and thus have to be filtered out in order to get smooth values out of it. Thus we have used a Kalman filter for filtering the values. Then both the flex sensor values and angles are fed into a pre-trained Support Vector Machine (SVM) with Radial Basis Function (Gaussian) Kernel. And thus the output is obtained. Figure 1: Principal Component Analysis of the dataset using all the features. Each of the colored cluster represents a particular gesture. As accelerations are also included the clusters are quite elongated. Figure 2: Principal Component Analysis of the dataset using just flex sensor values and angles. Here each colored cluster represents a particular gesture. Also these clusters are classifiable. The angles, liner accelerations, and gyroscopic accelerations are filtered using a Kalman Filter. The values are stores in a temporary file with each line representing one time point. Then every value is normalized column-wise. Then 50 time points are sampled out of them. After that they are linearized into one single vector of 800 dimensions. Then it is fed into a SVM with Radial Basis Function kernel (Gaussian). Because some gestures like ¡®Column Formation¡¯, ¡®Vehicle¡¯, ¡®Ammunition¡¯, and ¡®Rally-Point¡¯ are similar to each other we have grouped such similar features as one class. If the first SVM classifies into one of these groups then they are fed into another SVM which is trained just to classify the gestures in that group. Figure 3: Two samples of graph of x-axis acceleration the gesture door. Salient Features of the system: *Since we were told to show the output on a screen we have not used a Raspberry Pi Zero (microprocessor) for processing purposes. But it can be used for that and we have checked the feasibility of the algorithm¡¯s speed in that processor also.","Keyword(freq): gesture(8), value(7), angle(4), feature(4), sensor(4), acceleration(3), axe(2), cluster(2), soldier(2), algorithm(1)"
