"","site","date","headline","url_address","text","keyword"
"1","datacamp",2018-10-01,"Full Stack Data Science (Transcript)","https://www.datacamp.com/community/blog/full-stack-data-science-transcript","Here is a link to the podcast. Hugo:               Hi there, Vicki, and welcome to DataFramed. Vicki:                Thank you so much for having me. Hugo:               It's an absolute pleasure to have you on the show. I'm really excited to talk about your work in Python education, full stack data science, end-to-end data science, what these things actually mean, and your work in consulting. Before we get into all of that, I'd love to know a bit about you. I'm wondering what you're known for in the data community. Vicki:                Probably first and foremost, terrible puns and memes about all sorts of data and programming related things. Secondary is the content. My strategy is a little bit like BuzzFeed, right? Hit them with the memes and then sneak in serious content in between. Vicki:                I've written a lot of blog posts about how to do specific things in Python, how to do specific things in data, and then just talking about like where we are in the data community in general, so very high level articles, and talking about things that break down complicated concepts into easy to understand analogies. Hugo:               Fantastic. I love that secondary is the content and that primary are terrible puns and memes. I don't mean to put you on the spot, but what's one of the worst puns you've said or come up with or heard? Vicki:                They're all so terrible. I have this series of puns where it's basically me pretending to talk to a TV producer to pitch them on possible shows or movies, and so that series is s pretty terrible series of tweets. Hugo:               We'll definitely link to that in the show notes. That's primary. Secondary is the content. I thought I would just mention that you're also, in terms of content, in the process of creating a DataCamp course. Vicki:                Yeah, that's right. I'm working on a course that teaches object-oriented programming for Python, specifically in the context of a data setting. I'll be going throughout how to create objects and do manipulations with CSV files and digging into NumPy and pandas internals, so I'm pretty excited about that. Hugo:               Fantastic. Something you've also mentioned previously is that the educational stuff you do now is that you're essentially being the person you needed when you started out. Vicki:                Yep. Yeah, so the internet is a pretty big place and there's a lot of resources, but if you're just learning to program or you're just getting into data science, the best thing you can do is have an in-person mentor or someone who's ahead of you who you can ask questions. I didn't really have that person when I just started out, so my goal is to be that person for people just getting into the field. Hugo:               Fantastic. Actually, DataCamp itself has a similar origin story in that the CEO, our CEO Jonathan Cornelissen, when he was at grad school he was looking for something like DataCamp and just couldn't find it. He was like, ""Okay, when I finish grad school I'm going to make this thing,"" essentially. Vicki:                Yes. Hugo:               That's one of our several origin stories. That having been said, can you tell us what you do professionally at the moment? Vicki:                Yep, so I am a consultant. I work for CapTech Consulting. We do a bunch of different stuff. Part of our company deals with management consulting, and part of it is deeply technical consulting practice. Right now I do both data science and data engineering consulting depending on the project scope. Hugo:               That sounds very much like this idea of full stack data science, right? Vicki:                Yeah, so the idea is that a lot of companies will start out by not having the infrastructure set up to do data science, because really data science is kind of a mature product offering. We'll come in, we'll build out those pipelines, and then we'll get to the data science aspect, which is creating the models and presenting those results. Hugo:               Great, and we'll get to more of that later. In particular, I'm really interested in thinking about this job of building out the pipelines, doing that, but at the same time needing to demonstrate value as quickly as possible within an organization. This is something ... That's a little teaser for some things we'll chat about later. Hugo:               Before we get to that, though, data science is interesting because so many people have different avenues, all roads lead to data science in some sense. I'm wondering what your journey was. How did you get into data and data science, originally? Vicki:                I think I come from kind of a nontraditional, kind of a traditional background. It's kind of in the middle. I started out as an undergrad who majored in economics, and the reason I picked that was because I didn't want to be an English major and I didn't want to be a math major, and I like that econ kind of combined the two. I like using both sides of my brain a lot. That was my undergrad degree. Vicki:                Then after that, I actually got into economic consulting, which was pretty rare because I don't know a lot of people who focus on their major out of undergrad, so I guess I was lucky, or maybe unlucky in that sense. That's where I got tuned into doing stuff with data. Usually when you start right out of college you start doing stuff with spreadsheets, so I started doing stuff with spreadsheets. Then I heard about this new cool programming language that was free that was called R. I got exposed to that a little bit. I had a couple of roles that were analytics-based. Then my last role was as a data analyst where I learned SQL. Vicki:                Then I got tired of waiting for data to come into the SQL database for me, which is when I started really focusing on learning programing with Python and statistical methods, and then I became a data scientist as my next position. At the same time, I decided that I also wanted to get an MBA because I was interested in technical leadership. I actually don't have a statistics or development background in terms of a Master's program, but I kind of came to it through the job field. Hugo:               That's really interesting. Because a lot of people I speak to when thinking about advice to give to aspiring data scientists is one of the most important skills isn't to be able to build a thousand layer recurrent neural network, but to be able to learn on the job and pick up new skills as you go along, and it sounds like that was an integral part of your journey. Vicki:                Yeah, I think that's been really important for me the entire time in figuring out what to learn, because there's just so much to learn in data science. In consulting, that's one of the primary skills as well because you never know what kind of environment you're going to come into or what the client needs are going to be. Learning and a broad set of skills. Hugo:               Great. I'm just wondering, with your background in economics and your MBA, how do these play into your work as a data scientist in general? Do you find skills and tools you've developed and ways of thinking in economics and your MBA useful in your work in data science? Vicki:                Yeah, so economics and econometrics is actually pretty close to data science, and I think that's probably partly where data science came from. There's a lot of hypothesis testing, for example. There's a lot of statistic and econometrics that goes on. There's a lot of like the social science aspect where you have a hypothesis about how especially large scale systems would work, and that's what a lot of data scientists these days do, right? They test large scale social systems like social networks or platforms to see how things will perform, so that's part of it. Hugo:               Let's talk about your work in consulting. I presume you work across a variety of different industries, but which verticals do you see data science having the most impact on, in your experience? Vicki:                This is going to be a really consulting-y answer, but it really depends, and it's really a broad, broad variety of verticals. The ones that I focus on in my consulting career so far have been telecommunications, banking, and healthcare. Data science has an impact or a place in all of them as long as it's implemented correctly and as long as the business believes in data and sees it as a priority. Hugo:               What type of challenges have you found in demonstrating the value of data science across these industries? Vicki:                A lot of the times ... so we'll probably get to this later, but a lot of the times it's even building out that pipeline to get to the point where you can do data science, but a lot of the times, especially in larger companies, so my company deals a lot primarily with Fortune 500 companies, is getting to the point where you can demonstrate that your hypothesis or whatever it is that you said to do, your call to action, actually results in a change in the business. Hugo:               Great. Are you able to give any specific examples? I don't mean the names of companies or anything like that, but specific examples in telcos, bank, or healthcare, of actual data science projects? Vicki:                A lot of projects ... so this has been true for every industry I've been in. Every company wants to be able to measure churn or why customers are leaving or joining their platform, and especially tracing the fact of why companies are unhappy. For larger companies, this might result in an enormous amount of features, not all of which you can control. For example, the signup process, the billing process, issues they've had with your service or with their service, outside people that have approached them. You can create a model of what potentially causes customers to churn, but that might not necessarily be reflective of the real world. I think that ties back to econometrics, too, because in econometrics you're trying to create a model of the entire economy, but what you really have is a representation because you can't trace all of it. Hugo:               Yeah, great. This is a great example and something I've actually been thinking about a lot recently and talking about this morning, in fact, the churn example in particular, the potential for customers to take their business elsewhere, is the intersection between data science and decision science. Because you can build a model that may tell you or approximate what's happening in the world as to why customers are churning, but that doesn't tell you what to do, right? Vicki:                Right, so ultimately it's for the data scientist, in my opinion, to present a number of options, to present clearly what they think their view of the company is, and then a way for the company to move forward. That's kind of the point where we hand that off to a client. We'll recommend a couple of options, but we obviously won't say, ""Here's what you have to do."" Hugo:               Great. In the churn case, I can imagine several courses of action. The first would be to, if you think a customer's going to churn, reach out to them and make them some sort of offer dependent on how valuable a customer they are to your company. Another would be to try to nip it in the bud well before they're going to to churn. Are these the types of suggestions that you make or are there others as well? Vicki:                Yeah. Usually it's preventive, or you can change it when they're about to churn, or you can create preventive measures so that they can channel their frustration somewhere, for example, new support channels. Hugo:               Great. In your work across all these industries, what are common patterns you've seen in data science across them? Vicki:                One is that, I think we've heard this a lot, but getting the data to a point where you can actually do data science is always 80% of the work. Usually when we come into a company, a lot of the work will be getting the data to a point where we can do data science. The selection of tools and understanding what everybody else in the industry is doing. Kind of this need for understanding best practices. Are we picking the right tool? Is this what other people in the industry are doing? Is this what people in our industry are doing? Or people who are interested in having data science making the case that we need someone to come in and help us do this data science practice, that we actually need data science, that we actually need help making these decisions. Those are probably the big ones. Hugo:               Interesting. There are actually a lot of things that spring to mind there. The first I want to zoom in on is a lot of it's data preparation, getting into a form where you can do analytic or data science work with it. This amount of preparation you have to do, do you see this changing in the next 2, 5, 10 years? Will these types of things become more and more automated and hopefully productized? Vicki:                Some of that, but ultimately I think it's just the feature of data. Because usually unless you're working in manufacturing or some related field, what you have is you have humans generating the data, making sense of the data, defining how it's going to be in a business sense, and that kind of data is always going to be messy. Especially across larger organizations where you might have 5 or 10 or even 20 different data flows. Sometimes you have 2 data flows. They're exactly the same, but just with a little bit of difference. That kind of reconciliation is always going to be existing. Vicki:                What I do see happening more and more lately is a lot of organizations are calling for more data governance. More metadata management is becoming increasingly important in larger organizations. I think over the last 4 years or so, the push was to get stuff into a data lake. It doesn't matter how. It just needs to all be in one place so we can do something with it. Now the idea is we want to be able to manage our assets in a data lake. We need to be able to see them, represent them, and have the business be able to inventory like an S3 bucket or Hadoop cluster or something like that. Hugo:               Great. The other thing you mentioned that I'd like to discuss is you mentioned kind of the movement towards figuring out best practices for the industry, what other people are doing. I wanted to discuss this kind of in the sense that it appears to me that a lot of people ... a lot of data science work is occurring in silos across many different consulting groups, many different organizations, and that a lot of people seem to be reinventing the wheel in parallel in a lot of ways. Is that something you've seen as well? Vicki:                Yeah, I think that can definitely be true. What I've seen in a couple of my projects that were really successful is the organization or the client was dedicated to centralizing all of this stuff. What I've seen come up in larger organizations is something called a Center of Excellence where you have cross functional teams. You have engineers, you have data analysts, you have data scientists, and they all meet together to talk about what they're doing as a team. I've seen that kind of structure come up more and more recently. Hugo:               Is this the type of structure within an organization for data science teams that you think is the most effective? Vicki:                I think so. I'm a big proponent of always having all the stakeholders of any given data science project in the room, if it's practical. For example, if you have maybe 200 people that are going to impact, probably not, but I really always push for developers to sit with data analysts, and more importantly, with business users. Because usually the developers are the first part of the process, and the business users are all the way down there. It can be like a game of telephone where the developers build something, that gets put into some warehouse, that gets put into a dashboard. By the time it's built, the business users don't necessarily always want it and can't act on it. I always like to have all those people all in the same room. Hugo:               What do you think about the future of, I suppose, data literacy for business users? Will we increasingly see people in management, C-level, people using dashboards become more and more knowledgeable about what data is and how it works? Vicki:                I think so. I'm really optimistic about that, and not just because it's job security for me, as people want more and more data. I do believe that the popular press, or at least the tech press, has gotten to a point where ... and I've seen this in business literature like The Harvard Business Review or what have you, it's gotten to the point where a lot of executives now understand the need to be data-driven. Usually when meeting the clients, they say, ""We want to be data-driven."" I think the next two to three years will be ironing out what that means for them specifically. Hugo:               I presume it will mean some sort of computational literacy. I think it will probably mean a bit of statistics as well. Do you think people will need to learn like the basics of math even and linear algebra and logistic regression and these types of things, or is that expecting too much? Vicki:                No. I think the onus there is on the data scientist to present things for different audiences. If you're a data scientist and you're presenting to other data scientists, you can obviously talk about the specifics, the parameters that you have in your logistic regression or what have you. If you are talking to project managers, and especially executives, you should be speaking in a very different way, and you should be talking in a way that makes sense with what they're interested in. An executive is probably not going to be interested in the algorithm you used, but they're going to be interested in what you found and what kinds of actions you think that they should take. I am a firm believer in speaking to people in the language that they understand. Hugo:               I want to shift gears slightly and talk about your approach to building full stack end-to-end data science solutions. Before we do that though, I'm wondering if you could give us the elevator pitch or something analogous on what even full stack end-to-end data science is or means. Vicki:                Full stack to me means basically building out a data science product. You start with some kind of data flow, you transform that data in some environment, and then you output a model and you display that model. That, to me, is end-to-end data science and that's more of a product rather than a project, which I see as iterating on a specific model, for example. Hugo:               Great, so what's your approach to building these solutions then? Vicki:                I don't have a standard approach. It really depends. I usually come into the client's site and just kind of observe for the first week or so. I see what the team norms are, what kinds of tools they're using, where their pain points are. I get really annoying and I ask a ton of questions, and I do a lot of documentation. Then we usually start with looking at where the data flows into that team or that organization and seeing what we can leave behind that will be easy to maintain, reproducible, where you can understand the model that's going into it and where you can easily visualize the output. This is the golden ideal of an end-to-end project. Hugo:               Great. Can you give me an example of one you've worked on recently that you think was particularly valuable? Vicki:                Yep, so I did a project a couple of projects ago that was building predictive modeling capabilities into a Software as a Service platform. This client had a number of, let's say, a number of things that they wanted to predict about their clients. They had the descriptive capability, but they didn't have the predictive capability. My part was taking the data that they were already getting from their clients, putting that data through a model, so I used a Markov chain model that was kind of similar to modeling page views for this particular industry. Then I integrated that back into their existing software platform. Vicki:                Really my role there was, one, ingesting the data that the company was currently collecting in its task platform, analyzing that data, making sense of it because there had been no data analysis done before, figuring out what kind of model best to use to predict, and it turned out to be a Markov because, again, the product was similar to page views where you want to predict kind of the next move of the person or the client. Then wrapping that model around something that you could integrate back into their Software as a Service platform. Hugo:               Once this model is in production, who then is responsible for maintenance of it, and essentially also responsible for checking on model drift? Which, for our listeners out there, model drift is a phenomenon where when you have a productionized machine learning model, for example, it may not work, it may not give the results you're expecting after three to six months, for example. Who's responsible for this type of maintenance then? Vicki:                That depends on the type of project. Usually what we'll do with our company is we'll work with clients to stay on a month or so after and monitor the model, but usually we'll make it so that it can be easy to change on the client side, because ultimately it's theirs. We have to then make sure that it's easy to document and easy to change, which is why it's important to come in and observe it first, like I talked about, to see what toolsets they're comfortable with, what programming languages they use, what the statistical skillset of the people on the team are, so we can pass it back to them and not have it be a black box. Hugo:               Fantastic. That really is setting the expectation to make sure there's someone in house there who even has the capabilities to do this type of maintenance. Hugo:               Another thing that sprung to mind when you elucidated the process of building full stack end-to-end data science solutions was there are so many steps along the way. To be able to do this as one person as opposed to a team of people with different specialties, it seems like you ... one needs to be, and you are, a data science generalist in order to do this. Vicki:                Yeah, I think that's true. In general, I hate to propel the myth of the data science unicorn. I am certainly not a unicorn, but I do think there are generalists and specialists. For consulting particularly, it makes sense if you are a generalist and if you like to be a generalist, because you could be doing a bunch of different things. Vicki:                Recently I've done some prototyping in R. Right now I'm working on a data ingest into AWS. I've done, like I said, the Markov chain modeling before. All of that really is the skillset of understanding what the client needs and being able to figure out how to research and to get to the point where you can offer a solution versus a specialist who might be very, very knowledgeable in, for example, deep learning, for a specific industry. Hugo:               Yeah, and you mentioned R there, implicit in your work, of course, is that you work with SQL. In order to do what you need to do, I'm sure you need to do a bunch of command line stuff and you work in Python as well, so there's this kind of whole array of tools that you use to get the job done, right? Vicki:                Yep. Yeah, I would say my primary tool, when I can use it, is Python just because it's also kind of like the Swiss Army knife of languages. I actually read somewhere recently that Python is the second best language for almost anything, which I agree with. It's my personal favorite language. If you want to do almost anything, you can do it with Python. For my position particularly it works really well. Vicki:                Like I said, I've worked with R, I've worked with Scala, I do a bunch of command line stuff. Recently more and more I've been working with cloud platforms, AWS in particular, which is a whole new skillset, and more and more with engineering things like continuous integration, which is putting your model and making sure that you can keep building it and integrating it into the software. Hugo:               Actually, so I've referred to Python as the Swiss Army knife and I've heard it referred to as a Swiss Army knife for years now. I just had a brain flash, if that's even a term, that maybe we could call it the Dutch Army knife because of Guido. Vicki:                In honor, yes, in honor. Hugo:               Okay, great. I just want to also make clear to all our listeners that although Vicki ... Many guests I have on are data scientist generalists. Definitively not everyone is, and there is not a need to be a generalist, either. Something we may discuss later is that we actually are seeing a lot of specialization emerge within the discipline, right, Vicki? Vicki:                Yep, I totally agree with that. I think there's a place for both. I'm also a big proponent of data science teams as opposed to just one person doing it alone. I always work in teams. Usually it'll be someone who knows a little more statistics, someone who knows a little more engineering, and someone who's more business or business analyst oriented, and someone who's completely business facing. You have a combination of three or four of those kind of people. The best teams that I've been on complement each other in those ways. Hugo:               For people who want to get into this type of work building full stack end-to-end data science products and solutions, what advice with the respect to learning paths would you give them? Vicki:                I would say to just learn one thing that you're interested in. The bast advice I ever got was to just learn one language really well. It doesn't matter what language you're learning, although probably for the generalist Python would make more sense. Learn one language really well, and learn the internals of that language so then you can apply it to other things. Vicki:                Because what generalists do really well is to understand how different things apply to other things. For example, this is how objects work in R, this is how objects work in Python, this is how data flows into AWS, this is how data flows into Hadoop, this is how we would do something in Tableau versus D3. Generalists generally work well with patterns and are able to research different things. Vicki:                What I would suggest is, one, learning one language and then being able to extrapolate from that, and trying building a product or a project end-to-end. I had a tweet about this, which I can link to. Because it can sometimes be really hard to come up with project ideas and daunting, too. The way that I kind of scratch that itch for myself was I built a project called Soviet Art Bot, which tweets out socialist realism art. For that, I had to get that art from a website. I had to put it in AWS, and I had to have an AWS Lambda to create the bot to tweet. That kind of scratched my itch to figure out how all those different parts came together. Like I said, I have a tweet that I can link to that has a couple of different project ideas that you can ... Hugo:               I love that, and we'll definitely link to that in the show notes. Hugo:               Something that's in the cultural consciousness at the moment has been emerging for some time is this trade off in predictive analytics, machine learning and deep learning, between multiple forms, so how well a model is at predicting what it wants to predict, and being interpretable, so trying to figure out why it's making the predictions it does. I'm wondering in your work and your client work, what is the approach to this trade off, generally? Vicki:                My personal approach is to always create models that are a little bit simpler, but always easier to look in under the covers. The reason for that ... and I probably would have a different answer if I were full time at a company, but as a consultant you always need to be able to leave behind work that other people can look at, they can take apart, they can rely on, is easily documented. Especially for dealing with people that are not as technical, it's important to be able to explain those things really well. For me, I always err on the side of simpler is better. Hugo:               Something you spoke to earlier was the fact that more and more data science work is moving to the cloud, and I'd just love to pick your brain about that. This is a relatively large challenge for us as a community to do, and I was just wondering how you approach this in your work. Vicki:                Yeah, so what we've seen recently, while it's been a trend over the past couple years, but I've seen it come up in more and more projects is a lot of clients are starting to realize that they don't want to maintain infrastructure, and they want to take everything to the cloud. Of course, when they're doing this they want consider the fact that there's now things that you have to manage. For example, you have to manage the security of the cloud. Vicki:                Like there's been a lot of stories in the news lately with, for example, S3 buckets just kind of left wide open and all the data leaking out, so that's important to handle off. You need to handle some of the cloud management, and most importantly, you need to understand how all of these parts work together, because it can be harder than just, for example, creating a model in scikit, pickling it, and then putting it on some server. You have to understand how all the parts of the ecosystem work together, so that's becoming more important, too, in data science. I think specifically for data science in the cloud, the toolset is really just emerging at this point. For example, I know there's SageMaker and Google Cloud has some stuff and there's Azure Machine Learning, but all of these, I feel like, are just starting to come into their own, but they'll become more important components as people move in that direction. Hugo:               Also, I think the fact that these are emerging and rapidly developing technologies means that the barrier to entry might be slightly higher, right? Vicki:                It could be. Yeah, it could be in some ways, it's less in others. If you already know how to move in cloud environments, the barrier to entry to the cloud is low, and then the barrier to entry for machine learning is lower, too, because there's already some prototyped components that you can put together. If you don't know how to operate in those environments, in that sense the barrier to entry can be higher. What I've seen recently is a lot of people doing data science are kind of moving a little more towards the engineering path, even. Hugo:               Right. Yeah, I suppose I'm really thinking of the people who are working data scientists or are proficient in machine learning trying to go to the cloud, and it may not even be obvious even documentation wise what to do and how to do it. Vicki:                Right. Yeah, the documentation for a lot of these cloud services leaves a lot to be desired. Hugo:               We'll see that improving, surely. Vicki:                Yeah. In fact, I know AWS and I think also Microsoft have open sourced their documentation on GitHub, which a really positive. Hugo:               That's right, and I actually recently had Paige Bailey on the podcast who's a software developer advocate at Microsoft Azure, and she's instrumental in a lot of this work as well. Hugo:               Great, so we've talked a lot about kind of the data science landscape and your work currently. I'm wondering what the future of data science looks like to you. Vicki:                I think what we'll see is a lot of standardization and kind of like a narrowing out of the industry. The last five years have been about this explosive growth in this new field called data science, which nobody really knew what it was at first, and so we started to define that. There's a lot of now kind of shifting to data science. Everybody almost knows that data scientists are statisticians. Vicki:                What we're seeing now I think is a lot more, to your point, specialization. There's a lot of people specifically deep learning or specifically AI. A lot more movement to software development, like I mentioned. Especially as more stuff goes into the cloud, data scientists will need to know how to work in those environments. As always, I think the future belongs to people who can be flexible, who can write and read good code in whatever language, and who can teach themselves as the environment shifts. Hugo:               Great. Something you spoke to previously is trying to understand what best practices in data science look like. There isn't as of yet ... I mean people talk about certain things, but there isn't kind of solidified system of best practices like there is in front end software engineering, for example, right? Vicki:                Yeah, and I think that's just starting out. Like I've seen both Facebook and Google release guides on machine learning and things to look at. Google's is particularly good because it has things you should look at, and Facebook just released a bunch of videos. I think that will start to become more solidified. The other side of that is you also hear a lot of people talking about ethics in machine learning and data science, and I think there might be some pressure from that perspective as well to define just what data science means. Of course, there's GDPR regulations which will have us define what data we can collect. I think all those three things together will give us a little more fleshed out view of what that is. Hugo:               Yeah, great. I think the GDPR's an interesting example. We'll be seeing more and more of this. That's EU specific in a number of ways, if you have any data going through the EU potentially as well. As we see more and more countries adopting these types of things, I'm wondering if that will impact how we use cloud technologies as well. Vicki:                I'm sure it will to some extent. I think the big thing in cloud will be figuring out security... security and data flows first. Hugo:               Yeah. You mentioned ethics in data science. I'm wondering what you think the biggest concerns are in the ethical landscape. Vicki:                Personally I would say right now probably the biggest issue is data leaks. There's a number of different things, but I want to focus on the practical issue, which is a lot of people are not securing their data. The issue there is potentially collecting too much and then not monitoring it carefully enough. Hugo:               Okay. Yeah, I agree with that. We've talked a lot about different aspects of data science and the data science flow. I'm wondering in particular what's one of your favorite data science-y things to do, I mean techniques or methodologies? Vicki:                Yeah, so the ones that I enjoy doing the most because I get the most return out of them are probably decision trees. The reason I like them so much is because they're very easy to discuss with people who aren't necessarily data scientists. They're very easy to visualize and they give you a clear path to a call of action. If I can utilize them, I do. Hugo:               This really speaks again to something we're discussing earlier of one interpretability, you can actually show someone going down the tree and what decisions it makes at each branching point, but also ease of explicability or just being able to explain something to someone else. Vicki:                Yep, and the ease of porting between multiple platforms as well. Hugo:               In what sense? Vicki:                Implementation details so you can create a decision tree locally in scikit-learn. You can create one in R. You can create one on almost any platform that exists, so I like it with that. Hugo:               That's great. Of course in scikit-learn you can ... it's nice it's compatible with Graphviz, right, so you can visualize it immediately. Vicki:                Yep. Hugo:               What about with respect to data engineering? What really gets ... do you love doing there? Vicki:                I'm really into AWS Lambdas, which are basically... think of them as like virtual environments that exist ephemerally. They spin up, they do something, and then they go away. There's a lot of potential for use with them, and I'm really interested in exploring them a lot more. I've used them in my past two projects and I see them only growing. Hugo:               What's the gain? What's the big win to be made with AWS Lambda environments, do you think? Vicki:                They're kind of like functions that do things very quickly. They can move data. They can tweet. I use Lambda functions in my bot to tweet every certain amount of time. They're very easy to maintain. Once you set them up and have them going, they just kind of keep going. Hugo:               Fantastic. All right, so my last question is do you have a final call to action for our listeners out there? Vicki:                Yeah, so I'm on Twitter. I'm @vboykis. You can find my site there, my tech blog. If you're interested in more about what my company CapTech does, you can go to captechconsulting.com. We're always hiring and we're always taking on new clients. Hugo:               Fantastic. I suppose I do have a follow-up question there. In terms of the hiring process, this is a question I get a lot, do you have any advice or general rules of thumb for people entering an interview process, I mean with you or elsewhere? Vicki:                One, prepare well to understand the company that you're interviewing for. Especially in consulting it's a little bit different because we're looking for people who are good technically, but we're also looking for people who are interested in doing a lot of different things and are good at doing a lot of different things and can be self learners and do a lot of research. Vicki:                The second thing is to be enthusiastic about what you talk about. Tell me about what you're passionate about. Tell me about what kinds of projects you've done, if you've done projects outside of work. Tell me as much as you can about your work projects. Vicki:                Basically when I come into an interview with someone, I'm looking to have ... I'm not looking to trick you. I'm looking to have a conversation with you and to see if I can work with you, and that's it. Hugo:               Vicki, it's been an absolute pleasure having you on the show. Vicki:                Thank you for having me.","Keyword(freq): project(9), scientist(8), client(6), company(6), environment(5), organization(5), skill(5), team(5), customer(4), econometric(4)"
"2","mastery",2018-10-01,"How to Develop and Evaluate Naive Methods for Forecasting Household Electricity Consumption","https://machinelearningmastery.com/naive-methods-for-forecasting-household-electricity-consumption/","Given the rise of smart electricity meters and the wide adoption of electricity generation technology like solar panels, there is a wealth of electricity usage data available. This data represents a multivariate time series of power-related variables that in turn could be used to model and even forecast future electricity consumption. In this tutorial, you will discover how to develop a test harness for the ¡®household power consumption¡¯ dataset and evaluate three naive forecast strategies that provide a baseline for more sophisticated algorithms. After completing this tutorial, you will know: Let¡¯s get started. How to Develop and Evaluate Naive Forecast Methods for Forecasting Household Electricity ConsumptionPhoto by Philippe Put, some rights reserved. This tutorial is divided into four parts; they are: The ¡®Household Power Consumption¡® dataset is a multivariate time series dataset that describes the electricity consumption for a single household over four years. The data was collected between December 2006 and November 2010 and observations of power consumption within the household were collected every minute. It is a multivariate series comprised of seven variables (besides the date and time); they are: Active and reactive energy refer to the technical details of alternative current. A fourth sub-metering variable can be created by subtracting the sum of three defined sub-metering variables from the total active energy as follows: The dataset can be downloaded from the UCI Machine Learning repository as a single 20 megabyte .zip file: Download the dataset and unzip it into your current working directory. You will now have the file ¡°household_power_consumption.txt¡± that is about 127 megabytes in size and contains all of the observations. We can use the read_csv() function to load the data and combine the first two columns into a single date-time column that we can use as an index. Next, we can mark all missing values indicated with a ¡®?¡® character with a NaN value, which is a float. This will allow us to work with the data as one array of floating point values rather than mixed types (less efficient.) We also need to fill in the missing values now that they have been marked. A very simple approach would be to copy the observation from the same time the day before. We can implement this in a function named fill_missing() that will take the NumPy array of the data and copy values from exactly 24 hours ago. We can apply this function directly to the data within the DataFrame. Now we can create a new column that contains the remainder of the sub-metering, using the calculation from the previous section. We can now save the cleaned-up version of the dataset to a new file; in this case we will just change the file extension to .csv and save the dataset as ¡®household_power_consumption.csv¡®. Tying all of this together, the complete example of loading, cleaning-up, and saving the dataset is listed below. Running the example creates the new file ¡®household_power_consumption.csv¡® that we can use as the starting point for our modeling project. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this section, we will consider how we can develop and evaluate predictive models for the household power dataset. This section is divided into four parts; they are: There are many ways to harness and explore the household power consumption dataset. In this tutorial, we will use the data to explore a very specific question; that is: Given recent power consumption, what is the expected power consumption for the week ahead? This requires that a predictive model forecast the total active power for each day over the next seven days. Technically, this framing of the problem is referred to as a multi-step time series forecasting problem, given the multiple forecast steps. A model that makes use of multiple input variables may be referred to as a multivariate multi-step time series forecasting model. A model of this type could be helpful within the household in planning expenditures. It could also be helpful on the supply side for planning electricity demand for a specific household. This framing of the dataset also suggests that it would be useful to downsample the per-minute observations of power consumption to daily totals. This is not required, but makes sense, given that we are interested in total power per day. We can achieve this easily using the resample() function on the pandas DataFrame. Calling this function with the argument ¡®D¡® allows the loaded data indexed by date-time to be grouped by day (see all offset aliases). We can then calculate the sum of all observations for each day and create a new dataset of daily power consumption data for each of the eight variables. The complete example is listed below. Running the example creates a new daily total power consumption dataset and saves the result into a separate file named ¡®household_power_consumption_days.csv¡®. We can use this as the dataset for fitting and evaluating predictive models for the chosen framing of the problem. A forecast will be comprised of seven values, one for each day of the week ahead. It is common with multi-step forecasting problems to evaluate each forecasted time step separately. This is helpful for a few reasons: The units of the total power are kilowatts and it would be useful to have an error metric that was also in the same units. Both Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) fit this bill, although RMSE is more commonly used and will be adopted in this tutorial. Unlike MAE, RMSE is more punishing of forecast errors. The performance metric for this problem will be the RMSE for each lead time from day 1 to day 7. As a short-cut, it may be useful to summarize the performance of a model using a single score in order to aide in model selection. One possible score that could be used would be the RMSE across all forecast days. The function evaluate_forecasts() below will implement this behavior and return the performance of a model based on multiple seven-day forecasts. Running the function will first return the overall RMSE regardless of day, then an array of RMSE scores for each day. We will use the first three years of data for training predictive models and the final year for evaluating models. The data in a given dataset will be divided into standard weeks. These are weeks that begin on a Sunday and end on a Saturday. This is a realistic and useful way for using the chosen framing of the model, where the power consumption for the week ahead can be predicted. It is also helpful with modeling, where models can be used to predict a specific day (e.g. Wednesday) or the entire sequence. We will split the data into standard weeks, working backwards from the test dataset. The final year of the data is in 2010 and the first Sunday for 2010 was January 3rd. The data ends in mid November 2010 and the closest final Saturday in the data is November 20th. This gives 46 weeks of test data. The first and last rows of daily data for the test dataset are provided below for confirmation. The daily data starts in late 2006. The first Sunday in the dataset is December 17th, which is the second row of data. Organizing the data into standard weeks gives 159 full standard weeks for training a predictive model. The function split_dataset() below splits the daily data into train and test sets and organizes each into standard weeks. Specific row offsets are used to split the data using knowledge of the dataset. The split datasets are then organized into weekly data using the NumPy split() function. We can test this function out by loading the daily dataset and printing the first and last rows of data from both the train and test sets to confirm they match the expectations above. The complete code example is listed below. Running the example shows that indeed the train dataset has 159 weeks of data, whereas the test dataset has 46 weeks. We can see that the total active power for the train and test dataset for the first and last rows match the data for the specific dates that we defined as the bounds on the standard weeks for each set. Models will be evaluated using a scheme called walk-forward validation. This is where a model is required to make a one week prediction, then the actual data for that week is made available to the model so that it can be used as the basis for making a prediction on the subsequent week. This is both realistic for how the model may be used in practice and beneficial to the models allowing them to make use of the best available data. We can demonstrate this below with separation of input data and output/predicted data. The walk-forward validation approach to evaluating predictive models on this dataset is implement below, named evaluate_model(). The name of a function is provided for the model as the argument ¡°model_func¡°. This function is responsible for defining the model, fitting the model on the training data, and making a one-week forecast. The forecasts made by the model are then evaluated against the test dataset using the previously defined evaluate_forecasts() function. Once we have the evaluation for a model, we can summarize the performance. The function below named summarize_scores() will display the performance of a model as a single line for easy comparison with other models. We now have all of the elements to begin evaluating predictive models on the dataset. It is important to test naive forecast models on any new prediction problem. The results from naive models provide a quantitative idea of how difficult the forecast problem is and provide a baseline performance by which more sophisticated forecast methods can be evaluated. In this section, we will develop and compare three naive forecast methods for the household power prediction problem; they are: The first naive forecast that we will develop is a daily persistence model. This model takes the active power from the last day prior to the forecast period (e.g. Saturday) and uses it as the value of the power for each day in the forecast period (Sunday to Saturday). The daily_persistence() function below implements the daily persistence forecast strategy. Another good naive forecast when forecasting a standard week is to use the entire prior week as the forecast for the week ahead. It is based on the idea that next week will be very similar to this week. The weekly_persistence() function below implements the weekly persistence forecast strategy. Similar to the idea of using last week to forecast next week is the idea of using the same week last year to predict next week. That is, use the week of observations from 52 weeks ago as the forecast, based on the idea that next week will be similar to the same week one year ago. The week_one_year_ago_persistence() function below implements the week one year ago forecast strategy. We can compare each of the forecast strategies using the test harness developed in the previous section. First, the dataset can be loaded and split into train and test sets. Each of the strategies can be stored in a dictionary against a unique name. This name can be used in printing and in creating a plot of the scores. We can then enumerate each of the strategies, evaluating it using walk-forward validation, printing the scores, and adding the scores to a line plot for visual comparison. Tying all of this together, the complete example evaluating the three naive forecast strategies is listed below. Running the example first prints the total and daily scores for each model. We can see that the weekly strategy performs better than the daily strategy and that the week one year ago (week-oya) performs slightly better again. We can see this in both the overall RMSE scores for each model and in the daily scores for each forecast day. One exception is the forecast error for the first day (Sunday) where it appears that the daily persistence model performs better than the two weekly strategies. We can use the week-oya strategy with an overall RMSE of 465.294 kilowatts as the baseline in performance for more sophisticated models to be considered skillful on this specific framing of the problem. A line plot of the daily forecast error is also created. We can see the same observed pattern of the weekly strategies performing better than the daily strategy in general, except in the case of the first day. It is surprising (to me) that the week one-year-ago performs better than using the prior week. I would have expected that the power consumption from last week to be more relevant. Reviewing all strategies on the same plot suggests possible combinations of the strategies that may result in even better performance. Line Plot Comparing Naive Forecast Strategies for Household Power Forecasting This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop a test harness for the household power consumption dataset and evaluate three naive forecast strategies that provide a baseline for more sophisticated algorithms. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): model(13), strategy(11), score(7), observation(5), value(5), variable(5), implement(3), method(3), row(3), set(3)"
"3","mastery",2018-09-28,"How to Load and Explore Household Electricity Usage Data","https://machinelearningmastery.com/how-to-load-and-explore-household-electricity-usage-data/","Given the rise of smart electricity meters and the wide adoption of electricity generation technology like solar panels, there is a wealth of electricity usage data available. This data represents a multivariate time series of power-related variables, that in turn could be used to model and even forecast future electricity consumption. In this tutorial, you will discover a household power consumption dataset for multi-step time series forecasting and how to better understand the raw data using exploratory analysis. After completing this tutorial, you will know: Let¡¯s get started. How to Load and Explore Household Electricity Usage DataPhoto by Sheila Sund, some rights reserved. This tutorial is divided into five parts; they are: The Household Power Consumption dataset is a multivariate time series dataset that describes the electricity consumption for a single household over four years. The data was collected between December 2006 and November 2010 and observations of power consumption within the household were collected every minute. It is a multivariate series comprised of seven variables (besides the date and time); they are: Active and reactive energy refer to the technical details of alternative current. In general terms, the active energy is the real power consumed by the household, whereas the reactive energy is the unused power in the lines. We can see that the dataset provides the active power as well as some division of the active power by main circuit in the house, specifically the kitchen, laundry, and climate control. These are not all the circuits in the household. The remaining watt-hours can be calculated from the active energy by first converting the active energy to watt-hours then subtracting the other sub-metered active energy in watt-hours, as follows: The dataset seems to have been provided without a seminal reference paper. Nevertheless, this dataset has become a standard for evaluating time series forecasting and machine learning methods for multi-step forecasting, specifically for forecasting active power. Further, it is not clear whether the other features in the dataset may benefit a model in forecasting active power. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course The dataset can be downloaded from the UCI Machine Learning repository as a single 20 megabyte .zip file: Download the dataset and unzip it into your current working directory. You will now have the file ¡°household_power_consumption.txt¡± that is about 127 megabytes in size and contains all of the observations Inspect the data file. Below are the first five rows of data (and the header) from the raw data file. We can see that the data columns are separated by semicolons (¡®;¡®). The data is reported to have one row for each day in the time period. The data does have missing values; for example, we can see 2-3 days worth of missing data around 28/4/2007. We can start-off by loading the data file as a Pandas DataFrame and summarize the loaded data. We can use the read_csv() function to load the data. It is easy to load the data with this function, but a little tricky to load it correctly. Specifically, we need to do a few custom things: Putting all of this together, we can now load the data and summarize the loaded shape and first few rows. Next, we can mark all missing values indicated with a ¡®?¡¯ character with a NaN value, which is a float. This will allow us to work with the data as one array of floating point values rather than mixed types, which is less efficient. Now we can create a new column that contains the remainder of the sub-metering, using the calculation from the previous section. We can now save the cleaned-up version of the dataset to a new file; in this case we will just change the file extension to .csv and save the dataset as ¡®household_power_consumption.csv¡®. To confirm that we have not messed-up, we can re-load the dataset and summarize the first five rows. Tying all of this together, the complete example of loading, cleaning-up, and saving the dataset is listed below. Running the example first loads the raw data and summarizes the shape and first five rows of the loaded data. The dataset is then cleaned up and saved to a new file. We load this new file and again print the first five rows, showing the removal of the date and time columns and addition of the new sub-metered column. We can peek inside the new ¡®household_power_consumption.csv¡® file and check that the missing observations are marked with an empty column, that pandas will correctly read as NaN, for example around row 190,499: Now that we have a cleaned-up version of the dataset, we can investigate it further using visualizations. The data is a multivariate time series and the best way to understand a time series is to create line plots. We can start off by creating a separate line plot for each of the eight variables. The complete example is listed below. Running the example creates a single image with eight subplots, one for each variable. This gives us a really high level of the four years of one minute observations. We can see that something interesting was going on in ¡®Sub_metering_3¡® (environmental control) that may not directly map to hot or cold years. Perhaps new systems were installed. Interestingly, the contribution of ¡®sub_metering_4¡® seems to decrease with time, or show a downward trend, perhaps matching up with the solid increase in seen towards the end of the series for ¡®Sub_metering_3¡®. These observations do reinforce the need to honor the temporal ordering of subsequences of this data when fitting and evaluating any model. We might be able to see the wave of a seasonal effect in the ¡®Global_active_power¡® and some other variates. There is some spiky usage that may match up with a specific period, such as weekends. Line Plots of Each Variable in the Power Consumption Dataset Let¡¯s zoom in and focus on the ¡®Global_active_power¡®, or ¡®active power¡® for short. We can create a new plot of the active power for each year to see if there are any common patterns across the years. The first year, 2006, has less than one month of data, so will remove it from the plot. The complete example is listed below. Running the example creates one single image with four line plots, one for each full year (or mostly full years) of data in the dataset. We can see some common gross patterns across the years, such as around Feb-Mar and around Aug-Sept where we see a marked decrease in consumption. We also seem to see a downward trend over the summer months (middle of the year in the northern hemisphere) and perhaps more consumption in the winter months towards the edges of the plots. These may show an annual seasonal pattern in consumption. We can also see a few patches of missing data in at least the first, third, and fourth plots. Line Plots of Active Power for Most Years We can continue to zoom in on consumption and look at active power for each of the 12 months of 2007. This might help tease out gross structures across the months, such as daily and weekly patterns. The complete example is listed below. Running the example creates a single image with 12 line plots, one for each month in 2007. We can see the sign-wave of power consumption of the days within each month. This is good as we would expect some kind of daily pattern in power consumption. We can see that there are stretches of days with very minimal consumption, such as in August and in April. These may represent vacation periods where the home was unoccupied and where power consumption was minimal. Line Plots for Active Power for All Months in One Year Finally, we can zoom in one more level and take a closer look at power consumption at the daily level. We would expect there to be some pattern to consumption each day, and perhaps differences in days over a week. The complete example is listed below. Running the example creates a single image with 20 line plots, one for the first 20 days in January 2007. There is commonality across the days; for example, many days consumption starts early morning, around 6-7AM. Some days show a drop in consumption in the middle of the day, which might make sense if most occupants are out of the house. We do see some strong overnight consumption on some days, that in a northern hemisphere January may match up with a heating system being used. Time of year, specifically the season and the weather that it brings, will be an important factor in modeling this data, as would be expected. Line Plots for Active Power for 20 Days in One Month Another important area to consider is the distribution of the variables. For example, it may be interesting to know if the distributions of observations are Gaussian or some other distribution. We can investigate the distributions of the data by reviewing histograms. We can start-off by creating a histogram for each variable in the time series. The complete example is listed below. Running the example creates a single figure with a separate histogram for each of the 8 variables. We can see that active and reactive power, intensity, as well as the sub-metered power are all skewed distributions down towards small watt-hour or kilowatt values. We can also see that distribution of voltage data is strongly Gaussian. Histogram plots for Each Variable in the Power Consumption Dataset The distribution of active power appears to be bi-modal, meaning it looks like it has two mean groups of observations. We can investigate this further by looking at the distribution of active power consumption for the four full years of data. The complete example is listed below. Running the example creates a single plot with four figures, one for each of the years between 2007 to 2010. We can see that the distribution of active power consumption across those years looks very similar. The distribution is indeed bimodal with one peak around 0.3 KW and perhaps another around 1.3 KW. There is a long tail on the distribution to higher kilowatt values. It might open the door to notions of discretizing the data and separating it into peak 1, peak 2 or long tail. These groups or clusters for usage on a day or hour may be helpful in developing a predictive model. Histogram Plots of Active Power for Most Years It is possible that the identified groups may vary over the seasons of the year. We can investigate this by looking at the distribution for active power for each month in a year. The complete example is listed below. Running the example creates an image with 12 plots, one for each month in 2007. We can see generally the same data distribution each month. The axes for the plots appear to align (given the similar scales), and we can see that the peaks are shifted down in the warmer northern hemisphere months and shifted up for the colder months. We can also see a thicker or more prominent tail toward larger kilowatt values for the cooler months of December through to March. Histogram Plots for Active Power for All Months in One Year Now that we know how to load and explore the dataset, we can pose some ideas on how to model the dataset. In this section, we will take a closer look at three main areas when working with the data; they are: There does not appear to be a seminal publication for the dataset to demonstrate the intended way to frame the data in a predictive modeling problem. We are therefore left to guess at possibly useful ways that this data may be used. The data is only for a single household, but perhaps effective modeling approaches could be generalized across to similar households. Perhaps the most useful framing of the dataset is to forecast an interval of future active power consumption. Four examples include: Generally, these types of forecasting problems are referred to as multi-step forecasting. Models that make use of all of the variables might be referred to as a multivariate multi-step forecasting models. Each of these models is not limited to forecasting the minutely data, but instead could model the problem at or below the chosen forecast resolution. Forecasting consumption in turn, at scale, could aid in a utility company forecasting demand, which is a widely studied and important problem. There is a lot of flexibility in preparing this data for modeling. The specific data preparation methods and their benefit really depend on the chosen framing of the problem and the modeling methods. Nevertheless, below is a list of general data preparation methods that may be useful: There are many simple human factors that may be helpful in engineering features from the data, that in turn may make specific days easier to forecast. Some examples include: These factors may be significantly less important for forecasting monthly data, and perhaps to a degree for weekly data. More general features may include: There are perhaps four classes of methods that might be interesting to explore on this problem; they are: Naive methods would include methods that make very simple, but often very effective assumptions. Some examples include: Classical linear methods include techniques are very effective for univariate time series forecasting. Two important examples include: They would require that the additional variables be discarded and the parameters of the model be configured or tuned to the specific framing of the dataset. Concerns related to adjusting the data for daily and seasonal structures can also be supported directly. Machine learning methods require that the problem be framed as a supervised learning problem. This would require that lag observations for a series be framed as input features, discarding the temporal relationship in the data. A suite of nonlinear and ensemble methods could be explored, including: Careful attention is required to ensure that the fitting and evaluation of these models preserved the temporal structure in the data. This is important so that the method is not able to ¡®cheat¡¯ by harnessing observations from the future. These methods are often agnostic to large numbers of variables and may aid in teasing out whether the additional variables can be harnessed and add value to predictive models. Generally, neural networks have not proven very effective at autoregression type problems. Nevertheless, techniques such as convolutional neural networks are able to automatically learn complex features from raw data, including one-dimensional signal data. And recurrent neural networks, such as the long short-term memory network, are capable of directly learning across multiple parallel sequences of input data. Further, combinations of these methods, such as CNN LSTM and ConvLSTM, have proven effective on time series classification tasks. It is possible that these methods may be able to harness the large volume of minute-based data and multiple input variables. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered a household power consumption dataset for multi-step time series forecasting and how to better understand the raw data using exploratory analysis. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Great work Jason! Thanks. Thank you for the post. It is really helpful.
I was wondering how to frame the input data for Forecasting hourly consumption for the next day using SVM, ANN  and Randomforest. Is there any reference for multi-step multi-variate time series prediction?
Also, would Forecast hourly consumption for the next day be more accurate than Forecast hourly consumption for the next week? Yes, I have many examples. Here¡¯s a starting point:https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/ I also have many more examples in my book:https://machinelearningmastery.com/deep-learning-for-time-series-forecasting/ If there was ¡°IoT¡± in the title, people (including me) would faster recognize the immense value in your post/book. Thanks for the suggestion. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): plot(15), method(13), variable(10), observation(9), example(6), value(6), feature(5), model(5), row(5), distribution(3)"
"4","mastery",2018-09-26,"Deep Learning Models for Human Activity Recognition","https://machinelearningmastery.com/deep-learning-models-for-human-activity-recognition/","Human activity recognition, or HAR, is a challenging time series classification task. It involves predicting the movement of a person based on sensor data and traditionally involves deep domain expertise and methods from signal processing to correctly engineer features from the raw data in order to fit a machine learning model. Recently, deep learning methods such as convolutional neural networks and recurrent neural networks have shown capable and even achieve state-of-the-art results by automatically learning features from the raw sensor data. In this post, you will discover the problem of human activity recognition and the deep learning methods that are achieving state-of-the-art performance on this problem. After reading this post, you will know: Let¡¯s get started. Deep Learning Models for Human Activity RecognitionPhoto by Simon Harrod, some rights reserved. This post is divided into five parts; they are: Human activity recognition, or HAR for short, is a broad field of study concerned with identifying the specific movement or action of a person based on sensor data. Movements are often typical activities performed indoors, such as walking, talking, standing, and sitting. They may also be more focused activities such as those types of activities performed in a kitchen or on a factory floor. The sensor data may be remotely recorded, such as video, radar, or other wireless methods. Alternately, data may be recorded directly on the subject such as by carrying custom hardware or smart phones that have accelerometers and gyroscopes. Sensor-based activity recognition seeks the profound high-level knowledge about human activities from multitudes of low-level sensor readings <U+2014> Deep Learning for Sensor-based Activity Recognition: A Survey, 2018. Historically, sensor data for activity recognition was challenging and expensive to collect, requiring custom hardware. Now smart phones and other personal tracking devices used for fitness and health monitoring are cheap and ubiquitous. As such, sensor data from these devices is cheaper to collect, more common, and therefore is a more commonly studied version of the general activity recognition problem. The problem is to predict the activity given a snapshot of sensor data, typically data from one or a small number of sensor types. Generally, this problem is framed as a univariate or multivariate time series classification task. It is a challenging problem as there are no obvious or direct ways to relate the recorded sensor data to specific human activities and each subject may perform an activity with significant variation, resulting in variations in the recorded sensor data. The intent is to record sensor data and corresponding activities for specific subjects, fit a model from this data, and generalize the model to classify the activity of new unseen subjects from their sensor data. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course Traditionally, methods from the field of signal processing were used to analyze and distill the collected sensor data. Such methods were for feature engineering, creating domain-specific, sensor-specific, or signal processing-specific features and views of the original data. Statistical and machine learning models were then trained on the processed version of the data. A limitation of this approach is the signal processing and domain expertise required to analyze the raw data and engineer the features required to fit a model. This expertise would be required for each new dataset or sensor modality. In essence, it is expensive and not scalable. However, in most daily HAR tasks, those methods may heavily rely on heuristic handcrafted feature extraction, which is usually limited by human domain knowledge. Furthermore, only shallow features can be learned by those approaches, leading to undermined performance for unsupervised and incremental tasks. Due to those limitations, the performances of conventional [pattern recognition] methods are restricted regarding classification accuracy and model generalization. <U+2014> Deep Learning for Sensor-based Activity Recognition: A Survey, 2018. Ideally, learning methods could be used that automatically learn the features required to make accurate predictions from the raw data directly. This would allow new problems, new datasets, and new sensor modalities to be adopted quickly and cheaply. Recently, deep neural network models have started delivering on their promises of feature learning and are achieving stat-of-the-art results for human activity recognition. They are capable of performing automatic feature learning from the raw sensor data and out-perform models fit on hand-crafted domain-specific features. [¡¦] , the feature extraction and model building procedures are often performed simultaneously in the deep learning models. The features can be learned automatically through the network instead of being manually designed. Besides, the deep neural network can also extract high-level representation in deep layer, which makes it more suitable for complex activity recognition tasks. <U+2014> Deep Learning for Sensor-based Activity Recognition: A Survey, 2018. There are two main approaches to neural networks that are appropriate for time series classification and that have been demonstrated to perform well on activity recognition using sensor data from commodity smart phones and fitness tracking devices. They are Convolutional Neural Network Models and Recurrent Neural Network Models. RNN and LSTM are recommended to recognize short activities that have natural order while CNN is better at inferring long term repetitive activities. The reason is that RNN could make use of the time-order relationship between sensor readings, and CNN is more capable of learning deep features contained in recursive patterns. <U+2014> Deep Learning for Sensor-based Activity Recognition: A Survey, 2018. Before we dive into the specific neural networks that can be used for human activity recognition, we need to talk about data preparation. Both types of neural networks suitable for time series classification require that data be prepared in a specific manner in order to fit a model. That is, in a ¡®supervised learning¡® way that allows the model to associate signal data with an activity class. A straight-forward data preparation approach that was used both for classical machine learning methods on the hand-crafted features and for neural networks involves dividing the input signal data into windows of signals, where a given window may have one to a few seconds of observation data. This is often called a ¡®sliding window.¡¯ Human activity recognition aims to infer the actions of one or more persons from a set of observations captured by sensors. Usually, this is performed by following a fixed length sliding window approach for the features extraction where two parameters have to be fixed: the size of the window and the shift. <U+2014> A Dynamic Sliding Window Approach for Activity Recognition, 2011 Each window is also associated with a specific activity. A given window of data may have multiple variables, such as the x, y, and z axes of an accelerometer sensor. Let¡¯s make this concrete with an example. We have sensor data for 10 minutes; that may look like: If the data is recorded at 8 Hz, that means that there will be eight rows of data for one second of elapsed time performing an activity. We may choose to have one window of data represent one second of data; that means eight rows of data for an 8 Hz sensor. If we have x, y, and z data, that means we would have 3 variables. Therefore, a single window of data would be a 2-dimensional array with eight time steps and three features. One window would represent one sample. One minute of data would represent 480 sensor data points, or 60 windows of eight time steps. The total 10 minutes of data would represent 4,800 data points, or 600 windows of data. It is convenient to describe the shape of our prepared sensor data in terms of the number of samples or windows, the number of time steps in a window, and the number of features observed at each time step. Our example of 10 minutes of accelerometer data recorded at 8 Hz would be summarized as a three-dimensional array with the dimensions: There is no best window size, and it really depends on the specific model being used, the nature of the sensor data that was collected, and the activities that are being classified. There is a tension in the size of the window and the size of the model. Larger windows require large models that are slower to train, whereas smaller windows require smaller models that are much easier to fit. Intuitively, decreasing the window size allows for a faster activity detection, as well as reduced resources and energy needs. On the contrary, large data windows are normally considered for the recognition of complex activities <U+2014> Window Size Impact in Human Activity Recognition, 2014. Nevertheless, it is common to use one to two seconds of sensor data in order to classify a current fragment of an activity. From the results, reduced windows (2 s or less) are demonstrated to provide the most accurate detection performance. In fact, the most precise recognizer is obtained for very short windows (0.25<U+2013>0.5 s), leading to the perfect recognition of most activities. Contrary to what is often thought, this study demonstrates that large window sizes do not necessarily translate into a better recognition performance. <U+2014> Window Size Impact in Human Activity Recognition, 2014. There is some risk that the splitting of the stream of sensor data into windows may result in windows that miss the transition of one activity to another. As such, it was traditionally common to split data into windows with an overlap such that the first half of the window contained the observations from the last half of the previous window, in the case of a 50% overlap. [¡¦] an incorrect length may truncate an activity instance. In many cases, errors appear at the beginning or at the end of the activities, when the window overlaps the end of one activity and the beginning of the next one. In other cases, the window length may be too short to provide the best information for the recognition process. <U+2014> A Dynamic Sliding Window Approach for Activity Recognition, 2011 It is unclear whether windows with overlap are required for a given problem. In the adoption of neural network models, the use of overlaps, such as a 50% overlap, will double the size of the training data, which may aid in modeling smaller datasets, but may also lead to models that overfit the training dataset. An overlap between adjacent windows is tolerated for certain applications; however, this is less frequently used. <U+2014> Window Size Impact in Human Activity Recognition, 2014. Convolutional Neural Network models, or CNNs for short, are a type of deep neural network that were developed for use with image data, e.g. such as handwriting recognition. They have proven very effective on challenging computer vision problems when trained at scale for tasks such as identifying and localizing objects in images and automatically describing the content of images. They are models that are comprised of two main types of elements: convolutional layers and pooling layers. Convolutional layers read an input, such as a 2D image or a 1D signal, using a kernel that reads in small segments at a time and steps across the entire input field. Each read results in an the input that is projected onto a filter map and represents an internal interpretation of the input. Pooling layers take the feature map projections and distill them to the most essential elements, such as using a signal averaging or signal maximizing process. The convolution and pooling layers can be repeated at depth, providing multiple layers of abstraction of the input signals. The output of these networks is often one or more fully connected layers that interpret what has been read and map this internal representation to a class value. For more information on convolutional neural networks, can see the post: CNNs can be applied to human activity recognition data. The CNN model learns to map a given window of signal data to an activity where the model reads across each window of data and prepares an internal representation of the window. When applied to time series classification like HAR, CNN has two advantages over other models: local dependency and scale invariance. Local dependency means the nearby signals in HAR are likely to be correlated, while scale invariance refers to the scale-invariant for different paces or frequencies. <U+2014> Deep Learning for Sensor-based Activity Recognition: A Survey, 2018. The first important work using CNNs to HAR was by Ming Zeng, et al in their 2014 paper ¡°Convolutional Neural Networks for Human Activity Recognition using Mobile Sensors.¡± In the paper, the authors develop a simple CNN model for accelerometer data, where each axis of the accelerometer data is fed into separate convolutional layers, pooling layers, then concatenated before being interpreted by hidden fully connected layers. The figure below taken from the paper clearly shows the topology of the model. It provides a good template for how the CNN may be used for HAR problems and time series classification in general. Depiction of CNN Model for Accelerometer DataTaken from ¡°Convolutional Neural Networks for Human Activity Recognition using Mobile Sensors¡± There are many ways to model HAR problems with CNNs. One interesting example was by Heeryon Cho and Sang Min Yoon in their 2018 paper titled ¡°Divide and Conquer-Based 1D CNN Human Activity Recognition Using Test Data Sharpening.¡± In it, they divide activities into those that involve movement, called ¡°dynamic,¡± and those where the subject is stationary, called ¡°static,¡± then develop a CNN model to discriminate between these two main classes. Then, within each class, models are developed to discriminate between activities of that type, such as ¡°walking¡± for dynamic and ¡°sitting¡± for static. Separation of Activities as Dynamic or StaticTaken from ¡°Divide and Conquer-Based 1D CNN Human Activity Recognition Using Test Data Sharpening¡± They refer to this as a two-stage modeling approach. Instead of straightforwardly recognizing the individual activities using a single 6-class classifier, we apply a divide and conquer approach and build a two-stage activity recognition process, where abstract activities, i.e., dynamic and static activity, are first recognized using a 2-class or binary classifier, and then individual activities are recognized using two 3-class classifiers. <U+2014> Divide and Conquer-Based 1D CNN Human Activity Recognition Using Test Data Sharpening, 2018. Quite large CNN models were developed, which in turn allowed the authors to claim state-of-the-art results on challenging standard human activity recognition datasets. Another interesting approach was proposed by Wenchao Jiang and Zhaozheng Yin in their 2015 paper titled ¡°Human Activity Recognition Using Wearable Sensors by Deep Convolutional Neural Networks.¡± Instead of using 1D CNNs on the signal data, they instead combine the signal data together to create ¡°images¡± which are then fed to a 2D CNN and processed as image data with convolutions along the time axis of signals and across signal variables, specifically accelerometer and gyroscope data. Firstly, raw signals are stacked row-by-row into a signal image [¡¦.]. In the signal image, every signal sequence has the chance to be adjacent to every other sequence, which enables DCNN to extract hidden correlations between neighboring signals. Then, 2D Discrete Fourier Transform (DFT) is applied to the signal image and its magnitude is chosen as our activity image <U+2014> Human Activity Recognition Using Wearable Sensors by Deep Convolutional Neural Networks, 2015. Below is a depiction of the processing of raw sensor data into images, and then from images into an ¡°activity image,¡± the result of a discrete Fourier transform. Processing of Raw Sensor Data into an ImageTaken from ¡°Human Activity Recognition Using Wearable Sensors by Deep Convolutional Neural Networks¡± Finally, another good paper on the topic is by Charissa Ann Ronao and Sung-Bae Cho in 2016 titled ¡°Human activity recognition with smartphone sensors using deep learning neural networks.¡± Careful study of the use of CNNs is performed showing that larger kernel sizes of signal data are useful and limited pooling. Experiments show that convnets indeed derive relevant and more complex features with every additional layer, although difference of feature complexity level decreases with every additional layer. A wider time span of temporal local correlation can be exploited (1¡¿9 <U+2013> 1¡¿14) and a low pooling size (1¡¿2 <U+2013> 1¡¿3) is shown to be beneficial. <U+2014> Human activity recognition with smartphone sensors using deep learning neural networks, 2016. Usefully, they also provide the full hyperparameter configuration for the CNN models that may provide a useful starting point on new HAR and other sequence classification problems, summarized below. Table of CNN Model Hyperparameter ConfigurationTaken from ¡°Human activity recognition with smartphone sensors using deep learning neural networks.¡± Recurrent neural networks, or RNNs for short, are a type of neural network that was designed to learn from sequence data, such as sequences of observations over time, or a sequence of words in a sentence. A specific type of RNN called the long short-term memory network, or LSTM for short, is perhaps the most widely used RNN as its careful design overcomes the general difficulties in training a stable RNN on sequence data. LSTMs have proven effective on challenging sequence prediction problems when trained at scale for such tasks as handwriting recognition, language modeling, and machine translation. A layer in an LSTM model is comprised of special units that have gates that govern input, output, and recurrent connections, the weights of which are learned. Each LSTM unit also has internal memory or state that is accumulated as an input sequence is read and can be used by the network as a type of local variable or memory register. For more information on long short-term memory networks, see the post: Like the CNN that can read across an input sequence, the LSTM reads a sequence of input observations and develops its own internal representation of the input sequence. Unlike the CNN, the LSTM is trained in a way that pays specific attention to observations made and prediction errors made over the time steps in the input sequence, called backpropagation through time. For more information on backpropagation through time, see the post: LSTMs can be applied to the problem of human activity recognition. The LSTM learns to map each window of sensor data to an activity, where the observations in the input sequence are read one at a time, where each time step may be comprised of one or more variables (e.g. parallel sequences). There has been limited application of simple LSTM models to HAR problems. One example is by Abdulmajid Murad and Jae-Young Pyun in their 2017 paper titled ¡°Deep Recurrent Neural Networks for Human Activity Recognition.¡± Important, in the paper they comment on the limitation of CNNs in their requirement to operate on fixed-sized windows of sensor data, a limitation that LSTMs do not strictly have. However, the size of convolutional kernels restricts the captured range of dependencies between data samples. As a result, typical models are unadaptable to a wide range of activity-recognition configurations and require fixed-length input windows. <U+2014> Deep Recurrent Neural Networks for Human Activity Recognition, 2017. They explore the use of LSTMs that both process the sequence data forward (normal) and both directions (Bidirectional LSTM). Interestingly, the LSTM predicts an activity for each input time step of a subsequence of sensor data, which are then aggregated in order to predict an activity for the window. There will [be] a score for each time-step predicting the type of activity occurring at time t. The prediction for the entire window T is obtained by merging the individual scores into a single prediction <U+2014> Deep Recurrent Neural Networks for Human Activity Recognition, 2017. The figure below taken from the paper provides a depiction of the LSTM model followed by fully connected layers used to interpret the internal representation of the raw sensor data. Depiction of LSTM RNN for Activity RecognitionTaken from ¡°Deep Recurrent Neural Networks for Human Activity Recognition.¡± It may be more common to use an LSTM in conjunction with a CNN on HAR problems, in a CNN-LSTM model or ConvLSTM model. This is where a CNN model is used to extract the features from a subsequence of raw sample data, and output features from the CNN for each subsequence are then interpreted by an LSTM in aggregate. An example of this is in the 2016 paper by Francisco Javier Ordonez and Daniel Roggen titled ¡°Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition.¡± We introduce a new DNN framework for wearable activity recognition, which we refer to as DeepConvLSTM. This architecture combines convolutional and recurrent layers. The convolutional layers act as feature extractors and provide abstract representations of the input sensor data in feature maps. The recurrent layers model the temporal dynamics of the activation of the feature maps. <U+2014> Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition, 2016. A deep network architecture is used with four convolutional layers without any pooling layers, followed by two LSTM layers to interpret the extracted features over multiple time steps. The authors claim that the removal of the pooling layers is a critical part of their model architecture, where the use of pooling layers after the convolutional layers interferes with the convolutional layers¡¯ ability to learn to downsample the raw sensor data. In the literature, CNN frameworks often include convolutional and pooling layers successively, as a measure to reduce data complexity and introduce translation invariant features. Nevertheless, such an approach is not strictly part of the architecture, and in the time series domain [¡¦] DeepConvLSTM does not include pooling operations because the input of the network is constrained by the sliding window mechanism [¡¦] and this fact limits the possibility of downsampling the data, given that DeepConvLSTM requires a data sequence to be processed by the recurrent layers. However, without the sliding window requirement, a pooling mechanism could be useful to cover different sensor data time scales at deeper layers. <U+2014> Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition, 2016. The figure below taken from the paper makes the architecture clearer. Note that layers 6 and 7 in the image are in fact LSTM layers. Depiction of CNN LSTM Model for Activity RecognitionTaken from ¡°Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition.¡± This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered the problem of human activity recognition and the use of deep learning methods that are achieving state-of-the-art performance on this problem. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. can you implement something as an example to recognize any human activity. I don¡¯t see why not. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): network(26), layer(25), model(19), activity(18), feature(18), window(16), method(11), sensor(9), cnn(8), problem(8)"
"5","vidhya",2018-10-01,"Text Mining 101: A Stepwise Introduction to Topic Modeling using Latent Semantic Analysis (using Python)","https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 Have you ever been inside a well-maintained library? I¡¯m always incredibly impressed with the way the librarians keep everything organized, by name, content, and other topics. But if you gave these librarians thousands of books and asked them to arrange each book on the basis of their genre, they will struggle to accomplish this task in a day, let alone an hour! However, this won¡¯t happen to you if these books came in a digital format, right? All the arrangement seems to happen in a matter of seconds, without requiring any manual effort. All hail Natural Language Processing (NLP). Source: confessionsofabookgeek.com Have a look at the below text snippet: As you might gather from the highlighted text, there are three topics (or concepts) <U+2013> Topic 1, Topic 2, and Topic 3. A good topic model will identify similar words and put them under one group or topic. The most dominant topic in the above example is Topic 2, which indicates that this piece of text is primarily about fake videos. Intrigued, yet? Good! In this article, we will learn about a text mining approach called Topic Modeling. It is an extremely useful technique for extracting topics, and one you will work with a lot when faced with NLP challenges. Note: I highly recommend going through this article<U+00A0>to understand terms like SVD and UMAP. They are leveraged in this article so having a basic understanding of them will help solidify these concepts. A Topic Model can be defined as an unsupervised technique to discover topics across various text documents. These topics are abstract in nature, i.e., words which are related to each other form a topic. Similarly, there can be multiple topics in an individual document. For the time being, let¡¯s understand a topic model as a black box, as illustrated in the below figure: This black box (topic model) forms clusters of similar and related words which are called topics. These topics have a certain distribution in a document, and every topic is defined by the proportion of different words it contains. Recall the example we saw earlier of arranging similar books together. Now suppose you have to perform a similar task with a few digital text documents. You would be able to manually accomplish this, as long as the number of documents is manageable (aka not too many of them). But what happens when there¡¯s an impossible number of these digital text documents? That¡¯s where NLP techniques come to the fore. And for this particular task, topic modeling is the technique we will turn to. Source: topix.io/tutorial/tutorial.html Topic modeling helps in exploring large amounts of text data, finding clusters of words, similarity between documents, and discovering abstract topics. As if these reasons weren¡¯t compelling enough, topic modeling is also used in search engines wherein the search string is matched with the results. Getting interesting, isn¡¯t it? Well, read on then! All languages have their own intricacies and nuances which are quite difficult for a machine to capture (sometimes they¡¯re even misunderstood by us humans!). This can include different words that mean the same thing, and also the words which have the same spelling but different meanings. For example, consider the following two sentences: In the first sentence, the word ¡®novel¡¯ refers to a book, and in the second sentence it means new or fresh. We can easily distinguish between these words because we are able to understand the context behind these words. However, a machine would not be able to capture this concept as it cannot understand the context in which the words have been used. This is where Latent Semantic Analysis (LSA) comes into play as it attempts to leverage the context around the words to capture the hidden concepts, also known as topics.  So, simply mapping words to documents won¡¯t really help. What we really need is to figure out the hidden concepts or topics behind the words. LSA is one such technique that can find these hidden topics. Let¡¯s now deep dive into the inner workings of LSA. Let¡¯s say we have m number of text documents with n number of total unique terms (words). We wish to extract k topics from all the text data in the documents. The number of topics, k, has to be specified by the user. It¡¯s time to power up Python and understand how to implement LSA in a topic modeling problem. Once your Python environment is open, follow the steps I have mentioned below. Let¡¯s load the required libraries before proceeding with anything else. In this article, we will use the ¡¯20 Newsgroup¡¯ dataset from sklearn. You can download the dataset here, and follow along with the code. Output: 11,314 The dataset has 11,314 text documents distributed across 20 different newsgroups. To start with, we will try to clean our text data as much as possible. The idea is to remove the punctuations, numbers, and special characters all in one step using the regex replace(¡°[^a-zA-Z#]¡±, ¡± ¡°), which will replace everything, except alphabets with space. Then we will remove shorter words because they usually don¡¯t contain useful information. Finally, we will make all the text lowercase to nullify case sensitivity. It¡¯s good practice to remove the stop-words from the text data as they are mostly clutter and hardly carry any information. Stop-words are terms like ¡®it¡¯, ¡®they¡¯, ¡®am¡¯, ¡®been¡¯, ¡®about¡¯, ¡®because¡¯, ¡®while¡¯, etc. To remove stop-words from the documents, we will have to tokenize the text, i.e., split the string of text into individual tokens or words. We will stitch the tokens back together once we have removed the stop-words. This is the first step towards topic modeling. We will use sklearn¡¯s TfidfVectorizer to create a document-term matrix with 1,000 terms. We could have used all the terms to create this matrix but that would need quite a lot of computation time and resources. Hence, we have restricted the number of features to 1,000. If you have the computational power, I suggest trying out all the terms. The next step is to represent each and every term and document as a vector. We will use the document-term matrix and decompose it into multiple matrices. We will use sklearn¡¯s TruncatedSVD to perform the task of matrix decomposition. Since the data comes from 20 different newsgroups, let¡¯s try to have 20 topics for our text data. The number of topics can be specified by using the<U+00A0>n_components<U+00A0>parameter. The components of svd_model are our topics, and we can access them using svd_model.components_. Finally, let¡¯s print a few most important words in each of the 20 topics and see how our model has done. To find out how distinct our topics are, we should visualize them. Of course, we cannot visualize more than 3 dimensions, but there are techniques like PCA and t-SNE which can help us visualize high dimensional data into lower dimensions. Here we will use a relatively new technique called UMAP (Uniform Manifold Approximation and Projection). As you can see above, the result is quite beautiful. Each dot represents a document and the colours represent the 20 newsgroups. Our LSA model seems to have done a good job. Feel free to play around with the parameters of UMAP to see how the plot changes its shape. The entire code for this article can be found in this GitHub repository. Latent Semantic Analysis can be very useful as we saw above, but it does have its limitations. It¡¯s important to understand both the sides of LSA so you have an idea of when to leverage it and when to try something else. Pros: Cons: Apart from LSA, there are other advanced and efficient topic modeling techniques such as<U+00A0>Latent Dirichlet Allocation (LDA) and<U+00A0>lda2Vec. We have a wonderful article on LDA which you can check out here. lda2vec is a much more advanced topic modeling which is based on word2vec word embeddings. If you want to find out more about it, let me know in the comments section below and I¡¯ll be happy to answer your questions/. This article is an attempt to share my learnings with all of you. Topic modeling is quite an interesting topic and equips you with the skills and techniques to work with many text datasets. So, I urge you all to use the code given in this article and apply it to a different dataset. Let me know if you have any questions or feedback related to this article. Happy text mining!","Keyword(freq): topic(19), document(10), term(6), concept(4), technique(4), book(3), newsgroup(3), cluster(2), dimension(2), librarian(2)"
"6","vidhya",2018-09-27,"Building DataHack Summit 2018 <U+2013> India¡¯s Most Advanced AI Conference. Are you Ready?","https://www.analyticsvidhya.com/blog/2018/09/building-datahack-summit-2018-indias-most-advanced-ai-conference-are-you-ready/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 Usain Bolt created a World record by running 200m sprint in<U+00A0>19.30 seconds in 2008. What do you think he thought while he was preparing for 2009? He had his mind set to beat his own personal best and he did! Why am I talking about Bolt here? Well, I find myself in a similar situation. DataHack Summit 2017 was an unprecedented success. We created India¡¯s largest conference with unilateral focus on data science practitioners. The community loved the focus, the content and the knowledge sharing at the event. If you haven¡¯t seen already <U+2013> check out the highlights below. What are we thinking now as we are building India¡¯s most advanced data science conference? If you cut through my mind and get a peek inside <U+2013> this is what you will find <U+0001F642> The venue is bigger than last year but tickets are selling like hot cakes so make sure you grab yours before they¡¯re sold out. Prices go up on September 30th so avail the discount today! Head over here to book your seat<U+00A0>for India¡¯s most advanced conference on AI, Machine Learning, Deep Learning, and IoT! Let¡¯s take a quick tour around DHS 2018 to see how it¡¯s shaping up and what we have in store for you. If there is one place we bet our reputation on <U+2013> it is the content we create and we curate. DataHack Summit 2018 will be an epitome of this. To be honest, we are having a tough time saying no to very exciting talk proposals. Here are a few<U+00A0>eminent speakers in AI and ML who will be speaking at DataHack Sumit 2018: The most exciting thing which people would see are the<U+00A0>Hack sessions.<U+00A0>They saw a tremendous response from the audience last year, and have been expanded to reflect the latest breakthrough developments. Below are a few topics to whet your appetite (click on each session to read more about what will be covered): And here are a few awesome hackers, who will be performing live hack sessions: Check out the full speaker line-up<U+00A0>here. We will top up the sessions and Hack Sessions with an exclusive Startup Showcase and Research Track. We will showcase some of the most exciting AI and ML startups across the globe to showcase their offerings. Prepare to have your mind blown by some of the most amazing uses of AI and ML in a variety of domains. In addition to this, there is an entire track dedicated to cutting-edge research! We are giving individuals the opportunity to come and present their work in front of our community. This year¡¯s venue is none other than the NIMHANS Convention Center in Bengaluru. There are three auditoriums (yes, three!) <U+2013> so you are going to see 3 parallel tracks. So you can look forward to more sessions, more industry leaders, and more engagement! And all this space means an opportunity for even more events. There will be more hack sessions this year, and each session will have an even bigger audience than before. DataHack Summit 2018 will have bigger and swankier LED screens as well! So regardless of where you¡¯re sitting, the presentation and code will be visible from all corners of the room. Reserve your seat TODAY! There is an incredible deal on offer and prices will go up on September 30th. So act now and become a part of India¡¯s most advanced AI and ML conference.","Keyword(freq): session(6), price(2), auditorium(1), cake(1), corner(1), development(1), domain(1), event(1), hacker(1), highlight(1)"
"7","vidhya",2018-09-27,"A Multivariate Time Series Guide to Forecasting and Modeling (with Python codes)","https://www.analyticsvidhya.com/blog/2018/09/multivariate-time-series-guide-forecasting-modeling-python-codes/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 Time is the most critical factor that decides whether a business will rise or fall. That¡¯s why we see sales in stores and e-commerce platforms aligning with festivals. These businesses analyze years of spending data to understand the best time to throw open the gates and see an increase in consumer spending. But how can you, as a data scientist, perform this analysis? Don¡¯t worry, you don¡¯t need to build a time machine! Time Series modeling is a powerful technique that acts as a gateway to understanding and forecasting trends and patterns. But even a time series model has different facets. Most of the examples we see on the web deal with univariate time series. Unfortunately, real-world use cases don¡¯t work like that. There are multiple variables at play, and handling all of them at the same time is where a data scientist will earn his worth. In this article, we will understand what a multivariate time series is, and how to deal with it. We will also take a case study and implement it in Python to give you a practical understanding of the subject. This article assumes some familiarity with univariate time series, its properties and various techniques used for forecasting. Since this article will be focused on multivariate time series, I would suggest you go through the following articles which serve as a good introduction to univariate time series: But I¡¯ll give you a quick refresher of what a univariate time series is, before going into the details of a multivariate time series. Let¡¯s look at them one by one to understand the difference. A univariate time series, as the name suggests, is a series with a single time-dependent variable. For example, have a look at the sample dataset below that consists of the temperature values (each hour), for the past 2 years. Here, temperature is the dependent variable (dependent on Time). If we are asked to predict the temperature for the next few days, we will look at the past values and try to gauge and extract a pattern. We would notice that the temperature is lower in the morning and at night, while peaking in the afternoon. Also if you have data for the<U+00A0>past few years, you would observe that it is colder during the months of November to January, while being comparatively hotter in April to June. Such observations will help us in predicting future values. Did you notice that we used only one variable (the temperature of the past 2 years,)? Therefore, this is called Univariate Time Series Analysis/Forecasting. A Multivariate time series has more than one time-dependent variable. Each variable depends not only on its past values but also has some dependency on other variables. This dependency is used for forecasting future values. Sounds complicated? Let me explain. Consider the above example. Now suppose our dataset includes perspiration percent, dew point, wind speed, cloud cover percentage, etc. along with the temperature value for the past two years. In this case, there are multiple variables to be considered to optimally predict temperature. A series like this would fall under the category of multivariate time series. Below is an illustration of this: Now that we understand what a multivariate time series looks like, let us understand how can we use it to build a forecast. In this section, I will introduce you to one of the most commonly used methods for multivariate time series forecasting <U+2013> Vector Auto Regression (VAR). In a VAR model, each variable is a linear function of the past values of itself and the past values of all the other variables. To explain this in a better manner, I¡¯m going to use a simple visual example: We have two variables, y1 and y2. We need to forecast the value of these two variables at time t, from the given data for past n values. For simplicity, I have considered the lag value to be 1. For calculating y1(t), we will use the past value of y1 and y2. Similarly, to calculate y2(t), past values of both y1 and y2 will be used. Below is a simple mathematical way of representing this relation: Here, These equations are similar to the equation of an<U+00A0>AR process. Since the AR process is used for univariate time series data, the future values are linear combinations of their own past values only. Consider the AR(1) process: y(t) = a + w*y(t-1) +e In this case, we have only one variable <U+2013> y, a constant term <U+2013> a, an error term <U+2013> e, and a coefficient <U+2013> w. In order to accommodate the multiple variable terms in each equation for VAR, we will use vectors.<U+00A0> We can write the equations (1) and (2) in the following form : The two variables are y1 and y2, followed by a constant, a coefficient metric, lag value, and an error metric. This is the vector equation for a VAR(1) process. For a VAR(2) process, another vector term for time (t-2) will be added to the equation to generalize for p lags: The above equation represents a VAR(p) process with variables y1, y2 ¡¦yk. The same can be written as: The term ¥åt in the equation represents multivariate vector white noise. For a multivariate time series,<U+00A0>¥åt should be a continuous random vector that satisfies the following conditions: Recall the temperate forecasting example we saw earlier. An argument can be made for it to be treated as a multiple univariate series. We can solve it using simple univariate forecasting methods like AR. Since the aim is to predict the temperature, we can simply remove the other variables (except temperature) and fit a model on the remaining univariate series. Another simple idea is to forecast values for each series individually using the techniques we already know. This would make the work extremely straightforward! Then why should you learn another forecasting technique? Isn¡¯t this topic complicated enough already? From the above equations (1) and (2), it is clear that each variable is using the past values of every variable to make the predictions. Unlike AR, VAR is able to understand and use the relationship between several variables. This is useful for describing the dynamic behavior of the data and also provides better forecasting results. Additionally, implementing VAR is as simple as using any other univariate technique (which you will see in the last section). We know from studying the univariate concept that a stationary time series will more often than not give us a better set of predictions. If you are not familiar with the concept of stationarity, please go through this article first: A Gentle Introduction to handling non-stationary Time Series. To summarize, for a given univariate time series: y(t) = c*y(t-1) + ¥å t The series is said to be stationary if the value of |c| < 1. Now, recall the equation of our VAR process: Note: I is the identity matrix. Representing the equation in terms of Lag operators, we have: Taking all the y(t) terms on the left-hand side: The coefficient of y(t) is called the lag polynomial. Let us represent this as ¥Õ(L): For a series to be stationary, the eigenvalues of |¥Õ(L)-1| should be less than 1 in modulus. This might seem complicated given the number of variables in the derivation. This idea has been explained using a simple numerical example in the following video. I highly encourage watching it to solidify your understanding: Similar to the Augmented Dickey-Fuller test for univariate series, we have Johansen¡¯s test for checking the stationarity of any multivariate time series data. We will see how to perform the test in the last section of this article. If you have worked with univariate time series data before, you¡¯ll be aware of the train-validation sets. The idea of creating a validation set is to analyze the performance of the model before using it for making predictions. Creating a validation set for time series problems is tricky because we have to take into account the time component. One cannot directly use the train_test_split or k-fold validation since this will disrupt the pattern in the series. The validation set should be created considering the date and time values. Suppose we have to forecast the temperate, dew point, cloud percent, etc. for the next two months using data from the last two years. One possible method is to keep the data for the last two months aside and train the model on the remaining 22 months. Once the model has been trained, we can use it to make predictions on the validation set. Based on these predictions and the actual values, we can check how well the model performed, and the variables for which the model did not do so well. And for making the final prediction, use the complete dataset (combine the train and validation sets). In this section, we will implement the Vector AR model on a toy dataset. I have used the Air Quality dataset for this and you can download it from here. The data type of the<U+00A0>Date_Time column is object<U+00A0>and we need to change it to datetime. Also, for preparing the data, we need the index to have datetime. Follow the below commands: The next step is to deal with the missing values. Since the missing values in the data are replaced with a value -200, we will have to impute the missing value with a better number. Consider this <U+2013> if the present dew point value is missing, we can safely assume that it will be close to the value of the previous hour. Makes sense, right? Here, I will impute -200 with the previous value. You can choose to substitute the value using the average of a few previous values, or the value at the same time on the previous day (you can share your idea(s) of imputing missing values in the comments section below). Below is the result of the test: We can now go ahead and create the validation set to fit the model, and test the performance of the model: The predictions are in the form of an array, where each list represents the predictions of the row. We will transform this into a more presentable format. Output of the above code: After the testing on validation set, lets fit the model on the complete dataset Before I started this article, the idea of working with a multivariate time series seemed daunting in its scope. It is a complex topic, so take your time in understanding the details. The best way to learn is to practice, and so I hope the above Python implemenattion will be useful for you. I enocurage you to use this approach on a dataset of your choice. This will further cement your understanding of this complex yet highly useful topic. If you have any suggestions or queries, share them in the comments section. HI. Thanks for sharing the knowledge and the great article! Could you pls add some details regarding the stationarity test process described in the article : the test is done and the results are presented but it is not clear if it could be concluded that the data is stationary; after the test is done no further actions to make the data stationary are performed¡¦why so. Thanks Why not just use Random Forest for this?
Thank you Hi John, random forest can be used for supervised machine learning algorithms. In this case, we don¡¯t have a test set. Considering the example for weather prediction used in section 1 -if you consider temperature as target variable and the rest as independent variables, the test set must have the independent variables, which is not the case here. Using VAR , we predict all the variables .","Keyword(freq): value(19), variable(15), prediction(7), detail(3), equation(3), term(3), comment(2), method(2), result(2), set(2)"
"8","vidhya",2018-09-27,"The Winning Approaches from codeFest 2018 <U+2013> NLP, Computer Vision and Machine Learning!","https://www.analyticsvidhya.com/blog/2018/09/the-winning-approaches-from-codefest-2018-nlp-computer-vision-and-machine-learning/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 Analytics Vidhya¡¯s hackathons are one of the best ways to evaluate how far you¡¯ve traveled in your data science journey. And what better way than to put your skills to the test against the top data scientists from around the globe? Participating in these hackathons also helps you understand where you need to improve and what else you can learn to get a better score in the next competition. And a very popular demand after each hackathon is to see how the winning solution was designed and the thought process behind it. There¡¯s a lot to learn from this, including how you can develop your own unique framework for future hackathons. We are all about listening to our community, so we decided to curate the winning approaches from our recently concluded hackathon series, codeFest! This was a series of three hackathons in partnership with IIT-BHU, conducted between 31st August and 2nd September. The competition was intense, with more than 1,900 aspiring data scientists going head-to-head to grab the ultimate prize! Each hackathon had a unique element to it. Interested in finding out more? You can view the details of each competition below: It¡¯s time to check out the winners¡¯ approaches! Abhinav Gupta and Abhishek Sharma. The participants were given a list of tweets from customers about various tech firms who manufacture and sell mobiles, computers, laptops, etc. The challenge was to find the tweets which showed a negative sentiment towards such companies or products. The metric used for evaluating the performance of the classification model was weighted F1-Score. Abhinav and Abhishek have summarized their approach in a very intuitive manner, explaining everything from preprocessing and feature engineering to model building. Pre-processing: Feature Extraction: Classifiers used: They<U+00A0>hypertuned each of the above classifiers and found that LSTM (with attention mechanism) produced the best result. Ensemble Deepak Rawat. The Vista hackathon had a pretty intriguing problem statement. The participants had to build a model that counted the number of people in a given group selfie/photo. The dataset provided had already been split, wherein the training set consisted of images with coordinates of the bounding boxes and headcount for each image. The evaluation metric for this competition was RMSE (root mean squared error) over the headcounts predicted for test images. Check out Deepak¡¯s approach in his own words below: Mask R-CNN and<U+00A0>ResNet101 Both stages are connected to the backbone structure. Pre-processing  Model Building Raj Shukla. As a part of enigma competition, the target was to predict the number of upvotes on a question based on other information provided. For every question <U+2013> its tag, number of views received, number of answers, username and reputation of the question author, was provided. Using this information, the participant had to predict the upvote count that the question will receive. The evaluation metric for this competition was RMSE (root mean squared error). Below is the data dictionary for your reference: Here is Raj¡¯s approach to cracking the Enigma hackathon: Feature Engineering: My focus was on feature engineering, i.e., using the existing features to create new features. Below are some key features I cooked up: Model Building: A big thank you to everyone for participating in codeFest 2018! This competition was all about quick and structured thinking, coding, experimentation, and finding the one approach that got you up the leaderboard. In short, what machine learning is all about! Missed out this time? Don¡¯t worry, you can check out all upcoming hackathons on our DataHack platform and register yourself today!","Keyword(freq): hackathon(5), feature(3), approach(2), classifier(2), image(2), participant(2), scientist(2), tweet(2), analytics(1), answer(1)"
