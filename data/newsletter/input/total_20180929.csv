"","site","date","headline","url_address","text","keyword"
"1","datacamp",2018-09-25,"What is Data Engineering?","https://www.datacamp.com/community/blog/data-engineering","In order to start course creation, we¡¯ll need to pick a single definition of ¡°Data Engineer¡± to work from. After much deliberation and thought, we chose to paraphrase the American television show ¡°Law and Order¡±: In the world of Data Science, the data are represented by three separate yet equally important professions: For example, imagine that a company sells many different types of sofas on their website.  Each time a visitor to the website clicks on a particular sofa, a new piece of data is created.  A Data Engineer would define how to collect this data, what types of metadata should be appended to each click event, and how to store the data in an easy-to-access format.  A Data Analyst would create visualizations to help sales and marketing track who is buying each sofa and how much money the company is making.  A Data Scientist would take the data on which customers bought each sofa and use it to predict the perfect sofa for each new visitor to the website. For many organizations, data engineers are the first hires on a data team.  Before collected data can be analyzed and leveraged with predictive methods, it needs to be organized and cleaned. Data Engineers begins this process by making a list of what data is stored, called a data schema.  Next, they need to pick a reliable, easily accessible location, called a data warehouse, for storing the data. Examples of data warehousing systems include Amazon Redshift or Google Cloud.  Finally, Data Engineers create ETL (Extract, Transform and Load) processes to make sure that the data gets into the data warehouse. As an organization grows, Data Engineers are responsible for integrating new data sources into the data ecosystem, and sending the stored data into different analysis tools. When the data warehouse becomes very large, Data Engineers have to find new ways of making analyses performative, such as parallelizing analysis or creating smaller subsets for fast querying. Within the Data Science universe, there is always overlap between the three professions. Data Engineers are often responsible for simple Data Analysis projects or for transforming algorithms written by Data Scientists into more robust formats that can be run in parallel. Data Analysts and Data Scientists need to learn basic Data Engineering skills, especially if they¡¯re working in an early-stage startup where engineering resources are scarce. At DataCamp, we¡¯re excited to build out our Data Engineering course offerings. We know what we want to teach, and we¡¯re starting to recruit instructors to design these courses. If you¡¯re interested, check out our application and the list of courses we are currently prioritizing.","Keyword(freq): engineer(6), profession(2), scientist(2), type(2), algorithm(1), analysis(1), analyst(1), customer(1), example(1), format(1)"
"2","datacamp",2018-09-24,"Uncertainty in Data Science (Transcript)","https://www.datacamp.com/community/blog/uncertainty-data-science","Here is a link to the podcast. Hugo:               Hi, there, Allen, and welcome to DataFramed. Allen:               Hey, Hugo. Thank you very much. Hugo:               Such a pleasure to have you on the show, and I'm really excited to have you here to talk about uncertainty in data science, how we think about prediction, and how we can think probabilistically, and how we do it right, and how we can get it wrong as well, but before we get into that, I'd love to find out a bit about you, and so I'm wondering what you're known for in the data community. Allen:               Right. Well, I'm working on a book series that's called Think X, for all X, so hopefully some people know about that. Think Python is kind of the starting point, and then for data science, Think Stats and Think Bayes, for data science and for Bayesian statistics. Hugo:               Great, and so why Think? Allen:               Came about, roundabout, the original book was called How to Think Like a Computer Scientist, and it was originally a Java book, and then it became a Python book, and then it wasn't really about programming. It was about bigger ideas, and so then when I started the other books, the premise of the books is that you're using computation as a tool to learn something else, so it's a way of thinking, it's an approach to the topic, and so that's how we got to the schema that's always think something for various values of something. Hugo:               Right. I like that a lot, and speaking to this idea of computation, I know you're a huge proponent of the role of computation in helping us to think, so maybe you can speak to that for a minute. Allen:               Sure. I mean, it partly comes ... I've been teaching in an engineering program, and engineering education has been very math-focused for a long time, so the curriculum, you have to take a lot of calculus and linear algebra before you get to do any engineering, and it doesn't have to be that way at all. I think there are a lot of ideas in engineering that you can get to very quickly computationally that are much harder mathematically. Allen:               One of the examples that comes up all the time is integration, which is a little bit of a difficult idea. Students, when they see an integral sign, immediately there's gonna be some challenge there, but if you do everything discretely, you can take all of those integrals, you just turn them into summations, and then if you do it computationally, you take all of the summations and turn them into for loops, and then you can have very clear code where you're looping through space, you're adding up all of the elements. That's what an integral is. Hugo:               Absolutely, and I think another place that you've thought about a lot, and a lot of us have worked in where this rears its head is the idea of using computation and sampling and re-sampling datasets to get an idea about statistics. Right? Allen:               Right. Yeah. I think classical statistical inference, looking at things like confidence intervals and hypothesis tests, re-sampling is a very powerful tool. You're running simulations of the system, and you can compute things like sampling distribution or a p-value in a very straightforward way, meaning that it's easy to do, but it also just makes the concept transparent. It's really obvious what's going on. Hugo:               That's right, and you actually ... We've had a segment on the podcast previously, which is ... It's blog post of the week, and we had one on your blog post, There Is Only One Test, which really spells out the idea of that in the world of statistical hypothesis testing, there is really only one test, and the idea of you can actually see that, and this one of your great points, you can see that when you take the sampling, re-sampling, bootstrapping approach. Right? Allen:               Right. Yeah. I think it makes the framework visible, that hypothesis tests, there's a model of the null hypothesis, and that's gonna be different for different scenarios, and there's the test statistic, and that's gonna be different for different scenarios, but once you've specified those two pieces, everything else is the same. You're running the same framework. So, I think it makes the concept much clearer. Hugo:               Great, and we'll link to that in the show notes. We'll also link to your fantastic followup post called ""There Is Still Only One Test"". Allen:               Well, that's just because I didn't explain it very well the first time, so I had to try again. Hugo:               It also proves the point, though, that there is still only one test, and I'll repeat that, that there is still only one test. So, how did you get into data science originally? Allen:               Well, my background is computer science, so there are a lot of ways, a lot of doors into data science, but I think computer science is certainly one of the big ones. I did ... My master's thesis was on computer vision, so that was kind of a step in that direction. My PhD was all about measuring and modeling computational systems, so there are a lot of things that come in there like long tail distributions, and then in 2009 I did a sabbatical, and I was working at Google in a group that was working on internet performance, so we were doing a lot of measurement, modeling, statistical descriptions, and predictive modeling, so that's kind of where it started to get serious, and that's where I started when I was working on Think Stats for the first time. Hugo:               So, this origin story of you getting involved in data science I think makes an interesting point, that you've actually touched a lot of different types of data, and I know that you're a huge fan of the idea that data science isn't necessarily only for data scientists, that it actually could be of interest to everyone because it touches ... There are so many touch points with the way we live and data science. Right? Allen:               Right. Yeah. This is one of my things that I get a little upset about, is when people talk about data science, and then they talk about big data, and then they talk about quantitative finance and business analytics, like that's all there is, and I use a broader notion of what data science is. I'd like to push the idea that it's any time that you're using data to answer questions and to guide decision making, because that includes a lot of science, which is often about answering questions, a lot about engineering where you're designing a system to achieve a particular goal, and of course, decision making, both on an individual or a business or a national public policy level. So, I'd like to see data science involved in all of those pieces. Hugo:               Absolutely. So, we're here to talk about uncertainty today. One part of data science is making predictions, which we'll get to, but the fact that we live in an uncertain world is incredibly interesting because what we do as a culture and a society, we use probability to think about uncertainty, so I'm wondering your thoughts on whether we us humans are actually good at thinking probabilistically. Allen:               Right. It's funny because we are and we are not at the same time. Hugo:               I'm glad you didn't say we probably are. Allen:               Right. Yeah. That would've been good. So, we do seem to have some instinct for probabilistic thinking, even for young children. We do something that's like a Bayesian update. When we get new data, if we're uncertain about something, we get new evidence, we update our beliefs, and in some cases we actually do a pretty good approximation of an accurate Bayesian update, typically for things that are kind of in the middling range of probability, maybe from about 25% to 75%. At the same time, we're terrible at very rare things. Small probabilities we're pretty bad at, and then there are a bunch of ways that we can be consistently fooled because we're not actually doing the math. We're doing approximations to it, and those approximations fail consistently in ways that behavioral psychologists have pointed out, things like confirmation bias and other cognitive failures like that. Hugo:               Absolutely. So, I want to speak to an article you wrote on your blog called Why Are We So Surprised?, in which you stated, ¡°In theory, we should not be surprised by the outcome of the 2016 presidential election, but in practice, we are.¡± So, I'm wondering why you think we shouldn't have been surprised. Allen:               Right. Well, a lot of the forecasts, a lot of the models coming from FiveThirtyEight and from The New York Times, they were predicting that Trump had about a 25% chance, maybe more, of winning the election. So, if something's got a 25% chance, that's the same as flipping a coin twice and getting heads twice. You wouldn't be particularly surprised by that. So, in theory a 25% risk shouldn't be surprising, but in practice, I think people still don't really understand probabilistic predictions. Allen:               One reason we can see that is the lack of symmetry, which is, if I tell you that Trump has a 25% chance of winning, you think, ¡°Well, okay. That might happen,¡± but when FiveThirtyEight said that Hillary Clinton had a 70% chance of winning, I think a lot of people interpreted that as a deterministic prediction, that FiveThirtyEight was saying, ¡°Hillary Clinton is going to win,¡± and then when that didn't happen, they said, ¡°Well, then FiveThirtyEight was wrong,¡± and I don't think that's the right interpretation of a probabilistic prediction. If someone tells you there's a 70% chance and it doesn't happen, that should be mildly surprising, but it doesn't necessarily mean that the prediction was wrong. Hugo:               Yeah, and in your article, you actually make a related point that everybody predicted at some level, well, predicted that Hillary had over a 50% chance of winning, and you made the point that people interpreted this as there was consensus that Hillary would win with different degrees of confidence, but that's ... So, as you stated, that's interpreting it as deterministic predictions, not probabilistic predictions. Right? Allen:               Yeah, I think that's right, and it also ... It fails the symmetry test again because different predictions, they ranged all the way from 70% to 99%, and people reacted as if that was a consensus, but that's not a consensus. If you flip it around, that's the range from saying that Trump has anywhere between 1% and 30% chance of winning, and if the predictions had been expressed that way, I think people would've looked at that and said, ¡°Oh, clearly there's not a consensus there, because there's a big difference between 1% and 30%.¡± Hugo:               I really like this analogy to flipping coins, because it puts a lot of things in perspective, and another example, as you mention in your article, The New York Times gave Trump a 9% chance of winning, and if you flip a coin four times in a row and get four heads, that's relatively surprising, but you wouldn't be like, ¡°Oh, I can't believe that happened,¡± and that has a 6.25% chance of happening. Right? Allen:               Right. Yeah, I think that's a good way to get a sense for what these probabilities mean. Hugo:               Absolutely. So, you mentioned also that these models were actually relatively credible models, so maybe you can speak to that. Allen:               Yeah. I think going in, two reasons to think that these predictions were credible, one of them was just past performance, that FiveThirtyEight and The New York Times had done well in previous elections, but maybe more important, their methodology was transparent. They were showing you all of the poll data that they were using as inputs, and I think they weren't actually publishing the algorithms, but they gave a lot of detail about how these things were working. Some polls are more believable than others. They were applying correction factors, and they also had ... They were taking time into account. So, a more recent poll would be weighted more heavily than a poll that was farther into the past. So, all of those, I think ahead of the fact, we had good reasons to believe the predictions, and after the fact, even though the outcome wasn't what we expected, that really just doesn't mean that the models are wrong. Hugo:               So, with all of this knowledge around how uncertain we are about uncertainty and how we can be good and bad about thinking probabilistically, what approaches can we as a data reporting community take to communicate around uncertainty better in the future? Allen:               Right. I think we don't know yet, but one of the things that I think is good is that people are trying a lot of different things. So, again, taking the election as an example, The New York Times had the twitchy needle that was sort of famously maybe not the best way to represent that information. There were other examples. Nate Silver's predictions are based on running many simulations. So, he would show a histogram that would show the outcome of doing many, many simulations, and that I think probably works for some audiences. I think it's tough for other audience. Allen:               One of the suggestions I made that I would love to see someone try is instead of running many simulations and trying to summarize the results, I'd love to see one simulation per day with the results of one simulation presented in detail. So, thinking back to 2016, suppose that every day you looked in the paper, and it showed you one possible outcome of the election, and let's say that Nate Silver's predictions were right, and there was a 70% chance that Clinton would win. So, in a given week, you would see Clinton win maybe four or five times. You would see Trump win two or three times, and I think at the end of that week, your intuition would actually have a good sense for that probability. Hugo:               I think that's an incredible idea, because what it speaks to for me personally is you're not really looking at these simulations or these results in the abstract. You're actually experiencing them firsthand in some way. Allen:               Exactly. So, you get the emotional effect of opening the paper and seeing that Trump won, and if that's already happened a few times in simulation, then the reality would be a lot less surprising. Hugo:               Absolutely. Are there any other types of approaches or ways of thinking that you'd like to see more in the future? Allen:               Well, as I said, I think there are a lot of experiments going, so I think we will get better at communicating these ideas, and I think the audience is also learning, so different visualizations that wouldn't have worked very well a few years ago, now people are I think just better at interpreting data, interpreting visualizations, because it's become part of the media in a way that it wasn't. If you'd look back not that long ago, I don't know if you remember when USA Today started doing infographics, and that was a thing. People were really excited about those infographics, and you look back at those things now, and they're terrible. It'll be like- Hugo:               Mm-hmm (affirmative). We've come a long way. Allen:               It's something that's really just a bar chart, except that the bar is made up of stacked up apples and stacked up oranges, and that was data visualization, say, 20 years ago, and now you look at the things that The New York Times is doing with interactive visualizations. I saw one the other day, which is their three-dimensional visualization of the yield curve, which is a tough idea in finance and economics, and a 3-D visualization is tough, and interactive visualization is challenging, so maybe it doesn't work for every audience, but I really appreciated just the ambition of it. Hugo:               So, you mentioned the role of data science in decision making in general, and I think in a lot of ways, we make decisions based on all the data we have, and then a decision is made, but a lot of the time, the quality of the decision will be rated on the quality of the outcome, which isn't necessarily the correct way to think about these things. Right? Allen:               Right. I gave an example about Blackjack, that you can make the right play in Blackjack. You take a hit when you're supposed to take a hit, and if you go bust, it's tempting to say, ¡°Oh. Well, I guess I shouldn't have done that,¡± but that's not correct. You made the right play, and in the long run that's the right decision. Any specific outcome is not necessarily gonna go your way. Hugo:               Yeah, but we know that in that case because we can evaluate the predictions based on the theory we have and the simulations we have in our mind or computationally. Right? On long-term rates, essentially. Allen:               Right. Yeah. Blackjack is easy because every game of Blackjack is kind of the same, so you've got these identical trials. You've got long-term rates. We have a harder time with single-case predictions, single-case probabilities. Hugo:               Like election forecasting? Allen:               Like elections, right, but in that case, right, you can't evaluate a single prediction. You can't say specifically whether it's right or wrong, but you can evaluate the prediction process. You can check to make sure that probabilistic predictions are calibrated. So, maybe getting back to Nate Silver again, in The Signal and the Noise, he uses a nice example, which is the National Weather Service, which is, they make probabilistic predictions. They say, ¡°20% chance of rain, 80% chance of rain,¡± and on any given day, you don't know if they were wrong. Allen:               So, if they 20% then it rains, or if they say 80% and it doesn't rain, that's a little bit surprising, but it doesn't make them wrong. But in the long run, if you keep track of every single time that they say 20% and then you count up how many times does it actually rain on 20% days, and how many times does it rain on 80% days, if the answer is 20% and 80%, then that's a well-calibrated probabilistic prediction. Hugo:               Absolutely. So, this is another example. The weather is one. We've talked about election forecasting, and these are both examples where it's we really need to think about uncertainty. I'm wondering what other examples in society are where we need to think about uncertainty and why they're important. Allen:               Yep. Well, a big one ... Anything that's related to health and safety, those are all cases where we're talking about risks, we're talking about interventions that have certain probabilities of good outcomes, certain probabilities of side effects, and those are other cases, I think, where sometimes our heuristics are good, and other times we make really consistent cognitive errors. Hugo:               There are a lot of cognitive biases, and one that I fall prey to constantly is, I'm not even sure what it's called, but it's when you have a small sample size, and I see something occur several times, I'm like, ¡°Oh, that's probably the way things work.¡± Allen:               Right. Yeah. I guess that's a form of over-fitting. In statistics, there's sort of a joke that people talk about the law of small numbers, but that's right. I think that's a version of jumping to conclusions. That's an example where I think doctors have had a version of that in the past, which is they make decisions often about treatment that are based on their own patients, so, ¡°Such-and-such a drug has worked well for my patients, and I've seen bad outcomes with my patients,¡± as contrasted with using large randomized trials, which we've got a lot of evidence now that randomized trials are a more reliable form of evidence than the example that you gave of generalizing from small numbers. Hugo:               So, health and safety, as you said, are two relevant examples. What can we do to combat this, do you think? Allen:               That one's tough. I'm thinking about some of the ways that we get health wrong, some of the ways that we get safety. Certainly, one of the problems is that we're very bad at small risks, small probabilities. There's some evidence that we can do a little bit better if we express things in terms of natural frequencies, so if I tell you that something has a .01% probability, you might have a really hard time making sense of that, but if I tell you that it's something like one person out of 10,000, then you might have a way to picture that. You could say, ¡°Well, okay. At a baseball game, there might be 30,000 people, so there could be three people here right now how have such-and-such a condition.¡± So, I think expressing things in terms of natural frequencies might be one thing that helps. Hugo:               Interesting. So, essentially, these are, I suppose, linguistic technologies and adopting things that we know work in language. Allen:               Yeah, I think so. I think graphical visualizations are important, too. Certainly, we have this incredibly powerful tool, which is our vision system, that's able to take a huge amount of data and process it quickly, so that's, I think, one of the best ways to get information off a page and into someone's brain. Hugo:               Yeah. Look, this actually just reminded me of something I haven't thought about in years, but it must've been 10 or 15 years ago, I was at an art show in Melbourne, Australia, and there was an artwork which it was visualizing how many people had been in certain situations or done certain things using grains of rice. So, they had a bowl, like the total population of Australia, the total population of the US, and then the number of people who were killed during the Holocaust and the number of people who've stepped on the moon, and that type of stuff, and it was actually incredibly vivid and memorable, and you got a strong sense of magnitude there. Allen:               Yes. I think that works. There's a video I saw, we'll have to find this and maybe put in a link, about war casualties and showing a little individual person for each casualty, but then adding it up and showing colored rectangles of different casualties in different wars, the number of people from each country, and that was very effective, and then I'm reminded of XKCD has done several really nice examples to show the relative sizes of things, just by mapping them onto area on the page. One of the ones that I think is really good is different doses of radioactivity, where he was able to show many different orders of magnitude by starting with a small unit that was represented by a single square, and then scaling it up, and then scaling it up, so that you could see that there are orders of magnitude between things like dental x-rays that we really should not be worrying about, and other kinds of exposure that are actual health risks. Hugo:               Incredible. So, what are the most important misconceptions regarding uncertainty that you think we need to correct, those data-oriented educators? Allen:               Right. Well, we talked about probabilistic predictions. I think that's a big one. I think the other big one that I think about is the shapes of distributions, that when you try to summarize a distribution, if I just tell you the mean, then people generally assume that it's something like a bell-shaped curve, and we have some intuition for what that's like, that if I tell you that the average human being is about 165 centimeters tall, or I think it's more than that, but anyway, you get a sense of, ¡°Okay. So, probably there are some people who are over 200, and probably there are some people who are less than 60, but there probably isn't anybody who is a kilometer tall.¡± We have a sense of that distribution. Allen:               But then you get things like the Pareto distribution, and this is one of the examples I use in my book, is what I call Pareto World, which is same as our world, because the average height is about the same, but the distribution is shaped like a Pareto distribution, which is one of these crazy long-tailed distributions, and in Pareto World, the average height is between one and two meters, but the vast majority of people are only a centimeter tall, and if you have seven billion people in Pareto World, the tallest one is probably a hundred kilometers tall. Hugo:               That's incredible, and just quickly, what type of phenomena do Pareto distributions, what are they known to model? Allen:               Right. Well, I think wealth and income are two of the big ones. In fact, I think that's the original domain where Pareto was looking at these long-tailed distributions, and that's the case where a few people have almost all of the wealth, and the vast majority of people have almost none. So, that's a case where if I tell you the mean and you are imagining a bell-shaped distribution, you have totally the wrong picture of what's going on. The mean is really not telling you what a typical person has. In fact, there may be no typical person. Hugo:               Absolutely, and in fact, that's a great example. Another example is if you have a bimodal distribution with nothing in the middle, the mean. There could actually be no one with that particular quantity of whatever we're talking about. Allen:               Yeah, that's a good example. Hugo:               So Allen, when you were discussing the Pareto distribution and the normal distribution, then something really struck me that as stakeholders and decision makers and research scientists and data scientists, we seem to be more comfortable in thinking about summary statistics and concrete numbers instead of distribution. So what I mean by that is, we like to report the mean, the mode, the median and measures of spread such as the variance. And there seems to be some sort of discomfort we feel, and we're not great at thinking about distributions which seem kind of necessary to quantify and think about uncertainty. Allen:               No, I think that's right. It doesn't come naturally. You know, I work with students. It takes awhile to just understand the idea of what a distribution is. But I think it's important because it captures all of the information that you have about a prediction. You want to know all possible outcomes, and the probability for each possible outcome. That's what a distribution is. It captures exactly the information that you need as a decision maker. Hugo:               Exactly. So, I mean, instead of communicating, for example, P-values in hypothesis testing, we can actually show the distribution of the possible effect sizes, right? Allen:               Right, and this is the strength of Bayesian methods, because what you've got is a posterior distribution that captures this information. And if you now feed that into a decision making process, it answers all the questions that you might want to ask. If you only care about the central tendency you can get that, but very often there's a cost function that says, you know, if this value turns out to be very high, there's a cost associated with that. If it's low, there's a cost associated with that. So if you've got the whole distribution, you can feed that into a cost benefit analysis and make better decisions. Hugo:               Absolutely. And I love the point that you made, which I think about a lot of the time, and when I teach Bayesian thinking and Bayesian inference, I make this incredibly explicit all the time, that from the posterior, from the distribution, you can get out so many of the other things that you need and you would want to report. Allen:               Right, so maybe you care, you know, what's the probability of a given catastrophic output. So, in that case you would be looking at, you know, the tails of that distribution. Or something like, you know, what's the probability that I'll be off by a certain amount or again, you know, things like the mean and the spread. Whatever the number is, you can get it from the distribution. Hugo:               Absolutely. And this is actually ... this leads to another question which I wanted to talk about. Bayesian inference I think of in a number of ways, as a technology that we've developed to deal with these types of questions and concepts. I think also we have reached a point in the past decades where Bayesian inference now, because of computational power we have, is actually far more feasible to do in a robust and efficient manner. And I think we may get to that in a bit. But I'm wondering in general, so what technologies, to your mind, are best suited for thinking and communicating around uncertainty, Allen? Allen:               Well, you know, a couple of the visualizations that people use all the time, and of course, you know, the classic one is a histogram. And that one, I think, is most appropriate for a general audience. Most people understand histograms. Violin plots are kinda similar, that's just two histograms back-to-back. And I think those are good because people understand them, but problematic. I mean, I've seen a number of articles of people pointing out that you kinda have to get histograms right. If the bin size is too big, then you're smoothing away a lot of information that you might care about. If the bin size is too small, you're getting a lot of noise and it can be hard to see the shape of the distribution through the noise. Allen:               So, one of the things I advocate for is using CDFs instead of histograms, or PDFs, as the default visualization. And when I'm exploring a data set, I'm almost always looking at CDFs because you get the best view of the shape of the distribution, you can see modes, you can see central tendencies, you can see spread. But also if you've got weird outliers, they jump out, and if you've got repeated values, you can see those clearly in a CDF, with less visual noise that distracts you from the important stuff. So I love CDFs. The only problem is that people don't understand them. But I think this is another case where the audience is getting educated, that the more people are consuming data journalism, the more they're seeing visualizations like this. And there's some implicit learning that's going on. Allen:               I saw one example very recently, someone showing the altitude that human populations live at. 'Cause they were talking about sea levels rising and talking about the fraction of people who live less than four meters above sea level. But the visualization was kind of a sneaky CDF, they showed, it actually a CDF sideways. But it was done in a way where a person who doesn't necessarily have technical training would be able to figure out what that graph was showing. So I think that's a step in a good direction. Hugo:               I like that a lot. And just to clarify, a CDF is a cumulative distribution function? Allen:               Yes. Sorry, I should've said that. Hugo:               Yeah. Allen:               And in particular I'm talking about empirical CDFs, where you're just taking it straight from data and generating the cumulative distribution function. Hugo:               Fantastic. And one of the nice things there, so each point on the x-axis, the y value will correspond to the number of data points equal to a less than, that particular point. And one of the great things is, you can also read off all your percentiles, right? Allen:               Exactly, right. You can read it in both directions. So, if you start on the y-axis, you can pick the percentile you want, like the median, 50 percentile. And then read off the corresponding x value. Or, the flip side is exactly what you said. If you want to know what fraction of the values are below a certain threshold, then you just read off that threshold and get the corresponding y-value. Hugo:               Yeah. And one of the other things that I love, you mentioned a bunch of, well several very attractive characteristics of empirical CDF, ECDFs. I also love that you can plot, you know, your control and a lot of different experiments just on the same figure and actually see how they differ, as opposed to you try to plot a bunch of histograms together, you gotta do wacky transparencies and all this stuff, right? Allen:               Yes, that's exactly right. And you can stack lots of CDFs on the same axes, and the differences that you see are really the differences that matter. When you compare histograms, you're seeing a lot of noise and you can see differences between histograms that are just random. When you're looking at CDFs, you get a pretty robust view of what the differences are and where in the distribution those differences happen. Hugo:               Yeah. Fantastic. Look, I'm very excited for a day in which the general populace appreciates CDFs and they appear in the mainstream media. I think that's a bright future. Allen:               Yeah, and I think we're close. I've seen one example, there have got to be more. Hugo:               Are there any other technologies or ways of thinking about uncertainty that you think are useful? Allen:               Well we talked a little bit about visualizing simulations, I think that matters. There's one example maybe getting back to ... if we have to get back to the 2016 election, I think one of the issues that came up is that a lot of the predictions, when they showed you a map of the different states, they were showing a color scale where there would be a red state and a blue state, but also pink and light blue and purple. And they were trying to show uncertainty using that color map, but then that's, you know, and that's not how the electoral college works. The electoral college, every state is either all red or all blue, with just a couple of exceptions. So that was a case where the predictions ended up looking very different from what the final results looked like, and I think that's part of why we were uncomfortable with predictions and the results. Hugo:               Interesting. So what is a fix for that, do you think? Allen:               Well, again coming back to my suggestion about, you know, don't try to show me all possible simulation outcomes, but show me one simulation per day. And in that case, the result that you show me, the daily result, would be all red or all blue. So, the predictions in that sense would look exactly like the outcome. And then when you see the outcome, the chances are that it's gonna resemble at least one of the predictions that you made. Hugo:               Great. Now I just had kind of a future flash, a brainwave into a future where we can use virtual reality technologies to drop people into potential simulations. But that's definitely future music. Allen:               Yes. I think that's interesting. Hugo:               Yeah. So speaking of the future, we've talked a lot about modern data science and uncertainty. I'm wondering what the future of data science looks like to you? Allen:               I think a big part of it looks like more people being involved. So not just highly trained technical statisticians, but we've been talking like data journalists, for example, who are people who have a technical skill to look at data, but also the storytelling skill to ask interesting questions, get answers, and then communicate those answers. I'd love to see all of that become more a part of general education, starting in primary school. Starting in secondary school, working with data, working with some of these visualizations we've been talking about. Using data to answer questions. Using data to explore and find out about the world, you know, at the stage that's appropriate at different levels of education. Allen:               There's a lot of talk about trying to get maybe less calculus in the world and more data science, and I think that's gotta be the direction we go. If you look at what people really need to know and what they're likely to use, practically everybody is going to be a consumer of data science and I think more and more people are gonna be producers of data science. So I think that's gotta be part of a core education. And calculus, I love calculus. But, it's just not as important for as many people. Hugo:               Yeah. And arguably, for you in your engineering background, I mean, calculus is incredibly important for engineers and physicists, but other people who need to be quantitative, it is, I think your point is very strong that learning how to actually work with data and statistics around that, is arguably a lot more essential. Allen:               Yeah. I think, as I said, more and more people are gonna be doing at least some kind of data science where they're taking advantage of all of the data now that's freely available, and that's, you know, government agencies are producing huge volumes of data and often they don't have the resources to really do anything with it. They've got a mandate to produce the data, but they don't have the people to do that. But the flip side of that is there's a huge opportunity for anyone with basic data skills to get in there and find interesting things. Often, you're one of the first people to explore a data set, you know, if you jump in there on the day it's published, you can find all kinds of things, not necessarily using, you know, powerful or complex statistical methods, just basic exploratory data analysis. Hugo:               Yeah, and the ability now to get, you know, learners, students, people in education institutions, involved in data science by making it or letting them realize that it's relevant to them, that there's data about their lives or about their physiological systems that they can analyze and explore, I think, is a huge win. Allen:               It is. It's really empowering, and this is one of the reasons that I ... I call myself a data optimist. And what I mean by that is I think there are huge opportunities here to use data science for social good. Getting into these data sets, as you said, they are relevant to people's lives. You can find things. I saw a great example at a conference recently, I was talking to a young guy from Brazil, who had worked on an application that was going through government data that was available online and flagging evidence of corruption, evidence of budgets that were being misspent. And they would tweet about it. There was just a robot that would find suspicious things in these accounts, and tweet them out there, which is, you know, kind of transparency that I think makes governments better. So I think there's a lot of potential there. Hugo:               That's incredible. Actually, that reminded me. I met a lawyer who was non-technical awhile ago, and non-computational, but he was learning a bit of machine learning, a bit of Python. He was trying to figure out whether you could predict judgements handed down by the Supreme Court based on previous judgements, and who would vote in a particular way. And that's just because that's something that really interests him professionally and in terms of social justice, as well. Allen:               Right. And I think, you know, the fact that people can do that who are not necessarily experts in that field, but amateurs for lack of a better word, can get in there and really do useful work. I think, you know, there are a lot of concerns, too. And this is getting a lot of attention right now, I'm actually in the middle of reading Weapons of Math Destruction, Cathy O'Neill's book. And there are a lot of concerns and I think there are things that are scary that we should be thinking about, but one of the things I'm actually thinking about now and trying to figure out is, how do we balance this discussion? 'Cause I think we're having, or at least starting, a good public discussion about this. It's good to get the problems on the table and address them, but how do we get the right balance between the optimism that I think is appropriate, but also the concerns that we should be dealing with. Hugo:               Yeah, absolutely. And as you say, the more and more books being published, more and more conversations happening in public. I mean it's the past several weeks that Mike Loukides, Hilary Mason, and DJ Patil who have posted their series of articles on data ethics and what they would like to see adoption in culture and in tech, among other places. I do think Weapons of Math Destruction is very interesting as part of this conversation, because of course one of the key parts of the definition for Cathy O'Neil over Weapon of Math Destruction is that it's not transparent, as well, right? So all the cases we're talking about kind of involve necessary transparency, so if we see more of that going forward, we'll at least be able to have a conversation around it. Allen:               Right, and I agree with both O'Neill and with you. I think that's a crucial part of these algorithms and, you know, open science and reproducible science is based on transparency and open data, and you know, also open code and open methodology. Hugo:               Absolutely. And this actually brings me to another question, which is a through line here is, the ability of everybody, every citizen to interact with data science in some sense. And I'm wondering for you in your practice, and as a data scientist and an educator, what is the role of the open source in the ability of everybody to interact with data science? Allen:               Right, I think it's huge. You know, reproducible science doesn't work if your code is proprietary. If you, you know, if you only share your data but not your methods, that only goes so far. It also doesn't help very much if I publish my code but it's in a language that's not accessible to everybody, you know, languages that are very expensive to get your hands on. Even among relatively affluent countries, you're not necessarily gonna have access to that code. And then when you go worldwide, there are, you know, a great majority of people in the world that are not gonna have access to that as contrasted with languages like R and Python that are freely available, now you still have to access to technology and that's not universal, but it's better and I think free software is an important part of that. Hugo:               Yeah. Allen:               This is, you know, part of the reason that I put my books up under free licenses is I know that there are a lot of people in the world who are not gonna buy hard copies of these books, but I want to make them available, and I do, you know, I get a lot of correspondence from people who are using my labs in electronic forms, who would not have access to them in hard copy. Hugo:               So, Allen, we've talked about a bunch of techniques that are dear to your heart. I'm wondering what one of your favorite data science-y techniques or methodologies is. Allen:               Right. I have a lot. Hugo:               Let's do it. Allen:               This might not be a short list. Hugo:               Sure. Allen:               So I am at heart a Bayesian. I do a certain amount of computational inference, you know, you do in classical statistical inference, but I'm really interested in helping Bayesian methods spread. And I think one of the challenges there is just understanding the ideas. It's one of these ideas that seems hard when you first encounter it, and then at some point there's a breakthrough, and then it seems obvious. Once you've got it, it is such a beautiful simple idea that it changes how you see everything. So that's what I want to help readers get to, and my students, is get that transition from the initial confusion into that moment of clarity. Allen:               One of the methods I use for that, and this is what I use in Think Bayes a lot, is just grid algorithms where you take everything that's continuous and break it up into discrete chunks, and then all the integrals become for loops, and I think it makes the ideas very clear. And then I think the other part of it that's important is the algorithms, particularly MCMC algorithms, which, you know, that's what makes Bayesian methods practical for substantial problems. You mentioned earlier that, you know, the computational power has become available. And that's a big part of what makes Bayes practical. But I think the algorithms are just as important, and particularly when you start to get up into higher dimensions. It's just not feasible without modern algorithms that are really quite new, developed in the last decade or so. Hugo:               Yeah. And I just want to speak to the idea of grid methods and, you said, turning, you say integrals become for loops. And I think is something which has actually been behind a lot of what we've been discussing as well and something that actually attracted me to your pedagogy initially and all of your work, was this idea of turning math into computation. And we see the same with techniques such as the bootstrap and resampling, but taking concepts that seem, you know, relatively abstract and seeing how they actually play out in a computational structure and making that translational step there. Allen:               Right. Yeah, I've found that very powerful for me as a learner. I've had that experience over and over, of reading something expressed using mathematical concepts, and then I turn it into code and I feel like that's how I get to understand it. Partly because you get to see it happening, often it's very visual in a way that the math is not, at least for me. But the other is it's debuggable. That if you have a misunderstanding, then when you try to represent it in code, you're gonna see evidence of the misunderstanding. It's gonna pop up as a bug. So, when you're debugging your code, you're also debugging your understanding. Which, for me, builds the confidence that when I've got working code, it also makes me believe that I understand the thing. Hugo:               Absolutely, and a related concept is the idea that breaking it down into chunks of code allows you to understand smaller concepts and build up the entire concept in smaller steps. Allen:               Right, yeah. I think that's a good point, too. Hugo:               Great. So, are there any other favorite techniques? You can have one or two more if you'd like. Allen:               I'll mention one which is survival analysis. And partly because it doesn't come up in an introductory class most of the time, but it's something I keep coming back to. I've used it for several projects, not necessarily looking at survival or medicine, but things like a study I did of how long a marriage lasts. Or, how long it is until someone has a first child, or gets married for the first time, or how long the marriage itself lasts until a divorce. So, as I say, it's not an idea that everybody sees, but once you learn it, you start seeing a lot of applications for it. Hugo:               Absolutely. And this did make it into your Think Stats book, do I recall correctly, or? Allen:               Yes. Yeah, I've got a section on survival analysis. Hugo:               Yeah, fantastic. So I'll definitely link to that in the show notes, as well. So, my last question is, do you have a call to action for our listeners out there? Allen:               Maybe two. I think if you have not yet had a chance to study data science, you should. And I think there are a lot of great resources that are available now that just weren't around not too long ago. And especially if you took a statistics class in high school or college, and it did not connect with you, the problem is not necessarily you. The standard curriculum in statistics for a long time I think has just not been right for most people. I think it's just spent way too much time on esoteric hypothesis tests. It gets bogged down in some statistical philosophy that's actually not very good philosophy, it's not very good philosophy, it's science. Allen:               If you come back to it now from a data science point of view, it's much more likely that you're gonna find classes and educational resources that are much more relevant. They're gonna be based on data. They're gonna be much more compelling. So give it another shot. I think that's my first call to action. Hugo:               I would second that. Allen:               And then the other is, for people who have got data science skills, there are a lot of ways to use that to do social good in the world. I think a lot of data scientists end up doing, you know, quantitative finance and business analytics, those are kinda the two big application domains. And there's nothing wrong with that, but I also think there are a lot of ways to use the skills that you've got to do something good, to, you know, find stories about what's happening and get those stories out. To, you know, use those stories as a way to effect change. Or if nothing else, just to answer questions about the world. If there's something that interests you, very often you can find data and answer questions. Hugo:               And there are a lot of very interesting data for social good programs out there, which we've actually had Peter Bull on the podcast to talk about data for good in general, and I'll put some links in the show notes as well. Allen:               Yes, and then I've got actually a talk that I want to link to that I've done a couple of times, and it's called Data Science, Data Optimism. And the last part of the talk is my call for data science for social good. I've got a bunch of links there that I've collected, that are just really the people that I know and groups that I know who are working in this area, but it's not complete by any means. So I would love to hear more from people, and maybe help me to expand my list. Hugo:               Fantastic. And people can reach out to you on Twitter, as well? Is that right? Allen:               Yes. I'm Allen Downey. Hugo:               Fantastic. Allen, it's been an absolute pleasure having you on the show. Allen:               Thank you very much. It's been great talking with you.","Keyword(freq): prediction(20), time(14), question(8), simulation(8), algorithm(7), cdf(7), example(7), histogram(7), method(7), statistics(7)"
"3","mastery",2018-09-28,"How to Load and Explore Household Electricity Usage Data","https://machinelearningmastery.com/how-to-load-and-explore-household-electricity-usage-data/","Given the rise of smart electricity meters and the wide adoption of electricity generation technology like solar panels, there is a wealth of electricity usage data available. This data represents a multivariate time series of power-related variables, that in turn could be used to model and even forecast future electricity consumption. In this tutorial, you will discover a household power consumption dataset for multi-step time series forecasting and how to better understand the raw data using exploratory analysis. After completing this tutorial, you will know: Let¡¯s get started. How to Load and Explore Household Electricity Usage DataPhoto by Sheila Sund, some rights reserved. This tutorial is divided into five parts; they are: The Household Power Consumption dataset is a multivariate time series dataset that describes the electricity consumption for a single household over four years. The data was collected between December 2006 and November 2010 and observations of power consumption within the household were collected every minute. It is a multivariate series comprised of seven variables (besides the date and time); they are: Active and reactive energy refer to the technical details of alternative current. In general terms, the active energy is the real power consumed by the household, whereas the reactive energy is the unused power in the lines. We can see that the dataset provides the active power as well as some division of the active power by main circuit in the house, specifically the kitchen, laundry, and climate control. These are not all the circuits in the household. The remaining watt-hours can be calculated from the active energy by first converting the active energy to watt-hours then subtracting the other sub-metered active energy in watt-hours, as follows: The dataset seems to have been provided without a seminal reference paper. Nevertheless, this dataset has become a standard for evaluating time series forecasting and machine learning methods for multi-step forecasting, specifically for forecasting active power. Further, it is not clear whether the other features in the dataset may benefit a model in forecasting active power. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course The dataset can be downloaded from the UCI Machine Learning repository as a single 20 megabyte .zip file: Download the dataset and unzip it into your current working directory. You will now have the file ¡°household_power_consumption.txt¡± that is about 127 megabytes in size and contains all of the observations Inspect the data file. Below are the first five rows of data (and the header) from the raw data file. We can see that the data columns are separated by semicolons (¡®;¡®). The data is reported to have one row for each day in the time period. The data does have missing values; for example, we can see 2-3 days worth of missing data around 28/4/2007. We can start-off by loading the data file as a Pandas DataFrame and summarize the loaded data. We can use the read_csv() function to load the data. It is easy to load the data with this function, but a little tricky to load it correctly. Specifically, we need to do a few custom things: Putting all of this together, we can now load the data and summarize the loaded shape and first few rows. Next, we can mark all missing values indicated with a ¡®?¡¯ character with a NaN value, which is a float. This will allow us to work with the data as one array of floating point values rather than mixed types, which is less efficient. Now we can create a new column that contains the remainder of the sub-metering, using the calculation from the previous section. We can now save the cleaned-up version of the dataset to a new file; in this case we will just change the file extension to .csv and save the dataset as ¡®household_power_consumption.csv¡®. To confirm that we have not messed-up, we can re-load the dataset and summarize the first five rows. Tying all of this together, the complete example of loading, cleaning-up, and saving the dataset is listed below. Running the example first loads the raw data and summarizes the shape and first five rows of the loaded data. The dataset is then cleaned up and saved to a new file. We load this new file and again print the first five rows, showing the removal of the date and time columns and addition of the new sub-metered column. We can peek inside the new ¡®household_power_consumption.csv¡® file and check that the missing observations are marked with an empty column, that pandas will correctly read as NaN, for example around row 190,499: Now that we have a cleaned-up version of the dataset, we can investigate it further using visualizations. The data is a multivariate time series and the best way to understand a time series is to create line plots. We can start off by creating a separate line plot for each of the eight variables. The complete example is listed below. Running the example creates a single image with eight subplots, one for each variable. This gives us a really high level of the four years of one minute observations. We can see that something interesting was going on in ¡®Sub_metering_3¡® (environmental control) that may not directly map to hot or cold years. Perhaps new systems were installed. Interestingly, the contribution of ¡®sub_metering_4¡® seems to decrease with time, or show a downward trend, perhaps matching up with the solid increase in seen towards the end of the series for ¡®Sub_metering_3¡®. These observations do reinforce the need to honor the temporal ordering of subsequences of this data when fitting and evaluating any model. We might be able to see the wave of a seasonal effect in the ¡®Global_active_power¡® and some other variates. There is some spiky usage that may match up with a specific period, such as weekends. Line Plots of Each Variable in the Power Consumption Dataset Let¡¯s zoom in and focus on the ¡®Global_active_power¡®, or ¡®active power¡® for short. We can create a new plot of the active power for each year to see if there are any common patterns across the years. The first year, 2006, has less than one month of data, so will remove it from the plot. The complete example is listed below. Running the example creates one single image with four line plots, one for each full year (or mostly full years) of data in the dataset. We can see some common gross patterns across the years, such as around Feb-Mar and around Aug-Sept where we see a marked decrease in consumption. We also seem to see a downward trend over the summer months (middle of the year in the northern hemisphere) and perhaps more consumption in the winter months towards the edges of the plots. These may show an annual seasonal pattern in consumption. We can also see a few patches of missing data in at least the first, third, and fourth plots. Line Plots of Active Power for Most Years We can continue to zoom in on consumption and look at active power for each of the 12 months of 2007. This might help tease out gross structures across the months, such as daily and weekly patterns. The complete example is listed below. Running the example creates a single image with 12 line plots, one for each month in 2007. We can see the sign-wave of power consumption of the days within each month. This is good as we would expect some kind of daily pattern in power consumption. We can see that there are stretches of days with very minimal consumption, such as in August and in April. These may represent vacation periods where the home was unoccupied and where power consumption was minimal. Line Plots for Active Power for All Months in One Year Finally, we can zoom in one more level and take a closer look at power consumption at the daily level. We would expect there to be some pattern to consumption each day, and perhaps differences in days over a week. The complete example is listed below. Running the example creates a single image with 20 line plots, one for the first 20 days in January 2007. There is commonality across the days; for example, many days consumption starts early morning, around 6-7AM. Some days show a drop in consumption in the middle of the day, which might make sense if most occupants are out of the house. We do see some strong overnight consumption on some days, that in a northern hemisphere January may match up with a heating system being used. Time of year, specifically the season and the weather that it brings, will be an important factor in modeling this data, as would be expected. Line Plots for Active Power for 20 Days in One Month Another important area to consider is the distribution of the variables. For example, it may be interesting to know if the distributions of observations are Gaussian or some other distribution. We can investigate the distributions of the data by reviewing histograms. We can start-off by creating a histogram for each variable in the time series. The complete example is listed below. Running the example creates a single figure with a separate histogram for each of the 8 variables. We can see that active and reactive power, intensity, as well as the sub-metered power are all skewed distributions down towards small watt-hour or kilowatt values. We can also see that distribution of voltage data is strongly Gaussian. Histogram plots for Each Variable in the Power Consumption Dataset The distribution of active power appears to be bi-modal, meaning it looks like it has two mean groups of observations. We can investigate this further by looking at the distribution of active power consumption for the four full years of data. The complete example is listed below. Running the example creates a single plot with four figures, one for each of the years between 2007 to 2010. We can see that the distribution of active power consumption across those years looks very similar. The distribution is indeed bimodal with one peak around 0.3 KW and perhaps another around 1.3 KW. There is a long tail on the distribution to higher kilowatt values. It might open the door to notions of discretizing the data and separating it into peak 1, peak 2 or long tail. These groups or clusters for usage on a day or hour may be helpful in developing a predictive model. Histogram Plots of Active Power for Most Years It is possible that the identified groups may vary over the seasons of the year. We can investigate this by looking at the distribution for active power for each month in a year. The complete example is listed below. Running the example creates an image with 12 plots, one for each month in 2007. We can see generally the same data distribution each month. The axes for the plots appear to align (given the similar scales), and we can see that the peaks are shifted down in the warmer northern hemisphere months and shifted up for the colder months. We can also see a thicker or more prominent tail toward larger kilowatt values for the cooler months of December through to March. Histogram Plots for Active Power for All Months in One Year Now that we know how to load and explore the dataset, we can pose some ideas on how to model the dataset. In this section, we will take a closer look at three main areas when working with the data; they are: There does not appear to be a seminal publication for the dataset to demonstrate the intended way to frame the data in a predictive modeling problem. We are therefore left to guess at possibly useful ways that this data may be used. The data is only for a single household, but perhaps effective modeling approaches could be generalized across to similar households. Perhaps the most useful framing of the dataset is to forecast an interval of future active power consumption. Four examples include: Generally, these types of forecasting problems are referred to as multi-step forecasting. Models that make use of all of the variables might be referred to as a multivariate multi-step forecasting models. Each of these models is not limited to forecasting the minutely data, but instead could model the problem at or below the chosen forecast resolution. Forecasting consumption in turn, at scale, could aid in a utility company forecasting demand, which is a widely studied and important problem. There is a lot of flexibility in preparing this data for modeling. The specific data preparation methods and their benefit really depend on the chosen framing of the problem and the modeling methods. Nevertheless, below is a list of general data preparation methods that may be useful: There are many simple human factors that may be helpful in engineering features from the data, that in turn may make specific days easier to forecast. Some examples include: These factors may be significantly less important for forecasting monthly data, and perhaps to a degree for weekly data. More general features may include: There are perhaps four classes of methods that might be interesting to explore on this problem; they are: Naive methods would include methods that make very simple, but often very effective assumptions. Some examples include: Classical linear methods include techniques are very effective for univariate time series forecasting. Two important examples include: They would require that the additional variables be discarded and the parameters of the model be configured or tuned to the specific framing of the dataset. Concerns related to adjusting the data for daily and seasonal structures can also be supported directly. Machine learning methods require that the problem be framed as a supervised learning problem. This would require that lag observations for a series be framed as input features, discarding the temporal relationship in the data. A suite of nonlinear and ensemble methods could be explored, including: Careful attention is required to ensure that the fitting and evaluation of these models preserved the temporal structure in the data. This is important so that the method is not able to ¡®cheat¡¯ by harnessing observations from the future. These methods are often agnostic to large numbers of variables and may aid in teasing out whether the additional variables can be harnessed and add value to predictive models. Generally, neural networks have not proven very effective at autoregression type problems. Nevertheless, techniques such as convolutional neural networks are able to automatically learn complex features from raw data, including one-dimensional signal data. And recurrent neural networks, such as the long short-term memory network, are capable of directly learning across multiple parallel sequences of input data. Further, combinations of these methods, such as CNN LSTM and ConvLSTM, have proven effective on time series classification tasks. It is possible that these methods may be able to harness the large volume of minute-based data and multiple input variables. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered a household power consumption dataset for multi-step time series forecasting and how to better understand the raw data using exploratory analysis. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Great work Jason! Thanks. Thank you for the post. It is really helpful.
I was wondering how to frame the input data for Forecasting hourly consumption for the next day using SVM, ANN  and Randomforest. Is there any reference for multi-step multi-variate time series prediction?
Also, would Forecast hourly consumption for the next day be more accurate than Forecast hourly consumption for the next week? Yes, I have many examples. Here¡¯s a starting point:https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/ I also have many more examples in my book:https://machinelearningmastery.com/deep-learning-for-time-series-forecasting/ If there was ¡°IoT¡± in the title, people (including me) would faster recognize the immense value in your post/book. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): plot(15), method(13), variable(10), observation(9), example(6), value(6), feature(5), model(5), row(5), distribution(3)"
"4","mastery",2018-09-26,"Deep Learning Models for Human Activity Recognition","https://machinelearningmastery.com/deep-learning-models-for-human-activity-recognition/","Human activity recognition, or HAR, is a challenging time series classification task. It involves predicting the movement of a person based on sensor data and traditionally involves deep domain expertise and methods from signal processing to correctly engineer features from the raw data in order to fit a machine learning model. Recently, deep learning methods such as convolutional neural networks and recurrent neural networks have shown capable and even achieve state-of-the-art results by automatically learning features from the raw sensor data. In this post, you will discover the problem of human activity recognition and the deep learning methods that are achieving state-of-the-art performance on this problem. After reading this post, you will know: Let¡¯s get started. Deep Learning Models for Human Activity RecognitionPhoto by Simon Harrod, some rights reserved. This post is divided into five parts; they are: Human activity recognition, or HAR for short, is a broad field of study concerned with identifying the specific movement or action of a person based on sensor data. Movements are often typical activities performed indoors, such as walking, talking, standing, and sitting. They may also be more focused activities such as those types of activities performed in a kitchen or on a factory floor. The sensor data may be remotely recorded, such as video, radar, or other wireless methods. Alternately, data may be recorded directly on the subject such as by carrying custom hardware or smart phones that have accelerometers and gyroscopes. Sensor-based activity recognition seeks the profound high-level knowledge about human activities from multitudes of low-level sensor readings <U+2014> Deep Learning for Sensor-based Activity Recognition: A Survey, 2018. Historically, sensor data for activity recognition was challenging and expensive to collect, requiring custom hardware. Now smart phones and other personal tracking devices used for fitness and health monitoring are cheap and ubiquitous. As such, sensor data from these devices is cheaper to collect, more common, and therefore is a more commonly studied version of the general activity recognition problem. The problem is to predict the activity given a snapshot of sensor data, typically data from one or a small number of sensor types. Generally, this problem is framed as a univariate or multivariate time series classification task. It is a challenging problem as there are no obvious or direct ways to relate the recorded sensor data to specific human activities and each subject may perform an activity with significant variation, resulting in variations in the recorded sensor data. The intent is to record sensor data and corresponding activities for specific subjects, fit a model from this data, and generalize the model to classify the activity of new unseen subjects from their sensor data. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course Traditionally, methods from the field of signal processing were used to analyze and distill the collected sensor data. Such methods were for feature engineering, creating domain-specific, sensor-specific, or signal processing-specific features and views of the original data. Statistical and machine learning models were then trained on the processed version of the data. A limitation of this approach is the signal processing and domain expertise required to analyze the raw data and engineer the features required to fit a model. This expertise would be required for each new dataset or sensor modality. In essence, it is expensive and not scalable. However, in most daily HAR tasks, those methods may heavily rely on heuristic handcrafted feature extraction, which is usually limited by human domain knowledge. Furthermore, only shallow features can be learned by those approaches, leading to undermined performance for unsupervised and incremental tasks. Due to those limitations, the performances of conventional [pattern recognition] methods are restricted regarding classification accuracy and model generalization. <U+2014> Deep Learning for Sensor-based Activity Recognition: A Survey, 2018. Ideally, learning methods could be used that automatically learn the features required to make accurate predictions from the raw data directly. This would allow new problems, new datasets, and new sensor modalities to be adopted quickly and cheaply. Recently, deep neural network models have started delivering on their promises of feature learning and are achieving stat-of-the-art results for human activity recognition. They are capable of performing automatic feature learning from the raw sensor data and out-perform models fit on hand-crafted domain-specific features. [¡¦] , the feature extraction and model building procedures are often performed simultaneously in the deep learning models. The features can be learned automatically through the network instead of being manually designed. Besides, the deep neural network can also extract high-level representation in deep layer, which makes it more suitable for complex activity recognition tasks. <U+2014> Deep Learning for Sensor-based Activity Recognition: A Survey, 2018. There are two main approaches to neural networks that are appropriate for time series classification and that have been demonstrated to perform well on activity recognition using sensor data from commodity smart phones and fitness tracking devices. They are Convolutional Neural Network Models and Recurrent Neural Network Models. RNN and LSTM are recommended to recognize short activities that have natural order while CNN is better at inferring long term repetitive activities. The reason is that RNN could make use of the time-order relationship between sensor readings, and CNN is more capable of learning deep features contained in recursive patterns. <U+2014> Deep Learning for Sensor-based Activity Recognition: A Survey, 2018. Before we dive into the specific neural networks that can be used for human activity recognition, we need to talk about data preparation. Both types of neural networks suitable for time series classification require that data be prepared in a specific manner in order to fit a model. That is, in a ¡®supervised learning¡® way that allows the model to associate signal data with an activity class. A straight-forward data preparation approach that was used both for classical machine learning methods on the hand-crafted features and for neural networks involves dividing the input signal data into windows of signals, where a given window may have one to a few seconds of observation data. This is often called a ¡®sliding window.¡¯ Human activity recognition aims to infer the actions of one or more persons from a set of observations captured by sensors. Usually, this is performed by following a fixed length sliding window approach for the features extraction where two parameters have to be fixed: the size of the window and the shift. <U+2014> A Dynamic Sliding Window Approach for Activity Recognition, 2011 Each window is also associated with a specific activity. A given window of data may have multiple variables, such as the x, y, and z axes of an accelerometer sensor. Let¡¯s make this concrete with an example. We have sensor data for 10 minutes; that may look like: If the data is recorded at 8 Hz, that means that there will be eight rows of data for one second of elapsed time performing an activity. We may choose to have one window of data represent one second of data; that means eight rows of data for an 8 Hz sensor. If we have x, y, and z data, that means we would have 3 variables. Therefore, a single window of data would be a 2-dimensional array with eight time steps and three features. One window would represent one sample. One minute of data would represent 480 sensor data points, or 60 windows of eight time steps. The total 10 minutes of data would represent 4,800 data points, or 600 windows of data. It is convenient to describe the shape of our prepared sensor data in terms of the number of samples or windows, the number of time steps in a window, and the number of features observed at each time step. Our example of 10 minutes of accelerometer data recorded at 8 Hz would be summarized as a three-dimensional array with the dimensions: There is no best window size, and it really depends on the specific model being used, the nature of the sensor data that was collected, and the activities that are being classified. There is a tension in the size of the window and the size of the model. Larger windows require large models that are slower to train, whereas smaller windows require smaller models that are much easier to fit. Intuitively, decreasing the window size allows for a faster activity detection, as well as reduced resources and energy needs. On the contrary, large data windows are normally considered for the recognition of complex activities <U+2014> Window Size Impact in Human Activity Recognition, 2014. Nevertheless, it is common to use one to two seconds of sensor data in order to classify a current fragment of an activity. From the results, reduced windows (2 s or less) are demonstrated to provide the most accurate detection performance. In fact, the most precise recognizer is obtained for very short windows (0.25<U+2013>0.5 s), leading to the perfect recognition of most activities. Contrary to what is often thought, this study demonstrates that large window sizes do not necessarily translate into a better recognition performance. <U+2014> Window Size Impact in Human Activity Recognition, 2014. There is some risk that the splitting of the stream of sensor data into windows may result in windows that miss the transition of one activity to another. As such, it was traditionally common to split data into windows with an overlap such that the first half of the window contained the observations from the last half of the previous window, in the case of a 50% overlap. [¡¦] an incorrect length may truncate an activity instance. In many cases, errors appear at the beginning or at the end of the activities, when the window overlaps the end of one activity and the beginning of the next one. In other cases, the window length may be too short to provide the best information for the recognition process. <U+2014> A Dynamic Sliding Window Approach for Activity Recognition, 2011 It is unclear whether windows with overlap are required for a given problem. In the adoption of neural network models, the use of overlaps, such as a 50% overlap, will double the size of the training data, which may aid in modeling smaller datasets, but may also lead to models that overfit the training dataset. An overlap between adjacent windows is tolerated for certain applications; however, this is less frequently used. <U+2014> Window Size Impact in Human Activity Recognition, 2014. Convolutional Neural Network models, or CNNs for short, are a type of deep neural network that were developed for use with image data, e.g. such as handwriting recognition. They have proven very effective on challenging computer vision problems when trained at scale for tasks such as identifying and localizing objects in images and automatically describing the content of images. They are models that are comprised of two main types of elements: convolutional layers and pooling layers. Convolutional layers read an input, such as a 2D image or a 1D signal, using a kernel that reads in small segments at a time and steps across the entire input field. Each read results in an the input that is projected onto a filter map and represents an internal interpretation of the input. Pooling layers take the feature map projections and distill them to the most essential elements, such as using a signal averaging or signal maximizing process. The convolution and pooling layers can be repeated at depth, providing multiple layers of abstraction of the input signals. The output of these networks is often one or more fully connected layers that interpret what has been read and map this internal representation to a class value. For more information on convolutional neural networks, can see the post: CNNs can be applied to human activity recognition data. The CNN model learns to map a given window of signal data to an activity where the model reads across each window of data and prepares an internal representation of the window. When applied to time series classification like HAR, CNN has two advantages over other models: local dependency and scale invariance. Local dependency means the nearby signals in HAR are likely to be correlated, while scale invariance refers to the scale-invariant for different paces or frequencies. <U+2014> Deep Learning for Sensor-based Activity Recognition: A Survey, 2018. The first important work using CNNs to HAR was by Ming Zeng, et al in their 2014 paper ¡°Convolutional Neural Networks for Human Activity Recognition using Mobile Sensors.¡± In the paper, the authors develop a simple CNN model for accelerometer data, where each axis of the accelerometer data is fed into separate convolutional layers, pooling layers, then concatenated before being interpreted by hidden fully connected layers. The figure below taken from the paper clearly shows the topology of the model. It provides a good template for how the CNN may be used for HAR problems and time series classification in general. Depiction of CNN Model for Accelerometer DataTaken from ¡°Convolutional Neural Networks for Human Activity Recognition using Mobile Sensors¡± There are many ways to model HAR problems with CNNs. One interesting example was by Heeryon Cho and Sang Min Yoon in their 2018 paper titled ¡°Divide and Conquer-Based 1D CNN Human Activity Recognition Using Test Data Sharpening.¡± In it, they divide activities into those that involve movement, called ¡°dynamic,¡± and those where the subject is stationary, called ¡°static,¡± then develop a CNN model to discriminate between these two main classes. Then, within each class, models are developed to discriminate between activities of that type, such as ¡°walking¡± for dynamic and ¡°sitting¡± for static. Separation of Activities as Dynamic or StaticTaken from ¡°Divide and Conquer-Based 1D CNN Human Activity Recognition Using Test Data Sharpening¡± They refer to this as a two-stage modeling approach. Instead of straightforwardly recognizing the individual activities using a single 6-class classifier, we apply a divide and conquer approach and build a two-stage activity recognition process, where abstract activities, i.e., dynamic and static activity, are first recognized using a 2-class or binary classifier, and then individual activities are recognized using two 3-class classifiers. <U+2014> Divide and Conquer-Based 1D CNN Human Activity Recognition Using Test Data Sharpening, 2018. Quite large CNN models were developed, which in turn allowed the authors to claim state-of-the-art results on challenging standard human activity recognition datasets. Another interesting approach was proposed by Wenchao Jiang and Zhaozheng Yin in their 2015 paper titled ¡°Human Activity Recognition Using Wearable Sensors by Deep Convolutional Neural Networks.¡± Instead of using 1D CNNs on the signal data, they instead combine the signal data together to create ¡°images¡± which are then fed to a 2D CNN and processed as image data with convolutions along the time axis of signals and across signal variables, specifically accelerometer and gyroscope data. Firstly, raw signals are stacked row-by-row into a signal image [¡¦.]. In the signal image, every signal sequence has the chance to be adjacent to every other sequence, which enables DCNN to extract hidden correlations between neighboring signals. Then, 2D Discrete Fourier Transform (DFT) is applied to the signal image and its magnitude is chosen as our activity image <U+2014> Human Activity Recognition Using Wearable Sensors by Deep Convolutional Neural Networks, 2015. Below is a depiction of the processing of raw sensor data into images, and then from images into an ¡°activity image,¡± the result of a discrete Fourier transform. Processing of Raw Sensor Data into an ImageTaken from ¡°Human Activity Recognition Using Wearable Sensors by Deep Convolutional Neural Networks¡± Finally, another good paper on the topic is by Charissa Ann Ronao and Sung-Bae Cho in 2016 titled ¡°Human activity recognition with smartphone sensors using deep learning neural networks.¡± Careful study of the use of CNNs is performed showing that larger kernel sizes of signal data are useful and limited pooling. Experiments show that convnets indeed derive relevant and more complex features with every additional layer, although difference of feature complexity level decreases with every additional layer. A wider time span of temporal local correlation can be exploited (1¡¿9 <U+2013> 1¡¿14) and a low pooling size (1¡¿2 <U+2013> 1¡¿3) is shown to be beneficial. <U+2014> Human activity recognition with smartphone sensors using deep learning neural networks, 2016. Usefully, they also provide the full hyperparameter configuration for the CNN models that may provide a useful starting point on new HAR and other sequence classification problems, summarized below. Table of CNN Model Hyperparameter ConfigurationTaken from ¡°Human activity recognition with smartphone sensors using deep learning neural networks.¡± Recurrent neural networks, or RNNs for short, are a type of neural network that was designed to learn from sequence data, such as sequences of observations over time, or a sequence of words in a sentence. A specific type of RNN called the long short-term memory network, or LSTM for short, is perhaps the most widely used RNN as its careful design overcomes the general difficulties in training a stable RNN on sequence data. LSTMs have proven effective on challenging sequence prediction problems when trained at scale for such tasks as handwriting recognition, language modeling, and machine translation. A layer in an LSTM model is comprised of special units that have gates that govern input, output, and recurrent connections, the weights of which are learned. Each LSTM unit also has internal memory or state that is accumulated as an input sequence is read and can be used by the network as a type of local variable or memory register. For more information on long short-term memory networks, see the post: Like the CNN that can read across an input sequence, the LSTM reads a sequence of input observations and develops its own internal representation of the input sequence. Unlike the CNN, the LSTM is trained in a way that pays specific attention to observations made and prediction errors made over the time steps in the input sequence, called backpropagation through time. For more information on backpropagation through time, see the post: LSTMs can be applied to the problem of human activity recognition. The LSTM learns to map each window of sensor data to an activity, where the observations in the input sequence are read one at a time, where each time step may be comprised of one or more variables (e.g. parallel sequences). There has been limited application of simple LSTM models to HAR problems. One example is by Abdulmajid Murad and Jae-Young Pyun in their 2017 paper titled ¡°Deep Recurrent Neural Networks for Human Activity Recognition.¡± Important, in the paper they comment on the limitation of CNNs in their requirement to operate on fixed-sized windows of sensor data, a limitation that LSTMs do not strictly have. However, the size of convolutional kernels restricts the captured range of dependencies between data samples. As a result, typical models are unadaptable to a wide range of activity-recognition configurations and require fixed-length input windows. <U+2014> Deep Recurrent Neural Networks for Human Activity Recognition, 2017. They explore the use of LSTMs that both process the sequence data forward (normal) and both directions (Bidirectional LSTM). Interestingly, the LSTM predicts an activity for each input time step of a subsequence of sensor data, which are then aggregated in order to predict an activity for the window. There will [be] a score for each time-step predicting the type of activity occurring at time t. The prediction for the entire window T is obtained by merging the individual scores into a single prediction <U+2014> Deep Recurrent Neural Networks for Human Activity Recognition, 2017. The figure below taken from the paper provides a depiction of the LSTM model followed by fully connected layers used to interpret the internal representation of the raw sensor data. Depiction of LSTM RNN for Activity RecognitionTaken from ¡°Deep Recurrent Neural Networks for Human Activity Recognition.¡± It may be more common to use an LSTM in conjunction with a CNN on HAR problems, in a CNN-LSTM model or ConvLSTM model. This is where a CNN model is used to extract the features from a subsequence of raw sample data, and output features from the CNN for each subsequence are then interpreted by an LSTM in aggregate. An example of this is in the 2016 paper by Francisco Javier Ordonez and Daniel Roggen titled ¡°Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition.¡± We introduce a new DNN framework for wearable activity recognition, which we refer to as DeepConvLSTM. This architecture combines convolutional and recurrent layers. The convolutional layers act as feature extractors and provide abstract representations of the input sensor data in feature maps. The recurrent layers model the temporal dynamics of the activation of the feature maps. <U+2014> Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition, 2016. A deep network architecture is used with four convolutional layers without any pooling layers, followed by two LSTM layers to interpret the extracted features over multiple time steps. The authors claim that the removal of the pooling layers is a critical part of their model architecture, where the use of pooling layers after the convolutional layers interferes with the convolutional layers¡¯ ability to learn to downsample the raw sensor data. In the literature, CNN frameworks often include convolutional and pooling layers successively, as a measure to reduce data complexity and introduce translation invariant features. Nevertheless, such an approach is not strictly part of the architecture, and in the time series domain [¡¦] DeepConvLSTM does not include pooling operations because the input of the network is constrained by the sliding window mechanism [¡¦] and this fact limits the possibility of downsampling the data, given that DeepConvLSTM requires a data sequence to be processed by the recurrent layers. However, without the sliding window requirement, a pooling mechanism could be useful to cover different sensor data time scales at deeper layers. <U+2014> Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition, 2016. The figure below taken from the paper makes the architecture clearer. Note that layers 6 and 7 in the image are in fact LSTM layers. Depiction of CNN LSTM Model for Activity RecognitionTaken from ¡°Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition.¡± This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered the problem of human activity recognition and the use of deep learning methods that are achieving state-of-the-art performance on this problem. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): network(26), layer(25), model(19), activity(18), feature(18), window(16), method(11), sensor(9), cnn(8), problem(8)"
"5","mastery",2018-09-24,"How to Develop RNN Models for Human Activity Recognition Time Series Classification","https://machinelearningmastery.com/how-to-develop-rnn-models-for-human-activity-recognition-time-series-classification/","Human activity recognition is the problem of classifying sequences of accelerometer data recorded by specialized harnesses or smart phones into known well-defined movements. Classical approaches to the problem involve hand crafting features from the time series data based on fixed-sized windows and training machine learning models, such as ensembles of decision trees. The difficulty is that this feature engineering requires strong expertise in the field. Recently, deep learning methods such as recurrent neural networks like as LSTMs and variations that make use of one-dimensional convolutional neural networks or CNNs have been shown to provide state-of-the-art results on challenging activity recognition tasks with little or no data feature engineering, instead using feature learning on raw data. In this tutorial, you will discover three recurrent neural network architectures for modeling an activity recognition time series classification problem. After completing this tutorial, you will know: Let¡¯s get started. How to Develop RNN Models for Human Activity Recognition Time Series ClassificationPhoto by Bonnie Moreland, some rights reserved. This tutorial is divided into four parts; they are: Human Activity Recognition, or HAR for short, is the problem of predicting what a person is doing based on a trace of their movement using sensors. A standard human activity recognition dataset is the ¡®Activity Recognition Using Smart Phones Dataset¡¯ made available in 2012. It was prepared and made available by Davide Anguita, et al. from the University of Genova, Italy and is described in full in their 2013 paper ¡°A Public Domain Dataset for Human Activity Recognition Using Smartphones.¡± The dataset was modeled with machine learning algorithms in their 2012 paper titled ¡°Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine.¡± The dataset was made available and can be downloaded for free from the UCI Machine Learning Repository: The data was collected from 30 subjects aged between 19 and 48 years old performing one of six standard activities while wearing a waist-mounted smartphone that recorded the movement data. Video was recorded of each subject performing the activities and the movement data was labeled manually from these videos. Below is an example video of a subject performing the activities while their movement data is being recorded. The six activities performed were as follows: The movement data recorded was the x, y, and z accelerometer data (linear acceleration) and gyroscopic data (angular velocity) from the smart phone, specifically a Samsung Galaxy S II. Observations were recorded at 50 Hz (i.e. 50 data points per second). Each subject performed the sequence of activities twice; once with the device on their left-hand-side and once with the device on their right-hand side. The raw data is not available. Instead, a pre-processed version of the dataset was made available. The pre-processing steps included: Feature engineering was applied to the window data, and a copy of the data with these engineered features was made available. A number of time and frequency features commonly used in the field of human activity recognition were extracted from each window. The result was a 561 element vector of features. The dataset was split into train (70%) and test (30%) sets based on data for subjects, e.g. 21 subjects for train and nine for test. Experiment results with a support vector machine intended for use on a smartphone (e.g. fixed-point arithmetic) resulted in a predictive accuracy of 89% on the test dataset, achieving similar results as an unmodified SVM implementation. The dataset is freely available and can be downloaded from the UCI Machine Learning repository. The data is provided as a single zip file that is about 58 megabytes in size. The direct link for this download is below: Download the dataset and unzip all files into a new directory in your current working directory named ¡°HARDataset¡±. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this section, we will develop a Long Short-Term Memory network model (LSTM) for the human activity recognition dataset. LSTM network models are a type of recurrent neural network that are able to learn and remember over long sequences of input data. They are intended for use with data that is comprised of long sequences of data, up to 200 to 400 time steps. They may be a good fit for this problem. The model can support multiple parallel sequences of input data, such as each axis of the accelerometer and gyroscope data. The model learns to extract features from sequences of observations and how to map the internal features to different activity types. The benefit of using LSTMs for sequence classification is that they can learn from the raw time series data directly, and in turn do not require domain expertise to manually engineer input features. The model can learn an internal representation of the time series data and ideally achieve comparable performance to models fit on a version of the dataset with engineered features. This section is divided into four parts; they are: The first step is to load the raw dataset into memory. There are three main signal types in the raw data: total acceleration, body acceleration, and body gyroscope. Each has 3 axises of data. This means that there are a total of nine variables for each time step. Further, each series of data has been partitioned into overlapping windows of 2.65 seconds of data, or 128 time steps. These windows of data correspond to the windows of engineered features (rows) in the previous section. This means that one row of data has (128 * 9), or 1,152 elements. This is a little less than double the size of the 561 element vectors in the previous section and it is likely that there is some redundant data. The signals are stored in the /Inertial Signals/ directory under the train and test subdirectories. Each axis of each signal is stored in a separate file, meaning that each of the train and test datasets have nine input files to load and one output file to load. We can batch the loading of these files into groups given the consistent directory structures and file naming conventions. The input data is in CSV format where columns are separated by whitespace. Each of these files can be loaded as a NumPy array. The load_file() function below loads a dataset given the fill path to the file and returns the loaded data as a NumPy array. We can then load all data for a given group (train or test) into a single three-dimensional NumPy array, where the dimensions of the array are [samples, time steps, features]. To make this clearer, there are 128 time steps and nine features, where the number of samples is the number of rows in any given raw signal data file. The load_group() function below implements this behavior. The dstack() NumPy function allows us to stack each of the loaded 3D arrays into a single 3D array where the variables are separated on the third dimension (features). We can use this function to load all input signal data for a given group, such as train or test. The load_dataset_group() function below loads all input signal data and the output data for a single group using the consistent naming conventions between the directories. Finally, we can load each of the train and test datasets. The output data is defined as an integer for the class number. We must one hot encode these class integers so that the data is suitable for fitting a neural network multi-class classification model. We can do this by calling the to_categorical() Keras function. The load_dataset() function below implements this behavior and returns the train and test X and y elements ready for fitting and evaluating the defined models. Now that we have the data loaded into memory ready for modeling, we can define, fit, and evaluate an LSTM model. We can define a function named evaluate_model() that takes the train and test dataset, fits a model on the training dataset, evaluates it on the test dataset, and returns an estimate of the model¡¯s performance. First, we must define the LSTM model using the Keras deep learning library. The model requires a three-dimensional input with [samples, time steps, features]. This is exactly how we have loaded the data, where one sample is one window of the time series data, each window has 128 time steps, and a time step has nine variables or features. The output for the model will be a six-element vector containing the probability of a given window belonging to each of the six activity types. Thees input and output dimensions are required when fitting the model, and we can extract them from the provided training dataset. The model is defined as a Sequential Keras model, for simplicity. We will define the model as having a single LSTM hidden layer. This is followed by a dropout layer intended to reduce overfitting of the model to the training data. Finally, a dense fully connected layer is used to interpret the features extracted by the LSTM hidden layer, before a final output layer is used to make predictions. The efficient Adam version of stochastic gradient descent will be used to optimize the network, and the categorical cross entropy loss function will be used given that we are learning a multi-class classification problem. The definition of the model is listed below. The model is fit for a fixed number of epochs, in this case 15, and a batch size of 64 samples will be used, where 64 windows of data will be exposed to the model before the weights of the model are updated. Once the model is fit, it is evaluated on the test dataset and the accuracy of the fit model on the test dataset is returned. Note, it is common to not shuffle sequence data when fitting an LSTM. Here we do shuffle the windows of input data during training (the default). In this problem, we are interested in harnessing the LSTMs ability to learn and extract features across the time steps in a window, not across windows. The complete evaluate_model() function is listed below. There is nothing special about the network structure or chosen hyperparameters, they are just a starting point for this problem. We cannot judge the skill of the model from a single evaluation. The reason for this is that neural networks are stochastic, meaning that a different specific model will result when training the same model configuration on the same data. This is a feature of the network in that it gives the model its adaptive ability, but requires a slightly more complicated evaluation of the model. We will repeat the evaluation of the model multiple times, then summarize the performance of the model across each of those runs. For example, we can call evaluate_model() a total of 10 times. This will result in a population of model evaluation scores that must be summarized. We can summarize the sample of scores by calculating and reporting the mean and standard deviation of the performance. The mean gives the average accuracy of the model on the dataset, whereas the standard deviation gives the average variance of the accuracy from the mean. The function summarize_results() below summarizes the results of a run. We can bundle up the repeated evaluation, gathering of results, and summarization of results into a main function for the experiment, called run_experiment(), listed below. By default, the model is evaluated 10 times before the performance of the model is reported. Now that we have all of the pieces, we can tie them together into a worked example. The complete code listing is provided below. Running the example first prints the shape of the loaded dataset, then the shape of the train and test sets and the input and output elements. This confirms the number of samples, time steps, and variables, as well as the number of classes. Next, models are created and evaluated and a debug message is printed for each. Finally, the sample of scores is printed, followed by the mean and standard deviation. We can see that the model performed well, achieving a classification accuracy of about 89.7% trained on the raw dataset, with a standard deviation of about 1.3. This is a good result, considering that the original paper published a result of 89%, trained on the dataset with heavy domain-specific feature engineering, not the raw dataset. Note: given the stochastic nature of the algorithm, your specific results may vary. If so, try running the code a few times. Now that we have seen how to develop an LSTM model for time series classification, let¡¯s look at how we can develop a more sophisticated CNN LSTM model. The CNN LSTM architecture involves using Convolutional Neural Network (CNN) layers for feature extraction on input data combined with LSTMs to support sequence prediction. CNN LSTMs were developed for visual time series prediction problems and the application of generating textual descriptions from sequences of images (e.g. videos). Specifically, the problems of: You can learn more about the CNN LSTM architecture in the post: To learn more about the consequences of combining these models, see the paper: The CNN LSTM model will read subsequences of the main sequence in as blocks, extract features from each block, then allow the LSTM to interpret the features extracted from each block. One approach to implementing this model is to split each window of 128 time steps into subsequences for the CNN model to process. For example, the 128 time steps in each window can be split into four subsequences of 32 time steps. We can then define a CNN model that expects to read in sequences with a length of 32 time steps and nine features. The entire CNN model can be wrapped in a TimeDistributed layer to allow the same CNN model to read in each of the four subsequences in the window. The extracted features are then flattened and provided to the LSTM model to read, extracting its own features before a final mapping to an activity is made. It is common to use two consecutive CNN layers followed by dropout and a max pooling layer, and that is the simple structure used in the CNN LSTM model here. The updated evaluate_model() is listed below. We can evaluate this model as we did the straight LSTM model in the previous section. The complete code listing is provided below. Running the example summarizes the model performance for each of the 10 runs before a final summary of the models performance on the test set is reported. We can see that the model achieved a performance of about 90.6% with a standard deviation of about 1%. Note: given the stochastic nature of the algorithm, your specific results may vary. If so, try running the code a few times. A further extension of the CNN LSTM idea is to perform the convolutions of the CNN (e.g. how the CNN reads the input sequence data) as part of the LSTM. This combination is called a Convolutional LSTM, or ConvLSTM for short, and like the CNN LSTM is also used for spatio-temporal data. Unlike an LSTM that reads the data in directly in order to calculate internal state and state transitions, and unlike the CNN LSTM that is interpreting the output from CNN models, the ConvLSTM is using convolutions directly as part of reading input into the LSTM units themselves. For more information for how the equations for the ConvLSTM are calculated within the LSTM unit, see the paper: The Keras library provides the ConvLSTM2D class that supports the ConvLSTM model for 2D data. It can be configured for 1D multivariate time series classification. The ConvLSTM2D class, by default, expects input data to have the shape: Where each time step of data is defined as an image of (rows * columns) data points. In the previous section, we divided a given window of data (128 time steps) into four subsequences of 32 time steps. We can use this same subsequence approach in defining the ConvLSTM2D input where the number of time steps is the number of subsequences in the window, the number of rows is 1 as we are working with one-dimensional data, and the number of columns represents the number of time steps in the subsequence, in this case 32. For this chosen framing of the problem, the input for the ConvLSTM2D would therefore be: We can now prepare the data for the ConvLSTM2D model. The ConvLSTM2D class requires configuration both in terms of the CNN and the LSTM. This includes specifying the number of filters (e.g. 64), the two-dimensional kernel size, in this case (1 row and 3 columns of the subsequence time steps), and the activation function, in this case rectified linear. As with a CNN or LSTM model, the output must be flattened into one long vector before it can be interpreted by a dense layer. We can then evaluate the model as we did the LSTM and CNN LSTM models before it. The complete example is listed below. As with the prior experiments, running the model prints the performance of the model each time it is fit and evaluated. A summary of the final model performance is presented at the end of the run. We can see that the model does consistently perform well on the problem achieving an accuracy of about 90%, perhaps with fewer resources than the larger CNN LSTM model. Note: given the stochastic nature of the algorithm, your specific results may vary. If so, try running the code a few times. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered three recurrent neural network architectures for modeling an activity recognition time series classification problem. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hi Jason, So enjoy reading with your stuff, very helpful. As we can use CNN+LSTM to predict the spatial-temporal data, can we reverse the architecture as LSTM+CNN to do the same job? Any examples for LSTM + CNN? Not that I have seen. What application did you have in mind exactly? Sequence to image? Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): feature(21), step(18), model(11), result(10), sequence(7), window(7), subsequence(6), time(6), activity(5), lstm(5)"
"6","vidhya",2018-09-27,"Building DataHack Summit 2018 <U+2013> India¡¯s Most Advanced AI Conference. Are you Ready?","https://www.analyticsvidhya.com/blog/2018/09/building-datahack-summit-2018-indias-most-advanced-ai-conference-are-you-ready/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 Usain Bolt created a World record by running 200m sprint in<U+00A0>19.30 seconds in 2008. What do you think he thought while he was preparing for 2009? He had his mind set to beat his own personal best and he did! Why am I talking about Bolt here? Well, I find myself in a similar situation. DataHack Summit 2017 was an unprecedented success. We created India¡¯s largest conference with unilateral focus on data science practitioners. The community loved the focus, the content and the knowledge sharing at the event. If you haven¡¯t seen already <U+2013> check out the highlights below. What are we thinking now as we are building India¡¯s most advanced data science conference? If you cut through my mind and get a peek inside <U+2013> this is what you will find <U+0001F642> The venue is bigger than last year but tickets are selling like hot cakes so make sure you grab yours before they¡¯re sold out. Prices go up on September 30th so avail the discount today! Head over here to book your seat<U+00A0>for India¡¯s most advanced conference on AI, Machine Learning, Deep Learning, and IoT! Let¡¯s take a quick tour around DHS 2018 to see how it¡¯s shaping up and what we have in store for you. If there is one place we bet our reputation on <U+2013> it is the content we create and we curate. DataHack Summit 2018 will be an epitome of this. To be honest, we are having a tough time saying no to very exciting talk proposals. Here are a few<U+00A0>eminent speakers in AI and ML who will be speaking at DataHack Sumit 2018: The most exciting thing which people would see are the<U+00A0>Hack sessions.<U+00A0>They saw a tremendous response from the audience last year, and have been expanded to reflect the latest breakthrough developments. Below are a few topics to whet your appetite (click on each session to read more about what will be covered): And here are a few awesome hackers, who will be performing live hack sessions: Check out the full speaker line-up<U+00A0>here. We will top up the sessions and Hack Sessions with an exclusive Startup Showcase and Research Track. We will showcase some of the most exciting AI and ML startups across the globe to showcase their offerings. Prepare to have your mind blown by some of the most amazing uses of AI and ML in a variety of domains. In addition to this, there is an entire track dedicated to cutting-edge research! We are giving individuals the opportunity to come and present their work in front of our community. This year¡¯s venue is none other than the NIMHANS Convention Center in Bengaluru. There are three auditoriums (yes, three!) <U+2013> so you are going to see 3 parallel tracks. So you can look forward to more sessions, more industry leaders, and more engagement! And all this space means an opportunity for even more events. There will be more hack sessions this year, and each session will have an even bigger audience than before. DataHack Summit 2018 will have bigger and swankier LED screens as well! So regardless of where you¡¯re sitting, the presentation and code will be visible from all corners of the room. Reserve your seat TODAY! There is an incredible deal on offer and prices will go up on September 30th. So act now and become a part of India¡¯s most advanced AI and ML conference.","Keyword(freq): session(6), price(2), auditorium(1), cake(1), corner(1), development(1), domain(1), event(1), hacker(1), highlight(1)"
"7","vidhya",2018-09-27,"A Multivariate Time Series Guide to Forecasting and Modeling (with Python codes)","https://www.analyticsvidhya.com/blog/2018/09/multivariate-time-series-guide-forecasting-modeling-python-codes/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 Time is the most critical factor that decides whether a business will rise or fall. That¡¯s why we see sales in stores and e-commerce platforms aligning with festivals. These businesses analyze years of spending data to understand the best time to throw open the gates and see an increase in consumer spending. But how can you, as a data scientist, perform this analysis? Don¡¯t worry, you don¡¯t need to build a time machine! Time Series modeling is a powerful technique that acts as a gateway to understanding and forecasting trends and patterns. But even a time series model has different facets. Most of the examples we see on the web deal with univariate time series. Unfortunately, real-world use cases don¡¯t work like that. There are multiple variables at play, and handling all of them at the same time is where a data scientist will earn his worth. In this article, we will understand what a multivariate time series is, and how to deal with it. We will also take a case study and implement it in Python to give you a practical understanding of the subject. This article assumes some familiarity with univariate time series, its properties and various techniques used for forecasting. Since this article will be focused on multivariate time series, I would suggest you go through the following articles which serve as a good introduction to univariate time series: But I¡¯ll give you a quick refresher of what a univariate time series is, before going into the details of a multivariate time series. Let¡¯s look at them one by one to understand the difference. A univariate time series, as the name suggests, is a series with a single time-dependent variable. For example, have a look at the sample dataset below that consists of the temperature values (each hour), for the past 2 years. Here, temperature is the dependent variable (dependent on Time). If we are asked to predict the temperature for the next few days, we will look at the past values and try to gauge and extract a pattern. We would notice that the temperature is lower in the morning and at night, while peaking in the afternoon. Also if you have data for the<U+00A0>past few years, you would observe that it is colder during the months of November to January, while being comparatively hotter in April to June. Such observations will help us in predicting future values. Did you notice that we used only one variable (the temperature of the past 2 years,)? Therefore, this is called Univariate Time Series Analysis/Forecasting. A Multivariate time series has more than one time-dependent variable. Each variable depends not only on its past values but also has some dependency on other variables. This dependency is used for forecasting future values. Sounds complicated? Let me explain. Consider the above example. Now suppose our dataset includes perspiration percent, dew point, wind speed, cloud cover percentage, etc. along with the temperature value for the past two years. In this case, there are multiple variables to be considered to optimally predict temperature. A series like this would fall under the category of multivariate time series. Below is an illustration of this: Now that we understand what a multivariate time series looks like, let us understand how can we use it to build a forecast. In this section, I will introduce you to one of the most commonly used methods for multivariate time series forecasting <U+2013> Vector Auto Regression (VAR). In a VAR model, each variable is a linear function of the past values of itself and the past values of all the other variables. To explain this in a better manner, I¡¯m going to use a simple visual example: We have two variables, y1 and y2. We need to forecast the value of these two variables at time t, from the given data for past n values. For simplicity, I have considered the lag value to be 1. For calculating y1(t), we will use the past value of y1 and y2. Similarly, to calculate y2(t), past values of both y1 and y2 will be used. Below is a simple mathematical way of representing this relation: Here, These equations are similar to the equation of an<U+00A0>AR process. Since the AR process is used for univariate time series data, the future values are linear combinations of their own past values only. Consider the AR(1) process: y(t) = a + w*y(t-1) +e In this case, we have only one variable <U+2013> y, a constant term <U+2013> a, an error term <U+2013> e, and a coefficient <U+2013> w. In order to accommodate the multiple variable terms in each equation for VAR, we will use vectors.<U+00A0> We can write the equations (1) and (2) in the following form : The two variables are y1 and y2, followed by a constant, a coefficient metric, lag value, and an error metric. This is the vector equation for a VAR(1) process. For a VAR(2) process, another vector term for time (t-2) will be added to the equation to generalize for p lags: The above equation represents a VAR(p) process with variables y1, y2 ¡¦yk. The same can be written as: The term ¥åt in the equation represents multivariate vector white noise. For a multivariate time series,<U+00A0>¥åt should be a continuous random vector that satisfies the following conditions: Recall the temperate forecasting example we saw earlier. An argument can be made for it to be treated as a multiple univariate series. We can solve it using simple univariate forecasting methods like AR. Since the aim is to predict the temperature, we can simply remove the other variables (except temperature) and fit a model on the remaining univariate series. Another simple idea is to forecast values for each series individually using the techniques we already know. This would make the work extremely straightforward! Then why should you learn another forecasting technique? Isn¡¯t this topic complicated enough already? From the above equations (1) and (2), it is clear that each variable is using the past values of every variable to make the predictions. Unlike AR, VAR is able to understand and use the relationship between several variables. This is useful for describing the dynamic behavior of the data and also provides better forecasting results. Additionally, implementing VAR is as simple as using any other univariate technique (which you will see in the last section). We know from studying the univariate concept that a stationary time series will more often than not give us a better set of predictions. If you are not familiar with the concept of stationarity, please go through this article first: A Gentle Introduction to handling non-stationary Time Series. To summarize, for a given univariate time series: y(t) = c*y(t-1) + ¥å t The series is said to be stationary if the value of |c| < 1. Now, recall the equation of our VAR process: Note: I is the identity matrix. Representing the equation in terms of Lag operators, we have: Taking all the y(t) terms on the left-hand side: The coefficient of y(t) is called the lag polynomial. Let us represent this as ¥Õ(L): For a series to be stationary, the eigenvalues of |¥Õ(L)-1| should be less than 1 in modulus. This might seem complicated given the number of variables in the derivation. This idea has been explained using a simple numerical example in the following video. I highly encourage watching it to solidify your understanding: Similar to the Augmented Dickey-Fuller test for univariate series, we have Johansen¡¯s test for checking the stationarity of any multivariate time series data. We will see how to perform the test in the last section of this article. If you have worked with univariate time series data before, you¡¯ll be aware of the train-validation sets. The idea of creating a validation set is to analyze the performance of the model before using it for making predictions. Creating a validation set for time series problems is tricky because we have to take into account the time component. One cannot directly use the train_test_split or k-fold validation since this will disrupt the pattern in the series. The validation set should be created considering the date and time values. Suppose we have to forecast the temperate, dew point, cloud percent, etc. for the next two months using data from the last two years. One possible method is to keep the data for the last two months aside and train the model on the remaining 22 months. Once the model has been trained, we can use it to make predictions on the validation set. Based on these predictions and the actual values, we can check how well the model performed, and the variables for which the model did not do so well. And for making the final prediction, use the complete dataset (combine the train and validation sets). In this section, we will implement the Vector AR model on a toy dataset. I have used the Air Quality dataset for this and you can download it from here. The data type of the<U+00A0>Date_Time column is object<U+00A0>and we need to change it to datetime. Also, for preparing the data, we need the index to have datetime. Follow the below commands: The next step is to deal with the missing values. Since the missing values in the data are replaced with a value -200, we will have to impute the missing value with a better number. Consider this <U+2013> if the present dew point value is missing, we can safely assume that it will be close to the value of the previous hour. Makes sense, right? Here, I will impute -200 with the previous value. You can choose to substitute the value using the average of a few previous values, or the value at the same time on the previous day (you can share your idea(s) of imputing missing values in the comments section below). Below is the result of the test: We can now go ahead and create the validation set to fit the model, and test the performance of the model: The predictions are in the form of an array, where each list represents the predictions of the row. We will transform this into a more presentable format. Output of the above code: After the testing on validation set, lets fit the model on the complete dataset Before I started this article, the idea of working with a multivariate time series seemed daunting in its scope. It is a complex topic, so take your time in understanding the details. The best way to learn is to practice, and so I hope the above Python implemenattion will be useful for you. I enocurage you to use this approach on a dataset of your choice. This will further cement your understanding of this complex yet highly useful topic. If you have any suggestions or queries, share them in the comments section. HI. Thanks for sharing the knowledge and the great article! Could you pls add some details regarding the stationarity test process described in the article : the test is done and the results are presented but it is not clear if it could be concluded that the data is stationary; after the test is done no further actions to make the data stationary are performed¡¦why so. Thanks","Keyword(freq): value(19), variable(12), prediction(7), detail(3), equation(3), term(3), comment(2), method(2), result(2), set(2)"
"8","vidhya",2018-09-27,"The Winning Approaches from codeFest 2018 <U+2013> NLP, Computer Vision and Machine Learning!","https://www.analyticsvidhya.com/blog/2018/09/the-winning-approaches-from-codefest-2018-nlp-computer-vision-and-machine-learning/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 Analytics Vidhya¡¯s hackathons are one of the best ways to evaluate how far you¡¯ve traveled in your data science journey. And what better way than to put your skills to the test against the top data scientists from around the globe? Participating in these hackathons also helps you understand where you need to improve and what else you can learn to get a better score in the next competition. And a very popular demand after each hackathon is to see how the winning solution was designed and the thought process behind it. There¡¯s a lot to learn from this, including how you can develop your own unique framework for future hackathons. We are all about listening to our community, so we decided to curate the winning approaches from our recently concluded hackathon series, codeFest! This was a series of three hackathons in partnership with IIT-BHU, conducted between 31st August and 2nd September. The competition was intense, with more than 1,900 aspiring data scientists going head-to-head to grab the ultimate prize! Each hackathon had a unique element to it. Interested in finding out more? You can view the details of each competition below: It¡¯s time to check out the winners¡¯ approaches! Abhinav Gupta and Abhishek Sharma. The participants were given a list of tweets from customers about various tech firms who manufacture and sell mobiles, computers, laptops, etc. The challenge was to find the tweets which showed a negative sentiment towards such companies or products. The metric used for evaluating the performance of the classification model was weighted F1-Score. Abhinav and Abhishek have summarized their approach in a very intuitive manner, explaining everything from preprocessing and feature engineering to model building. Pre-processing: Feature Extraction: Classifiers used: They<U+00A0>hypertuned each of the above classifiers and found that LSTM (with attention mechanism) produced the best result. Ensemble Deepak Rawat. The Vista hackathon had a pretty intriguing problem statement. The participants had to build a model that counted the number of people in a given group selfie/photo. The dataset provided had already been split, wherein the training set consisted of images with coordinates of the bounding boxes and headcount for each image. The evaluation metric for this competition was RMSE (root mean squared error) over the headcounts predicted for test images. Check out Deepak¡¯s approach in his own words below: Mask R-CNN and<U+00A0>ResNet101 Both stages are connected to the backbone structure. Pre-processing  Model Building Raj Shukla. As a part of enigma competition, the target was to predict the number of upvotes on a question based on other information provided. For every question <U+2013> its tag, number of views received, number of answers, username and reputation of the question author, was provided. Using this information, the participant had to predict the upvote count that the question will receive. The evaluation metric for this competition was RMSE (root mean squared error). Below is the data dictionary for your reference: Here is Raj¡¯s approach to cracking the Enigma hackathon: Feature Engineering: My focus was on feature engineering, i.e., using the existing features to create new features. Below are some key features I cooked up:","Keyword(freq): hackathon(4), feature(3), approach(2), classifier(2), image(2), participant(2), scientist(2), tweet(2), analytics(1), answer(1)"
"9","vidhya",2018-09-24,"Reinforcement Learning Guide: Solving the Multi-Armed Bandit Problem from Scratch in Python","https://www.analyticsvidhya.com/blog/2018/09/reinforcement-multi-armed-bandit-scratch-python/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 Do you have a favorite coffee place in town? When you think of having a coffee, you might just go to this place as you¡¯re almost sure that you will get the best coffee. But this means you¡¯re missing out on the coffee served by this place¡¯s cross-town competitor. And if you try out all the coffee places one by one, the probability of tasting the worse coffee of your life would be pretty high! But then again, there¡¯s a chance you¡¯ll find an even better coffee brewer. But what does all of this have to do with reinforcement learning? Cafe Coffee Day vs Starbucks I¡¯m glad you asked. The dilemma in our coffee tasting experiment arises from incomplete information. In other words, we need to gather enough information to formulate the best overall strategy and then explore new actions. This will eventually lead to minimizing the overall bad experiences. A multi-armed bandit is a simplified form of this analogy. It is used to represent similar kinds of problems and finding a good strategy to solve them is already helping a lot of industries. In this article, we will first understand what actually is a multi-armed bandit problem, it¡¯s various use cases in the real-world, and then explore some strategies on how to solve it. I will then show you how to solve this challenge in Python using a click-through rate optimization dataset. A bandit is defined as someone who steals your money. A one-armed bandit is a simple slot machine wherein you insert a coin into the machine, pull a lever, and get an immediate reward. But why is it called a bandit? It turns out all casinos configure these slot machines in such a way that all gamblers end up losing money! A multi-armed bandit is a complicated slot machine wherein instead of 1, there are several levers which a gambler can pull, with each lever giving a different return. The probability distribution for the reward corresponding to each lever is different and is unknown to the gambler. The task is to identify which lever to pull in order to get maximum reward after a given set of trials. This problem statement is like a single step Markov decision process, which I discussed in this article. Each arm chosen is equivalent to an action, which then leads to an immediate reward. Exploration Exploitation in the context of<U+00A0> Bernoulli MABP The below table shows the sample results for a 5-armed Bernoulli bandit with arms labelled as 1, 2, 3, 4 and 5: This is called Bernoulli, as the reward returned is either 1 or 0. In this example, it looks like the arm number 3 gives the maximum return and hence one idea is to keep playing this arm in order to obtain the maximum reward (pure exploitation). Just based on the knowledge from the given sample, 5 might look like a bad arm to play, but we need to keep in mind that we have played this arm only once and maybe we should play it a few more times (exploration) to be more confident. Only then should we decide which arm to play (exploitation). Bandit algorithms are being used in a lot of research projects in the industry. I have listed some of their use cases in this section. The well being of patients during clinical trials is as important as the actual results of the study. Here, exploration is equivalent to identifying the best treatment, and exploitation is treating patients as effectively as possible during the trial. Clinical Trials Routing is the process of selecting a path for traffic in a network, such as telephone networks or computer networks (internet). Allocation of channels to the right users, such that the overall throughput is maximised, can be formulated as a MABP. Network Routing The goal of an advertising campaign is to maximise revenue from displaying ads. The advertiser makes revenue every time an offer is clicked by a web user. Similar to MABP, there is a trade-off between exploration, where the goal is to collect information on an ad¡¯s performance using click-through rates, and exploitation, where we stick with the ad that has performed the best so far. Online Ads Building a hit game is challenging. MABP can be used to test experimental changes in game play/interface and exploit the changes which show positive experiences for players. Game Designing In this section, we will discuss some strategies to solve a multi-armed bandit problem. But before that, let¡¯s get familiar with a few terms we¡¯ll be using from here on. The expected payoff or expected reward can also be called an action-value function. It is represented by q(a) and defines the average reward for each action at a time t. Suppose the reward probabilities for a K-armed bandit are given by {P1, P2, P3 ¡¦¡¦ Pk}. If the ith arm is selected at time t, then Qt(a) = Pi. The question is, how do we decide whether a given strategy is better than the rest? One direct way is to compare the total or average reward which we get for each strategy after n trials. If we already know the best action for the given bandit problem, then an interesting way to look at this is the concept of regret. Let¡¯s say that we are already aware of the best arm to pull for the given bandit problem. If we keep pulling this arm repeatedly, we will get a maximum expected reward which can be represented as a horizontal line (as shown in the figure below): But in a real problem statement, we need to make repeated trials by pulling different arms till we am approximately sure of the arm to pull for maximum average return at a time t. The loss that we incur due to time/rounds spent due to the learning is called regret. In other words, we want to maximise my reward even during the learning phase.<U+00A0>Regret is very aptly named, as it quantifies exactly how much you regret not picking the optimal arm. Now, one might be curious as to how does the regret change if we are following an approach that does not do enough exploration and ends exploiting a suboptimal arm. Initially there might be low regret but overall we are far lower than the maximum achievable reward for the given problem as shown by the green curve in the following figure. Based on how exploration is done, there are several ways to solve the MABP. Next, we will discuss some possible solution strategies. A naive approach could be to calculate the q, or action value function, for all arms at each timestep. From that point onwards, select an action which gives the maximum q. The action values for each action will be stored at each timestep by the following function: It then chooses the action at each timestep that maximises the above expression, given by: However, for evaluating this expression at each time t, we will need to do calculations over the whole history of rewards. We can avoid this by doing a running sum.<U+00A0>So, at each time t, the q-value for each action can be calculated using the reward: The problem here is this approach only exploits, as it always picks the same action without worrying about exploring other actions that might return a better reward. Some exploration is necessary to actually find an optimal arm, otherwise we might end up pulling a suboptimal arm forever. One potential solution could be to now, and we can then explore new actions so that we ensure we are not missing out on a better choice of arm. With epsilon probability, we will choose a random action (exploration) and choose an action with maximum qt(a) with probability 1-epsilon. With probability 1- epsilon <U+2013> we choose action with maximum value (argmaxa Qt(a)) With probability epsilon <U+2013> we randomly choose an action from a set of all actions A For example, if we have a problem with two actions <U+2013> A and B, the epsilon greedy algorithm works as shown below: This is much better than the greedy approach as we have an element of exploration here. However, if two actions have a very minute difference between their q values, then even this algorithm will choose only that action which has a probability higher than the others. The solution is to make the probability of choosing an action proportional to q. This can be done using the softmax function, where the probability of choosing action a at each step is given by the following expression: The value of epsilon is very important in deciding how well the epsilon greedy works for a given problem. We can avoid setting this value by keeping epsilon dependent on time. For example, epsilon can be kept equal to 1/log(t+0.00001). It will keep reducing as time passes, to the point where we starting exploring less and less as we become more confident of the optimal action or arm. The problem with random selection of actions is that after sufficient timesteps even if we know that some arm is bad, this algorithm will keep choosing that with probability epsilon/n. Essentially, we are exploring a bad action which does not sound very efficient. The approach to get around this could be to favour exploration of arms with a strong potential in order to get an optimal value. Upper Confidence Bound (UCB) is the most widely used solution method for multi-armed bandit problems. This algorithm is based on the principle of optimism in the face of uncertainty. In other words, the more uncertain we are about an arm, the more important it becomes to explore that arm. The intuitive reason this works is that when acting optimistically in this way, one of two things happen: UCB is actually a family of algorithms. Here, we will discuss UCB1. Steps involved in UCB1: We will not go into the mathematical proof for UCB. However, it is important to understand the expression that corresponds to our selected action. Remember, in the random exploration we just had Q(a) to maximise, while here we have two terms. First is the action value function, while the second is the confidence term. Regret Comparison Among all the algorithms given in this article, only the UCB algorithm provides a strategy where the regret increases as log(t), while in the other algorithms we get linear regret with different slopes. An important assumption we are making here is that we are working with the same bandit and distributions from which rewards are being sampled at each timestep stays the same. This is called a stationary problem. To explain it with another example, say you get a reward of 1 every time a coin is tossed, and the result is head. Say after 1000 coin tosses due to wear and tear the coin becomes biased then this will become a non-stationary problem. To solve a non-stationary problem, more recent samples will be important and hence we could use a constant discounting factor alpha and we can rewrite the update equation like this: Note that we have replaced Nt(at) here with a constant alpha, which ensures that the recent samples are given higher weights, and increments are decided more by such recent samples. There are other techniques which provide different solutions to bandits with non-stationary rewards. You can read more about them in this paper. As mentioned in the use cases section, MABP has a lot of applications in the online advertising domain. Suppose an advertising company is running 10 different ads targeted towards a similar set of population on a webpage. We have results for which ads were clicked by a user here.<U+00A0>Each column index represents a different ad. We have a 1 if the ad was clicked by a user, and 0 if it was not. A sample from the original dataset is shown below: This is a simulated dataset and it has Ad #5 as the one which gives the maximum reward. First, we will try a random selection technique, where we randomly select any ad and show it to the user. If the user clicks the ad, we get paid and if not, there is no profit. Total reward for the random selection algorithm comes out to be 1170. As this algorithm is not learning anything, it will not smartly select any ad which is giving the maximum return. And hence even if we look at the last 1000 trials, it is not able to find the optimal ad. Now, let¡¯s try the Upper Confidence Bound algorithm to do the same: The total_reward for UCB comes out to be 2125. Clearly, this is much better than random selection and indeed a smart exploration technique that can significantly improve our strategy to solve a MABP.  After just 1500 trials, UCB is already favouring Ad #5 (index 4) which happens to be the optimal ad, and gets the maximum return for the given problem.","Keyword(freq): action(7), trial(7), algorithm(4), arm(4), case(3), result(3), reward(3), sample(3), strategy(3), work(3)"
"10","vidhya",2018-09-23,"10 Mind-Blowing TED Talks on Artificial Intelligence Every Data Scientist & Business Leader Must Watch","https://www.analyticsvidhya.com/blog/2018/09/best-ted-talks-artificial-intelligence-must-watch/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 TED talks are simply fascinating. They provide tightly knit stories in short doses with mind-blowing information and experiences. It is amazing how much knowledge has been shared in this world using this simple and powerful medium. With Artificial Intelligence and Machine Learning getting so much attention in the spheres of research and business, I started looking out for TED talks on Artificial Intelligence in particular. I was in for such a treat <U+2013> information treat to be precise. I gained much more from watching these TED talks than I have from following some of the most popular YouTube channels out there. Hence, I thought of sharing this incredible content with our community. To save your time <U+2013> I have done all the hard work of watching all the talks on this topic till date and I have shortlisted the best ones for you. Ready for high-quality knowledge enrichment? Dig in! Note: If you are looking to understand what AI is all about and how it¡¯ll impact your career, there¡¯s no better place than our ¡®AI and ML for Business Leaders¡® course! It¡¯s a comprehensive course that gives you the lowdown on what AI and ML are, the common techniques used in the industry, which functions and roles are getting impacted and how, among other things. Check it out today! We all know how creative AI can be, but some of the things Maurice Conti and his team came up with are mindblowing. Using AI for designing new things, like cars, bridges, drones, entire buildings, etc. is no longer limited to the silver screen. This video paints a vivid picture of how AI and humans can (and hopefully will) work together in the future to accomplish tasks neither could perform by themselves. The digital era we find ourselves in the midst of is run by algorithms. They¡¯re everywhere <U+2013> from predicting stock prices to recommending the next movie you should watch. And these algorithms are only going to embed themselves even deeper into our lifestream. This is a thoughtfully curated talk by Kevin Slavin looks at how these algos are shaping our world, and asks a very pressing question <U+2013> at what point do we admit we¡¯ve lost control of them? Nick Bostrom, author of the popular book ¡®Superintelligence¡¯, looks at a future where machines will become smarter than humans. Will machines rule us then? Will humans have any power in that world? These are just some of the questions Nick wants all of us to think about before we heedlessly build AI systems. It¡¯s a deep topic, and quite fitting that it comes from a philosopher. Keeping up the trend of superintelligent AI, Sam Harris delivers a riveting presentation on why we should be scared of building smart AI systems. Sam is a neuroscientist and philosopher, and he delves into both these domains to present a concerning perspective on the future. One of my favorite talks in this list. Can you imagine a world where drivers don¡¯t exist? Or at least, they aren¡¯t human? Well, you better buckle up, because that world is transforming into a reality in front of us. Chris Urmson, former head of Google¡¯s driverless car program, gives us the lowdown on how autonomous<U+00A0>vehicles take decisions in real-time about what to do next. Computer vision is the hottest research field in machine learning right now. It has spawned applications like real-time facial recognition and object detection. But how does it work? The wonderful Fei-Fei Li delivers this thrilling talk on how machines are being trained using computer vision techniques. This talk was delivered three years ago, and the state-of-the-art algorithms have since come quite a long way. Shows how quickly AI is advancing! Continuing our theme of computer vision talks, here¡¯s Joseph Redmon demonstrating how object detection works in real-time. I remember watching this a few months back and being left awed by the demo. It still has the same effect! Joseph built his model using the YOLO framework. Pretty cool, right? Driverless cars are the primary beneficiaries of these advances. Anthony Goldbloom is the co-founder and CEO of Kaggle and an all-around nice person. He¡¯s as good a person as any to give a perspective on the jobs machines will automate in the future (in fact some of the things he mentioned are already happening as I type this!). The key takeaway from this talk is that we need to upskill ourselves at every opportunity we get, otherwise the risk of being left behind will always hang like a shadow over us. Tom Gruber is the co-creator of Siri, so he knows what he¡¯s talking about. He takes a more positive view on the advances in AI and how it can help us enhance the way we live our day-to-day lives.<U+00A0>He shares his vision for a future where AI helps us achieve superhuman performance in perception, creativity, and cognitive function. Music and AI <U+2013> a perfect combination. Pierre Barreau demonstrates AIVA, an artificial intelligence system that creates musical scores! The system has been trained on over 30,000 music compositions (including from the likes of Mozart). Pierre shows us a glimpse of how AIVA was designed using deep neural networks, and the visualizations are spectacular. This is by no means an exhaustive list. There are plenty more Ted Talks available on both the official platform and YouTube that relate to AI. But the reason I¡¯ve chosen these talks is to give you a sense of what industry leaders and experts think about this topic. They are at the forefront of this domain and control a lot about the way we approach things.","Keyword(freq): talk(8), machine(4), algorithm(3), advance(2), car(2), leader(2), system(2), technique(2), algo(1), application(1)"
