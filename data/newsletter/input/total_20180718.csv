"site","date","headline","url_address","text","keyword"
"kaggle",2018-07-12,"Data Notes: How to Forecast the S&P 500 with Prophet","http://blog.kaggle.com/2018/07/12/data-notes-how-to-forecast-the-sp-500-with-prophet/","Stock markets, political parties, and extraterrestrials: Enjoy these new, intriguing, and overlooked datasets and kernels. 1. S&P 500 Simple Forecasting with Prophet (link) 2.<U+00A0><U+0001F4DA> arXiv Data Analysis: Computation and Language Papers (link) 3.<U+00A0><U+0001F47D> Dysonian SETI with Machine Learning (link) 4.<U+00A0><U+0001F3A6> Content-based Recommender Using Text Mining (link) 5.<U+00A0><U+0001F3AC> Predicting Cast in TV's Frasier Based on Dialog (link) 6.<U+00A0><U+0001F1EE><U+0001F1F1> Shift in Votes Between Political Parties (link) 7.<U+00A0><U+0001F1F0> Data Science Glossary on Kaggle (link) 8.<U+00A0><U+0001F697> Kaggle Dataset #1: Stanford Cars Dataset (link) 9.<U+00A0><U+0001F50E> Kaggle Dataset #2: Columbia Object Image Library (link) 10.<U+00A0><U+0001F921> Kaggle Dataset #3: CelebFaces Attributes Dataset (link) Got unlabeled data? Unsupervised methods like PCA and t-SNE can uncover hidden structure! Copyright ¨Ï 2018 Kaggle, All rights reserved.","Keyword(freq): data(3), party(2), car(1), celebface(1), dataset(1), extraterrestrial(1), kernel(1), market(1), method(1), pca(1)"
"datacamp",2018-07-16,"Data Science at McKinsey (Transcript)","https://www.datacamp.com/community/blog/data-science-mckinsey","Here is a link to the podcast. Hugo:    Hi there Taras, and welcome to DataFramed. Taras:    Hi Hugo. I'm really glad to be here. Hugo:    It's great to have you on the show. I'm really excited to be talking about management consulting, analytics for businesses, and data science. So you're a Senior Analytics Manager at McKinsey and I want to know what that looks like on the ground. But before we get there, I'd like to know what your colleagues think that you do. Taras:    My colleagues, especially that don't do analytics themselves, don't always have a very clear view. Most of them think I spent most of my time building models. Some of my analytics colleagues think that I spend most of my time talking to clients but the truth is a lot broader than that. Hugo:    And I'm excited to get to that. I'm wondering, for the non-technical people, your colleagues, what does building models mean to them? What do they think building models looks like? Taras:    It's one of those sometimes scary, sometimes black box type of process where they see it as a whole bunch of random data from different client systems coming in and the outcome comes out that hopefully tells them some insights into what drives performance of the clients or what they should advise clients to do but for most non-technical people, most but not all, this process is not very, very transparent. Hugo:    Yeah, and I suppose they think there's some sort of computation involved; some sort of mathematics involved but they don't have a strong sense of what that may mean. Taras:    Exactly. But there are some people that do a lot of analytic work, even if they are not data scientist themselves, and for them they actually know exactly what's going on, they just don't write the code. It's actually impressive how well they understand the underlying data science. Hugo:    And I think McKinsey does an incredible job in a lot of work in thinking about how to explain technical model building, data science, analytic techniques, to a broader audience. I saw recently there's a great interactive, web-based explanation you have called an Executive's Guide to AI, which really explains the nuts and bolts to what type of models are considered artificial intelligence these days. Taras:    That's exactly right. We actually worked on that: one expert from our team really drove development of most of the content for the guide. We get this question asked all the time and we cannot use technical language to executives because they need to understand it, and the same time we really want to be factually accurate. Things like that, even explaining what machine learning, what deep learning is in simple terms is, is very very important for us. I think it's actually very important more generally, so people can make decisions, actually understand what they're trying to accomplish and they feel comfortable acting on it. Hugo:    Yeah, I like that, and I like the fact that a subtle overtone of what you just said then is that the machines don't necessarily make decisions yet. We do have certain types of learning algorithms, reinforcement learning in particular, but the fact that when we're looking at predictive analytics, we've got AI making predictions but then it's up to human responsibility to then make decisions and take action. Taras:    It's a great point. I think it's one of the key things to understand how different is model prediction from the actual decision. Model prediction might say that a customer is likely to buy certain product if it's offered at a certain discount or a customer is likely to leave the company because of interruptions in services. But that has nothing to do with the business decision that the senior executive needs to make, which usually is what type of discounts to offer to any particular customer, because there's a lot more going on. There's a lot more strategic, as well as operational practices.Taras:    Sometimes people think predicting the outcome, through the model, is the same as acting on it. It's just the beginning. There's a really, really long chain of thinking, usually qualitative thinking, that need to happen before you act on a decision. Not always. In some cases decisions need to be made in real time. For example, banking fraud or credit card fraud, and then algorithms actually act but for management consulting, when you talk about strategy and really high impact decisions, models need to be interpreted. We like to use the word translated, and the implications need to be assessed and pressure-tested before anyone can make any decision because the stakes are just too high. Hugo:    I love it and I look forward to returning to these ideas when we talk about specific verticals and industries where you see the most demand for data science analytics and management consulting. Let's get back to you before we get there. You told us that you colleagues generally will think that you build models to talk to clients but you also told us that what you do is a lot more broad then that. So what do you actually do? Taras:    I spend time in 4 to 5 different areas. As somebody who built and is leading a fairly large data science team, probably the most important part of my work is to make sure that we have the right capabilities that we can serve our clients on what their needs are. That involves quite a lot of strategic thinking in terms of what problems data science will be solving tomorrow or next year, and working backwards from there: what people you need to have on a team, what technologies you need to test out, what use cases and what clients you want to work with. There's a little bit of that.Taras:    A second piece is extremely important for anybody, which is the essential piece around relationship management. For us, that means working with clients, managing strategic relationships so we can actually deploy data science tools and derive real change. Those are internal. As a data science function, you need to work with technology function, with data function, with business development function, with marketing function, with individual businesses because data science is by nature is so embedded into the rest of the organization that you cannot be effective unless you spend significant time, 30-50% of your time just managing the relationships.Taras:    It's still a people world so you need to help people be successful and that starts at just the first interview that you interview somebody and hopefully if they get hired, then it's onboarding, then it's finding the right projects, then it's doing mentorship on the project, handling conflicts, helping people grow in what role they are in - how experienced they are - because consulting is people business and the only way to create impact is by mentoring, developing people, creating opportunities for them.Taras:    And then of course there is the actual data science part which I really, really enjoy doing. For me that's usually involves working with data scientist from the team, helping them structure their approach, and then helping them, if things don't work, to understand why so they are able to build models and deliver the outcomes. I still sometimes try to be hands-on data scientist write code and build models myself but it's really, really hard to find time. Hugo:    I'm sure. Taras:    Last piece, is of course, whenever I serve clients, it's making sure they can digest the models we are building for them. They know what the models mean. They can act on them, they can implement them, they actually most importantly measure the outcomes post implementation. They can see this really works and it's actual, tangible outcomes benefits either financial or other benefits to their organizations because that's the heart of what we do as consultants in data science. Hugo:    So really that last point speaks to something very essential to the job that you do which is taking the results of the data analytic and data science process, and as we said, turning them into decisions and actionables. But even before that I suppose, you need to translate the business problem into a data science question, solve it to a certain extent and then translate it back to an actionable. You have this two bi-directional process, in and out of data science, right? Taras:    That's absolutely right. There is always a process of ... even understanding the business problem, and then translating that into an analytics approach and in the consulting settings, that's actually not that straightforward because at the beginning when we just start the project, we don't always know everything we would like to know about the client business and especially about their data environment and how they make decisions. The process becomes very iterative. Based on what we know, we structure an analytics process, figure out which models need to be built, at what level of granularity, with what data, and then we try that and while we're trying that, we see what works and what doesn't. We show it to clients. We get their input and based on that we refine the  analytics approach until we actually get to something that's as meaningful and impactful as possible. So that translation happens at the beginning but it continues throughout the project. Hugo:    Does the question arise ¡°is data science or data analytics even the appropriate approach to the business question posed?¡±? Taras:    Absolutely and I think it's the right question to ask. If you can solve the client question with a simpler approach, there is no need for data science or advanced analytics. I think advanced analytics is a great tool but we shouldn't try to force it to be used everywhere and if a simpler approach works or there's not enough data, then some high level strategic qualitative thinking or some interview-based approach or some case study based approach works much better. On the other hand there are real cases where you have to use fairly advanced modeling to get to an understanding of the environment or market or business that's deep enough and that's accurate enough. That's the case for data science.Taras:    And by the way, what was really interesting is that it happens both ways. Sometimes you start an advanced analytics project that you think you build a lot of models, and as you learn client business or data availability more, you realize that's just not going to work, so you go back to basic qualitative analysis. Sometimes you start this strategy project that has none of these components, and in the middle you realize that there's so much to be learned from applying advanced machine learning that you can re-scope the project and bring that in. And that's why it's so important to have people involved in the project who actually understand data science as well as understand the business question; and then they can make these decisions on what's the best approach to go forward. Hugo:    This actually speaks to something else, that different people on your team have different skills, and you mentioned that you need a fairly large team of data scientists, and my question there is, have we as a community figured out what data science teams should look like? And what I mean by that is with backend engineers, we know what those teams generally look like and how they work together and best practices. Have we figured this out in data science yet? Taras:    It's an excellent question. I don't think we have and I believe it's not just McKinsey or consulting. I think it's in general. It's evolving very, very quickly. Our own journey was that in the beginning we had more people who were predictive modelers and who had very broad backgrounds. They could build a wide range of predictive models. Increasingly what I see is that on one hand you need to start specializing much deeper so you need to start having people that only do NLP or only do deep learning or only do anomaly detection.Taras:    On the other hand, analytics is found beyond predictive modeling. For many questions that we need to answer now, somebody understand complexity theory and can do real advanced simulations is actually invaluable. I only see this process of changing the needs for data science skills accelerating with every year. For us, not only have we not figured out the steady state but I don't think there is a steady state.Taras:    On the other hand, analytics is found beyond predictive modeling. For many questions that we need to answer now, somebody understand complexity theory and can do real advanced simulations is actually invaluable. I only see this process of changing the needs for data science skills accelerating with every year. For us, not only have we not figured out the steady state but I don't think there is a steady state. Taras:    Maybe for some organizations that have much more fixed business model that you only need certain types of skills sets, maybe that's more steady, but even then I don't believe, given how quickly data science changes, you can be static about it. You need to constantly be adding skill sets and moving along with the field. Hugo:    And something you've spoken to there, is not only is the skill set changing so rapidly and the techniques, but even what happens on a daily basis. As we see more feature selection automated, more data manipulation, data munging, data cleaning, automated machine learning, for example, we're going to see what data scientists do on the ground evolving incredibly quickly. Taras:    That's absolutely right. I think it's amazing to have tools now that we didn't have two years ago. Even for computing infrastructure, you can go to Google Cloud or AWS and spin the system of clusters with all the software that you need in one click of a button, which was completely impossible before. That changes the speed with which we work and the nature of the models we can build and how easy it is, so much that suddenly it opens up new possibilities. There's only strong influence of technology on what skill sets data scientists need to have as well. Hugo:    Fantastic. I want to jump in now and find out through the lens of your work at McKinsey, which verticals and/or industries do you see the most demand for data science, data analytics, management consulting, at the moment? Taras:    If I look at the moment, I think about it in 3 broad buckets of industries. The first one is the one that really interacts with consumers directly. Industries like retail, industries like telecom, some media, some banking, and for them the need to set up - it's actually really driven by personalization - the need to heavily customize their products, their marketing messages to individual consumers ideally and do it in real time with as much data and analytics as possible is really key. Taras:    The other bucket is essentially organizations for which risk management is a big deal. Insurance for example. Insurance is all about price and risk properly and managing claim processing very efficiently. Setting up the rates very efficiently and for them you can't do risk management without quantitative methods. That is another really, really interesting application of analytics. Taras:    And then finally many other organizations generate a lot of data from their operations. For example, think about a semi-conductive fab that has literally hundreds of different pieces of equipment and each piece of equipment for each process step, making chips, generates real time data with millisecond precision from all the sensors. It's extremely complex. The data is very large and all real time and the stakes are really high to optimize manufacturing and do it well. I think for those organizations there is huge need for advanced analytics to be driving better decisions. Another piece similar is genomics in bioinformatics: huge amount of data coming in. Taras:    Another piece is health care where you have now structured data on health care claims but also unstructured data from medical testing, images, etc. This is a little bit more where data is there but analytic methods are not always there. The data has not been used yet, fully, and that's another area where there is increasing pool for data science. Hugo:    There is so much interesting stuff in there that I'm at a loss to figure out quite which direction to go. You stated industry verticals that address a lot of consumer interactions, those that need excellent risk management. You also mentioned industries where data is being generated. Semi-conductors are really interesting because I think a lot of people, when they think about data science, they think about data science in tech where we kind of know what we're doing a lot of the time. But when we've got real time data flowing straight in and we need to make decisions straight away or automate those decisions, that's a very different game isn't it? Taras:    Absolutely. We always talk about digital born companies and they do amazing stuff don't get me wrong, but if you're a completely digital company, it's actually easy because all your information is digital. You can access it and you can test your ideas really quickly. It's just natural. Try to actually do analytics if you're a semi-conductor player or if you're agriculture provider. Some of my most fascinating experienced were actually in old school field like agriculture but imagine that you have a tractor, driving in a farm. The tractor has a whole bunch of sensors. Now tractor has a video camera that can film the fields and you have a drone that flies on top and it takes real time data of the crop. All of this needs to be transferred, in a very low cost, from the farm to a central processing unit or all the computations have to be on premise, on tractor, as an edge system. Taras:    And you need to optimize an extremely complex set of real, physical equipment actions. It's actually really, really hard and you need to do it in such a way that the farmer would not mind using the advanced analytics to improve his yield of crops. I think very often we don't give enough credit for traditional industries that operate in the physical world because it's so much harder.. To me that is were a lot of value of analytics in the future will be coming from. Hugo:    That agriculture example is so fantastic because it speaks to another very interesting concern that ... lets say that you have a drone taking photograph and you want to do some image analysis, pattern recognition, object detection, bounding boxes, whatever it may be. You might say I can just throw a huge convolutional neural net at it. Having said that, if you want to do this in real time in the drone, sure if you¡¯ve got clusters in the cloud, you can do this, but if you want to do this in real time in the drone, it will actually change what type of model you build to do it, right? Taras:    Exactly because literally in that time, you start thinking about time it takes to classify single image and you actually literally start counting how many milliseconds your convolutional network can run. Different between 100 milliseconds and 300 milliseconds means certain speed of flying for the drone and you just cannot go sometimes with the 300 because otherwise to take image of 100 acre fields will take you forever. You go with simpler models that can work in the ag computing environment that maybe not as accurate as possible or you spend a lot of time thinking through your deploying architecture, how to keep accuracy up but reduce computing time down dramatically. Taras:    That's another fascinating problem that's very different from what conventional data scientists will have to deal with. Hugo:    Right. Now I want to jump in and find out about organizations that you've worked with or industries. I do understand that there are certain ... there's a lot of privacy you need to respect with respect to your client base but I was wondering if you'll tell me about a few key examples of organizations and/or industries that you've worked with. Taras:    Yeah, absolutely. Let me switch gears and talk again about another real low tech line. I work with a correctional facility and the problem that they had was problem of violence. The inmates were sometimes really violent and they were seeing increases in violence and the administration could not understand why. It's very challenging environment. It's always high pressure and the stakes are really high. Literally people might get injured or killed. On the other hand, you have ability to act and a lot of data you can use to make better decisions. Taras:    What we've done is we looked at what actually drives violence in each group. It's not necessarily about individuals. It's about if you put certain number of individual in the same cell, it's the relative composition of people that go in there that increases or decreases the violence. If you start thinking about it from the quantitative perspective and understand what drives that, you can actually reduce violence quite dramatically. Taras:    It's a great example where looking at industry that traditionally does not use analytics leads to a huge impact and it really makes a difference in a major way. Hugo:    So that sounds like a success story. I do think when an organization has challenges or problems where data analytics can help to solve, there is some sort of barrier. The value needs to be created and it needs to be demonstrated. I'm wondering what does it take to change an organization in terms of their decision making process, to change them through data science? Taras:    That's a great question. It's a very challenging task because many things need to happen and all of them need to happen. If one link is missing, chances are it's not going to work. First of all you need to have a vision of what analytics should do for an organization. Why do you actually want to have data science or analytics in the organization and people need to believe it will add value, and it is connected to the business strategy. Not just analytic teams but throughout the organization.Taras:    Secondly, you need to have support from very senior executives, ideally C-level, to actually create that excitement in the organization, to make sure we have the right visibility and funding and right resources to do it; and to actually help analytics or data science team work with other parts of the business. Because with all of that, very often data scientists just build models in isolation and models never get used. Then you need to have the right data and data environment, and that is relatively a slow process that can be expensive. You always need to start building that environment but you shouldn't wait until that is done before you start deploying analytics.Taras:    You want to identify areas where you can drive real measurable results very quickly so that there is excitement in the organization about the analytics and more and more people want to try. That's what we mean by use cases. You find out where there is the biggest value for analytics, and you go, and you build the models. You start making decisions different. You capture that value for 3, 4, 5 use cases quickly and then everybody wants to do it for their own part of business. Taras:    And then you need to make sure it actually sticks; it's not something that you've done once. People use it for three months and then they went back to the old ways. To make something stick, you need to redesign the processes of making decisions that usually involve some kind of technology solution - software or interfaces - to make analytics digestible and it involves retraining people. It involves new ways to measure the outcomes of the decisions. Taras:    And finally, organizations that are really great at analytics, they always do that. You need to change the culture of the organization so that every time you're on a business meeting and you want to propose something, people will ask what data do we have to act on this proposal. Is it really backed up by hard numbers, well-designed models or not? And once you get to that level, then analytics truly becomes part of the company DNA and you accomplish what you're trying to do but that is multi-year journey for most organizations, and it's not easy. Hugo:    Right. I want to zoom in on this idea of early stage value extraction, in particular through several use cases. I suppose this essentially is having a few proofs of concepts, demonstrating their value, and gaining trust of people at different levels throughout the organization. Taras:    Yes. That's fair. Think about it from the perspective of a business executive who doesn't necessarily understand technology. It's a buzz, everybody talks about it but you want to know if this thing is real or not. What real means is if I start doing what the model tells me to do, is my business doing better or worse? You need to convince somebody to give you a shot, which usually happens through the proof of concept, and then you need to very rigorously measure what happened so that it's so clear, that there was impact and the impact is directly attributable to analytics. That use case does two things for you: A, the business partner that you work with becomes a champion for you and uses more analytics within their own part of business; but B, creates a much broader visibility that other people see it and other people say, ""You know what, this thing actually really, really works and I'm just going to go and try to do it.""Taras:    Then finally, what's important is if you can show that you spend $2 million on a use case but it generated $50 million in revenue or in cost savings, suddenly you can claim that analytics becomes self-funded so organizations can allocate a bigger budget to do another 3 or 4 use cases and to fund new technology or new software or new data management systems and that's how you get going in real organizations because you always need to justify the budget that you spent on certain activities. Hugo:    Right. So you mentioned if one of these moving parts is missing, the change in the organization will very likely fail. There's a lot of these moving parts. You mentioned creating the vision for analytics, having strong support particularly at C-level, early value extraction, process redesign, culture change, data foundation. It seems like a lot to get working together. How often are you able to do that? How often are you able to see it all work together? Taras:    It's a great question. Another question to ask, another way to ask this is, how many organizations have actually been able to achieve it? There is not many. In each industry there is maybe 2, 3, 4 organizations that are ahead of the pack and sometimes none of them actually achieve this wide acceptance of analytics in every decision making that we talk about. It's actually tough but what we're seeing is that where clients are now, it's very different from where they were 2 years ago. Everybody is moving in the right direction and most companies are making significant progress. All of them are not there yet and I think this journey will take a little bit longer for them. Taras:    There's too many things that need to happen and each of these steps is quite a long step. Hugo:    So Taras, you've been leveraging data science to help Fortune 500 companies, for example, improve their performance for over a decade now. How have the different moving parts of data science evolved over this time? Taras:    It's fascinating. I would say that there's so many changes that happen in so many ways. The simplest one to see is software. Ten years ago we were using mostly SAS, and then open source became really prominent and people moved to R, and then to Python and there is just continuous emergence of new software tools and that's very easy to see and track. Taras:    Second piece I think is for algorithms themselves. Ten years ago we were still doing a lot of BI, and we were a doing a little bit of statistical modeling, but it was mostly linear models. It was classical statistics but then machine learning became much more prominent, and then every quickly we started being able to work with unstructured data through deep learning, through NLP, and to me that's the next wave that's happening now, moving to the unstructured information. Taras:    Another change that I noticed is in what domains analytics is used. Ten years ago market analytics was big. Risk analytics was big in financial sector because financial sector really needed that. And then some of the heavily operational companies would use supply chains in inventory management, but those are three big areas. Now if you look at where analytics is, it's literally everywhere. It's in HR organization function. It's understanding what people to hire and when they are likely to leave.Taras:    It's every interaction we have with customers, it¡¯s customization. It's just so broad and that's relatively new. This spread of analytics into every decision making is something that happened in the last 10 years and to me that's probably the most fascinating part. Hugo:    I think the HR example is really interesting and we¡¯re seeing increases in machines helping out with the hiring process, for example, of course machines can encode human biases, machines can create their own type of biases as well. I'm wondering what your view on these types of biases that may occur, what the major challenges are, and how important it is to have a human in the loop as much as possible. Taras:    That's another great question. I will tell you my own perspective. I do think there's many different points of views on this. I think that human plus machine works a lot better than just machine alone. Some of my colleagues that are data scientist in tech firms actually disagree and they see their role as getting human out of the loop and fully automating everything. I think there's a difference in the nature of the problems we are trying to solve with analytics. But for me, on the one hand, it's really good to design algorithms with as little bias as possible, and bias usually just comes from your input data. Taras:    You need to think about is your data is actually representative or is it biased in any particular way. But on the other hand, once you get your model predictions, what do you do with it? We still come back to the issue of translation and to act properly on output algorithms, that's where humans really add value. If I go to the HR example, for example, I spend a lot of time looking at McKinsey's recruiting and understanding what performance characteristics people display during interviews and predict their long-term success but it's not like I take output of predictive model and allow algorithm decide who is going to get hired or not.Taras:    It's just for us to inform our recruiting process to focus on the relevant things but humans still need to process, digest, assess how precise the predictions are and assess false positives and true positives and based on that, design the right recruiting process. You can't just take human out and let algorithm design. It will not work very well for much qualitative, strategic decision you need to make. Hugo:    You actually said something very interesting, well a lot of very interesting things in there, but something I want to zoom in on is you said a model may optimize long term success, and we might not even know how to define long-term success correctly. That may be something that evolves over time. Trying to figure out what we're actually optimizing for or what we're learning is just as important as implementing the algorithm. Taras:    Absolutely. The performance metrics matter tremendously and sometimes we see if you change one metric to the other, suddenly the models change a lot. That's also why it's so important to keep an open mind and have a human involved because sometimes we build models for different outcomes. The outcome may be, if I take a slightly more traditional commercial example, outcome might be revenue growth. It might be profitability. It might be market share, and you realize if you build 3 different models with 3 different outcomes, you get 3 different set of drivers and if you start analyzing the differences between them, you really get a much deeper understanding of your market dynamics because you look at multiple dimension of the problem. That's absolutely critical. Hugo:    So you got some really nice insight into how you're seeing the different moving parts of data science evolve over the past decade and more. How do you see the different parts of data science evolving in the future? Taras:    It's fascinating. For one thing, I think open source is here to stay. I just can't imagine doing analytics without open source, given how complex it gets and how increasingly powerful it becomes. We all have to share the code, share algorithms, and it will remain relevant for a very long time.
Taras:    Secondly, I think unstructured data will become more and more important. It's already most information that's been generated has been unstructured, but we are just at the beginning to use it and I think the advances that can come with improving, for example, NLP techniques, will be truly transformative and it's actually beginning very, very real. Even the work we do now in NLP, it's so different now than what you could do 2 years ago, that if i just fast forward the pace of development I know within the next 5 years we'll have a very different face of analytics just because of that. Taras:    To me it's also a lot about interpretability. Take deep learning for example. Amazing techniques. It allows you to do things that are not possible completely. Analyzing videos, analyzing images that are not possible to do well before deep learning. At the same time, the applicability of deep learning, in truly strategic decision making, is not that high primarily because of that. But I spent a lot of time looking at symbolic equations. Something that can automatically discover features just as deep learning does but it presents those features as symbolic equations. Taras:    That becomes extremely powerful because then you understand what's going on and what drives your outcomes and you can actually explain it to the executives. To me I think there'll be a lot of advances made in our representation of models and in making models more transparent, while using complex models. Simple models we have done. It is really now about complex models but making them understandable. To me that will be another big change that will hopefully happen in another 5 years. Hugo:    Great, having seen how the different parts of data science will evolve in the future, in your mind, how do you see the use of data science within organizations evolving in the coming years? Taras:    It's another fascinating question. I think that data science will be democratized. Part of it is that isolated data teams that we have now in many organizations will become more much embedded within the rest of the organizations. Literally data science will have connections to every business, every function, and as a result will be impacting a lot more decisions. Another piece is that I think data science will be used by some of the non  data scientists, just like when Excel and Spreadsheets came around, suddenly business analysts were able to use it and you didn't need to be a programmer to do it. Taras:    With good graphical interfaces, with algorithms that are much more intelligent, you can actually have people who are business managers or business analysts to use data science tools. I think we'll see a lot more of that in just core data, so data science as well. There'll be hard problems and real data scientists will be solving them and there'll be more standard problem for which we will have really good tools and lots of other people will be able to use them. Hugo:    This is somewhat of a controversial question in a lot of circles but my question for you is can now or is there a future in which data science can be done in a GUI? Taras:    We¡¯re all data scientists, and the reason we don't like GUI is because it's an inefficient way to work but at the same time, if all you do is multivariate regression, and all you need is look at your p-values and look at your regression coefficients, why would you as a data scientist want to do it yourself - writing code in Python? Just give the GUI to business analysts, let them upload it, build in the right checks and balances, and to make sure right significances are done, and help business analysts take intro statistics courses and you're done. I think DataCamp, for example, has done a lot democratizing that knowledge because I know online learning data scientists are so powerful and I think it's not just data scientists that take the courses. At some level we as a data scientist don't want to let it go and sometimes we just don't trust other people to do it right but for us to really continue to have impact, we need to work on really hard problems with continuously new methods and approaches and that means stop working on simpler problems that are very easily solvable that somebody else can do. For me I just see it as a natural progression. Hugo:    I love it. I'd like to know one of your favorite data science techniques or methodologies is. Taras:    I am huge fan of autonomous feature or hypothesis generation. Again to me the ability to find features that drive outcomes, but yet unlike deep learning, ability to express those features through symbolic equations, is hugely powerful, at least in a strategic decision making where we need to have more understanding than precision. I have seen this space evolving very, very rapidly in the last 2-3 years from pretty much zero from before that, to actually now having companies and products that can do it quite well and I think we're just scratching the surface of that. For me the interface of powerful future engineering and interpretability is definitely my favorite area of data science now. Taras:    Another one is, which is my second favorite, is the science of complexity. We talk about building models but if you start thinking about simulating behavior of complex and non-linear dynamic systems that sometimes behave in chaotic ways, they're still guided by rules that can be learned and can be developed. It's fascinating and its impact is huge because our life is essential a complex system and every organization is a complex system. Human bodies are complex systems. Most engineering processes are complex systems so the ability to do it well, with precise mathematics, is absolutely fascinating. And again, I'm beginning to see some data scientists and some software tools being developed that can handle complexity without excessive amount of effort, I would say. To me that's another emerging trend that I think we'll be talking more about 4 or 5 years from now. Hugo:    I really like that the favorite techniques and methodology you mention are very forward looking because the automatic feature hypothesis generation is something which is now and in the future will change what all data scientists do on the daily basis. It will automate a lot of the drudgery, the 80%, so we can focus on the far more exciting and creative work. Taras:    Absolutely. Absolutely. I have no doubt on that because it's already happening and we will just continue to do this in more classes of problems and with more impact. Hugo:    So Taras, to wrap up, do you have a final call to action for our listeners out there? Taras:    My final call is really things are moving so fast now and all of us as data scientists, if we're doing now what we were doing a year ago, we're really falling behind. My call for action, and that's something that I try to do everyday, continue to learn new things everyday. Keep in touch with new software development, keep in touch with new algorithms, understand mathematics and just deploy it because to me that's the biggest fun part about being a data scientist is this learning and the application to new domains, new areas all the time. Taras:    It's very, very nice that we actually have to do that ourselves to stay relevant. Don't do the same thing over and over. Keep looking out for new opportunities. Hugo:    I couldn't agree more and I think the learning new things, especially as you said in a field that's moving so quickly. You also mentioned to learn some of the math as well, which is incredibly important, and to also not be so scared of math because math can be overwhelming in a lot of ways. When you're writing code, bit by bit, the basics of what you're doing, maybe try to re-implement a few of your favorite algorithms, whatever they may be. In terms of learning new things, I may be biased but I think DataCamp is an incredible platform to do that on. I also think McKinsey has a lot of fantastic online resources. As I said, this interactive Executive's Guide to AI, you've actually got a whole bunch of stuff, commentary stuff, released on the economics of AI which I think would provide a wonderful counterpoint to people actually writing code as well. I think all of that is really fantastic and we'll include a link to a bunch of those resources in the show notes. Taras:    Perfect. Could not agree more. Hugo:    Absolutely. So Taras, thank you so much for joining me on the show. It's been absolute pleasure. Taras:    Yeah. Pleasure is mine. Thanks so much for inviting.","Keyword(freq): data(96), analytics(45), tara(33), model(29), decision(16), organization(16), scientist(13), client(10), industry(9), algorithm(8)"
"mastery",2018-07-18,"The Role of Randomization to Address Confounding Variables in Machine Learning","https://machinelearningmastery.com/confounding-variables-in-machine-learning/","A large part of applied machine learning is about running controlled experiments to discover what algorithm or algorithm configuration to use on a predictive modeling problem. A challenge is that there are aspects of the problem and the algorithm called confounding variables that cannot be controlled (held constant) and must be controlled-for. An example is the use of randomness in a learning algorithm, such as random initialization or random choices during learning. The solution is to use randomness in a way that has become a standard in applied machine learning. We can learn more about the rationale for using randomness in controlled experiments by looking briefly at why randomness is used to manage confounding variables in medicine through the use of randomized clinical trials. In this post, you will discover confounding variables and how we can address them using the tool of randomization. After reading this post, you will know: Let¡¯s get started. The Role of Randomization to Address Confounding Variables in Machine LearningPhoto by Funk Dooby, some rights reserved. This post is divided into four parts;l they are: Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In an experiment, we are often interested in the effect of an independent variable on a dependent variable. A confounding variable is a variable that confuses the relationship between the independent and the dependent variable. Confounding, sometimes referred to as confounding bias, is mostly described as a ¡®mixing¡¯ or ¡®blurring¡¯ of effects. <U+2014> Confounding: What it is and how to deal with it, 2008. A confounding variable can influence the outcome of an experiment in many ways, such as: A confounding variable may be known or unknown. They are often characterized as having an association or correlation with both the independent and dependent variables. Another characterization is that the confounding variable affects groups or observations differently. Confounding variables or confounders are often defined as the variables correlate (positively or negatively) with both the dependent variable and the independent variable. A Confounder is an extraneous variable whose presence affects the variables being studied so that the results do not reflect the actual relationship between the variables under study. <U+2014> How to control confounding effects by statistical analysis, 2012. The deeper difficulty of confounding variables is that it may not be obvious that they exist and are impacting results. The effects of confounding variables are often not obvious or even identifiable unless they are specifically addressed in the design of the experiment or data collection method. Confounding variables are traditionally a concern in applied statistics. This is because in statistics we are often concerned with the effect of independent variables on dependent variables in data. Statistical methods are designed to discover and describe these relationships and confounding variables can essentially corrupt or invalidate discoveries. Machine learning practitioners are typically interested in the skill of a predictive model and less concerned with the statistical correctness or interpretability of the model. As such, confounding variables are an important topic when it comes to data selection and preparation, but less important than they may be when developing descriptive statistical models. Nevertheless, confounding variables are critically important in applied machine learning. The evaluation of a machine learning model is an experiment with independent and dependent variables. As such, it is subject to confounding variables. What may be surprising is that you already know this and that the gold-standard practices in applied machine learning address this. Therefore, being intimately aware of the confounding variables in machine learning experiments is required to understand the choice and interpretation of machine learning model evaluation. Consider, what impacts the evaluation of a machine learning model, what are the independent variables? Some examples include: Each of these choices will impact the dependent variable in a machine learning experiment, which is the chosen metric used to estimate the skill of the model when making predictions. The evaluation of a machine learning model involves the design and execution of controlled experiments. A controlled experiment holds all elements constant except one element under study. The two most common types of controlled experiments in machine learning are: Nevertheless, there are confounding variables that the controlled experiments cannot hold constant. Specifically, there are sources of randomness, that if they were held constant would result in an invalid evaluation of the model. Three examples include: For example, weights in a neural network are initialized to random values. Stochastic gradient descent randomizes the order of samples in an epoch to vary the types of updates performed. Random subsets of features are selected for each possible cut point in random forest. And many more examples. Randomization in machine learning algorithms is not a bug; it is a feature intended to improve the performance of the model on average over classical deterministic methods. Randomness can be present in ML at many different levels, usually enhancing performance or alleviating problems and difficulties of classical methods. <U+2014> Randomized Machine Learning Approaches: Recent Developments and Challenges, 2017. These are confounding variables that we cannot hold constant. If they are held constant, the evaluation of the model will no longer provide insight into the generalizability of the result. We will know how well the model performs on a specific data sample or initialization of sequence of decisions during learning, but little idea on how the model will perform in general. The way that we can handle confounding variables that we cannot control is by using randomization. Randomization is a technique used in experimental design to give control over confounding variables that cannot (should not) be held constant. For example, randomization is used in clinical experiments to control-for the biological differences between individual human beings when evaluating a treatment. It is the reason why a treatment must be evaluated on multiple individuals rather than on a single individual before the findings can be generalized. In randomization the random assignment of study subjects to exposure categories to breaking any links between exposure and confounders. This reduces potential for confounding by generating groups that are fairly comparable with respect to known and unknown confounding variables. <U+2014> How to control confounding effects by statistical analysis, 2012. Randomization is a simple tool in experimental design that allows the confounding variables to have their effect across a sample. It shifts the experiment from looking at an individual case to a collection of observations, where statistical tools are used to interpret the finding. In medicine, randomization is the gold standard for evaluating a treatment and is called the randomized clinical trial. It is designed to remove not only the confounding effects of biological differences, but also the bias, such as the effect of the experimenter choosing the members of the treatment and non-treatment groups. You can imagine that a treatment would look very successful if the least-sick members of a cohort were chosen to be administered. An [Randomized clinical trial] is a special kind of cohort study, with the characteristic that patients are randomly assigned to the experimental group (with exposure) and the control group (without exposure). [¡¦] Therefore, randomization helps to prevent selection by the clinician, and helps to establish groups that are equal with respect to relevant prognostic factors. <U+2014> The randomized clinical trial: An unbeatable standard in clinical research?, 2007. There are still confounding variables when using a randomized clinical trial. An example is the case where the experimenters know what treatment participants of the study are receiving. This can impact the way the experimenters interact with the participants, which in turn can impact the results of the experiment. The answer is to use blinding where participants or experimenters do not know the treatment. Ideally, a double-blind experiment is adopted, ensuring that both participates and experimenters are unaware of their treatment. When feasible, it is strongly recommended that also after randomization, patients and clinicians do not know who receives the intervention and who does not. Studies may be single blind (either the patient or the clinician does not know who receives the treatment and who does not) or double blind (both the patient and the clinician do not know who receives the treatment). <U+2014> The randomized clinical trial: An unbeatable standard in clinical research?, 2007. Note, before we move on to look at the use of randomization in machine learning, consider that there are other approaches to managing the effect of confounding variables. Wikipedia has a good list here. Randomization is used in the evaluation of machine learning models to manage the uncontrollable confounding variables. It is key to the standard ways described for evaluating machine learning models and the rationale for using methods such as data resampling and repeating experiments. Randomization allows the machine learning practitioner to generalize a finding, to make it useful and applicable. It¡¯s the reason why careful design of the test harness and resampling method is important. It is the reason why we repeat the evaluation of a model and the reason we don¡¯t fix the seed on the pseudorandom number generator. I talk more about these topics in the posts: When we take a closer look at why we use randomization, to control for confounding variables, it raises questions about the other confounders that we may not be controlling for. For example, the machine learning practitioner knowing the skill of models prior to giving each model a chance to do its best via data preparation and hyperparameter tuning. Perhaps practitioners should blind themselves to remove the possibility of biasing the choice of final model. The risk is that the practitioner that really likes artificial neural networks will ¡°discover¡± a neural network configuration that outperforms other models. At best it is a statistical fluke or violation of Occam¡¯s Razor for a parsimonious solution to a predictive modeling project; at worst, it is scientific fraud. The reason that clinicians aggressively removed this bias is people¡¯s lives were at risk. We may get to that point with machine learning algorithms, e.g. in cars. In practice, today, I think this is good motivation for front-loading an experiment with a large and careful design and automating the execution and statistical interpretation of the results. This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered confounding variables and how we can address them using the tool of randomization. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦by writing lines of code in python Discover how in my new Ebook:Statistical Methods for Machine Learning It provides self-study tutorials on topics like:Hypothesis Tests, Correlation, Nonparametric Stats, Resampling, and much more¡¦ Skip the Academics. Just Results. Click to learn more. Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning.","Keyword(freq): variable(32), experiment(8), data(6), effect(5), method(5), model(5), result(5), experimenter(4), confounder(3), example(3)"
"mastery",2018-07-16,"All of Statistics for Machine Learning","https://machinelearningmastery.com/all-of-statistics-for-machine-learning/","A foundation in statistics is required to be effective as a machine learning practitioner. The book ¡°All of Statistics¡± was written specifically to provide a foundation in probability and statistics for computer science undergraduates that may have an interest in data mining and machine learning. As such, it is often recommended as a book to machine learning practitioners interested in expanding their understanding of statistics. In this post, you will discover the book ¡°All of Statistics¡±, the topics it covers, and a reading list intended for machine learning practitioners. After reading this post, you will know: Let¡¯s get started. All of Statistics for Machine LearningPhoto by Chris Sorge, some rights reserved. The book ¡°All of Statistics: A Concise Course in Statistical Inference¡± was written by Larry Wasserman and released in 2004. All of Statistics Wasserman is a professor of statistics and data science at Carnegie Mellon University. The book is ambitious. It seeks to quickly bring computer science students up-to-speed with probability and statistics. As such, the topics covered by the book are very broad, perhaps broader than the average introductory textbooks. Taken literally, the title ¡°All of Statistics¡± is an exaggeration. But in spirit, the title is apt, as the book does cover a much broader range of topics than a typical introductory book on mathematical statistics. This book is for people who want to learn probability and statistics quickly. <U+2014> Page vii, All of Statistics: A Concise Course in Statistical Inference, 2004. The book is not for the average practitioner; it is intended for computer science undergraduate students. It does assume some prior knowledge in calculus and linear algebra. If you don¡¯t like equations or mathematical notation, this book is not for you. Interestingly, Wasserman wrote the book in response to the rise of data mining and machine learning in computer science occurring outside of classical statistics. He asserts in the preface the importance of having a grounding in statistics in order to be effective in machine learning. Using fancy tools like neural nets, boosting, and support vector machines without understanding basic statistics is like doing brain surgery before knowing how to use a band-aid. <U+2014> Pages vii-viii, All of Statistics: A Concise Course in Statistical Inference, 2004. The material is presented in a very clear and concise manner. A systematic approach is taken with brief descriptions of a method, equations describing its implementation, and worked examples to motivate the use of the method with sample code in R. In fact, the material is so compact that it often reads like a series of encyclopedia examples. This is great if you want to know how to implement a method, but very challenging if you are new to the methods and seeking intuitions. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course The choice of topics covered by the book is very broad, as mentioned in the previous section. This is great on the one hand as the reader is given exposure to advanced subjects early on. The downside of this aggressive scope is that topics are touched on briefly with very little hand holding. You are left to re-read sections until you get it. Let¡¯s look at the topics covered by the book. This is helpful to both get an idea of the presented scope of the field and the context for the topics that may interest you as a machine learning practitioner. The book is divided into three parts; they are: The first part of the book focuses on probability theory and formal language for describing uncertainty. The second part is focused on statistical inference. The third part focuses on specific methods and problems raised in the second part. The book does have a reference or encyclopedia feeling. As such, there are a lot of chapters, but each chapter is reasonably standalone. The book is divided into 24 chapters; they are: The preface for the book provides a useful glossary of terms mapping them from statistics to computer science. This ¡°Statistics/Data Mining Dictionary¡± is reproduced below. Statistics/Data Mining DictionaryTaken from ¡°All of Statistics¡°. All of the R code and datasets used in the worked examples in the book are available from Wasserman¡¯s homepage. This is very helpful as you can focus on experimenting with the examples rather than typing in the code and hoping that you got the syntax correct. I would not recommend this book to developers who have not touched statistics before. It¡¯s too challenging. I would recommend this book to computer science students who are in math-learning-mode. I would also recommend it to machine learning practitioners with some previous background in statistics or a strong mathematical foundation. If you are comfortable with mathematical notation and you know what you¡¯re looking for, this book is an excellent reference. You can flip to the topic or the method and get a crisp presentation. The problem is, for a machine learning practitioner, you do need to know about many of these topics, just not at the level of detail presented. Perhaps a shade lighter, at the intuition level. If you are up to it, it would be worth reading (or skimming) the following chapters in order to build a solid foundation in probability for statistics: Again, these are important topics, but you require a concept-level understanding only. For coverage of statistical hypothesis tests that you may use to interpret data and compare the skill of models, the following chapters are recommended reading: I would also recommend the chapter on the Bootstrap. It¡¯s just a great method to have in your head, but with a focus for either better understanding bagging and random forest or as a procedure for estimating confidence intervals of model skill. Finally, a statistical approach is used to present machine learning algorithms. I would recommend these chapters if you prefer a more mathematical treatment of regression and classification algorithms: I can read the mathematical presentation of statistics, but I prefer intuitions and working code. I am less likely to pick up this book from my bookcase, in favor of gentler treatments such as ¡°Statistics in Plain English¡± or application focused treatments such as ¡°Empirical Methods for Artificial Intelligence¡°. Do you agree with this reading list?
Let me know in the comments below. This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered the book ¡°All of Statistics¡± that provides a broad and concise introduction to statistics. Specifically, you learned: Have you read this book?
What did you think of it? Let me know in the comments below. Are you thinking of picking up a copy of this book?
Let me know in the comments. ¡¦by writing lines of code in python Discover how in my new Ebook:Statistical Methods for Machine Learning It provides self-study tutorials on topics like:Hypothesis Tests, Correlation, Nonparametric Stats, Resampling, and much more¡¦ Skip the Academics. Just Results. Click to learn more. Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning.","Keyword(freq): statistics(27), topic(10), data(5), example(4), method(4), comment(3), practitioner(3), student(3), algorithm(2), developer(2)"
"mastery",2018-07-13,"A Gentle Introduction to Statistical Power and Power Analysis in Python","https://machinelearningmastery.com/statistical-power-and-power-analysis-in-python/","The statistical power of a hypothesis test is the probability of detecting an effect, if there is a true effect present to detect. Power can be calculated and reported for a completed experiment to comment on the confidence one might have in the conclusions drawn from the results of the study. It can also be used as a tool to estimate the number of observations or sample size required in order to detect an effect in an experiment. In this tutorial, you will discover the importance of the statistical power of a hypothesis test and now to calculate power analyses and power curves as part of experimental design. After completing this tutorial, you will know: Let¡¯s get started. A Gentle Introduction to Statistical Power and Power Analysis in PythonPhoto by Kamil Porembi<U+0144>ski, some rights reserved. This tutorial is divided into four parts; they are: Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course A statistical hypothesis test makes an assumption about the outcome, called the null hypothesis. For example, the null hypothesis for the Pearson¡¯s correlation test is that there is no relationship between two variables. The null hypothesis for the Student¡¯s t test is that there is no difference between the means of two populations. The test is often interpreted using a p-value, which is the probability of observing the result given that the null hypothesis is true, not the reverse, as is often the case with misinterpretations. In interpreting the p-value of a significance test, you must specify a significance level, often referred to as the Greek lower case letter alpha (a). A common value for the significance level is 5% written as 0.05. The p-value is interested in the context of the chosen significance level. A result of a significance test is claimed to be ¡°statistically significant¡± if the p-value is less than the significance level. This means that the null hypothesis (that there is no result) is rejected. Where: We can see that the p-value is just a probability and that in actuality the result may be different. The test could be wrong. Given the p-value, we could make an error in our interpretation. There are two types of errors; they are: In this context, we can think of the significance level as the probability of rejecting the null hypothesis if it were true. That is the probability of making a Type I Error or a false positive. Statistical power, or the power of a hypothesis test is the probability that the test correctly rejects the null hypothesis. That is, the probability of a true positive result. It is only useful when the null hypothesis is rejected. ¡¦ statistical power is the probability that a test will correctly reject a false null hypothesis. Statistical power has relevance only when the null is false. <U+2014> Page 60, The Essential Guide to Effect Sizes: Statistical Power, Meta-Analysis, and the Interpretation of Research Results, 2010. The higher the statistical power for a given experiment, the lower the probability of making a Type II (false negative) error. That is the higher the probability of detecting an effect when there is an effect. In fact, the power is precisely the inverse of the probability of a Type II error. More intuitively, the statistical power can be thought of as the probability of accepting an alternative hypothesis, when the alternative hypothesis is true. When interpreting statistical power, we seek experiential setups that have high statistical power. Experimental results with too low statistical power will lead to invalid conclusions about the meaning of the results. Therefore a minimum level of statistical power must be sought. It is common to design experiments with a statistical power of 80% or better, e.g. 0.80. This means a 20% probability of encountering a Type II area. This different to the 5% likelihood of encountering a Type I error for the standard value for the significance level. Statistical power is one piece in a puzzle that has four related parts; they are: All four variables are related. For example, a larger sample size can make an effect easier to detect, and the statistical power can be increased in a test by increasing the significance level. A power analysis involves estimating one of these four parameters given values for three other parameters. This is a powerful tool in both the design and in the analysis of experiments that we wish to interpret using statistical hypothesis tests. For example, the statistical power can be estimated given an effect size, sample size and significance level. Alternately, the sample size can be estimated given different desired levels of significance. Power analysis answers questions like ¡°how much statistical power does my study have?¡± and ¡°how big a sample size do I need?¡±. <U+2014> Page 56, The Essential Guide to Effect Sizes: Statistical Power, Meta-Analysis, and the Interpretation of Research Results, 2010. Perhaps the most common use of a power analysis is in the estimation of the minimum sample size required for an experiment. Power analyses are normally run before a study is conducted. A prospective or a priori power analysis can be used to estimate any one of the four power parameters but is most often used to estimate required sample sizes. <U+2014> Page 57, The Essential Guide to Effect Sizes: Statistical Power, Meta-Analysis, and the Interpretation of Research Results, 2010. As a practitioner, we can start with sensible defaults for some parameters, such as a significance level of 0.05 and a power level of 0.80. We can then estimate a desirable minimum effect size, specific to the experiment being performed. A power analysis can then be used to estimate the minimum sample size required. In addition, multiple power analyses can be performed to provide a curve of one parameter against another, such as the change in the size of an effect in an experiment given changes to the sample size. More elaborate plots can be created varying three of the parameters. This is a useful tool for experimental design. We can make the idea of statistical power and power analysis concrete with a worked example. In this section, we will look at the Student¡¯s t test, which is a statistical hypothesis test for comparing the means from two samples of Gaussian variables. The assumption, or null hypothesis, of the test is that the sample populations have the same mean, e.g. that there is no difference between the samples or that the samples are drawn from the same underlying population. The test will calculate a p-value that can be interpreted as to whether the samples are the same (fail to reject the null hypothesis), or there is a statistically significant difference between the samples (reject the null hypothesis). A common significance level for interpreting the p-value is 5% or 0.05. The size of the effect of comparing two groups can be quantified with an effect size measure. A common measure for comparing the difference in the mean from two groups is the Cohen¡¯s d measure. It calculates a standard score that describes the difference in terms of the number of standard deviations that the means are different. A large effect size for Cohen¡¯s d is 0.80 or higher, as is commonly accepted when using the measure. We can use the default and assume a minimum statistical power of 80% or 0.8. For a given experiment with these defaults, we may be interested in estimating a suitable sample size. That is, how many observations are required from each sample in order to at least detect an effect of 0.80 with an 80% chance of detecting the effect if it is true (20% of a Type II error) and a 5% chance of detecting an effect if there is no such effect (Type I error). We can solve this using a power analysis. The statsmodels library provides the TTestIndPower class for calculating a power analysis for the Student¡¯s t test with independent samples. Of note is the TTestPower class that can perform the same analysis for the paired Student¡¯s t test. The function solve_power() can be used to calculate one of the four parameters in a power analysis. In our case, we are interested in calculating the sample size. We can use the function by providing the three pieces of information we know (alpha, effect, and power) and setting the size of argument we wish to calculate the answer of (nobs1) to ¡°None¡°. This tells the function what to calculate. A note on sample size: the function has an argument called ratio that is the ratio of the number of samples in one sample to the other. If both samples are expected to have the same number of observations, then the ratio is 1.0. If, for example, the second sample is expected to have half as many observations, then the ratio would be 0.5. The TTestIndPower instance must be created, then we can call the solve_power() with our arguments to estimate the sample size for the experiment. The complete example is listed below. Running the example calculates and prints the estimated number of samples for the experiment as 25. This would be a suggested minimum number of samples required to see an effect of the desired size. We can go one step further and calculate power curves. Power curves are line plots that show how the change in variables, such as effect size and sample size, impact the power of the statistical test. The plot_power() function can be used to create power curves. The dependent variable (x-axis) must be specified by name in the ¡®dep_var¡® argument. Arrays of values can then be specified for the sample size (nobs), effect size (effect_size), and significance (alpha) parameters. One or multiple curves will then be plotted showing the impact on statistical power. For example, we can assume a significance of 0.05 (the default for the function) and explore the change in sample size between 5 and 100 with low, medium, and high effect sizes. The complete example is listed below. Running the example creates the plot showing the impact on statistical power (y-axis) for three different effect sizes (es) as the sample size (x-axis) is increased. We can see that if we are interested in a large effect that a point of diminishing returns in terms of statistical power occurs at around 40-to-50 observations. Power Curves for Student¡¯s t Test Usefully, statsmodels has classes to perform a power analysis with other statistical tests, such as the F-test, Z-test, and the Chi-Squared test. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered the statistical power of a hypothesis test and how to calculate power analyses and power curves as part of experimental design. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦by writing lines of code in python Discover how in my new Ebook:Statistical Methods for Machine Learning It provides self-study tutorials on topics like:Hypothesis Tests, Correlation, Nonparametric Stats, Resampling, and much more¡¦ Skip the Academics. Just Results. Click to learn more. nIce article thank u Thanks. Nice article as always¡¦ Thanks. Excellent Thanks. Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning.","Keyword(freq): sample(10), curve(7), parameter(7), result(7), observation(5), size(5), student(5), analysis(4), variable(4), axi(3)"
"vidhya",2018-07-16,"An Introductory Guide to Maximum Likelihood Estimation (with a case study in R)","https://www.analyticsvidhya.com/blog/2018/07/introductory-guide-maximum-likelihood-estimation-case-study-r/","Interpreting how a model works is one of the most basic yet critical aspects of data science. You build a model which is giving you pretty impressive results, but what was the process behind it? As a data scientist, you need to have an answer to this oft-asked question. For example, let¡¯s say you built a model to predict the stock price of a company. You observed that the stock price increased rapidly over night. There could be multiple reasons behind it. Finding the likelihood of the most probable reason is what Maximum Likelihood Estimation is all about. This concept is used in economics, MRIs, satellite imaging, among other things. Source: YouTube In this post we will look into how Maximum Likelihood Estimation (referred as MLE hereafter) works and how it can be used to determine coefficients of a model with any kind of distribution. Understanding MLE would involve probability and mathematics, but I will try to make it easier with examples. Note: As mentioned, this article assumes that you know the basics of maths and probability. You can refresh your concepts by going through this article first <U+2013><U+00A0>6 Common Probability Distributions every data science professional should know. Let us say we want to predict the sale of tickets for an event. The data has the following histogram and density. How would you model such a variable? The variable is not normally distributed and is asymmetric and hence it violates the assumptions of linear regression. A popular way is to transform the variable with log, sqrt, reciprocal, etc. so that the transformed variable is normally distributed and can be modelled with linear regression. <U+00A0><U+00A0><U+00A0> Let¡¯s try these transformations and see how the results are: With Log transformation:  With Square Root Transformation: With Reciprocal:  None of these are close to a normal distribution. How should we model such data so that the basic assumptions of the model are not violated? How about modelling this data with a different distribution rather than a normal one? If we do use a different distribution, how will we estimate the coefficients?  This is where Maximum Likelihood Estimation (MLE) has such a major advantage. While studying stats and probability, you must have come across problems like <U+2013> What is the probability of x > 100, given that x follows a normal distribution with mean 50 and standard deviation (sd) 10. In such problems, we already know the distribution (normal in this case) and its parameters (mean and sd) but in real life problems these quantities are unknown and must be estimated from the data. MLE is the technique which helps us in determining the parameters of the distribution that best describe the given data. Let¡¯s understand this with an example: Suppose we have data points representing the weight (in kgs) of students in a class. The data points are shown in the figure below (the R code that was used to generate the image is provided as well): Figure 1 This appears to follow a normal distribution. But how do we get the mean and standard deviation (sd) for this distribution? One way is to directly compute the mean and sd of the given data, which comes out to be 49.8 Kg and 11.37 respectively. These values are a good representation of the given data but may not best describe the population. We can use MLE in order to get more robust parameter estimates. Thus, MLE can be defined as a method for estimating population parameters (such as the mean and variance for Normal, rate (lambda) for Poisson, etc.) from sample data such that the probability (likelihood) of obtaining the observed data is maximized.  In order to get an intuition of MLE, try to guess which of the following would maximize the probability of observing the data in the above figure? Clearly, it is not very likely we¡¯ll observe the above data shape if the population mean is 100. Now that you got an intuition of what MLE can do, we can get into the details of what actually likelihood is and how it can be maximized. But first, let¡¯s start with a quick review of distribution parameters. Let us first understand distribution parameters. Wikipedia¡¯s definition of this term is as follows: ¡°It is a quantity that indexes a family of probability distributions¡±. It can be regarded as a numerical characteristic of a population or a statistical model. We can understand it by the following diagram: Figure 2, Source: Wikipedia The width and height of the bell curve is governed by two parameters <U+2013> mean and variance. These are known as distribution parameters for normal distribution. Similarly, Poisson distribution is governed by one parameter <U+2013> lambda, which is the<U+00A0>number of times an event occurs in an interval of time or space. Figure 3, Source: Wikipedia Most of the distributions have one or two parameters, but some distributions can have up to 4 parameters, like a 4 parameter beta distribution.  From Fig. 2 and 3 we can see that given a set of distribution parameters, some data values are more probable than other data. From Fig. 1, we have seen that the given data is more likely to occur when the<U+00A0>mean is 50, rather than<U+00A0>100. In reality however, we have already observed the data. Accordingly, we are faced with an inverse problem: Given the observed data and a model of interest, we need to find the one Probability Density Function/Probability Mass Function (f(x|¥è)), among all the probability densities that are most likely to have produced the data. To solve this inverse problem, we define the likelihood function by reversing the roles of the data vector x and the (distribution) parameter vector ¥è in f(x| ¥è),<U+00A0>i.e., L(¥è;x) = f(x| ¥è) In MLE, we can assume that we have a likelihood function L(¥è;x), where ¥è<U+00A0>is the distribution parameter vector and x is the set of observations. We are interested in finding the value of ¥è that maximizes the likelihood with given observations (values of x).  The mathematical problem at hand becomes simpler if we assume that the observations (xi) are independent and identically distributed random variables drawn from a Probability Distribution, f0 (where f0<U+00A0>= Normal Distribution for example in Fig.1). This reduces the Likelihood function to:<U+00A0><U+00A0> To find the maxima/minima of this function, we can take the derivative of this function w.r.t ¥è<U+00A0>and equate it to 0 (as zero slope indicates maxima or minima). Since we have terms in product here, we need to apply the chain rule which is quite cumbersome with products. A clever trick would be to take log of the likelihood function and maximize the same. This will convert the product to sum and since log is a strictly increasing function, it would not impact the resulting value of ¥è. So we have: To find the maxima of the log likelihood function LL(¥è; x), we can: There are many situations where calculus is of no direct help in maximizing a likelihood, but a maximum can still be readily identified. There¡¯s nothing that gives setting the first derivative equal to zero any kind of ¡®primacy¡¯ or special place in finding the parameter value(s) that maximize log-likelihood. It¡¯s simply a convenient tool when a few parameters need to be estimated.  As a general principle, pretty much any valid approach for identifying the argmax of a function may be suitable to find maxima of the log likelihood function. This is an unconstrained non-linear optimization problem. We seek an optimization algorithm that behaves in the following manner: It¡¯s very common to use optimization techniques to maximize likelihood; there are a large variety of methods (Newton¡¯s method, Fisher scoring, various conjugate gradient-based approaches, steepest descent, Nelder-Mead type (simplex) approaches, BFGS and a wide variety of other techniques).  It turns out that when the model is assumed to be Gaussian as in the examples above, the MLE estimates are equivalent to the ordinary least squares method. You can refer to the proof here. Let us now look at how MLE can be used to determine the coefficients of a predictive model.<U+00A0><U+00A0> Suppose that we have a sample of n observations y1, y2, . . . , yn which can be treated as realizations of independent Poisson random variables, with Yi ¡­ P(¥ìi). Also, suppose that we want to let the mean ¥ìi (and therefore the variance!) depend on a vector of explanatory variables xi . We could form a simple linear model as follows <U+2013>  where<U+00A0>¥è<U+00A0>is the vector of model coefficients. This model has the disadvantage that the linear predictor on the right-hand side can assume any real value, whereas the Poisson mean on the left-hand side, which represents an expected count, has to be non-negative. A straightforward solution to this problem is to model the logarithm of the mean using a linear model. Thus, we consider a generalized linear model with log link log, which can be written as follows <U+2013>  Our aim is to find ¥è by using MLE.  Now, Poisson distribution is given by: We can apply the log likelihood concept that we learnt in the previous section to find the ¥è. Taking logs of the above equation and ignoring a constant involving log(y!), we find that the log-likelihood function is <U+2013> where ¥ìi depends on the covariates xi and a vector of<U+00A0>¥è coefficients.<U+00A0>We can substitute ¥ìi = exp(xi¡¯¥è) and solve the equation to get<U+00A0>¥è<U+00A0>that maximizes the likelihood.<U+00A0>Once we have the<U+00A0>¥è<U+00A0>vector, we can then predict the expected value of the mean by multiplying the xi and ¥è vector. In this section, we will use a real-life dataset to solve a problem using the concepts learnt earlier.<U+00A0>You can download the dataset from this link.<U+00A0><U+00A0>A sample from the dataset is as follows:  <U+00A0><U+00A0><U+00A0>Datetime  Count of tickets sold 25-08-2012 00:00   <U+00A0><U+00A0><U+00A0> 8 25-08-2012 01:00 <U+00A0><U+00A0><U+00A0><U+00A0> 2 25-08-2012 02:00 <U+00A0><U+00A0><U+00A0><U+00A0> 6 25-08-2012 03:00 <U+00A0><U+00A0><U+00A0><U+00A0> 2 25-08-2012 04:00 <U+00A0><U+00A0><U+00A0><U+00A0> 2 25-08-2012 05:00 <U+00A0><U+00A0><U+00A0><U+00A0> 2 It has the count of tickets sold in each hour from 25th Aug 2012 to 25th Sep 2014<U+00A0> (about 18K records). Our aim is to predict the number of tickets sold in each hour. This is the same dataset which was discussed in the first section of this article.  The problem can be solved using techniques like regression, time series, etc. Here we will use the statistical modeling technique that we have learnt above using R. Let¡¯s first analyze the data. In statistical modelling, we are concerned more with how the target variable is distributed. Let¡¯s have a look at the distribution of counts: <U+00A0> This could be treated as a Poisson distribution or we could even try fitting an exponential distribution.  Since the variable at hand is count of tickets, Poisson is a more suitable model for this. Exponential distribution is generally used to model time interval between events. Let¡¯s plot the count of tickets sold over these 2 years: Looks like there is a significant increase in sale of tickets over time.<U+00A0>In order to keep things simple, let¡¯s model the outcome by only using age as a factor, where age is the defined no. of weeks elapsed since 25th Aug 2012. We can write this as: where, ¥ì (Count of tickets sold) is assumed to follow the mean of Poisson distribution and ¥è0<U+00A0>and ¥è1 are the coefficients that we need to estimate.  Combining Eq. 1 and 2, we get the log likelihood function as follows: We can use the mle() function in R stats4 package to estimate the coefficients ¥è0<U+00A0>and ¥è1. It needs the following primary parameters:  For our example, the negative log likelihood function can be coded as follows: I have divided the data into train and test set so that we can objectively evaluate the performance of the model. idx is the indices of the rows which are in test set. Next let¡¯s call the mle function to get the parameters: This gives us the estimate of the coefficients. Let¡¯s use RMSE as the evaluation metric for getting results on the test set: Now let¡¯s see how our model fairs against the standard linear model (with errors normally distributed), modelled with log of count.  As you can see, RMSE for the standard linear model is higher than our model with Poisson distribution.<U+00A0>Let¡¯s compare the residual plots for these 2 models on a held out sample to see how the models perform in different regions: We see that the errors using Poisson regression are much closer to zero when compared to Normal linear regression.  Similar thing can be achieved in Python by using the<U+00A0>scipy.optimize.minimize() function which accepts objective function to minimize, initial guess for the parameters and methods like BFGS, L-BFGS, etc. Its further simpler to model popular distributions in R using the glm function from the<U+00A0>stats package. It supports Poisson, Gamma, Binomial, Quasi, Inverse Gaussian, Quasi Binomial, Quasi Poisson distributions out of the box. For the example shown above, you can get the coefficients directly using the below command: Same can be done in Python using pymc.glm() and setting the family as pm.glm.families.Poisson().  One way to think of the above example is that there exist better coefficients in the parameter space than those estimated by a standard linear model. Normal distribution is the default and most widely used form of distribution, but we can obtain better results if the correct distribution is used instead. Maximum likelihood estimation is a technique which can be used to estimate the distribution parameters irrespective of the distribution used. So next time you have a modelling problem at hand, first look at the distribution of data and see if something other than normal makes more sense! The detailed code and data is present on my Github repository.<U+00A0>Refer to the ¡°Modelling single variables.R¡± file for an example that covers data reading, formatting and modelling using only age variables. I have also modelled using multiple variables, which is present in the<U+00A0>¡°Modelling multiple variables.R¡± file.","Keyword(freq): data(24), parameter(15), coefficient(10), ticket(8), variable(7), distribution(6), observation(4), result(4), bfg(3), problem(3)"
"vidhya",2018-07-12,"13 Common Mistakes Amateur Data Scientists Make and How to Avoid Them?","https://www.analyticsvidhya.com/blog/2018/07/13-common-mistakes-aspiring-fresher-data-scientists-make-how-to-avoid-them/","So you¡¯ve decided data science is the field for you. More and more businesses are becoming data driven, the world is increasingly becoming more connected and looks like every business will need a data science practice. So, the demand for data scientists is huge. Even better,<U+00A0>everyone acknowledges the shortfall of talent in the industry. However, becoming a data scientist does not come easy. It needs a mix of problem solving, structured thinking, coding and various technical skills among others to be truly successful. If you are from a non-technical and non-mathematical background, there¡¯s a good chance a lot of your learning happens through books and video courses. Most of these resources don¡¯t teach you what the industry is looking for in a data scientist. This is one of the reasons why aspiring data scientists are struggling to bridge the gap between self education and real-world jobs. In this article, I discuss the top mistakes amateur data scientists make (I have made some of them myself). I have also provided resources wherever applicable with the aim of helping you avoid these pitfalls on your data science journey. Source: Cognitive Class <U+2013> YouTube As I mentioned in my article on AV¡¯s practice problems<U+00A0><U+2013> it¡¯s good to get a grasp of the theory behind machine learning techniques. But if you don¡¯t apply them, they are only theoretical concepts. When I started out learning data science, I made the same mistake <U+2013> I studied books and online courses but didn¡¯t always apply them to solve a problem. So when I was faced with a challenge or problem where I had the chance to apply all that I had learned, I couldn¡¯t remember half of it! There¡¯s so much to learn <U+2013> algorithms, derivations, research papers, etc. There¡¯s a high chance you¡¯ll lose your motivation halfway through and give up. I have personally seen this happen to a lot of people who attempt to enter this field. It¡¯s imperative that your learning process should be a healthy balance between theoretical and practical. As soon as you learn a concept, head over to Google and find a dataset or problem where you can use it. You¡¯ll find that you are retaining that concept way better than before. You can also use AV¡¯s DataHack platform to take part in practice problems and ongoing competitions. You will have to accept that you cannot learn everything in one go. Fill in the gaps as you practice and you will learn a whole lot more! Source: Imperial College London <U+2013> YouTube The majority of folks who want to become a data scientist are inspired by videos of robots, or awesome predictive models, and in some cases even the high salaries. Sadly (sorry to disappoint!), there is a long road you need to travel, before you reach there. You should get to know how techniques work before you apply them in a problem.<U+00A0>Learning this will help you understand how an algorithm works, what you can do to fine tune it, and will also help you build on existing techniques. Mathematics plays an important role here so it¡¯s always helpful to know certain concepts. In a day-to-day corporate data scientist role you may not need to know advanced calculus, but having a high-level overview definitely helps. In case you have a curious mind, or want to get into a research role, the four key components you need to know before diving into core machine learning are: Just as a house is built brick-by-brick, a data scientist is also the sum of all the individual parts. There are tons of resources out there which will help you learn these topics. I have mentioned one resource of each topic below which should get you started: You can also check out Analytics Vidhya¡¯s ¡®Introduction to Data Science¡® course which includes a comprehensive module on statistics and probability. Source: CIO.com Ah, the pet-peeve of hiring managers and recruiters. Ever since data science became ultra popular, certifications and degrees have cropped up just about everywhere. A glance through my LinkedIn feed shows up at least 5 certification images proudly being displayed. While achieving that certification is no easy feat, relying solely on it is a recipe for disaster. There are too many of these courses online being poured over and completed by thousands upon thousands of aspiring data scientists. If they ever added a unique value to your data science CV, that is no longer the case. Hiring managers do not care much for these pieces of paper <U+2013> they place far more emphasis on your knowledge, and how you¡¯ve applied it in real-life practical situations. This is because dealing with clients, handling deadlines, understanding how a data science project lifecycle works, how to design your model to fit into the existing business framework <U+2013> these are just some of the things you will need to know to succeed as a data scientist. Just a certification or degree will not qualify you for it. Don¡¯t get me wrong <U+2013> certifications are valuable, but only when you apply that knowledge outside the classroom and put it out in the open. Use real-world datasets and whatever analysis you do, make sure you write about it. Create your own blog, post it on LinkedIn, and ask for feedback from the community. This shows that you are willing to learn and are flexible enough to ask for suggestions and work them into your projects. You should be open to the idea of internships (regardless of your experience level). You will learn a lot about how a data science team works, which will benefit you when you sit for another interview. If you¡¯re looking for that next project, you¡¯ve come to the right place. We have an awesome list of projects here divided by the degree of difficulty. Get started NOW. This is one of the biggest misconceptions aspiring data scientists have these days. Competitions and hackathons provide us with datasets that are clean and spotless (okay <U+2013> I went a little overboard, but you get the hang of it). You download them, and start working on the problem. Even those datasets that have columns with missing values don¡¯t require you to work your brain cells off <U+2013> figure out an imputation technique and fill in the blanks. Unfortunately, real-world projects don¡¯t work like that. There is an end-to-end pipeline which involves working with a bunch of people. You will almost always have to work with messy and unclean data. The old saying about spending 70-80% of your time just collecting and cleaning data is true. It¡¯s the grueling part and you will (most likely) not enjoy but it¡¯s something that eventually becomes part of a routine. Also, and we will cover this in more detail in the next point, the simpler model will win precedence over any complex stacked ensemble model. Accuracy isn¡¯t always the end goal, and this is one of the most contrasting things you¡¯ll learn on the job. One of the key factors to negate this misunderstanding is, ironically, experience. The more experience you gain (internships help a lot in this case), the better you¡¯ll be able to distinguish between the two. This is where social media comes in handy <U+2013> reach out to data scientists and ask them their experience. Additionally, I suggest going through this Quora thread where data scientists from around the world provide their input on this exact question. Getting a good score on a competition leaderboard is excellent for measuring your learning progress, but interviewers will want to know how you can optimize your algorithm for impact, not for optimization. Learn about how a data science project works, what different types of roles a team has (from a data engineer to a data architect), and structure your answer in that sense. Go through this LinkedIn post which explains the standard methodology for analytical models. Source: Design Shack As mentioned above, accuracy isn¡¯t always what the business is after. Sure a model that predicts loan default with 95% accuracy is good, but if you can¡¯t explain how the model got there, which features led it there, and what your thinking was when building the model, your client will reject it. You will rarely, if ever, find a deep neural network being used in commercial applications. It¡¯s just not possible to explain to the client how a neural network (let alone a deep one) worked with hidden layers, convolutions layers, etc. The first preference is, and will always be, on ensuring that we are able to understand what¡¯s going on underneath the model. If you can¡¯t tell whether age, or number of family members, or previous credit history went into rejecting a loan application, how will the business run? Another key aspect is whether your model will fit within the organization¡¯s existing framework. Using 10 different types of tools and libraries will fail spectacularly if the production environment cannot support it. You will have to redesign and retrain the model from scratch with a simpler approach. The best way to prevent yourself from making this mistake is speaking to people working in the industry. There is no better teacher than experience. Pick a domain (finance, HR, marketing, sales, operations, etc.) and reach out to people to understand how their project works. Apart from that, practice making simpler models and then explaining them to non-technical people. Then add complexity to your model and keep doing this until even you don¡¯t understand what¡¯s going on beneath. This will teach you when to stop, and why simple models are always given preference in real-life applications. If you have done this before, you will know what I¡¯m talking about. If your resume currently has this problem, rectify it immediately! You may know a plethora of techniques and tools but simply listing them down will turn off potential hiring managers. Your resume is a profile of what you have accomplished and how you did it <U+2013> not a list of things to simply jot down. When a recruiter looks at your resume, he/she wants to understand your background and what all you have accomplished in a neat and summarized manner. If half the page is filled with vague data science terms like linear regression, XGBoost, LightGBM, without any explanation, your resume might not clear the screening round. The simplest way to eliminate resume clutter is to use bullet points. Only list the techniques which you have used to accomplish something (could be a project or a competition). Write a line about how you used it <U+2013> this helps the recruiter understand your thinking. When you¡¯re applying for fresher or entry-level jobs, your resume needs to reflect what potential impact you can add to the business. You will be applying to roles in different domains so perhaps having a set template will help <U+2013> just change the story to relfect your interest in that particular industry. This article by Kunal Jain is an excellent resource for preparing an outstanding CV for data science roles. Source: Data Science Lab Let¡¯s take an example to understand why this is a mistake. Imagine you¡¯ve been given a dataset on house prices and you need to predict the value of future real estate. There are over 200 variables, including number of buildings, rooms, number of tenants, family size, size of the courtyard, whether faucets are available, etc. There¡¯s a good chance you might not be aware of what some variables mean. You can still build a model with a good accuracy, but you have no idea why a certain variable was dropped. As it turns out, that variable was a crucial element in a real-world scenario. It¡¯s a calamitous mistake. Having a solid knowledge of tools and libraries is excellent, but it will only take you so far. Combining that knowledge with the business problem posed by the domain is where a true data scientist steps in. You should be aware of at least the basic challenges in the industry you are interested in (or are applying to). There are plenty of options to explore here: Data visualization is such a wonderful facet of data science, yet a lot of aspiring data scientists prefer to skim over it and get to the model building stage. This approach might work out in competitions, but is bound to fail in a real job. Understanding the data you¡¯re given is the single most important thing you will do, and your model¡¯s results will reflect that. By spending time on getting to know the dataset and trying out different charts, you will gain a deeper knowledge of the challenge or problem you¡¯ve been tasked with solving. You¡¯d be surprised to know how much insight you can gain just by doing this! Pattern and trends emerge, stories are told and the best part? Visualizations are the best way to present your findings to the client. As a data scientist, you need to be inherently curious. It¡¯s one of the great things about data science <U+2013> the more curious you are, the more questions you¡¯ll ask. This leads to a much better understanding of the data you are given and also helps solve problems you didn¡¯t know existed in the first place! Practice! Next time you work on a dataset, spend more time on this step. You will be stunned at the amount of insight it will generate for you. Ask questions! Ask your manager, ask domain experts, search for solutions on the internet and if you don¡¯t find any, ask on social media. So many options! To help you get started, I have mentioned a few resources below which you should refer to: Source: MindMatters.co.in Structured thinking helps a data scientist in many ways: There are many more reasons why having a structured thinking mindset helps. As you can imagine, not having a structured thinking mindset is counter intuitive. Your work and approach to a problem will be haphazard, you will lose track of your own steps when faced with a complex problem, etc. When you go for a data science interview, you will inevitably be given a case study, guess estimate and puzzle problem(s). Because of the pressure filled atmosphere in an interview room and the time constraint, the interviewer looks at how well you structure your thoughts to arrive at a final result. In many cases, this can be a deal breaker or deal sealer for getting the job. You can acquire a structured thinking mindset through simple training and and a disciplined approach. I have listed a few articles below which will help you get started on this crucial aspect: I¡¯ve seen this one too many times. Because of the dilemma and the unique features each tool offers, people tend to attempt learning all the tools at once. This is a bad idea <U+2013> you will end up mastering none of them. Tools are a means to perform data science, they are not the end goal. Pick one tool and stick to it until you have mastery over it. If you¡¯ve already started learning R, then don¡¯t be tempted by Python (yet). Stick with R, learn it end-to-end and only then try to incorporate another tool into your skillset. You will learn more with this approach. Each tool has a great user community which you can tap into whenever you get stuck. Use our discussion forum to ask questions, search stuff online, and don¡¯t give up. The aim is to learn data science through the tool, not the tool through data science. If you are still undecided on which tool you should use, check out this wonderful article which lists down each tool¡¯s advantages and shortcomings (it also includes SAS in case you are interested in that). Source: The Brooks Group This one applies to all data scientists, not just freshers. We have a tendency to get distracted easily. We study for a period of time (say, a month), then we give it a break for the next 2 months. Trying to get back into the groove of things after that is a nightmare. Most of the earlier concepts are forgotten, notes are lost and it feels like we just wasted the last few months. I have personally experienced this as well. Due to various things we have going on, we find excuses and reasons not to get back to studying. But this is eventually our loss <U+2013> if data science was as easy as opening a text book and cramming everything, everyone would be a data scientist today. It demands consistent effort and learning, something which people don¡¯t appreciate until it¡¯s too late. Set goals for yourself. Map out a time table and stick it on your wall. Plan how and what you want to study and set deadlines for yourself. For example, when I wanted to learn about neural networks, I gave myself a couple of weeks and then tested what I¡¯d learned by competing in a hackathon. You have decided to become a data scientist so you should be ready to put in the hours. If you continually keep finding excuses not to study, this might not be the field for you. Source: Interview Skills Consulting This is a combination of a few things we¡¯ve seen in the above points. Aspiring data scientists tend to shy away from posting their analysis online in fear of being criticized. But if you don¡¯t receive feedback from the community, you will not grow as a data scientist. Data science is a field where discussions, ideas and brainstorming is of utter importance. You cannot sit in a silo and work <U+2013> you need to collaborate and understand other data scientists¡¯ perspective. Similarly, people don¡¯t take part in competitions because they feel they won¡¯t win. This is a wrong mindset! You participate in these competitions to learn, not to win. Winning is a bonus, learning is the goal. It¡¯s fairly straightforward <U+2013> start participating in discussions and competitions! It¡¯s okay to not come in the top 5%. If you learn a new technique out of the whole thing, you have won in your own right. Source: Jim Harvey Communications skills are one of the most under-rated and least talked about aspects a data scientist absolutely MUST possess. I am yet to come across a course that places a solid emphasis on this. You can learn all the latest techniques, master multiple tools and make the best graphs, but if you cannot explain your analysis to your client, you will fail as a data scientist. And not just clients, you will also be working with team members who are not well versed with data science <U+2013> IT, HR, finance, operations, etc.<U+00A0>You can be sure that the interviewer will be monitoring this aspect throughout. Assume you¡¯ve built a credit risk model using logistic regression. As a thought exercise, take a minute to think how you would explain to a non-technical person how you came to the final conclusion. If you used any technical words, you need to work on this ASAP! Most data scientists these days are coming from a computer science background so I understand this can be a daunting skill to acquire. But to become a successful data scientist and climb up the ladder, you don¡¯t have a choice but to polish this part of your personality. One of the things I find most helpful is explaining data science terms to a non-technical person. It helps me gauge how well I have articulated the problem. If you¡¯re working in a small to medium-sized company, find a person in the marketing or sales department and do this exercise with them. It will help you immensely in the long term. There are plenty of free resources available on the internet to get you started but remember, practice is key when it comes to soft skills. Ensure you start doing this TODAY. This is most definitely NOT an exhaustive list <U+2013> there are plenty of other mistakes aspiring data scientists tend to make. But these were the most common ones I have seen and my aim, as stated earlier, is to help others avoid it (as much as possible). I would love to hear your thoughts on these pointers, and also your personal experience with similar problems. Use the comments section below to let me know! Great observations Pranav and the way you have penned down the same is just amazing. Keep up the good work. Thank you pranav¡¦ Now this will help me a lot This article is too helpful for pioneers Great Thank you very much for this article Really awesome article, thanks! Excellent article. I will revisit again and again..! Really very helpful article.. Thanks pranav Nicely written article, very useful tips, especially no. 10 and 11, I have been the victim of both. Excellent. I see number 2 constantly! Good summary and nicely written! Thanks Very Helpful article. Thanks Pranav. It¡¯s really helpful. There are situation where I have encountered with these mistakes. Nicely articulated the knowledge , great work Great article. Thanks !! One more awesome guide. We students always do the first mistake. We have a lot of theoretical knowledge, but we don¡¯t practice. I found a website where we can practice with real time data. https://learn.analyttica.com/ Hi Sujit, You can also look at the datahack platform of analytics vidhya. You can take part in hackathons and practice problems to practice on real time data. Excellent article Pranav. Thanks a lot !!!! Excellent article Pranav !!!! Please, keep up this great work.","Keyword(freq): data(44), scientist(13), tool(7), competition(6), technique(6), model(5), problem(5), resource(5), thank(5), skill(4)"
