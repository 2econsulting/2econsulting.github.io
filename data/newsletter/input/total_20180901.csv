"","site","date","headline","url_address","text","keyword"
"1","mastery",2018-08-31,"How and When to Use ROC Curves and Precision-Recall Curves for Classification in Python","https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/","It can be more flexible to predict probabilities of an observation belonging to each class in a classification problem rather than predicting classes directly. This flexibility comes from the way that probabilities may be interpreted using different thresholds that allow the operator of the model to trade-off concerns in the errors made by the model, such as the number of false positives compared to the number of false negatives. This is required when using models where the cost of one error outweighs the cost of other types of errors. Two diagnostic tools that help in the interpretation of probabilistic forecast for binary (two-class) classification predictive modeling problems are ROC Curves and Precision-Recall curves. In this tutorial, you will discover ROC Curves, Precision-Recall Curves, and when to use each to interpret the prediction of probabilities for binary classification problems. After completing this tutorial, you will know: Let¡¯s get started. How and When to Use ROC Curves and Precision-Recall Curves for Classification in PythonPhoto by Giuseppe Milo, some rights reserved. This tutorial is divided into 6 parts; they are: In a classification problem, we may decide to predict the class values directly. Alternately, it can be more flexible to predict the probabilities for each class instead. The reason for this is to provide the capability to choose and even calibrate the threshold for how to interpret the predicted probabilities. For example, a default might be to use a threshold of 0.5, meaning that a probability in [0.0, 0.49] is a negative outcome (0) and a probability in [0.5, 1.0] is a positive outcome (1). This threshold can be adjusted to tune the behavior of the model for a specific problem. An example would be to reduce more of one or another type of error. When making a prediction for a binary or two-class classification problem, there are two types of errors that we could make. By predicting probabilities and calibrating a threshold, a balance of these two concerns can be chosen by the operator of the model. For example, in a smog prediction system, we may be far more concerned with having low false negatives than low false positives. A false negative would mean not warning about a smog day when in fact it is a high smog day, leading to health issues in the public that are unable to take precautions. A false positive means the public would take precautionary measures when they didn¡¯t need to. A common way to compare models that predict probabilities for two-class problems us to use a ROC curve. A useful tool when predicting the probability of a binary outcome is the Relative Operating Characteristic curve, or ROC curve. It is a plot of the false positive rate (x-axis) versus the true positive rate (y-axis) for a number of different candidate threshold values between 0.0 and 1.0. Put another way, it plots the false alarm rate versus the hit rate. The true positive rate is calculated as the number of true positives divided by the sum of the number of true positives and the number of false negatives. It describes how good the model is at predicting the positive class when the actual outcome is positive. The true positive rate is also referred to as sensitivity. The false positive rate is calculated as the number of false positives divided by the sum of the number of false positives and the number of true negatives. It is also called the false alarm rate as it summarizes how often a positive class is predicted when the actual outcome is negative. The false positive rate is also referred to as the inverted specificity where specificity is the total number of true negatives divided by the sum of the number of true negatives and false positives. Where: The ROC curve is a useful tool for a few reasons: The shape of the curve contains a lot of information, including what we might care about most for a problem, the expected false positive rate, and the false negative rate. To make this clear: If you are confused, remember, when we predict a binary outcome, it is either a correct prediction (true positive) or not (false positive). There is a tension between these options, the same with true negative and false positives. A skilful model will assign a higher probability to a randomly chosen real positive occurrence than a negative occurrence on average. This is what we mean when we say that the model has skill. Generally, skilful models are represented by curves that bow up to the top left of the plot. A model with no skill is represented at the point [0.5, 0.5]. A model with no skill at each threshold is represented by a diagonal line from the bottom left of the plot to the top right and has an AUC of 0.0. A model with perfect skill is represented at a point [0.0 ,1.0]. A model with perfect skill is represented by a line that travels from the bottom left of the plot to the top left and then across the top to the top right. An operator may plot the ROC curve for the final model and choose a threshold that gives a desirable balance between the false positives and false negatives. We can plot a ROC curve for a model in Python using the roc_curve() scikit-learn function. The function takes both the true outcomes (0,1) from the test set and the predicted probabilities for the 1 class. The function returns the false positive rates for each threshold, true positive rates for each threshold and thresholds. The AUC for the ROC can be calculated using the roc_auc_score() function. Like the roc_curve() function, the AUC function takes both the true outcomes (0,1) from the test set and the predicted probabilities for the 1 class. It returns the AUC score between 0.0 and 1.0 for no skill and perfect skill respectively. A complete example of calculating the ROC curve and AUC for a logistic regression model on a small test problem is listed below. Running the example prints the area under the ROC curve. A plot of the ROC curve for the model is also created showing that the model has skill. Line Plot of ROC Curve There are many ways to evaluate the skill of a prediction model. An approach in the related field of information retrieval (finding documents based on queries) measures precision and recall. These measures are also useful in applied machine learning for evaluating binary classification models. Precision is a ratio of the number of true positives divided by the sum of the true positives and false positives. It describes how good a model is at predicting the positive class. Precision is referred to as the positive predictive value. or Recall is calculated as the ratio of the number of true positives divided by the sum of the true positives and the false negatives. Recall is the same as sensitivity. or Reviewing both precision and recall is useful in cases where there is an imbalance in the observations between the two classes. Specifically, there are many examples of no event (class 0) and only a few examples of an event (class 1). The reason for this is that typically the large number of class 0 examples means we are less interested in the skill of the model at predicting class 0 correctly, e.g. high true negatives. Key to the calculation of precision and recall is that the calculations do not make use of the true negatives. It is only concerned with the correct prediction of the minority class, class 1. A precision-recall curve is a plot of the precision (y-axis) and the recall (x-axis) for different thresholds, much like the ROC curve. The no-skill line is defined by the total number of positive cases divide by the total number of positive and negative cases. For a dataset with an equal number of positive and negative cases, this is a straight line at 0.5. Points above this line show skill. A model with perfect skill is depicted as a point at [1.0,1.0]. A skilful model is represented by a curve that bows towards [1.0,1.0] above the flat line of no skill. There are also composite scores that attempt to summarize the precision and recall; three examples include: In terms of model selection, F1 summarizes model skill for a specific probability threshold, whereas average precision and area under curve summarize the skill of a model across thresholds, like ROC AUC. This makes precision-recall and a plot of precision vs. recall and summary measures useful tools for binary classification problems that have an imbalance in the observations for each class. Precision and recall can be calculated in scikit-learn via the precision_score() and recall_score() functions. The precision and recall can be calculated for thresholds using the precision_recall_curve() function that takes the true output values and the probabilities for the positive class as output and returns the precision, recall and threshold values. The F1 score can be calculated by calling the f1_score() function that takes the true class values and the predicted class values as arguments. The area under the precision-recall curve can be approximated by calling the auc() function and passing it the recall and precision values calculated for each threshold. Finally, the average precision can be calculated by calling the average_precision_score() function and passing it the true class values and the predicted class values. The complete example is listed below. Running the example first prints the F1, area under curve (AUC) and average precision (AP) scores. The precision-recall curve plot is then created showing the precision/recall for each threshold compared to a no skill model. Line Plot of Precision-Recall Curve Generally, the use of ROC curves and precision-recall curves are as follows: The reason for this recommendation is that ROC curves present an optimistic picture of the model on datasets with a class imbalance. However, ROC curves can present an overly optimistic view of an algorithm¡¯s performance if there is a large skew in the class distribution. [¡¦] Precision-Recall (PR) curves, often used in Information Retrieval , have been cited as an alternative to ROC curves for tasks with a large skew in the class distribution. <U+2014> The Relationship Between Precision-Recall and ROC Curves, 2006. Some go further and suggest that using a ROC curve with an imbalanced dataset might be deceptive and lead to incorrect interpretations of the model skill. [¡¦] the visual interpretability of ROC plots in the context of imbalanced datasets can be deceptive with respect to conclusions about the reliability of classification performance, owing to an intuitive but wrong interpretation of specificity. [Precision-recall curve] plots, on the other hand, can provide the viewer with an accurate prediction of future classification performance due to the fact that they evaluate the fraction of true positives among positive predictions <U+2014> The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets, 2015. The main reason for this optimistic picture is because of the use of true negatives in the False Positive Rate in the ROC Curve and the careful avoidance of this rate in the Precision-Recall curve. If the proportion of positive to negative instances changes in a test set, the ROC curves will not change. Metrics such as accuracy, precision, lift and F scores use values from both columns of the confusion matrix. As a class distribution changes these measures will change as well, even if the fundamental classifier performance does not. ROC graphs are based upon TP rate and FP rate, in which each dimension is a strict columnar ratio, so do not depend on class distributions. <U+2014> ROC Graphs: Notes and Practical Considerations for Data Mining Researchers, 2003. We can make this concrete with a short example. Below is the same ROC Curve example with a modified problem where there is a 10:1 ratio of class=0 to class=1 observations. Running the example suggests that the model has skill. Indeed, it has skill, but much of that skill is measured as making correct false negative predictions and there are a lot of false negative predictions to make. A plot of the ROC Curve confirms the AUC interpretation of a skilful model for most probability thresholds. Line Plot of ROC Curve Imbalanced Dataset We can also repeat the test of the same model on the same dataset and calculate a precision-recall curve and statistics instead. The complete example is listed below. Running the example first prints the F1, AUC and AP scores. The scores do not look encouraging, given skilful models are generally above 0.5. From the plot, we can see that after precision and recall crash fast. Line Plot of Precision-Recall Curve Imbalanced Dataset This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered ROC Curves, Precision-Recall Curves, and when to use each to interpret the prediction of probabilities for binary classification problems. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of scikit-learn code Discover how in my new Ebook:Machine Learning Mastery With Python Covers self-study tutorials and end-to-end projects like:Loading data, visualization, modeling, tuning, and much more¡¦ Skip the Academics. Just Results. Click to learn more. I don¡¯t think a diagonal straight line is the right baseline for P/R curve. The baseline ¡°dumb¡± classifier should be a straight line with precision=positive% You¡¯re right, thanks!  Fixed. Great tutorial. Thanks! How about the Mathews Correlation Coefficient ? I¡¯ve not used it, got some refs? Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning.","Keyword(freq): curve(17), positive(15), negative(11), probability(11), value(10), threshold(6), measure(5), model(5), problem(5), score(5)"
"2","mastery",2018-08-29,"How to Predict Room Occupancy Based on Environmental Factors","https://machinelearningmastery.com/how-to-predict-room-occupancy-based-on-environmental-factors/","Small computers, such as Arduino devices, can be used within buildings to record environmental variables from which simple and useful properties can be predicted. One example is predicting whether a room or rooms are occupied based on environmental measures such as temperature, humidity, and related measures. This is a type of common time series classification problem called room occupancy classification. In this tutorial, you will discover a standard multivariate time series classification problem for predicting room occupancy using the measurements of environmental variables. After completing this tutorial, you will know: Let¡¯s get started. This tutorial is divided into four parts; they are: A standard time series classification data set is the ¡°Occupancy Detection¡± problem available on the UCI Machine Learning repository. It is a binary classification problem which requires that an observation of environmental factors such as temperature and humidity be used to classify whether a room is occupied or unoccupied. It appears that the data was originally recorded by Zheng Yang, et al. at University of Southern California and described in their 2012 paper ¡°A Multi-Sensor Based Occupancy Estimation Model for Supporting Demand Driven HVAC Operations¡°. In the paper, they describe the use of two Arduino units to collect sensor data across multiple research labs over 20 days. The sensor data was collected for 20 consecutive days, starting from 00:00 AM, Sep. 12th to 00:00 AM, Oct. 1st. At a one-minute sampling rate, after excluding all corrupted data points due to wireless connection breaks, a total of 25,898 data points were collected in both labs. Their objective of the original project appeared to estimate the total occupancy of the rooms based on the sensor data. Arduino Black Widow Sensor NodeTaken from ¡°A Multi-Sensor Based Occupancy Estimation Model for Supporting Demand Driven HVAC Operations¡± The data was somehow retrieved, restructured, and made available on the UCI website. The number of observations and dates don¡¯t appear to match the original paper. It is quite possible that the source paper is unrelated or only partially related to the dataset. Data is provided with date-time information and six environmental measures taken each minute over multiple days, specifically: This dataset has been used in many simple modeling machine learning papers. For example, see the paper ¡°Visible Light Based Occupancy Inference Using Ensemble Learning,¡± 2018 for further references. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course The data is available in CSV format in three files, claimed to be a split of data for training, validation and testing. The three files are as follows: What is obvious at first is that the split in the data is not contiguous in time and that there are gaps. The test dataset is before the train and validation datasets in time. Perhaps this was an error in the naming convention of the files. We can also see that the data extends from Feb 2 to Feb 18, which spans 17 calendar days, not 20. Download the files from here and place them in your current working directory: Each file contains a header line, but includes a column for the row number that does not include an entry in the header line. In order to load the data files correctly, update the header line of each file to read as follows: From: To: Below is a sample of the first five lines of datatraining.txt file with the modification. We can then load the data files using the Pandas read_csv() function, as follows: Once loaded, we can create a plot for each of the six series, clearly showing the separation of the three datasets in time. The complete example is listed below. Running the example creates a plot with a different color for each dataset: We can see the small gap between the test and train sets and the larger gap between the train and validation sets. We can also see corresponding structures (peaks) in the series for each variable with the room occupancy. Line Plot Showing Time Series Plots for all variables and each dataset We can simplify the dataset by preserving the temporal consistency of the data and concatenating all three sets into a single dataset, dropping the ¡°no¡± column. This will allow ad hoc testing of simple direct framings of the problem (in the next section) that can be tested on a temporally consistent way with ad hoc train/test set sizes. Note: This simplification does not account for the temporal gaps in the data and algorithms that rely on a sequence of observations at prior time steps may require a different organization of the data. The example below loads the data, concatenates it into a temporally consistent dataset, and saves the results to a new file named ¡°combined.csv¡°. Running the example saves the concatenated dataset to the new file ¡®combined.csv¡®. The simplest formulation of the problem is to predict occupancy based on the environmental conditions at the current time. I refer to this as a direct model as it does not make use of the observations of the environmental measures at prior time steps. Technically, this is not sequence classification, it is just a straight classification problem where the observations are temporally ordered. This seems to be the standard formulation of the problem from my skim of the literature, and disappointingly, the papers seem to use the train/validation/test data as labeled on the UCI website. We will use the combined dataset described in the previous section and evaluate model skill by holding back the last 30% of the data as a test set. For example: Next, we can evaluate some models of the dataset, starting with a naive prediction model. A simple model for this formulation of the problem is to predict the most prominent class outcome. This is called the Zero Rule, or the naive prediction algorithm. We will evaluate predicting all 0 (unoccupied) and all 1 (occupied) for each example in the test set and evaluate the approach using the accuracy metric. Below is a function that will perform this naive prediction given a test set and a chosen outcome variable The complete example is listed below. Running the example prints the naive prediction and the related score. We can see that the baseline score is about 82% accuracy by predicting all 0, e.g. all no occupancy. For any model to be considered skilful on the problem, it must achieve a skill of 82% or better. A skim of the literature shows a range of sophisticated neural network models applied on this problem. To start with, let¡¯s try a simple logistic regression classification algorithm. The complete example is listed below. Running the example fits a logistic regression model on the training dataset and predicts the test dataset. The skill of the model is about 99% accurate, showing skill over the naive method. Normally, I would recommend centering and normalizing the data prior to modeling, but some trial and error demonstrated that a model on the unscaled data was more skilful. This is an impressive result at first glance. Although the test-setup is different to that presented in the research literature, the reported skill of a very simple model outperforms more sophisticated neural network models. A closer look at the time series plot shows a clear relationship between the times when the rooms are occupied and peaks in the environmental measures. This makes sense and explains why this problem is in fact so easy to model. We can further simplify the model by testing a simple logistic regression model on each environment measure in isolation. The idea is that we don¡¯t need all of the data to predict occupancy; that perhaps just one of the measures is sufficient. This is the simplest type of feature selection where a model is created and evaluated with each feature in isolation. More advanced methods may consider each subgroup of features. The complete example testing a logistic model with each of the five input features in isolation is listed below. Running the example prints the feature index, name, and the skill of a logistic model trained on that feature and evaluated on the test set. We can see that only the ¡°Light¡± variable is required in order to achieve 99% accuracy on this dataset. It is very likely that the lab rooms in which the environmental variables were recorded had a light sensor that turned internal lights on when the room was occupied. Alternately, perhaps the light is recorded during the daylight hours (e.g. sunshine through windows), and the rooms are occupied on each day, or perhaps each week day. At the very least, the results of this tutorial ask some hard questions about any research papers that use this dataset, as clearly it is not a challenging prediction problem. This data may still be interesting for further investigation. Some ideas include: I tried each of these models briefly without exciting results. If you explore any of these extensions or find some examples online, let me know in the comments below. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered a standard multivariate time series classification problem for predicting room occupancy using the measurements of environmental variables. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hi! Awesome blog!. Thanks! One thing, why don¡¯t you use plt.tight_layout() at the end of the script in order to have non-overlapping graphs? Great suggestion, thanks! Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning.","Keyword(freq): file(6), measure(6), room(5), variable(5), model(4), observation(4), result(4), question(3), set(3), uci(3)"
"3","mastery",2018-08-27,"How to Predict Whether a Persons Eyes are Open or Closed Using Brain Waves","https://machinelearningmastery.com/how-to-predict-whether-eyes-are-open-or-closed-using-brain-waves/","Evaluating machine learning models on time series forecasting problems is challenging. It is easy to make a small error in the framing of a problem or in the evaluation of models that give impressive results but result in an invalid finding. An interesting time series classification problem is predicting whether a subject¡¯s eyes are open or closed based only on their brain wave data (EEG). In this tutorial, you will discover the problem of predicting whether eyes are open or closed based on brain waves and a common methodological trap when evaluating time series forecasting models. Working through this tutorial, you will have an idea of how to avoid common traps when evaluating machine learning algorithms on time series forecast problems. These are traps that catch both beginners, expert practitioners, and academics alike. After completing this tutorial, you will know: Let¡¯s get started. This tutorial is divided into seven parts; they are: In this post, we are going to take a closer look at a problem that involves predicting whether the subjects eyes are open or closed based on brain wave data. The problem was described and data collected by Oliver Rosler and David Suendermann for their 2013 paper titled ¡°A First Step towards Eye State Prediction Using EEG¡°. I saw this dataset and I had to know more. Specifically, an electroencephalography (EEG) recording was made of a single person for 117 seconds (just under two minutes) while the subject opened and closed their eyes, which was recorded via a video camera. The open/closed state was then recorded against each time step in the EEG trace manually. The EEG was recorded using a Emotiv EEG Neuroheadset, resulting in 14 traces. Cartoon of where EEG sensors were located on the subjectTaken from ¡°A First Step towards Eye State Prediction Using EEG¡±, 2013. The output variable is binary, meaning that this is a two-class classification problem. A total of 14,980 observations (rows) were made over the 117 seconds, meaning that there were about 128 observations per second. The corpus consists of 14,977 instances with 15 attributes each (14 attributes representing the values of the electrodes and the eye state). The instances are stored in the corpus in chronological order to be able to analyze temporal dependencies. 8,255 (55.12%) instances of the corpus correspond to the eye open and 6,722 (44.88%) instances to the eye closed state. There were also some EEG observations that have a much larger than expected amplitude. These are likely outliers and can be identified and removed using a simple statistical method such as removing rows that have an observation 3-to-4 standard deviations from the mean. The simplest framing of the problem is to predict the eye-state (open/closed) given the EEG trace at the current time step. More advanced framings of the problem may seek to model the multivariate time series of each EEG trace in order to predict the current eye state. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course The dataset can be downloaded for free from the UCI Machine Learning repository: The raw data is in ARFF format (used in Weka), but can be converted to CSV by deleting the ARFF header. Below is a sample of the first five lines of the data with the ARFF header removed. We can load the data as a DataFrame and plot the time series for each EEG trace and the output variable (open/closed state). The complete code example is listed below. The example assumes that you have a copy of the dataset in CSV format with the filename ¡®EEG_Eye_State.csv¡® in the same directory as the code. Running the example creates a line plot for each EEG trace and the output variable. We can see the outliers washing out the data in each trace. We can also see the open/closed state of the eyes over time with 0/1 respectively. Line Plot for each EEG trace and the output variable It is useful to remove the outliers to better understand the relationship between the EEG traces and the open/closed state of the eyes. The example below removes all rows that have an EEG observation that is four standard deviations or more from the mean. The dataset is saved to a new file called ¡®EEG_Eye_State_no_outliers.csv¡®. It is a quick and dirty implementation of outlier detection and removal, but gets the job done. I¡¯m sure you could engineer a more efficient implementation. Running the example summarizes the rows deleted as each column in the EEG data is processed for outliers above and below the mean. We can now visualize the data without outliers by loading the new ¡®EEG_Eye_State_no_outliers.csv¡® file. Running the example creates a better plot, clearly showing little positive peaks when eyes are closed (1) and negative peaks when eyes are open (0). Line Plot for each EEG trace and the output variable without outliers The simplest predictive model is to predict the eye open/closed state based on the current EEG observation, ignoring the trace information. Intuitively, one would not expect this to be effective, nevertheless, it was the approach used in Rosler and Suendermann¡¯s 2013 paper. Specifically, they evaluated a large suite of classification algorithms in the Weka software using 10-fold cross-validation of this framing of the problem. They achieved better than 90% accuracy with multiple methods, including instance based methods such as k-nearest neighbors and KStar. However, instance-based learners such as IB1 and KStar outperformed decision trees yet again substantially. The latter achieved the clearly best performance with a classification error rate of merely 3.2%. <U+2014> A First Step towards Eye State Prediction Using EEG, 2013. A similar methodology and finding was used with the same and similar datasets in a number of other papers. I was surprised when I read this and so reproduced the result. The complete example is listed below with a k=3 KNN. Running the example prints the score for each fold of the cross validation and the mean score of 97% averaged across all 10 folds. Very impressive! But something felt wrong. I was interested to see how models that took into account the clear peaks in the data at each transition from open-to-closed and closed-to-open performed. Every model I tried using my own test harness that respected the temporal ordering of the data performed much worse. Why? Hint: think about the chosen model evaluation strategy and the type of algorithm that performed the best. Disclaimer: I am not calling out the authors of the paper or related papers. I don¡¯t care. In my experience, most published papers cannot be reproduced or have major methodological flaws (including a lot of the stuff that I have written). I¡¯m only interested in learning and teaching. There is a methodological flaw in the way that time series models are evaluated. I coach against this flaw, but after reading the paper and reproducing the result, it still tripped me up. I hope by working through this example that it will help not trip you up on your own forecast problems. The methodological flaw in the evaluation of the models is the use of k-fold cross-validation. Specifically, the evaluation of the models in a way that does not respect the temporal ordering of the observations. Key to this problem is the finding of instance-based methods, such as k-nearest neighbors, as being skillful on the problem. KNN will seek out the k most similar rows in the dataset and calculate the mode of the output state as the prediction. By not respecting the temporal order of instances when evaluating models, it allows the models to use information from the future in making the prediction. This is pronounced specifically in the KNN algorithm. Because of the high frequency of observations (128 per second), the most similar rows will be those adjacent in time to the instance being predicted, both in the past and in the future. We can make this clearer with some small experiments. The first test we can do is to evaluate the skill of a KNN model with a train/test split both when the dataset is shuffled, and when it is not. In the case when the data is shuffled prior to the split, we expect the result to be similar to the cross-validation result in the previous section, specifically if the test set is 10% of the dataset. If the theory about the importance of temporal ordering and instance-based methods using adjacent examples in the future is true, we would expect the test where the dataset is not shuffled prior to the split to be worse. First, the example below splits the dataset into train/test split with 90%/10% of the data respectively. The dataset is shuffled prior to the split. Running the example, we can see that indeed, the skill matches what we see in the cross-validation example, or close to it, at 96% accuracy. Next, we repeat the experiment without shuffling the dataset prior to the split. This means that the training data are the first 90% of the data respecting the temporal ordering of the observations, and the test dataset is the last 10%, or about 1,400 observations of the data. Running the example shows model skill that is much worse at 52%. This is a good start, but not definitive. It is possible that the last 10% of the dataset is hard to predict, given the very short open/close intervals we can see on the plot of the outcome variable. We can repeat the experiment and use the first 10% of the data in time for test and the last 90% for train. We can do this by reversing the order of the rows prior to splitting the data using the flip() function. Running the experiment produces similar results at about 52% accuracy. This gives more evidence that it is not the specific contiguous block of observations that results in the poor model skill. It looks like immediately adjacent observations are required to make good predictions. It is possible that the model requires the adjacent observations in the past (but not the future) in order to make skillful predictions. This sounds reasonable at first, but also has a problem. Nevertheless we can achieve this using walk-forward validation over the test set. This is where the model is permitted to use all observations prior to the time step being predicted as we validate a new model at each new time step in the test dataset. For more on walk-forward validation, see the post: The example below evaluates the skill of KNN using walk-forward validation over the last 10% of the dataset (about 10 seconds), respecting temporal ordering. Running the example gives an impressive model skill at about 95% accuracy. We can push this test further and only make the previous 10 observations available to the model when making a prediction. The complete example is listed below. Running the example results in a further improved model skill of nearly 99% accuracy. I would expect that the only errors being made are those at the inflection points in the EEG series when the trace transitions from open-to-closed or closed-to-open, the actual hard part of the problem. This aspect requires further investigation. Indeed, we have confirmed that the model requires adjacent observations and their outcome in order to make a prediction, and that it can do very well with only adjacent observations in the past, not the future. This is interesting. But this finding is not useful in practice. If this model was deployed, it would require the model to know the eye open/closed state in the very recent past, such as the previous 128th of a second. This will not be available. The whole idea of a model for predicting eye state based on brain waves is to have it operate without such confirmation. Let¡¯s review what we have learned so far: 1. The model evaluation methodology must take the temporal ordering of observations into account. This means that it is methodologically invalid to use k-fold cross-validation that does not stratify by time (e.g. shuffles or uses a random selection of rows). This also means that it is methodologically invalid to use a train/test split that shuffles the data prior to splitting. We saw this in the evaluation of the high skill of the model with k-fold cross-validation and shuffled train/test split compared to the low skill of the model when directly adjacent observations in time were not available at prediction time. 2. The model evaluation methodology must make sense for the use of the final model. This means that even if you use a methodology that respects the temporal ordering of the observations, the model should only have information available that it would have if the model were being used in practice. We saw this in the high skill of the model under a walk-forward validation methodology that respected the order of observations, but made information available, such as eye-state, that would not be available if the model were being used in practice. The key is to start with a framing of the problem based in the use of the final model and work backwards to the data that would be available and a methodology for evaluating the model in its framing that only operates under information that would be available in that framing. This applies doubly when you are trying to understand other people¡¯s work. Going Forward Hopefully, this helps, both when you are evaluating your own forecast models and when you are evaluating the models of others. So, how would you work this problem if presented with the raw data? I think the keys to this problem are the positive/negative peaks that are obvious in the EEG data at the times when there is a transition from eyes open-to-closed or closed-to-open. I would expect an effective model would exploit this feature, perhaps using half a second or similar of prior EEG observations. This might even be possible with a single trace, rather than 15, and a simple peak detection method from signal processing, rather than a machine learning method. Let me know if you have a go at this; I¡¯d love to see what you discover. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered the problem of predicting whether eyes are open or closed based on brain waves and a common methodological trap when evaluating time series forecasting models. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hi Jason, Thank you for this nice case study! I hope you¡¯ll be tackling more nuanced cases in the future. Best,
Elie Thanks. I may be looking at this wrong, but in the example where you have above 99% accuracy and looking at past eye state, isn¡¯t your model just learning that the current state is most likely the past state as the frequency of open and closing the eye is very slow. Yes, I believe so. It was more about the dangers of not thinking through methodology. Thanks!  Great article! Thanks Tony. Fantastic article about machine learning and evaluation pitfalls!!!
Please,give us more analysis like this,where we can learn from mistakes. Thanks. Happy it helped. Did you try using time series forecasting on stock market?hope to learn more on the financial topic! I try to avoid finance applications, there¡¯s too much emotion around the topic. In your data cleaning, you should probably use standard deviations from the median, since deviations from mean will inherently be biased because outliers will shift the mean. It probably won¡¯t make much of a difference, but I used this code: 
filtered = data[data.apply(lambda x: np.abs(x - x.median()) / x.std() < 4).all(axis=1)]
I used a 2 layer stateful LSTM and got 99%, probably because of overfitting (it predicts a constant output). Someone better than me at LSTMs could probably troubleshoot my approach. It would be good if there was more data. My code is here: https://gist.github.com/JonnoFTW/f94f8d97e57f6796da83b834ce66aa45 Very nice Jonathan! Amazing work. Thanks. Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning.","Keyword(freq): observation(19), model(12), row(8), outlier(7), result(5), thank(5), deviation(4), method(4), peak(4), problem(3)"
"4","vidhya",2018-08-30,"Build High Performance Time Series Models using Auto ARIMA in Python and R","https://www.analyticsvidhya.com/blog/2018/08/auto-arima-time-series-modeling-python-r/","

MEGA-LAUNCH Offer on Computer Vision Using Deep Learning | Use COUPON CODE: CVLAUNCH60 for 60% Discount |
Course Starts 4th September | 
Buy Now 

 Picture this <U+2013> You¡¯ve been tasked with forecasting the price of the next iPhone and have been provided with historical data. This includes features like quarterly sales, month-on-month expenditure, and a whole host of things that come with Apple¡¯s balance sheet. As a data scientist, which kind of problem would you classify this as? Time series modeling, of course. From predicting the sales of a product to estimating the electricity usage of households, time series forecasting is one of the core skills any data scientist is expected to know, if not master. There are a plethora of different techniques out there which you can use, and we will be covering one of the most effective ones, called Auto ARIMA, in this article. We will first understand the concept of ARIMA which will lead us to our main topic <U+2013> Auto ARIMA. To solidify our concepts, we will take up a dataset and implement it in both Python and R. If you are familiar with time series and it¡¯s techniques (like moving average, exponential smoothing, and ARIMA), <U+00A0>you can skip directly to section 4. For beginners, start from the below section which is a brief introduction to time series and various forecasting techniques. Before we learn about the techniques to work on time series data, we must first understand what a time series actually is and how is it different from any other kind of data. Here is the formal definition of time series <U+2013> It is a series of data points measured at consistent time intervals. This simply means that particular values are recorded at a constant interval which may be hourly, daily, weekly, every 10 days, and so on. What makes time series different is that each data point in the series is dependent on the previous data points. Let us understand the difference more clearly by taking a couple of examples. Example 1: Suppose you have a dataset of people who have taken a loan from a particular company (as shown in the table below). Do you think each row will be related to the previous rows? Certainly not! The loan taken by a person will be based on his financial conditions and needs (there could be other factors such as the family size etc., but for simplicity we are considering only income and loan type) . Also, the data was not collected at any specific time interval. It depends on when the company received a request for the loan. Example 2: Let¡¯s take another example. Suppose you have a dataset that contains the level of CO2 in the air per day (screenshot below). Will you be able to predict the approximate amount of CO2 for the next day by looking at the values from the past few days? Well, of course. If you observe, the data has been recorded on a daily basis, that is, the time interval is constant (24 hours). You must have got an intuition about this by now <U+2013> the first case is a simple regression problem and the second is a time series problem. Although the time series puzzle here can also be solved using linear regression, but that isn¡¯t really the best approach as it neglects the relation of the values with all the relative past values. Let¡¯s now look at some of the common techniques used for solving time series problems. There are a number of methods for time series forecasting and we will briefly cover them in this section. The detailed explanation and python codes for all the below mentioned techniques can be found in this article: 7 techniques for time series forecasting (with python codes). In this section we will do a quick introduction to ARIMA which will be helpful in understanding Auto Arima. A detailed explanation of Arima, parameters (p,q,d), plots (ACF PACF) and implementation is included in this article : Complete tutorial to Time Series. ARIMA is a very popular statistical method for time series forecasting. ARIMA stands for Auto-Regressive Integrated Moving Averages. ARIMA models work on the following assumptions <U+2013> ARIMA has three components <U+2013> AR (autoregressive term), I (differencing term) and MA (moving average term). Let us understand each of these components <U+2013> The general steps to implement an ARIMA model are <U+2013> Although ARIMA is a very powerful model for forecasting time series data, the data preparation and parameter tuning processes end up being really time consuming. Before implementing ARIMA, you need to make the series stationary, and determine the values of p and q using the plots we discussed above. Auto ARIMA makes this task really simple for us as it eliminates steps 3 to 6 we saw in the previous section. Below are the steps you should follow for implementing auto ARIMA: We completely bypassed the selection of p and q feature as you can see. What a relief! In the next section, we will implement auto ARIMA using a toy dataset. We will be using the International-Air-Passenger dataset. This dataset contains monthly total of number of passengers (in thousands). It has two columns <U+2013> month and count of passengers. You can download the dataset from this link. Below is the R Code for the same problem: In the above code, we simply used the .fit() command to fit the model without having to select the combination of p, q, d. But how did the model figure out the best combination of these parameters? Auto ARIMA takes into account the AIC and BIC values generated (as you can see in the code) to determine the best combination of parameters. AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) values are estimators to compare models. The lower these values, the better is the model. Check out these links if you are interested in the maths behind AIC and BIC. I have found auto ARIMA to be the simplest technique for performing time series forecasting. Knowing a shortcut is good but being familiar with the math behind it is also important. In this article I have skimmed through the details of how ARIMA works but do make sure that you go through the links provided in the article. For your easy reference, here are the links again: I would suggest practicing what we have learned here on this practice problem: Time Series Practice Problem. You can also take our training course created on the same practice problem,<U+00A0>Time series forecasting, to provide you a head start. Good luck, and feel free to provide your feedback and ask questions in the comments section below. Hi Aishwarya,
Thanks for another nice article.
There are few things which I would like to clarify. could you please through some light on below points? While introducing ARIMA you mentioned that q is calculated by PACF and p is calculated by ACF, I believe its vice versa. Please confirm
what are the selected p,d,q values after the auto arima model decided the best params for prediction?
Forecasted line seems to be just an average of the validation set which completely miss the seasonality and the trend. Is it a good forecast?
which smoothing technique(MA, WA, Holts or Holts winter) is used while applying ARiMA on the training dataset?
How to interpret the score on the basis of AIC and BIC? Please elaborate
Do we need to do any preprocessing before feeding training set to the auto arima tool?
How can we improve the rmse score? Regards
Nitin Hi Nitin, Thank you for the feedback. You are right about the calculation of p and q. I will update the same in the article.  Secondly, when you use model.fit(), it will print the selected p,q,d.  Regarding the forecast, if you change the parameters of auto arima and put Seasonality = True, Auto arima will take into account the seasonality as well. You will certainly give a better result. Same is the answer to your last question about rmse score, set a range of p,q and P,Q (after setting seasonality=True) and you will see an improvement in the rmse score. This is actually a great point and if time permits, I will include parameter tuning of auto arima in this article.  Lastly, AIC and BIC values are used to compare models; the lower the AIC/BIC, better is the model. Thanks for this article¡¦ Glad you liked it! sounds great, but bit unclear about cons n pros, is this always good to prefer than simply ARIMA? Hi Ayush,","Keyword(freq): value(10), technique(7), parameter(4), link(3), model(3), point(3), step(3), code(2), component(2), holt(2)"
"5","vidhya",2018-08-30,"A Simple Introduction to Facial Recognition (with Python codes)","https://www.analyticsvidhya.com/blog/2018/08/a-simple-introduction-to-facial-recognition-with-python-codes/","

MEGA-LAUNCH Offer on Computer Vision Using Deep Learning | Use COUPON CODE: CVLAUNCH60 for 60% Discount |
Course Starts 4th September | 
Buy Now 

 Did you know that every time you upload a photo to Facebook, the platform uses facial recognition algorithms to identify the people in that image? Or that certain governments around the world use face recognition technology to identify and catch criminals? I don¡¯t need to tell you that you can now unlock smartphones with your face! The applications of this sub-domain of computer vision are vast and businesses around the world are already reaping the benefits. The usage of face recognition models is only going to increase in the next few years so why not teach yourself how to build one from scratch? In this article, we are going to do just that. We will first understand the inner workings of face recognition, and then take a simple case study and implement it in Python. By the end of the article you will have built your very first facial recognition model! In order to understand how Face Recognition works, let us first get an idea of the concept of a feature vector. Every Machine Learning algorithm takes a dataset as input and learns from this data. The algorithm goes through the data and identifies patterns in the data. For instance, suppose we wish to identify whose face is present in a given image, there are multiple things we can look at as a pattern: Clearly, there is a pattern here <U+2013> different faces have different dimensions like the ones above. Similar faces have similar dimensions. The challenging part is to convert a particular face into numbers <U+2013> Machine Learning algorithms only understand numbers. This numerical representation of a ¡°face¡± (or an element in the training set) is termed as a feature vector. A feature vector comprises of various numbers in a specific order. As a simple example, we can map a ¡°face¡± into a feature vector which can comprise various features like: Essentially, given an image, we can map out various features and convert it into a feature vector like: So, our image is now a vector that could be represented as (23.1, 15.8, 255, 224, 189, 5.2, 4.4). Of course there could be countless other features that could be derived from the image (for instance, hair color, facial hair, spectacles, etc). However, for the example, let us consider just these 5 simple features. Now, once we have encoded each image into a feature vector, the problem becomes much simpler. Clearly, when we have 2 faces (images) that represent the same person, the feature vectors derived will be quite similar. Put it the other way, the ¡°distance¡± between the 2 feature vectors will be quite small. Machine Learning can help us here with 2 things: Now that we have a basic understanding of how Face Recognition works, let us build our own Face Recognition algorithm using some of the well-known Python libraries. We are given a bunch of faces <U+2013> possibly of celebrities like Mark Zuckerberg, Warren Buffett, Bill Gates, Shah Rukh Khan, etc. Call this bunch of faces as our ¡°corpus¡±. Now, we are given image of yet another celebrity (¡°new celebrity¡±). The task is simple <U+2013> identify if this ¡°new celebrity¡± is among those present in the ¡°corpus¡±. Here are some of the images in the corpus: As you can see, we have celebrities like Barack Obama, Bill Gates, Jeff Bezos, Mark Zuckerberg, Ray Dalio and Shah Rukh Khan. Now, here is the ¡°new celebrity¡±: Note: all of the above images have been taken from Google images. It is obvious that this is Shah Rukh Khan. However, for a computer this is a challenging task. The challenge is because of the fact that for us humans, it is easy to combine so many features of the images to see which one is which celebrity. However, for a computer, it isn¡¯t straightforward to learn how to recognize these faces. There is an amazingly simple Python library that encapsulates all of what we learn above <U+2013> creating feature vectors out of faces and knowing how to differentiate across faces. This Python library is called as face_recognition and deep within, it employs dlib <U+2013> a modern C++ toolkit that contains several machine learning algorithms that help in writing sophisticated C++ based applications. face_recognition library in Python can perform a large number of tasks: Here, we will talk about the 3rd use case <U+2013> identify faces in images. You can find the source code of face_recognition library here on Github: https://github.com/ageitgey/face_recognition. In fact, there is also a tutorial on how to install face_recognition library: https://github.com/ageitgey/face_recognition#installation-options. Before you install face_recognition, you need to install dlib as well. You can find the instructions to install dlib over here: https://gist.github.com/ageitgey/629d75c1baac34dfa5ca2a1928a7aeaf. This section contains the code for a building a straightforward face recognition system using the<U+00A0>face_recognition library. This is the implementation part, we will go through the code to understand it in more detail in the next section. The folder structure is as follows: facialrecognition: Our root directory, facialrecognition contains: When you create the folder structure as above and run the above code, here is what you get as the output: Clearly, the ¡°new celebrity¡± is Shah Rukh Khan and our face recognition system is able to detect it! Now, let us go through the code to understand how it works: These are simply the imports. We will be using the built-in os library to read all the images in our corpus and we will use face_recognition for the purpose of writing the algorithm. This simple code helps us identify the path of all of the images in the corpus. Once this line is executed, we will have: Now, the code below loads the new celebrity¡¯s image: To make sure that the algorithms are able to interpret the image, we convert the image to a feature vector: The rest of the code now is fairly easy: Here, we are: The output as shown above clearly suggests that this simple face recognition algorithm works amazingly well. Let us try replacing my_image with another image: When you run the algorithm again, you will see the following output: Clearly, the system did not identify Jack Ma as any of the above celebrities. This indicates that our algorithm is quite good in both: Face Recognition is a well researched problem and is widely used in both industry and in academia. As an example, a criminal in China was caught because a Face Recognition system in a mall detected his face and raised an alarm. Clearly, Face Recognition can be used to mitigate crime. There are many other interesting use cases of Face Recognition: To summarize, Face Recognition is an interesting problem with lots of powerful use cases which can significantly help society across various dimensions. While there will always be an ethical risk attached to commercialzing such techniques, that is a debate we will shelve for another time. I hope you found this article useful. Please provide you feedback and suggestions in the comments section below!","Keyword(freq): face(8), image(8), feature(5), algorithm(4), celebrity(4), dimension(3), http(3), number(3), vector(3), application(2)"
"6","vidhya",2018-08-27,"The Ultimate Guide to 12 Dimensionality Reduction Techniques (with Python codes)","https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/","

MEGA-LAUNCH Offer on Computer Vision Using Deep Learning | Use COUPON CODE: CVLAUNCH60 for 60% Discount |
Course Starts 4th September | 
Buy Now 

 Have you ever worked on a dataset with more than a thousand features? How about over 50,000 features? I have, and let me tell you it¡¯s a very challenging task, especially if you don¡¯t know where to start! Having a high number of variables is both a boon and a curse. It¡¯s great that we have loads of data for analysis, but it is challenging due to size. It¡¯s not feasible to analyze each and every variable at a microscopic level. It might take us days or months to perform any meaningful analysis and we¡¯ll lose a ton of time and money for our business! Not to mention the amount of computational power this will take. We need a better way to deal with high dimensional data so that we can quickly extract patterns and insights from it. So how do we approach such a dataset? Using dimensionality reduction techniques, of course. You can use this concept to reduce the number of features in your dataset without having to lose much information and keep (or improve) the model¡¯s performance. It¡¯s a really powerful way to deal with huge datasets, as you¡¯ll see in this article. This is a comprehensive guide to various dimensionality reduction techniques that can be used in practical scenarios. We will first understand what this concept is and why we should use it, before diving into the 12 different techniques I have covered. Each technique has it¡¯s own implementation in Python to get you well acquainted with it. We are generating a tremendous amount of data daily. In fact, 90% of the data in the world has been generated in the last 3-4 years! The numbers are truly mind boggling. Below are just some of the examples of the kind of data being collected: As data generation and collection keeps increasing, visualizing it and drawing inferences becomes more and more challenging. One of the most common ways of doing visualization is through charts. Suppose we have 2 variables, Age and Height. We can use a scatter or line plot between Age and Height and visualize their relationship easily: Now consider a case in which we have, say 100 variables (p=100). In this case, we can have 100(100-1)/2 = 5000 different plots. It does not make much sense to visualize each of them separately, right? In such cases where we have a large number of variables, it is better to select a subset of these variables (p<<100) which captures as much information as the original set of variables. Let us understand this with a simple example. Consider the below image: Here we have weights of similar objects in Kg (X1) and Pound (X2). If we use both of these variables, they will convey similar information. So, it would make sense to use only one variable. We can convert the data from 2D (X1 and X2) to 1D (Y1) as shown below: Similarly, we can reduce p dimensions of the data into a subset of k dimensions (k<<n). This is called dimensionality reduction. Here are some of the benefits of applying dimensionality reduction to a dataset: Time to dive into the crux of this article <U+2013> the various dimensionality reduction techniques! We will be using the dataset from AV¡¯s<U+00A0>Practice Problem: Big Mart Sales III<U+00A0>(register on this link and download the dataset from the data section). Dimensionality reduction can be done in two different ways: We will now look at various dimensionality reduction techniques and how to implement each of them in Python. Suppose you¡¯re given a dataset. What would be your first step? You would naturally want to explore the data first before building model. While exploring the data, you find that your dataset has some missing values. Now what? You will try to find out the reason for these missing values and then impute them or drop the variables entirely which have missing values (using appropriate methods). What if we have too many missing values (say more than 50%)? Should we impute the missing values or drop the variable? I would prefer to drop the variable since it will not have much information. However, this isn¡¯t set in stone. We can set a threshold value and if the percentage of missing values in any variable is more than that threshold, we will drop the variable. Let¡¯s implement this approach in Python. First, let¡¯s load the data: Note: The path of the file should be added while reading the data. Now, we will check the percentage of missing values in each variable. We can use .isnull().sum() to calculate this. As you can see in the above table, there aren¡¯t too many missing values (just 2 variables have them actually). We can impute the values using appropriate methods, or we can set a threshold of, say 20%, and remove the variable having more than 20% missing values. Let¡¯s look at how this can be done in Python: So the variables to be used are stored in ¡°variable¡±, which contains only those features where the missing values are less than 20%. Consider a variable in our dataset where all the observations have the same value, say 1. If we use this variable, do you think it can improve the model we will build? The answer is no, because this variable will have zero variance. So, we need to calculate the variance of each variable we are given. Then drop the variables having low variance as compared to other variables in our dataset. The reason for doing this, as I mentioned above, is that variables with a low variance will not affect the target variable. Let¡¯s first impute the missing values in the Item_Weight column using the median value of the known Item_Weight observations. For the<U+00A0>Outlet_Size column, we will use the mode of the known Outlet_Size values to impute the missing values: Let¡¯s check whether all the missing values have been filled: Voila! We are all set. Now let¡¯s calculate the variance of all the numerical variables. As the above output shows, the variance of Item_Visibility is very less as compared to the other variables. We can safely drop this column. This is how we apply low variance filter. Let¡¯s implement this in Python: The above code gives us the list of variables that have a variance greater than 10. High correlation between two variables means they have similar trends and are likely to carry similar information. This can bring down the performance of some models drastically (linear and logistic regression models, for instance). We can calculate the correlation between independent numerical variables that are numerical in nature. If the correlation coefficient crosses a certain threshold value, we can drop one of the variables (dropping a variable is highly subjective and should always be done keeping the domain in mind). As a general guideline, we should keep those variables which show a decent or high correlation with the target variable. Let¡¯s perform the correlation calculation in Python. We will drop the dependent variable (Item_Outlet_Sales) first and save the remaining variables in a new dataframe (df). Wonderful, we don¡¯t have any variables with a high correlation in our dataset. Generally, if the correlation between a pair of variables is greater than 0.5-0.6, we should seriously consider dropping one of those variables. Random Forest is one of the most widely used algorithms for feature selection. It comes packaged with in-built feature importance so you don¡¯t need to program that separately. This helps us select a smaller subset of features. We need to convert the data into numeric form by applying one hot encoding, as Random Forest (Scikit-Learn Implementation) takes only numeric inputs. Let¡¯s also drop the ID variables (Item_Identifier and Outlet_Identifier) as these are just unique numbers and hold no significant importance for us currently. After fitting the model, plot the feature importance graph: Based on the above graph, we can hand pick the top-most features to reduce the dimensionality in our dataset. Alernatively, we can use the SelectFromModel of sklearn to do so. It selects the features based on the importance of their weights. Follow the below steps to understand and use the ¡®Backward Feature Elimination¡¯ technique: This method can be used when building Linear Regression or Logistic Regression models. Let¡¯s look at it¡¯s Python implementation: We need to specify the algorithm and number of features to select, and we get back the list of variables obtained from backward feature elimination. We can also check the ranking of the variables using the ¡°rfe.ranking_¡± command. This is the opposite process of the Backward Feature Elimination we saw above. Instead of eliminating features, we try to find the best features which improve the performance of the model. This technique works as follows: Let¡¯s implement it in Python: This returns an array containing the F-values of the variables and the p-values corresponding to each F value. Refer to<U+00A0>this link to learn more about F-values. For our purpose, we will select the variables having F-value greater than 10: This gives us the top most variables based on the forward feature selection algorithm. NOTE : Both Backward Feature Elimination and Forward Feature Selection are time consuming and computationally expensive.They are practically only used on datasets that have a small number of input variables. The techniques we have seen so far are generally used when we do not have a very large number of variables in our dataset. These are more or less feature selection techniques. In the upcoming sections, we will be working with the Fashion MNIST dataset, which consists of images belonging to different types of apparel, e.g. T-shirt, trousers, bag, etc. The dataset can be downloaded from the ¡°IDENTIFY THE APPAREL¡± practice problem. The dataset has a total of 70,000 images, out of which 60,000 are in the training set and the remaining 10,000 are test images. For the scope of this article, we will be working only on the training images. The train file is in a zip format. Once you extract the zip file, you will get a .csv file and a train folder which includes these 60,000 images. The corresponding label of each image can be found in the ¡®train.csv¡¯ file. Suppose we have two variables: Income and Education. These variables will potentially have a high correlation as people with a higher education level tend to have significantly higher income, and vice versa. In the Factor Analysis technique, variables are grouped by their correlations, i.e., all variables in a particular group will have a high correlation among themselves, but a low correlation with variables of other group(s). Here, each group is known as a factor. These factors are small in number as compared to the original dimensions of the data. However, these factors are difficult to observe. Let¡¯s first read in all the images contained in the train folder: NOTE: You must replace the path inside the glob function with the path of your train folder. Now we will convert these images into a<U+00A0>numpy array format so that we can perform mathematical operations and also plot the images. (60000, 28, 28, 3) As you can see above, it¡¯s a 3-dimensional array. We must convert it to 1-dimension as all the upcoming techniques only take 1-dimensional input. To do this, we need to flatten the images: Let us now create a dataframe containing the pixel values of every individual pixel present in each image, and also their corresponding labels (for labels, we will make use of the train.csv file). Now we will decompose the dataset using Factor Analysis: Here, n_components will decide the number of factors in the transformed data. After transforming the data, it¡¯s time to visualize the results: Looks amazing, doesn¡¯t it? We can see all the different factors in the above graph. Here, the x-axis and y-axis represent the values of decomposed factors. As I mentioned earlier, it is hard to observe these factors individually but we have been able to reduce the dimensions of our data successfully. PCA is a technique which helps us in extracting a new set of variables from an existing large set of variables. These newly extracted variables are called Principal Components. You can refer to this article to learn more about PCA. For your quick reference, below are some of the key points you should know about PCA before proceeding further: Before moving further, we¡¯ll randomly plot some of the images from our dataset: Let¡¯s implement PCA using Python and transform the dataset: In this case, n_components will decide the number of principal components in the transformed data. Let¡¯s visualize how much variance has been explained using these 4 components. We will use explained_variance_ratio_ to calculate the same. In the above graph, the blue line represents component-wise explained variance while the orange line represents the cumulative explained variance. We are able to explain around 60% variance in the dataset using just four components. Let us now try to visualize each of these decomposed components: Each additional dimension we add to the PCA technique captures less and less of the variance in the model. The first component is the most important one, followed by the second, then the third, and so on. We can also use Singular Value Decomposition<U+00A0>(SVD) to decompose our original dataset into its constituents, resulting in dimensionality reduction. To learn the mathematics behind SVD, refer to<U+00A0>this article. SVD decomposes the original variables into three constituent matrices. It is essentially used to remove redundant features from the dataset. It uses the concept of Eigenvalues and Eigenvectors to determine those three matrices. We will not go into the mathematics of it due to the scope of this article, but let¡¯s stick to our plan, i.e. reducing the dimensions in our dataset. Let¡¯s implement SVD and decompose our original variables: Let us visualize the transformed variables by plotting the first two principal components: The above scatter plot shows us the decomposed components very neatly. As described earlier, there is not much correlation between these components. Independent Component Analysis (ICA) is based on information-theory and is also one of the most widely used dimensionality reduction techniques. The major difference between PCA and ICA is that PCA looks for uncorrelated factors while ICA looks for independent factors. If two variables are uncorrelated, it means there is no linear relation between them. If they are independent, it means they are not dependent on other variables. For example, the age of a person is independent of what that person eats, or how much television he/she watches. This algorithm assumes that the given variables are linear mixtures of some unknown latent variables. It also assumes that these latent variables are mutually independent, i.e., they are not dependent on other variables and hence they are called the independent components of the observed data. Let¡¯s compare PCA and ICA visually to get a better understanding of how they are different: Here, image (a) represents the PCA results while image (b) represents the ICA results on the same dataset. The equation of PCA is x = W¥ö. Here, Now we have to find an un-mixing matrix such that the components become as independent as possible. Most common method to measure independence of components is Non-Gaussianity: The above distribution is non-gaussian which in turn makes the components independent. Let¡¯s try to implement ICA in Python: Here, n_components will decide the number of components in the transformed data. We have transformed the data into 3 components using ICA. Let¡¯s visualize how well it has transformed the data: The data has been separated into different independent components which can be seen very clearly in the above image. X-axis and Y-axis represent the value of decomposed independent components. Now we shall look at some of the methods which reduce the dimensions of the data using projection techniques. To start off, we need to understand what projection is. Suppose we have two vectors, vector a and vector b, as shown below: We want to find the projection of a on b. Let the angle between a and b be <U+2205>. The projection (a1) will look like: a1 is the vector parallel to b. So, we can get the projection of vector a on vector b using the below equation: Here, By projecting one vector onto the other, dimensionality can be reduced. In projection techniques, multi-dimensional data is represented by projecting its points onto a lower-dimensional space. Now we will discuss different methods of projections: Once upon a time, it was assumed that the Earth was flat. No matter where you go on Earth, it keeps looking flat (let¡¯s ignore the mountains for a while). But if you keep walking in one direction, you will end up where you started. That wouldn¡¯t happen if the Earth was flat. The Earth only looks flat because we are minuscule as compared to the size of the Earth. These small portions where the Earth looks flat are manifolds, and if we combine all these manifolds we get a large scale view of the Earth, i.e., original data. Similarly for an n-dimensional curve, small flat pieces are manifolds and a combination of these manifolds will give us the original n-dimensional curve. Let us look at the steps for projection onto manifolds: Let us understand manifold projection technique with an example. If a manifold is continuously differentiable to any order, it is known as smooth or differentiable manifold. ISOMAP is an algorithm which aims to recover full low-dimensional representation of a non-linear manifold. It assumes that the manifold is smooth. It also assumes that for any pair of points on manifold, the geodesic distance (shortest distance between two points on a curved surface) between the two points is equal to the Euclidean distance (shortest distance between two points on a straight line). Let¡¯s first visualize the geodesic and Euclidean distance between a pair of points: Here, ISOMAP assumes both of these distances to be equal. Let¡¯s now look at a more detailed explanation of this technique. As mentioned earlier, all these techniques work on a three-step approach. We will look at each of these steps in detail: Let¡¯s implement it in Python and get a clearer picture of what I¡¯m talking about. We will perform non-linear dimensionality reduction through Isometric Mapping. For visualization, we will only take a subset of our dataset as running it on the entire dataset will require a lot of time. Parameters used: Visualizing the transformed data: You can see above that the correlation between these components is very low. In fact, they are even less correlated as compared to the components we obtained using SVD earlier! So far we have learned that PCA is a good choice for dimensionality reduction and visualization for datasets with a large number of variables. But what if we could use something more advanced? What if we can easily search for patterns in a non-linear way? t-SNE is one such technique. There are mainly two types of approaches we can use to map the data points: You can refer to<U+00A0>this article to learn about t-SNE in more detail. We will now implement it in Python and visualize the outcomes: n_components will decide the number of components in the transformed data. Time to visualize the transformed data: Here you can clearly see the different components that have been transformed using the powerful t-SNE technique. t-SNE works very well on large datasets but it also has it¡¯s limitations, such as loss of large-scale information, slow computation time, and inability to meaningfully represent very large datasets. Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can preserve as much of the local, and more of the global data structure as compared to t-SNE, with a shorter runtime. Sounds intriguing, right? Some of the key advantages of UMAP are: This method uses the concept of k-nearest neighbor and optimizes the results using stochastic gradient descent. It first calculates the distance between the points in high dimensional space, projects them onto the low dimensional space, and calculates the distance between points in this low dimensional space. It then uses Stochastic Gradient Descent to minimize the difference between these distances. To get a more in-depth understanding of how UMAP works, check out<U+00A0>this paper. Refer here<U+00A0>to see the documentation and installation guide of UMAP. We will now implement it in Python: Here, Let us visualize the transformation: The dimensions have been reduced and we can visualize the different transformed components. There is very less correlation between the transformed variables. Let us compare the results from UMAP and t-SNE: We can see that the correlation between the components obtained from UMAP is quite less as compared to the correlation between the components obtained from t-SNE. Hence, UMAP tends to give better results. As mentioned in UMAP¡¯s GitHub repository, it often performs better at preserving aspects of the global structure of the data than t-SNE. This means that it can often provide a better ¡°big picture¡± view of the data as well as preserving local neighbor relations. Take a deep breath. We have covered quite a lot of the dimensionality reduction techniques out there. Let¡¯s briefly summarize where each of them can be used. In this section, we will briefly summarize the use cases of each dimensionality reduction technique that we covered. It¡¯s important to understand where you can, and should, use a certain technique as it helps save time, effort and computational power. This is as comprehensive an article on dimensionality reduction as you¡¯ll find anywhere! I had a lot of fun writing it and found a few new ways of dealing with high number of variables I hadn¡¯t used before (like UMAP). Dealing with thousands and millions of features is a must-have skill for any data scientist. The amount of data we are generating each day is unprecedented and we need to find different ways to figure out how to use it. Dimensionality reduction is a very useful way to do this and has worked wonders for me, both in a professional setting as well as in machine learning hackathons. I¡¯m looking forward to hearing your feedback and ideas in the comments section below. Its really a wonderful article . Appreciate for sharing. Hi, Thanks for the feedback. Glad you liked the article. Wonder-full,In one word ¡°awesome¡±..can you please publish this with R code please Hi Sayam, Glad you liked the article. I am not very familiar with R however will let you know if i find any resource containing R codes. Excellent article, have gone through it once and clarified my long standing doubts. Keeping it under favorites for another detailed read. Thank you Om! Hi Regarding removing features with low variance The magnitude of variance is dependent on the scale of the values, right? If that¡¯s the case, how can one compare variance between two features having a different order of magnitude (when choosing which one to drop) ? Hi Clyton, Great point. We can normalize the variables and bring them down to same scale. Then there will not be any bias based on the scale of the values. Excellent! Or you may go for standard deviation instead of variance. Yes you can. Choice is yours, both will give similar results. muy bueno el posting! graicas Hi Jimmy, gracias por apreciar! Thanks for you article, really wonderfull ! I would like to know in part 3.3, why we need to delete a variable if the correlation is superior to 0.6 ? Hi Laurent,  If the independent variables are correlated to each other, they will have almost same effect on the target variable. So we can only keep one variable out of all correlated variables which will help us to reduce the dimensions of our dataset and will also not affect the performance of our model. Thanks a lot for writing such a comprehensive review! Thank you Sahar On random forest technique, you do: indices = np.argsort(importances[0:9]) # top 10 features but that takes only the first 9 elements of the model.feature_importances_ and sort those 9 elements.
Shouldn¡¯t you take all elements, sort them and then take the 9 more important ?
This way: indices = np.argsort(importance)[-9:]","Keyword(freq): variable(55), component(23), value(22), feature(15), technique(13), image(10), point(10), dimension(8), factor(8), result(7)"
