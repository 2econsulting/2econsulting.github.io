"site","date","headline","url_address","text"
"vidhya",2018-09-20,"Let¡¯s Think in Graphs: Introduction to Graph Theory and its Applications using Python","https://www.analyticsvidhya.com/blog/2018/09/introduction-graph-theory-applications-python/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 ¡°There is a magic in graphs. The pro<U+FB01>le of a curve reveals in a <U+FB02>ash a whole situation <U+2014> the life history of an era of prosperity. The curve informs the mind, awakens the imagination, convinces.¡± <U+2013> Henry D. Hubbard Visualizations are a powerful way to simplify and interpret the underlying patterns in data. The first thing I do, whenever I work on a new dataset is to explore it through visualization. And this approach has worked well for me. Sadly, I don¡¯t see many people using visualizations as much. That is why I thought I will share some of my ¡°secret sauce¡± with the world! Use of graphs is one such visualization technique. It is incredibly useful and helps businesses make better data-driven decisions. But to understand the concepts of graphs in detail, we must first understand it¡¯s base <U+2013> Graph Theory. In this article, we will be learning the concepts of graphs and graph theory. We will also look at the fundamentals and basic properties of graphs, along with different types of graphs. We will then work on a case study to solve a commonly seen problem in the aviation industry by applying the concepts of Graph Theory using Python. Let¡¯s get started! Consider the plot shown below: It¡¯s a nice visualization of the store-wise sales of a particular item. But this isn¡¯t a graph, it¡¯s a chart. Now you might be wondering why is this a chart and not a graph, right? Well, a chart represents the graph of a function. Let me explain this by expanding on the above example. Out of the total units of a particular item, 15.1% are sold from store A, 15.4% from store B, and so on. We can represent it using a table: Corresponding to each store is their contribution (in %) to the overall sales. In the above chart, we mapped store A with 15.1% contribution, store B with 15.4%, so on and so forth. Finally, we visualized it using a pie chart. But then what¡¯s the difference between this chart and a graph? To answer this, consider the visual shown below: The points in the above visual represent the characters of Game of Thrones, while the lines joining these points represent the connection between them. Jon Snow has connections with multiple characters, and the same goes for Tyrion, Cersei, Jamie, etc. And this is what a graph looks like. A single point might have connections with multiple points, or even a single point. Typically, a graph is a combination of vertices (nodes) and edges. In the above GOT visual, all the characters are vertices and the connections between them are edges. We now have an idea of what graphs are, but why do we need graphs in the first place? We¡¯ll look at this pertinent question in the next section. Suppose you booked an Uber cab. One of the most important things that is critical to Uber¡¯s functioning is its ability to match drivers with riders in an efficient way. Consider there are 6 possible rides that you can be matched with. So, how does Uber allocate a ride to you? We can make use of graphs to visualize how the process of allotting a ride might be: As you can interpret, there are 6 possible rides (Ride 1, Ride 2, ¡¦. Ride 6) which the rider can be matched with. Representing this in graph form makes it easier to visualize and finally fulfill our aim, i.e., to match the closest ride to the user. The numbers in the above graph represent the distance (in kilometers) between the rider and his/her corresponding ride. We (and of course Uber) can clearly visualize that Ride 3 is the closest option. Note: For simplicity, I have taken only the distance metric to decide which ride will be allotted to the rider. Whearas in a real life scenario, there are multiple metrics through which the allotment of a ride is decided, such as rating of the rider and driver, traffic between different routes, time for which the rider is idle, etc. Similarly, online food delivery aggregators like Zomato can select a rider who will pick up our orders from the corresponding restaurant and deliver it to us. This is one of the many use cases of graphs through which we can solve a lot of challenges. Graphs make visualizations easier and more interpretable. To understand the concept of graphs in detail, we must first understand graph theory. We¡¯ll first discuss the origins of graph theory to get an intuitive understanding of graphs. There is an interesting story behind its origin, and I aim to make it even more intriguing using plots and visualizations. It all started with the Seven Bridges of Konigsberg. The challenge (or just a brain teaser) with Konigsberg¡¯s bridges, was to be able to walk through the city by crossing all the seven bridges only once. Let us visualize it first to have a clear understanding of the problem: Give it a try and see if you can walk through the city with this restraint. You have to keep two things in mind while trying to solve the above problem (or should i say riddle?): You can try any number of combinations, but it remains an impossible challenge to crack. There is no way in which one can walk through the city by crossing each bridge only once.<U+00A0>Leonhard Euler delved deep into this puzzle to come up with the reason why this is such an impossible task. Let¡¯s analyze how he did this: There are four distinct places in the above image: two islands (B and D), and two parts of the mainland (A and C) and a total of seven bridges. Let us first look at each land separately and try to find patterns (if any exist at all) : One inference from the above image is that each land is connected with an<U+00A0>odd number of bridges. If you wish to cross each bridge only once, then you can enter and leave a land only if it is connected to an even number of bridges. In other words, we can generalize that if there are even number of bridges, it¡¯s possible to leave the land, while it¡¯s impossible to do so with an odd number. Let¡¯s try to add one more bridge to the current problem and see whether it can crack open this problem: Now we have 2 lands connected with an even number of bridges, and 2 lands connected with an odd number of bridges. Let¡¯s draw a new route after the addition of the new bridge: The addition of a single bridge solved the problem! You might be wondering if the number of bridges played a significant part in solving this problem? Should it be even all the time? Well, that¡¯s not always the case. Euler explained that along with the number of bridges, the number of pieces of land with an odd number of connected bridges matters as well. Euler converted this problem from land and bridges to graphs, where he represented the land as vertices and the bridges as edges: Here, the visualization is simple and crystal clear. Before we move further and delve deeper into this problem, let us first understand the fundamentals and basic properties of a graph. There are many key points and key words that we should keep in mind when we are dealing with graphs. In this section, we will discuss all those keywords in detail. These are some of the fundamentals which you must keep in mind when dealing with graphs. Now onto understanding the basic properties of a graph. Till this point, we have seen what a graph looks like and it¡¯s different components. Now we will turn our focus to some basic properties and terminologies related to a graph. We will be using the below given graph (referred to as G) and understand each terminology using the same: Take a moment and think about possible solutions to the following questions: I will try to answer all these questions using basic graph terminologies: These are some of the terminologies related to graphs. Next we will discuss the different types of graphs. There are vairous and diverse types of graphs. In this section, we will discuss some of the most commonly used ones. Now that we have an understanding of the different types of graphs, their components, and some of the basic graph-related terminologies, let¡¯s get back to the problem which we were trying to solve, i.e. the Seven Bridges of Konigsberg. We shall explore in even more detail how Leonhard Euler approached and explained his reasoning. We saw earlier that Euler transformed this problem using graphs: Here, A, B, C, and D represent the land, and the lines joining them are the bridges. We can calculate the degree of each vertex. deg(B) = 5 deg(A) = deg(C) = deg(D) = 3 Euler showed that the possibility of walking through a graph (city) using each edge (bridge) only once, strictly depends on the degree of vertices (land). And such a path, which contains each edge of a graph only once, is called Euler¡¯s path. Can you figure out Euler¡¯s path for our problem? Let¡¯s try! And this is how the classic Seven Bridges of Konigsberg challenge can be solved using graphs and Euler¡¯s path. And this is basically the origin of Graph Theory. Thanks to Leonhard Euler! Trees are one of the most powerful and effective ways of representing a graph. In this section, we will learn what binary search trees are, how they work, and how they make visualizations more interpretable. But before all that, take a moment to understand what trees actually are in this context. Trees are graphs which do not contain even a single cycle: In the above example, the first graph has no cycle (aka a tree), while the second graph has a cycle (A-B-E-C-E, hence it¡¯s not a tree). The elements of a tree are called nodes. (A, B, C, D, and E) are the nodes in the above tree. The first node (or the topmost node) of a tree is known as the root node, while the last node (node C, D and E in the above example) is known as the leaf node. All the remaining nodes are known as child nodes (node B in our case). It¡¯s time to move on to one of the most important topics in Graph Theory, i.e., Graph Traversal. Suppose we want to identify the location of a particular node in a graph. What might me the possible solution to identify nodes of a graph? How to start? What should be the starting point? Once we know the starting point, how to proceed further? I will try to answer all these questions in this section by explaining the concepts of Graph Traversal. Graph Traversal refers to visiting every vertex and edge of a graph exactly once in a well-defined order. As the aim of traversing is to visit each vertex only once, we keep a track of vertices covered so that we do not cover same vertex twice. There are various methods for graph traversal and we will discuss some of the famous methods: We start from the source node (root node) and traverse the graph, layer wise. Steps for Breadth First Search are: Let me explain it with a visualization: So in Breadth First Search, we start from the Source Node (A in our case) and move down to the first layer, i.e. Layer 1. We cover all the nodes in that layer by moving horizontally (B -> C). Then we go to the next layer, i.e. Layer 2 and repeat the same step (we move from D -> E -> F). We continue this step until all the layers and vertices are covered. Key advantage of this approach is that we will always find the shortest path to the goal. This is appropriate for small graphs and trees but for more complex and larger graphs, its performance is very slow and it also takes a lot of memory. We will look at another traversing approach which takes less memory space as compared to BFS. Let us first look at the steps involved in this approach: The sequence for Depth First Search for the above example will be: A -> B -> D -> E -> C -> F Once a path has been fully explored it can be removed from memory, so DFS only needs to store the root node, all the children of the root node and where it currently is. Hence, it overcomes the memory problem of BFS. In this approach all the nodes of a tree are arranged in a sorted order. Let¡¯s have a look at an example of Binary Search Tree: As mentioned earlier, all the nodes in the above tree are arranged based on a condition. Suppose we want to access the node with value 45. If we would have followed BFS or DFS, we would have required a lot of computational time to reach to it. Now let¡¯s look at how a Binary Search Tree will help us to reach to the required node using least number of steps. Steps to reach to the node with value 45 using Binary Search Tree: This approach is very fast and takes very less memory as well. Till now we have seen most of the concepts of Graph Theory. Next we will try to implement these concepts to solve a real life problem using Python. And finally, we get to work with data in Python! In this dataset, we have records of over 7 million flights from the USA. The below variables have been provided: It is a gigantic dataset and I have taken only a sample from it for this article. The idea is to give you an understanding of the concepts using this sample dataset, and you can then apply them to the entire dataset. Download the dataset which we will be using for the case study from here. We will first import the usual libraries, and read the dataset, which is provided in a .csv format: Let¡¯s have a look at the first few rows of the dataset using the head() function: Here, CRSDepTime, CRSArrTime, DepTime, and ArrTime represent the scheduled time of departure, the scheduled time of arrival, the actual time of departure, and the actual time of arrival respectively. Origin and Dest are the Origin and Destination of the journey. There can often be multiple paths from one airport to another, and the aim is to find the shortest possible path between all the airports. There are two ways in which we can define a path as the shortest: We can solve such problems using the concepts of graph theory which we have learned so far. Can you recall what we need to do to make a graph? The answer is identifying the vertices and edges! We can convert the problem to a graph by representing all the airports as vertices, and the route between them as edges. We will be using NetworkX for creating and visualizing graphs. NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. You can refer to the documentation of NetworkX here. After installing NetworkX, we will create the edges and vertices for our graph using the dataset: It will store the vertices and edges automatically. Take a quick look at the edges and vertices of the graph which we have created: Let us plot and visualize the graph using the<U+00A0>matplotlib and draw_networkx() functions of networkx. The above amazing visualization represents the different flight routes. Suppose a passenger wants to take the shortest route from AMA to PBI. Graph theory comes to the rescue once again! Let¡¯s try to calculate the shortest path based on the airtime between the airports AMA and PBI. We will be using Dijkstra¡¯s shortest path algorithm. This algorithm finds the shortest path from a source vertex to all the vertices of the given graph. Let me give you a brief run through of the steps this algorithm follows: Let us take an example to understand this algorithm in a better way: Here the source vertex is A. The numbers represent the distance between the vertices. Initially, the sptSet is empty so we will assign distances to all the vertices. The distances are: {0, INF, INF, INF, INF, INF}, where INF represents INFINITE. Now, we will pick the vertex with the minimum distance, i.e., A and it will be included in the sptSet. So, the new sptSet is {A}. The next step is to pick a vertex which is not in the sptSet and is closest to the source vertex. This, in our case, is B with a distance value of 2. So this will be added to the sptSet. sptSet = {A,B} Now we will update the distances of vertices adjacent to vertex B: The distance value of the vertex F becomes 6. We will again pick the vertex with the minimum distance value which is not already included in SPT (C with a distance value of 4). sptSet = {A,B,C} We will follow similar steps until all the vertices are included in the sptSet. Let¡¯s implement this algorithm and try to calculate the shortest distance between the airports. We will use the<U+00A0>dijkstra_path() function of networkx to do so: This is the shortest possible path between the two airports based on the distance between them. We can also calculate the shortest path based on the airtime just by changing the hyperparameter weight=¡¯AirTime¡¯: This is the shortest path based on the airtime. Intuitive and easy to understand, this was all about graph theory! This is just one of the many applications of Graph Theory. We can apply it to almost any kind of problem and get solutions and visualizations. Some of the application of Graph Theory which I can think of are: These are some of the applications. You can come up with many more. Feel free to share them in the comment section below. I hope you have enjoyed the article. Looking forward to your responses. Awesome Article.
Written in an intuitive and lucid manner. Thank you Ankur! Thanks Pulkit for the post. Did you find any way of drawing edge thickness? Glad you liked the article!
There is a concept called weighted graphs in which you can give weights to edges. For more details on this, refer here. Pulkit,
Well written! It is a good introduction to the computer science students. Python integration is excellent.
However, I think the quote of Henry D. Hubbard is not on graphs of Graph Theory bu t is on the graph created out of a function. Hi Joseph, Thanks for the feedback.
I just added that quote to get an intuition of how graphs can be useful for visualization. Great article! Will there be others? It would be great if you could point some books and courses about it too in the end of the article. Hi, Glad you liked the article!!
You can refer ¡°Introduction to Graph Theory¡± course of coursera to learn more about graph theory. Thanks for this nice article. I have some question as below.
1) Can we implemented travelling sales man problem using graph theory in python?
2) Need some more example of Real life project case study. Hi Ashish, 1. To solve the traveling salesman problem, you can consider the cities as vertices and the distance between them as edges. Then you can solve it using Graph Theory. For more details, refer here."
"vidhya",2018-09-18,"Nuts & Bolts of Reinforcement Learning: Model Based Planning using Dynamic Programming","https://www.analyticsvidhya.com/blog/2018/09/reinforcement-learning-model-based-planning-dynamic-programming/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 Deep Reinforcement learning is responsible for the two biggest AI wins over human professionals <U+2013> Alpha Go and OpenAI Five. Championed by Google and Elon Musk, interest in this field has gradually increased in recent years to the point where it¡¯s a thriving area of research nowadays. In this article, however, we will not talk about a typical RL setup but explore Dynamic Programming (DP). DP is a collection of algorithms that<U+00A0> can solve a problem where we have the perfect model of the environment (i.e. probability distributions of any change happening in the problem setup are known) and where an agent can only take discrete actions. DP essentially solves a planning problem rather than a more general RL problem. The main difference, as mentioned, is that for an RL problem the environment can be very complex and its specifics are not known at all initially. But before we dive into all that, let¡¯s understand why you should learn dynamic programming in the first place using an intuitive example. Apart from being a good starting point for grasping reinforcement learning, dynamic programming can help find optimal solutions to planning problems faced in the industry, with an important assumption that the specifics of the environment are known.<U+00A0>DP presents a good starting point to understand RL algorithms that can solve more complex problems. Sunny manages a motorbike rental company in Ladakh. Being near the highest motorable road in the world, there is a lot of demand for motorbikes on rent from tourists. Within the town he has 2 locations where tourists can come and get a bike on rent. If he is out of bikes at one location, then he loses business. The problem that Sunny is trying to solve is to find out how many bikes he should move each day from 1 location to another so that he can maximise his earnings. Here, we exactly know the environment (g(n) & h(n)) and this is the kind of problem in which dynamic programming can come in handy.<U+00A0>Similarly, if you can properly model the environment of your problem where you can take discrete actions, then DP can help you find the optimal solution.<U+00A0>In this article, we will use DP to train an agent using Python to traverse a simple environment, while touching upon key concepts in RL such as policy, reward, value function and more. Most of you must have played the tic-tac-toe game in your childhood. If not, you can grasp the rules of this simple game from its wiki page. Suppose tic-tac-toe is your favourite game, but you have nobody to play it with. So you decide to design a bot that can play this game with you. Some key questions are: Can you define a rule-based framework to design an efficient bot? You sure can, but you will have to hardcode a lot of rules for each of the possible situations that might arise in a game. However, an even more interesting question to answer is: Can you train the bot to learn by playing against you several times? And that too without being explicitly programmed to play tic-tac-toe efficiently? A few considerations for this are: For more clarity on the aforementioned reward, let us consider a match between bots O and X: Consider the following situation encountered in tic-tac-toe: If bot X puts X in the bottom right position for example, it results in the following situation: Bot O would be rejoicing (Yes! They are programmed to show emotions) as it can win the match with just one move. Now, we need to teach X not to do this again. So we give a negative reward or punishment to reinforce the correct behaviour in the next trial. We say that this action in the given state would correspond to a negative reward and should not be considered as an optimal action in this situation. Similarly, a positive reward would be conferred to X if it stops O from winning in the next move: Now that we understand the basic terminology, let¡¯s talk about formalising this whole process using a concept called a Markov Decision Process or MDP. A Markov Decision Process (MDP) model contains: Now, let us understand the markov or ¡®memoryless¡¯ property. Any random process in which the probability of being in a given state depends only on the previous state, is a markov process. In other words, in the markov decision process setup, the environment¡¯s response at time t+1 depends only on the state and action representations at time t, and is independent of whatever happened in the past. The above diagram clearly illustrates the iteration at each time step wherein the agent receives a reward Rt+1 and ends up in state St+1<U+00A0>based on its action At at a particular state St. The overall goal for the agent is to maximise the cumulative reward it receives in the long run. Total reward at any time instant t is given by: where T is the final time step of the episode. In the above equation, we see that all future rewards have equal weight which might not be desirable. That¡¯s where an additional concept of discounting comes into the picture. Basically, we define ¥ã as a discounting factor and each reward after the immediate reward is discounted by this factor as follows: For discount factor < 1, the rewards further in the future are getting diminished. This can be understood as a tuning parameter which can be changed based on how much one wants to consider the long term (¥ã close to 1) or short term (¥ã close to 0). Can we use the reward function defined at each time step to define how good it is, to be in a given state for a given policy? The value function denoted as v(s) under a policy ¥ð represents how good a state is for an agent to be in. In other words, what is the average reward that the agent will get starting from the current state under policy ¥ð? E in the above equation represents the expected reward at each state if the agent follows policy ¥ð and S represents the set of all possible states. Policy, as discussed earlier, is the mapping of probabilities of taking each possible action at each state (¥ð(a/s)). The policy might also be deterministic when it tells you exactly what to do at each state and does not give probabilities. Now, it¡¯s only intuitive that ¡®the optimum policy¡¯ can be reached if the value function is maximised for each state. This optimal policy is then given by: The above value function only characterizes a state. Can we also know how good an action is at a particular state? A state-action value function, which is also called the q-value, does exactly that. We define the value of action a, in state s, under a policy ¥ð, as: This is the expected return the agent will get if it takes action At at time t, given state St, and thereafter follows policy ¥ð. Bellman was an applied mathematician who derived equations that help to solve an Markov Decision Process. Let¡¯s go back to the state value function v and state-action value function q. Unroll the value function equation to get: In this equation, we have the value function for a given policy ¥ð represented in terms of the value function of the next state. Choose an action a, with probability ¥ð(a/s) at the state s, which leads to state s¡¯ with prob p(s¡¯/s,a). This gives a reward [r + ¥ã*v¥ð(s)] as given in the square bracket above. This is called the Bellman Expectation Equation. The value information from successor states is being transferred back to the current state, and this can be represented efficiently by something called a backup diagram as shown below. The Bellman expectation equation averages over all the possibilities, weighting each by its probability of occurring. It states that the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way. We have n (number of states) linear equations with unique solution to solve for each state s. The goal here is to find the optimal policy, which when followed by the agent gets the maximum cumulative reward. In other words, find a policy ¥ð, such that for no other ¥ð can the agent get a better expected return. We want to find a policy which achieves maximum value for each state. Note that we might not get a unique policy, as under any situation there can be 2 or more paths that have the same return and are still optimal. Optimal value function can be obtained by finding the action a which will lead to the maximum of q*. This is called the bellman optimality equation for v*. Intuitively, the Bellman optimality equation says that the value of each state under an optimal policy must be the return the agent gets when it follows the best action as given by the optimal policy. For optimal policy ¥ð*, the optimal value function is given by: Given a value function q*, we can recover an optimum policy as follows: The value function for optimal policy can be solved through a non-linear system of equations. We can can solve these efficiently using iterative methods that fall under the umbrella of dynamic programming. Dynamic programming algorithms solve a category of problems called planning problems. Herein given the complete model and specifications of the environment (MDP), we can successfully find an optimal policy for the agent to follow. It contains two main steps: To solve a given MDP, the solution must have the components to: Policy evaluation answers the question of how good a policy is. Given an MDP and an arbitrary policy ¥ð, we will compute the state-value function. This is called policy evaluation in the DP literature. The idea is to turn bellman expectation equation discussed earlier to an update. To produce each successive approximation vk+1 from vk, iterative policy evaluation applies the same operation to each state s. It replaces the old value of s with a new value obtained from the old values of the successor states of s, and the expected immediate rewards, along all the one-step transitions possible under the policy being evaluated, until it converges to the true value function of a given policy ¥ð. Let us understand policy evaluation using the very popular example of Gridworld. A bot is required to traverse a grid of 4¡¿4 dimensions to reach its goal (1 or 16). Each step is associated with a reward of -1. There are 2 terminal states here: 1 and 16 and 14 non-terminal states given by [2,3,¡¦.,15]. Consider a random policy for which, at every state, the probability of every action {up, down, left, right} is equal to 0.25. We will start with initialising v0 for the random policy to all 0s. This is definitely not very useful. Let¡¯s calculate v2 for all the states of 6: Similarly, for all non-terminal states, v1(s) = -1. For terminal states p(s¡¯/s,a) = 0 and hence vk(1) = vk(16) = 0 for all k. So v1 for the random policy is given by: Now, for v2(s) we are assuming ¥ã or the discounting factor to be 1: As you can see, all the states marked in red in the above diagram are identical to 6 for the purpose of calculating the value function. Hence, for all these states, v2(s) = -2. For all the remaining states, i.e., 2, 5, 12 and 15, v2 can be calculated as follows: If we repeat this step several times, we get v¥ð: Using policy evaluation we have determined the value function v for an arbitrary policy ¥ð. We know how good our current policy is. Now for some state s, we want to understand what is the impact of taking an action a that does not pertain to policy ¥ð.<U+00A0> Let¡¯s say we select a in s, and after that we follow the original policy ¥ð. The value of this way of behaving is represented as: If this happens to be greater than the value function v¥ð(s), it implies that the new policy ¥ð¡¯ would be better to take. We do this iteratively for all states to find the best policy. Note that in this case, the agent would be following a greedy policy in the sense that it is looking only one step ahead. Let¡¯s get back to our example of gridworld. Using v¥ð, the value function obtained for random policy ¥ð, we can improve upon ¥ð by following the path of highest value (as shown in the figure below). We start with an arbitrary policy, and for each state one step look-ahead is done to find the action leading to the state with the highest value. This is done successively for each state. As shown below for state 2, the optimal action is left which leads to the terminal state having a value . This is the highest among all the next states (0,-18,-20). This is repeated for all states to find the new policy. Overall, after the policy improvement step using v¥ð, we get the new policy ¥ð¡¯: Looking at the new policy, it is clear that it¡¯s much better than the random policy. However, we should calculate v¥ð¡¯ using the policy evaluation technique we discussed earlier to verify this point and for better understanding. Once the policy has been improved using v¥ð to yield a better policy ¥ð¡¯, we can then compute v¥ð¡¯ to improve it further to ¥ð¡¯¡¯. Repeated iterations are done to converge approximately to the true value function for a given policy ¥ð (policy evaluation). Improving the policy as described in the policy improvement section is called policy iteration. In this way, the new policy is sure to be an improvement over the previous one and given enough iterations, it will return the optimal policy. This sounds amazing but there is a drawback <U+2013> each iteration in policy iteration itself includes another iteration of policy evaluation that may require multiple sweeps through all the states. Value iteration technique discussed in the next section provides a possible solution to this. We saw in the gridworld example that at around k = 10, we were already in a position to find the optimal policy. So, instead of waiting for the policy evaluation step to converge exactly to the value function v¥ð, we could stop earlier. We can also get the optimal policy with just 1 step of policy evaluation followed by updating the value function repeatedly (but this time with the updates derived from bellman optimality equation). Let¡¯s see how this is done as a simple backup operation: This is identical to the bellman update in policy evaluation, with the difference being that we are taking the maximum over all actions. Once the updates are small enough, we can take the value function obtained as final and estimate the optimal policy corresponding to that. Some important points related to DP: It is of utmost importance to first have a defined environment in order to test any kind of policy for solving an MDP efficiently. Thankfully, OpenAI, a non profit research organization provides a large number of environments to test and play with various reinforcement learning algorithms. To illustrate dynamic programming here, we will use it to navigate the Frozen Lake environment. The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile. The surface is described using a grid like the following: (S: starting point, safe),<U+00A0><U+00A0>(F: frozen surface, safe),<U+00A0>(H: hole, fall to your doom),<U+00A0>(G: goal) The idea is to reach the goal from the starting point by walking only on frozen surface and avoiding all the holes. Installation details and documentation is available at this link. Once gym library is installed, you can just open a jupyter notebook to get started. Now, the env variable contains all the information regarding the frozen lake environment. Before we move on, we need to understand what an episode is. An episode represents a trial by the agent in its pursuit to reach the goal. An episode ends once the agent reaches a terminal state which in this case is either a hole or the goal. Description of parameters for policy iteration function policy: 2D array of a size n(S) x n(A), each cell represents a probability of taking action a in state s. environment: Initialized OpenAI gym environment object discount_factor: MDP discount factor theta: A threshold of a value function change. Once the update to value function is below this number max_iterations: Maximum number of iterations to avoid letting the program run indefinitely This function will return a vector of size nS, which represent a value function for each state. Let¡¯s start with the policy evaluation step. The objective is to converge to the true value function for a given policy ¥ð. We will define a function that returns the required value function. Now coming to the policy improvement part of the policy iteration algorithm. We need a helper function that does one step lookahead to calculate the state-value function. This will return an array of length nA containing expected value of each action Now, the overall policy iteration would be as described below. This will return a tuple (policy,V) which is the optimal policy matrix and value function for each state. The parameters are defined in the same manner for value iteration.<U+00A0>The value iteration algorithm can be similarly coded: Finally, let¡¯s compare both methods to look at which of them works better in a practical setting. To do this, we will try to learn the optimal policy for the frozen lake environment using both techniques described above. Later, we will check which technique performed better based on the average return after 10,000 episodes. We observe that value iteration has a better average reward and higher number of wins when it is run for 10,000 episodes. In this article, we became familiar with model based planning using dynamic programming, which given all specifications of an environment, can find the best policy to take. I want to particularly mention the brilliant book on RL by Sutton and Barto which is a bible for this technique and encourage people to refer it. More importantly, you have taken the first step towards mastering reinforcement learning. Stay tuned for more articles covering different algorithms within this exciting domain."
"vidhya",2018-09-17,"DataHack Radio #10: The Role of Computer Science in the Data Science World with Dr. Jeannette M. Wing","https://www.analyticsvidhya.com/blog/2018/09/datahack-radio-data-science-podcast-jeanette-wing/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 Have you noticed that the recent surge of data scientists have a background in computer science? It¡¯s not a coincidence. These two domains are important in their own right but when merged together, they produce powerful results. We are thrilled to announce the release of episode 10 of our DataHack Radio podcast with none other than Professor Jeannette M. Wing! She has over 4 decades of experience in academia and the industry, and there is no one better to give a perspective on how computer science has evolved, and how it meshes with the data science world. I have briefly summarized the key takeaways from this episode below. I recommend listening to the podcast to truly get a feel for how computer science and data science are a powerful combination when used together. Enjoy this episode! Subscribe to DataHack Radio NOW and listen to this, as well as all previous episodes, on any of the below platforms: Professor Wing has always been fascinated by mathematics and engineering since her childhood. She went to graduation school at MIT and started majoring in electrical engineering there. During her initial days at the university, she was introduced to the world of computer science and that prompted her to change majors. And there was no looking back from that point on. Post her days at MIT (where she also successfully completed her Ph.D in computer science), she worked at the University of Southern California for a couple of years before joining Carnegie Mellon University. She was the computer science department head twice at Carnegie Mellon. In between those two stints, she worked at the National Science Foundation (NSF). During her second time as the department head at Carnegie Mellon, Microsoft approached her and she took up a role there in 2013. Within a year of joining, she was put in charge of all the basic research labs, including in Silicon Valley, New York, Bangalore, and Beijing, among others. And then last year came Columbia University and a chance to work in academia again. At Columbia, she is the Avanessians Director of the Data Science Institute and Professor of Computer Science. She reports directly to the President of the University. Although there has been decades of research done in computer science to formally show how one can prove how a program is correct, this is all with respect to mathematical logic. What data science is now bringing is the complexity for proving how a property is correct with respect to inherently probabilistic and statistical methods. Professor Wing firmly believes that a lot of the new data science methods should be revisited by the formal methods techniques. Its a challenge for the formal methods community to help data science grow using these concepts, something which hasn¡¯t yet happened. In case you are not aware, formal methods are mathematics based techniques especially used in computer science. You can read more about them here. Professor Wing, in her current role at Columbia University, is working with the AI community to understand what methods and logic are required to specify the relevant properties that these machine learned models should have. She feels this will help build safe and trustworthy AI systems for the future, a topic Professor Wing is a strong advocate of. At Microsoft, she was overlooking several research projects in multiple locations as I mentioned above. The Bangalore lab, in particular, had a couple of big strengths: ¡°I¡¯m really just an academic at heart.¡± <U+2013> Professor Wing A very common question from folks new to data science is <U+2013> ¡°what¡¯s the difference between working in academia versus getting industry experience¡±? And Professor Wing was kind enough to cover this topic. She echoes the wide-held belief that being a scholar has it¡¯s own distinct advantages. You have more freedom to explore questions like why something works, rather than just focusing on how it works (which is what happens in most industry roles). The science part of both computer and data science comes from research and academia far more than the industry. It was a privilege hosting Professor Wing on our podcast. Her explanation of formal methods and the important part they are playing in the software industry was a true delight to listen to. Fans of mathematics will surely love this episode. Happy listening!"
