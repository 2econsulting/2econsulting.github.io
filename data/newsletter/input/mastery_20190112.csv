"site","date","headline","url_address","text"
"mastery",2019-01-11,"How to Fix Vanishing Gradients Using the Rectified Linear Activation Function","https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/","The vanishing gradients problem is one example of unstable behavior that you may encounter when training a deep neural network. It describes the situation where a deep multilayer feed-forward network or a recurrent neural network is unable to propagate useful gradient information from the output end of the model back to the layers near the input end of the model. The result is the general inability of models with many layers to learn on a given dataset or to prematurely converge to a poor solution. Many fixes and workarounds have been proposed and investigated, such as alternate weight initialization schemes, unsupervised pre-training, layer-wise training, and variations on gradient descent. Perhaps the most common change is the use of the rectified linear activation function that has become the new default, instead of the hyperbolic tangent activation function that was the default through the late 1990s and 2000s. In this tutorial, you will discover how to diagnose a vanishing gradient problem when training a neural network model and how to fix it using an alternate activation function and weight initialization scheme. After completing this tutorial, you will know: Let¡¯s get started. How to Fix the Vanishing Gradient By Using the Rectified Linear Activation FunctionPhoto by Liam Moloney, some rights reserved. This tutorial is divided into five parts; they are: Neural networks are trained using stochastic gradient descent. This involves first calculating the prediction error made by the model and using the error to estimate a gradient used to update each weight in the network so that less error is made next time. This error gradient is propagated backward through the network from the output layer to the input layer. It is desirable to train neural networks with many layers, as the addition of more layers increases the capacity of the network, making it capable of learning a large training dataset and efficiently representing more complex mapping functions from inputs to outputs. A problem with training networks with many layers (e.g. deep neural networks) is that the gradient diminishes dramatically as it is propagated backward through the network. The error may be so small by the time it reaches layers close to the input of the model that it may have very little effect. As such, this problem is referred to as the ¡°vanishing gradients¡± problem. Vanishing gradients make it difficult to know which direction the parameters should move to improve the cost function ¡¦ <U+2014> Page 290, Deep Learning, 2016. In fact, the error gradient can be unstable in deep neural networks and not only vanish, but also explode, where the gradient exponentially increases as it is propagated backward through the network. This is referred to as the ¡°exploding gradient¡± problem. The term vanishing gradient refers to the fact that in a feedforward network (FFN) the backpropagated error signal typically decreases (or increases) exponentially as a function of the distance from the final layer. <U+2014> Random Walk Initialization for Training Very Deep Feedforward Networks, 2014. Vanishing gradients is a particular problem with recurrent neural networks as the update of the network involves unrolling the network for each input time step, in effect creating a very deep network that requires weight updates. A modest recurrent neural network may have 200-to-400 input time steps, resulting conceptually in a very deep network. The vanishing gradients problem may be manifest in a Multilayer Perceptron by a slow rate of improvement of a model during training and perhaps premature convergence, e.g. continued training does not result in any further improvement. Inspecting the changes to the weights during training, we would see more change (i.e. more learning) occurring in the layers closer to the output layer and less change occurring in the layers close to the input layer. There are many techniques that can be used to reduce the impact of the vanishing gradients problem for feed-forward neural networks, most notably alternate weight initialization schemes and use of alternate activation functions. Different approaches to training deep networks (both feedforward and recurrent) have been studied and applied [in an effort to address vanishing gradients], such as pre-training, better random initial scaling, better optimization methods, specific architectures, orthogonal initialization, etc. <U+2014> Random Walk Initialization for Training Very Deep Feedforward Networks, 2014. In this tutorial, we will take a closer look at the use of an alternate weight initialization scheme and activation function to permit the training of deeper neural network models. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course As the basis for our exploration, we will use a very simple two-class or binary classification problem. The scikit-learn class provides the make_circles() function that can be used to create a binary classification problem with the prescribed number of samples and statistical noise. Each example has two input variables that define the x and y coordinates of the point on a two-dimensional plane. The points are arranged in two concentric circles (they have the same center) for the two classes. The number of points in the dataset is specified by a parameter, half of which will be drawn from each circle. Gaussian noise can be added when sampling the points via the ¡°noise¡± argument that defines the standard deviation of the noise, where 0.0 indicates no noise or points drawn exactly from the circles. The seed for the pseudorandom number generator can be specified via the ¡°random_state¡± argument that allows the exact same points to be sampled each time the function is called. The example below generates 1,000 examples from the two circles with noise and a value of 1 to seed the pseudorandom number generator. We can create a graph of the dataset, plotting the x and y coordinates of the input variables (X) and coloring each point by the class value (0 or 1). The complete example is listed below. Running the example creates a plot showing the 1,000 generated data points with the class value of each point used to color each point. We can see points for class 0 are blue and represent the outer circle, and points for class 1 are orange and represent the inner circle. The statistical noise of the generated samples means that there is some overlap of points between the two circles, adding some ambiguity to the problem, making it non-trivial. This is desirable as a neural network may choose one of among many possible solutions to classify the points between the two circles and always make some errors. Scatter Plot of Circles Dataset With Points Colored By Class Value Now that we have defined a problem as the basis for our exploration, we can look at developing a model to address it. We can develop a Multilayer Perceptron model to address the two circles problem. This will be a simple feed-forward neural network model, designed as we were taught in the late 1990s and early 2000s. First, we will generate 1,000 data points from the two circles problem and rescale the inputs to the range [-1, 1]. The data is almost already in this range, but we will make sure. Normally, we would prepare the data scaling using a training dataset and apply it to a test dataset. To keep things simple in this tutorial, we will scale all of the data together before splitting it into train and test sets. Next, we will split the data into train and test sets. Half of the data will be used for training and the remaining 500 examples will be used as the test set. In this tutorial, the test set will also serve as the validation dataset so we can get an idea of how the model performs on the holdout set during training. Next, we will define the model. The model will have an input layer with two inputs, for the two variables in the dataset, one hidden layer with five nodes, and an output layer with one node used to predict the class probability. The hidden layer will use the hyperbolic tangent activation function (tanh) and the output layer will use the logistic activation function (sigmoid) to predict class 0 or class 1 or something in between. Using the hyperbolic tangent activation function in hidden layers was the best practice in the 1990s and 2000s, performing generally better than the logistic function when used in the hidden layer. It was also good practice to initialize the network weights to small random values from a uniform distribution. Here, we will initialize weights randomly from the range [0.0, 1.0]. The model uses the binary cross entropy loss function and is optimized using stochastic gradient descent with a learning rate of 0.01 and a large momentum of 0.9. The model is trained for 500 training epochs and the test dataset is evaluated at the end of each epoch along with the training dataset. After the model is fit, it is evaluated on both the train and test dataset and the accuracy scores are displayed. Finally, the accuracy of the model during each step of training is graphed as a line plot, showing the dynamics of the model as it learned the problem. Tying all of this together, the complete example is listed below. Running the example fits the model in just a few seconds. The model performance on the train and test sets is calculated and displayed. Your specific results may vary given the stochastic nature of the learning algorithm. Consider running the example a few times. We can see that in this case, the model learned the problem well, achieving an accuracy of about 81.6% on both the train and test datasets. A line plot of model accuracy on the train and test sets is created, showing the change in performance over all 500 training epochs. The plot suggests, for this run, that the performance begins to slow around epoch 300 at about 80% accuracy for both the train and test sets. Line Plot of Train and Test Set Accuracy Over Training Epochs for MLP in the Two Circles Problem Now that we have seen how to develop a classical MLP using the tanh activation function for the two circles problem, we can look at modifying the model to have many more hidden layers. Traditionally, developing deep Multilayer Perceptron models was challenging. Deep models using the hyperbolic tangent activation function do not train easily, and much of this poor performance is blamed on the vanishing gradient problem. We can attempt to investigate this using the MLP model developed in the previous section. The number of hidden layers can be increased from 1 to 5; for example: We can then re-run the example and review the results. The complete example of the deeper MLP is listed below. Running the example first prints the performance of the fit model on the train and test datasets. Your specific results may vary given the stochastic nature of the learning algorithm. Consider running the example a few times. In this case, we can see that performance is quite poor on both the train and test sets achieving around 50% accuracy. This suggests that the model as configured could not learn the problem nor generalize a solution. The line plots of model accuracy on the train and test sets during training tell a similar story. We can see that performance is bad and actually gets worse as training progresses. Line Plot of Train and Test Set Accuracy of Over Training Epochs for Deep MLP in the Two Circles Problem The rectified linear activation function has supplanted the hyperbolic tangent activation function as the new preferred default when developing Multilayer Perceptron networks, as well as other network types like CNNs. This is because the activation function looks and acts like a linear function, making it easier to train and less likely to saturate, but is, in fact, a nonlinear function, forcing negative inputs to the value 0. It is claimed as one possible approach to addressing the vanishing gradients problem when training deeper models. When using the rectified linear activation function (or ReLU for short), it is good practice to use the He weight initialization scheme. We can define the MLP with five hidden layers using ReLU and He initialization, listed below. Tying this together, the complete code example is listed below. Running the example prints the performance of the model on the train and test datasets. Your specific results may vary given the stochastic nature of the learning algorithm. Consider running the example a few times. In this case, we can see that this small change has allowed the model to learn the problem, achieving about 84% accuracy on both datasets, outperforming the single layer model using the tanh activation function. A line plot of model accuracy on the train and test sets over training epochs is also created. The plot shows quite different dynamics to what we have seen so far. The model appears to rapidly learn the problem, converging on a solution in about 100 epochs. Line Plot of Train and Test Set Accuracy of Over Training Epochs for Deep MLP with ReLU in the Two Circles Problem Use of the ReLU activation function has allowed us to fit a much deeper model for this simple problem, but this capability does not extend infinitely. For example, increasing the number of layers results in slower learning to a point at about 20 layers where the model is no longer capable of learning the problem, at least with the chosen configuration. For example, below is a line plot of train and test accuracy of the same model with 15 hidden layers that shows that it is still capable of learning the problem. Line Plot of Train and Test Set Accuracy of Over Training Epochs for Deep MLP with ReLU with 15 Hidden Layers Below is a line plot of train and test accuracy over epochs with the same model with 20 layers, showing that the configuration is no longer capable of learning the problem. Line Plot of Train and Test Set Accuracy of Over Training Epochs for Deep MLP with ReLU with 20 Hidden Layers Although use of the ReLU worked, we cannot be confident that use of the tanh function failed because of vanishing gradients and ReLU succeed because it overcame this problem. This section assumes that you are using the TensorFlow backend with Keras. If this is not the case, you can skip this section. In the cases of using the tanh activation function, we know the network has more than enough capacity to learn the problem, but the increase in layers has prevented it from doing so. It is hard to diagnose a vanishing gradient as a cause for bad performance. One possible signal is to review the average size of the gradient per layer per training epoch. We would expect layers closer to the output to have a larger average gradient than those layers closer to the input. Keras provides the TensorBoard callback that can be used to log properties of the model during training such as the average gradient per layer. These statistics can then be reviewed using the TensorBoard interface that is provided with TensorFlow. We can configure this callback to record the average gradient per-layer per-training epoch, then ensure the callback is used as part of training the model. We can use this callback to first investigate the dynamics of the gradients in the deep model fit using the hyperbolic tangent activation function, then later compare the dynamics to the same model fit using the rectified linear activation function. First, the complete example of the deep MLP model using tanh and the TensorBoard callback is listed below. Running the example creates a new ¡°logs/¡± subdirectory with a file containing the statistics recorded by the callback during training. We can review the statistics in the TensorBoard web interface. The interface can be started from the command line, requiring that you specify the full path to your logs directory. For example, if you run the code in a ¡°/code¡± directory, then the full path to the logs directory will be ¡°/code/logs/¡°. Below is the command to start the TensorBoard interface to be executed on your command line (command prompt). Be sure to change the path to your logs directory. Next, open your web browser and enter the following URL: If all went well, you will see the TensorBoard web interface. Plots of the average gradient per layer per training epoch can be reviewed under the ¡°Distributions¡± and ¡°Histograms¡± tabs of the interface. The plots can be filtered to only show the gradients for the Dense layers, excluding the bias, using the search filter ¡°kernel_0_grad¡°. I have provided a copy of the plots below, although your specific results may vary given the stochastic nature of the learning algorithm. First, line plots are created for each of the 6 layers (5 hidden, 1 output). The names of the plots indicate the layer, where ¡°dense_1¡± indicates the hidden layer after the input layer and ¡°dense_6¡± represents the output layer. We can see that the output layer has a lot of activity over the entire run, with average gradients per epoch at around 0.05 to 0.1. We can also see some activity in the first hidden layer with a similar range. Therefore, gradients are getting through to the first hidden layer, but the last layer and last hidden layer is seeing most of the activity. TensorBoard Line Plots of Average Gradients Per Layer for Deep MLP With Tanh TensorBoard Density Plots of Average Gradients Per Layer for Deep MLP With Tanh We can collect the same information from the deep MLP with the ReLU activation function. The complete example is listed below. The TensorBoard interface can be confusing if you are new to it. To keep things simple, delete the ¡°logs¡± subdirectory prior to running this second example. Once run, you can start the TensorBoard interface the same way and access it through your web browser. The plots of the average gradient per layer per training epoch show a different story as compared to the gradients for the deep model with tanh. We can see that the first hidden layer sees more gradients, more consistently with larger spread, perhaps 0.2 to 0.4, as opposed to 0.05 and 0.1 seen with tanh. We can also see that the middle hidden layers see large gradients. TensorBoard Line Plots of Average Gradients Per Layer for Deep MLP With ReLU TensorBoard Density Plots of Average Gradients Per Layer for Deep MLP With ReLU The ReLU activation function is allowing more gradient to flow backward through the model during training, and this may be the cause for improved performance. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to diagnose a vanishing gradient problem when training a neural network model and how to fix it using an alternate activation function and weight initialization scheme. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Better Deep Learning It provides self-study tutorials on topics like: weight decay, batch normalization, dropout, model stacking and much more¡¦ Skip the Academics. Just Results. Click to learn more. Comment  Name (required)  Email (will not be published) (required)  Website"
"mastery",2019-01-09,"A Gentle Introduction to the Rectified Linear Activation Function for Deep Learning Neural Networks","https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/","In a neural network, the activation function is responsible for transforming the summed weighted input from the node into the activation of the node or output for that input. The rectified linear activation function is a piecewise linear function that will output the input directly if is positive, otherwise, it will output zero. It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance. In this tutorial, you will discover the rectified linear activation function for deep learning neural networks. After completing this tutorial, you will know: Let¡¯s get started. A Gentle Introduction to the Rectified Linear Activation Function for Deep Learning Neural NetworksPhoto by Bureau of Land Management, some rights reserved. This tutorial is divided into six parts; they are: A neural network is comprised of layers of nodes and learns to map examples of inputs to outputs. For a given node, the inputs are multiplied by the weights in a node and summed together. This value is referred to as the summed activation of the node. The summed activation is then transformed via an activation function and defines the specific output or ¡°activation¡± of the node. The simplest activation function is referred to as the linear activation, where no transform is applied at all. A network comprised of only linear activation functions is very easy to train, but cannot learn complex mapping functions. Linear activation functions are still used in the output layer for networks that predict a quantity (e.g. regression problems). Nonlinear activation functions are preferred as they allow the nodes to learn more complex structures in the data. Traditionally, two widely used nonlinear activation functions are the sigmoid and hyperbolic tangent activation functions. The sigmoid activation function, also called the logistic function, is traditionally a very popular activation function for neural networks. The input to the function is transformed into a value between 0.0 and 1.0. Inputs that are much larger than 1.0 are transformed to the value 1.0, similarly, values much smaller than 0.0 are snapped to 0.0. The shape of the function for all possible inputs is an S-shape from zero up through 0.5 to 1.0. For a long time, through the early 1990s, it was the default activation used on neural networks. The hyperbolic tangent function, or tanh for short, is a similar shaped nonlinear activation function that outputs values between -1.0 and 1.0. In the later 1990s and through the 2000s, the tanh function was preferred over the sigmoid activation function as models that used it were easier to train and often had better predictive performance. ¡¦ the hyperbolic tangent activation function typically performs better than the logistic sigmoid. <U+2014> Page 195, Deep Learning, 2016. A general problem with both the sigmoid and tanh functions is that they saturate. This means that large values snap to 1.0 and small values snap to -1 or 0 for tanh and sigmoid respectively. Further, the functions are only really sensitive to changes around their mid-point of their input, such as 0.5 for sigmoid and 0.0 for tanh. The limited sensitivity and saturation of the function happen regardless of whether the summed activation from the node provided as input contains useful information or not. Once saturated, it becomes challenging for the learning algorithm to continue to adapt the weights to improve the performance of the model. ¡¦ sigmoidal units saturate across most of their domain<U+2014>they saturate to a high value when z is very positive, saturate to a low value when z is very negative, and are only strongly sensitive to their input when z is near 0. <U+2014> Page 195, Deep Learning, 2016. Finally, as the capability of hardware increased through GPUs¡¯ very deep neural networks using sigmoid and tanh activation functions could not easily be trained. Layers deep in large networks using these nonlinear activation functions fail to receive useful gradient information. Error is back propagated through the network and used to update the weights. The amount of error decreases dramatically with each additional layer through which it is propagated, given the derivative of the chosen activation function. This is called the vanishing gradient problem and prevents deep (multi-layered) networks from learning effectively. Vanishing gradients make it difficult to know which direction the parameters should move to improve the cost function <U+2014> Page 290, Deep Learning, 2016. Although the use of nonlinear activation functions allows neural networks to learn complex mapping functions, they effectively prevent the learning algorithm from working with deep networks. Workarounds were found in the late 2000s and early 2010s using alternate network types such as Boltzmann machines and layer-wise training or unsupervised pre-training. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In order to use stochastic gradient descent with backpropagation of errors to train deep neural networks, an activation function is needed that looks and acts like a linear function, but is, in fact, a nonlinear function allowing complex relationships in the data to be learned. The function must also provide more sensitivity to the activation sum input and avoid easy saturation. The solution had been bouncing around in the field for some time, although was not highlighted until papers in 2009 and 2011 shone a light on it. The solution is to use the rectified linear activation function, or ReL for short. A node or unit that implements this activation function is referred to as a rectified linear activation unit, or ReLU for short. Often, networks that use the rectifier function for the hidden layers are referred to as rectified networks. Adoption of ReLU may easily be considered one of the few milestones in the deep learning revolution, e.g. the techniques that now permit the routine development of very deep neural networks. [another] major algorithmic change that has greatly improved the performance of feedforward networks was the replacement of sigmoid hidden units with piecewise linear hidden units, such as rectified linear units. <U+2014> Page 226, Deep Learning, 2016. The rectified linear activation function is a simple calculation that returns the value provided as input directly, or the value 0.0 if the input is 0.0 or less. We can describe this using a simple if-statement: We can describe this function g() mathematically using the max() function over the set of 0.0 and the input z; for example: The function is linear for values greater than zero, meaning it has a lot of the desirable properties of a linear activation function when training a neural network using backpropagation. Yet, it is a nonlinear function as negative values are always output as zero. Because rectified linear units are nearly linear, they preserve many of the properties that make linear models easy to optimize with gradient-based methods. They also preserve many of the properties that make linear models generalize well. <U+2014> Page 175, Deep Learning, 2016. Because the rectified function is linear for half of the input domain and nonlinear for the other half, it is referred to as a piecewise linear function or a hinge function. However, the function remains very close to linear, in the sense that is a piecewise linear function with two linear pieces. <U+2014> Page 175, Deep Learning, 2016. Now that we are familiar with the rectified linear activation function, let¡¯s look at how we can implement it in Python. We can implement the rectified linear activation function easily in Python. Perhaps the simplest implementation is using the max() function; for example: We expect that any positive value will be returned unchanged whereas an input value of 0.0 or a negative value will be returned as the value 0.0. Below are a few examples of inputs and outputs of the rectified linear activation function. Running the example, we can see that positive values are returned regardless of their size, whereas negative values are snapped to the value 0.0. We can get an idea of the relationship between inputs and outputs of the function by plotting a series of inputs and the calculated outputs. The example below generates a series of integers from -10 to 10 and calculates the rectified linear activation for each input, then plots the result. Running the example creates a line plot showing that all negative values and zero inputs are snapped to 0.0, whereas the positive outputs are returned as-is, resulting in a linearly increasing slope, given that we created a linearly increasing series of positive values (e.g. 1 to 10). Line Plot of Rectified Linear Activation for Negative and Positive Inputs The derivative of the rectified linear function is also easy to calculate. Recall that the derivative of the activation function is required when updating the weights of a node as part of the backpropagation of error. The derivative of the function is the slope. The slope for negative values is 0.0 and the slope for positive values is 1.0. Traditionally, the field of neural networks has avoided any activation function that was not completely differentiable, perhaps delaying the adoption of the rectified linear function and other piecewise-linear functions. Technically, we cannot calculate the derivative when the input is 0.0, therefore, we can assume it is zero. This is not a problem in practice. For example, the rectified linear function g(z) = max{0, z} is not differentiable at z = 0. This may seem like it invalidates g for use with a gradient-based learning algorithm. In practice, gradient descent still performs well enough for these models to be used for machine learning tasks. <U+2014> Page 192, Deep Learning, 2016. Using the rectified linear activation function offers many advantages; let¡¯s take a look at a few in the next section. The rectified linear activation function has rapidly become the default activation function when developing most types of neural networks. As such, it is important to take a moment to review some of the benefits of the approach, first highlighted by Xavier Glorot, et al. in their milestone 2012 paper on using ReLU titled ¡°Deep Sparse Rectifier Neural Networks¡°. The rectifier function is trivial to implement, requiring a max() function. This is unlike the tanh and sigmoid activation function that require the use of an exponential calculation. Computations are also cheaper: there is no need for computing the exponential function in activations <U+2014> Deep Sparse Rectifier Neural Networks, 2011. An important benefit of the rectifier function is that it is capable of outputting a true zero value. This is unlike the tanh and sigmoid activation functions that learn to approximate a zero output, e.g. a value very close to zero, but not a true zero value. This means that negative inputs can output true zero values allowing the activation of hidden layers in neural networks to contain one or more true zero values. This is called a sparse representation and is a desirable property in representational learning as it can accelerate learning and simplify the model. An area where efficient representations such as sparsity are studied and sought is in autoencoders, where a network learns a compact representation of an input (called the code layer), such as an image or series, before it is reconstructed from the compact representation. One way to achieve actual zeros in h for sparse (and denoising) autoencoders [¡¦] The idea is to use rectified linear units to produce the code layer. With a prior that actually pushes the representations to zero (like the absolute value penalty), one can thus indirectly control the average number of zeros in the representation. <U+2014> Page 507, Deep Learning, 2016. The rectifier function mostly looks and acts like a linear activation function. In general, a neural network is easier to optimize when its behavior is linear or close to linear. Rectified linear units [¡¦] are based on the principle that models are easier to optimize if their behavior is closer to linear. <U+2014> Page 194, Deep Learning, 2016. Key to this property is that networks trained with this activation function almost completely avoid the problem of vanishing gradients, as the gradients remain proportional to the node activations. Because of this linearity, gradients flow well on the active paths of neurons (there is no gradient vanishing effect due to activation non-linearities of sigmoid or tanh units). <U+2014> Deep Sparse Rectifier Neural Networks, 2011. Importantly, the (re-)discovery and adoption of the rectified linear activation function meant that it became possible to exploit improvements in hardware and successfully train deep multi-layered networks with a nonlinear activation function using backpropagation. In turn, cumbersome networks such as Boltzmann machines could be left behind as well as cumbersome training schemes such as layer-wise training and unlabeled pre-training. ¡¦ deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training. <U+2014> Deep Sparse Rectifier Neural Networks, 2011. In this section, we¡¯ll take a look at some tips when using the rectified linear activation function in your own deep learning neural networks. For a long time, the default activation to use was the sigmoid activation function. Later, it was the tanh activation function. For modern deep learning neural networks, the default activation function is the rectified linear activation function. Prior to the introduction of rectified linear units, most neural networks used the logistic sigmoid activation function or the hyperbolic tangent activation function. <U+2014> Page 195, Deep Learning, 2016. Most papers that achieve state-of-the-art results will describe a network using ReLU. For example, in the milestone 2012 paper by Alex Krizhevsky, et al. titled ¡°ImageNet Classification with Deep Convolutional Neural Networks,¡± the authors developed a deep convolutional neural network with ReLU activations that achieved state-of-the-art results on the ImageNet photo classification dataset. ¡¦ we refer to neurons with this nonlinearity as Rectified Linear Units (ReLUs). Deep convolutional neural networks with ReLUs train several times faster than their equivalents with tanh units. If in doubt, start with ReLU in your neural network, then perhaps try other piecewise linear activation functions to see how their performance compares. In modern neural networks, the default recommendation is to use the rectified linear unit or ReLU <U+2014> Page 174, Deep Learning, 2016. The ReLU can be used with most types of neural networks. It is recommended as the default for both Multilayer Perceptron (MLP) and Convolutional Neural Networks (CNNs). The use of ReLU with CNNs has been investigated thoroughly, and almost universally results in an improvement in results, initially, surprisingly so. ¡¦ how do the non-linearities that follow the filter banks influence the recognition accuracy. The surprising answer is that using a rectifying non-linearity is the single most important factor in improving the performance of a recognition system. <U+2014> What is the best multi-stage architecture for object recognition?, 2009 Work investigating ReLU with CNNs is what provoked their use with other network types. [others] have explored various rectified nonlinearities [¡¦] in the context of convolutional networks and have found them to improve discriminative performance. <U+2014> Rectified Linear Units Improve Restricted Boltzmann Machines, 2010. When using ReLU with CNNs, they can be used as the activation function on the filter maps themselves, followed then by a pooling layer. A typical layer of a convolutional network consists of three stages [¡¦] In the second stage, each linear activation is run through a nonlinear activation function, such as the rectified linear activation function. This stage is sometimes called the detector stage. <U+2014> Page 339, Deep Learning, 2016. Traditionally, LSTMs use the tanh activation function for the activation of the cell state and the sigmoid activation function for the node output. Given their careful design, ReLU were thought to not be appropriate for Recurrent Neural Networks (RNNs) such as the Long Short-Term Memory Network (LSTM) by default. At first sight, ReLUs seem inappropriate for RNNs because they can have very large outputs so they might be expected to be far more likely to explode than units that have bounded values. <U+2014> A Simple Way to Initialize Recurrent Networks of Rectified Linear Units, 2015. Nevertheless, there has been some work on investigating the use of ReLU as the output activation in LSTMs, the result of which is a careful initialization of network weights to ensure that the network is stable prior to training. This is outlined in the 2015 paper titled ¡°A Simple Way to Initialize Recurrent Networks of Rectified Linear Units.¡± The bias is the input on the node that has a fixed value. The bias has the effect of shifting the activation function and it is traditional to set the bias input value to 1.0. When using ReLU in your network, consider setting the bias to a small value, such as 0.1. ¡¦ it can be a good practice to set all elements of [the bias] to a small, positive value, such as 0.1. This makes it very likely that the rectified linear units will be initially active for most inputs in the training set and allow the derivatives to pass through. <U+2014> Page 193, Deep Learning, 2016. There are some conflicting reports as to whether this is required, so compare performance to a model with a 1.0 bias input. Before training a neural network,the weights of the network must be initialized to small random values. When using ReLU in your network and initializing weights to small random values centered on zero, then by default half of the units in the network will output a zero value. For example, after uniform initialization of the weights, around 50% of hidden units continuous output values are real zeros <U+2014> Deep Sparse Rectifier Neural Networks, 2011. There are many heuristic methods to initialize the weights for a neural network, yet there is no best weight initialization scheme and little relationship beyond general guidelines for mapping weight initialization schemes to the choice of activation function. Prior to the wide adoption of ReLU, Xavier Glorot and Yoshua Bengio proposed an initialization scheme in their 2010 paper titled ¡°Understanding the difficulty of training deep feedforward neural networks¡± that quickly became the default when using sigmoid and tanh activation functions, generally referred to as ¡°Xavier initialization¡°. Weights are set at random values sampled uniformly from a range proportional to the size of the number of nodes in the previous layer (specifically +/- 1/sqrt(n) where n is the number of nodes in the prior layer). Kaiming He, et al. in their 2015 paper titled ¡°Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification¡± suggested that Xavier initialization and other schemes were not appropriate for ReLU and extensions. Glorot and Bengio proposed to adopt a properly scaled uniform distribution for initialization. This is called ¡°Xavier¡± initialization [¡¦]. Its derivation is based on the assumption that the activations are linear. This assumption is invalid for ReLU <U+2014> Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, 2015. They proposed a small modification of Xavier initialization to make it suitable for use with ReLU, now commonly referred to as ¡°He initialization¡± (specifically +/- 2/sqrt(n) where n is the number of nodes in the prior layer). In practice, both Gaussian and uniform versions of the scheme can be used. It is good practice to scale input data prior to using a neural network. This may involve standardizing variables to have a zero mean and unit variance or normalizing each value to the scale 0-to-1. Without data scaling on many problems, the weights of the neural network can grow large, making the network unstable and increasing the generalization error. This good practice of scaling inputs applies whether using ReLU for your network or not. By design, the output from ReLU is unbounded in the positive domain. This means that in some cases, the output can continue to grow in size. As such, it may be a good idea to use a form of weight regularization, such as an L1 or L2 vector norm. Another problem could arise due to the unbounded behavior of the activations; one may thus want to use a regularizer to prevent potential numerical problems. Therefore, we use the L1 penalty on the activation values, which also promotes additional sparsity <U+2014> Deep Sparse Rectifier Neural Networks, 2011. This can be a good practice to both promote sparse representations (e.g. with L1 regularization) and reduced generalization error of the model. The ReLU does have some limitations. Key among the limitations of ReLU is the case where large weight updates can mean that the summed input to the activation function is always negative, regardless of the input to the network. This means that a node with this problem will forever output an activation value of 0.0. This is referred to as a ¡°dying ReLU¡°. the gradient is 0 whenever the unit is not active. This could lead to cases where a unit never activates as a gradient-based optimization algorithm will not adjust the weights of a unit that never activates initially. Further, like the vanishing gradients problem, we might expect learning to be slow when training ReL networks with constant 0 gradients. <U+2014> Rectifier Nonlinearities Improve Neural Network Acoustic Models, 2013. Some popular extensions to the ReLU relax the non-linear output of the function to allow small negative values in some way. The Leaky ReLU (LReLU or LReL) modifies the function to allow small negative values when the input is less than zero. The leaky rectifier allows for a small, non-zero gradient when the unit is saturated and not active <U+2014> Rectifier Nonlinearities Improve Neural Network Acoustic Models, 2013. The Exponential Linear Unit, or ELU, is a generalization of the ReLU that uses a parameterized exponential function to transition from the positive to small negative values. ELUs have negative values which pushes the mean of the activations closer to zero. Mean activations that are closer to zero enable faster learning as they bring the gradient closer to the natural gradient <U+2014> Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs), 2016. The Parametric ReLU, or PReLU, learns parameters that control the shape and leaky-ness of the function. ¡¦ we propose a new generalization of ReLU, which we call Parametric Rectified Linear Unit (PReLU). This activation function adaptively learns the parameters of the rectifiers <U+2014> Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, 2015. Maxout is an alternative piecewise linear function that returns the maximum of the inputs, designed to be used in conjunction with the dropout regularization technique. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout¡¯s fast approximate model averaging technique. <U+2014> Maxout Networks, 2013. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered the rectified linear activation function for deep learning neural networks. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Better Deep Learning It provides self-study tutorials on topics like: weight decay, batch normalization, dropout, model stacking and much more¡¦ Skip the Academics. Just Results. Click to learn more. This was a great explanation. Thank you. Thanks, I¡¯m glad it helped. Excellent explanation and supplemented well with additional resources. Thank you, Jason. I¡¯m glad it helped. How can we analyse the performance of nn. Is it when mean squared error is minimum and validation testing and training graphs coincide. In what all ways we can analyse it¡¯s performance. It really depends on the specifics of the problem.  Mean MSE across multiple runs might make sense for a regression predictive modeling problem. Comment  Name (required)  Email (will not be published) (required)  Website"
"mastery",2019-01-07,"How to Create an Equally, Linearly, and Exponentially Weighted Average of Neural Network Model Weights in Keras","https://machinelearningmastery.com/polyak-neural-network-model-weight-ensemble/","The training process of neural networks is a challenging optimization process that can often fail to converge. This can mean that the model at the end of training may not be a stable or best-performing set of weights to use as a final model. One approach to address this problem is to use an average of the weights from multiple models seen toward the end of the training run. This is called Polyak-Ruppert averaging and can be further improved by using a linearly or exponentially decreasing weighted average of the model weights. In addition to resulting in a more stable model, the performance of the averaged model weights can also result in better performance. In this tutorial, you will discover how to combine the weights from multiple different models into a single model for making predictions. After completing this tutorial, you will know: Let¡¯s get started. How to Create an Equally, Linearly, and Exponentially Weighted Average of Neural Network Model Weights in KerasPhoto by netselesoobrazno, some rights reserved. This tutorial is divided into seven parts; they are: Learning the weights for a deep neural network model requires solving a high-dimensional non-convex optimization problem. A challenge with solving this optimization problem is that there are many ¡°good¡± solutions and it is possible for the learning algorithm to bounce around and fail to settle in on one. In the area of stochastic optimization, this is referred to as problems with the convergence of the optimization algorithm on a solution, where a solution is defined by a set of specific weight values. A symptom you may see if you have a problem with the convergence of your model is train and/or test loss value that shows higher than expected variance, e.g. it thrashes or bounces up and down over training epochs. One approach to address this problem is to combine the weights collected towards the end of the training process. Generally, this might be referred to as temporal averaging and is known as Polyak Averaging or Polyak-Ruppert averaging, named for the original developers of the method. Polyak averaging consists of averaging together several points in the trajectory through parameter space visited by an optimization algorithm. <U+2014> Page 322, Deep Learning, 2016. Averaging multiple noisy sets of weights during the learning process may paradoxically sound less desirable than tuning the optimization process itself, but may prove a desirable solution, especially for very large neural networks that may take days, weeks, or even months to train. The essential advancement was reached on the basis of the paradoxical idea: a slow algorithm having less than optimal convergence rate must be averaged. <U+2014> Acceleration of Stochastic Approximation by Averaging, 1992. Averaging the weights of multiple models from a single training run has the effect of calming down the noisy optimization process that may be noisy because of the choice of learning hyperparameters (e.g. learning rate) or the shape of the mapping function that is being learned. The result is a final model or set of weights that may offer a more stable, and perhaps more accurate result. The basic idea is that the optimization algorithm may leap back and forth across a valley several times without ever visiting a point near the bottom of the valley. The average of all of the locations on either side should be close to the bottom of the valley though. <U+2014> Page 322, Deep Learning, 2016. The simplest implementation of Polyak-Ruppert averaging involves calculating the average of the weights of the models over the last few training epochs. This can be improved by calculating a weighted average, where more weight is applied to more recent models, which is linearly decreased through prior epochs. An alternative and more widely used approach is to use an exponential decay in the weighted average. Polyak-Ruppert averaging has been shown to improve the convergence of standard SGD [¡¦] . Alternatively, an exponential moving average over the parameters can be used, giving higher weight to more recent parameter value. <U+2014> Adam: A Method for Stochastic Optimization, 2014. Using an average or weighted average of model weights in the final model is a common technique in practice for ensuring the very best results are achieved from the training run. The approach is one of many ¡°tricks¡± used in the Google Inception V2 and V3 deep convolutional neural network models for photo classification, a milestone in the field of deep learning. Model evaluations are performed using a running average of the parameters computed over time. <U+2014> Rethinking the Inception Architecture for Computer Vision, 2015. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course We will use a small multi-class classification problem as the basis to demonstrate the model weight ensemble. The scikit-learn class provides the make_blobs() function that can be used to create a multi-class classification problem with the prescribed number of samples, input variables, classes, and variance of samples within a class. The problem has two input variables (to represent the x and y coordinates of the points) and a standard deviation of 2.0 for points within each group. We will use the same random state (seed for the pseudorandom number generator) to ensure that we always get the same data points. The results are the input and output elements of a dataset that we can model. In order to get a feeling for the complexity of the problem, we can plot each point on a two-dimensional scatter plot and color each point by class value. The complete example is listed below. Running the example creates a scatter plot of the entire dataset. We can see that the standard deviation of 2.0 means that the classes are not linearly separable (separable by a line) causing many ambiguous points. This is desirable as it means that the problem is non-trivial and will allow a neural network model to find many different ¡°good enough¡± candidate solutions resulting in a high variance. Scatter Plot of Blobs Dataset With Three Classes and Points Colored by Class Value Before we define a model, we need to contrive a problem that is appropriate for the ensemble. In our problem, the training dataset is relatively small. Specifically, there is a 10:1 ratio of examples in the training dataset to the holdout dataset. This mimics a situation where we may have a vast number of unlabeled examples and a small number of labeled examples with which to train a model. We will create 1,100 data points from the blobs problem. The model will be trained on the first 100 points and the remaining 1,000 will be held back in a test dataset, unavailable to the model. The problem is a multi-class classification problem, and we will model it using a softmax activation function on the output layer. This means that the model will predict a vector with three elements with the probability that the sample belongs to each of the three classes. Therefore, we must one hot encode the class values before we split the rows into the train and test datasets. We can do this using the Keras to_categorical() function. Next, we can define and compile the model. The model will expect samples with two input variables. The model then has a single hidden layer with 25 nodes and a rectified linear activation function, then an output layer with three nodes to predict the probability of each of the three classes and a softmax activation function. Because the problem is multi-class, we will use the categorical cross entropy loss function to optimize the model and stochastic gradient descent with a small learning rate and momentum. The model is fit for 500 training epochs and we will evaluate the model each epoch on the test set, using the test set as a validation set. At the end of the run, we will evaluate the performance of the model on the train and test sets. Then finally, we will plot learning curves of the model accuracy over each training epoch on both the training and validation datasets. Tying all of this together, the complete example is listed below. Running the example prints the performance of the final model on the train and test datasets. Your specific results will vary (by design!) given the high variance nature of the model. In this case, we can see that the model achieved about 86% accuracy on the training dataset, which we know is optimistic, and about 81% on the test dataset, which we would expect to be more realistic. A line plot is also created showing the learning curves for the model accuracy on the train and test sets over each training epoch. We can see that training accuracy is more optimistic over most of the run, as we also noted with the final scores. Importantly, we do see a reasonable amount of variance in the accuracy during training on both the train and test datasets, potentially providing a good basis for using model weight averaging. Line Plot Learning Curves of Model Accuracy on Train and Test Dataset over Each Training Epoch One approach to the model weight ensemble is to keep a running average of model weights in memory. There are three downsides to this approach: An alternative is to save model weights to file during training as a first step, and later combine the weights from the saved models in order to make a final model. Perhaps the simplest way to implement this is to manually drive the training process, one epoch at a time, then save models at the end of the epoch if we have exceeded an upper limit on the number of epochs. For example, with our test problem, we will train the model for 500 epochs and perhaps save models from epoch 490 onwards (e.g. between and including epochs 490 and 499). Models can be saved to file using the save() function on the model and specifying a filename that includes the epoch number. Note, saving and loading neural network models in Keras requires that you have the h5py library installed. You can install this library using pip as follows: Tying all of this together, the complete example of fitting the model on the training dataset and saving all models from the last 10 epochs is listed below. Running the example saves 10 models into the current working directory. We can create a new model from multiple existing models with the same architecture. First, we need to load the models into memory. This is reasonable as the models are small. If you are working with very large models, it might be easier to load models one at a time and average the weights in memory. The load_model() Keras function can be used to load a saved model from file. The function load_all_models() below will load models from the current working directory. It takes the start and end epochs as arguments so that you can experiment with different groups of models saved over contiguous epochs. We can call the function to load all of the models. Once loaded, we can create a new model with the weighted average of the model weights. Each model has a get_weights() function that returns a list of arrays, one for each layer in the model. We can enumerate each layer in the model, retrieve the same layer from each model, and calculate the weighted average. This will give us a set of weights. We can then use the clone_model() Keras function to create a clone of the architecture and call set_weights() function to use the average weights we have prepared. The model_weight_ensemble() function below implements this. Tying these elements together, we can load the 10 models and calculate the equally weighted average (arithmetic average) of the model weights. The complete listing is provided below. Running the example first loads the 10 models from file. A model weight ensemble is created from these 10 models giving equal weight to each model and a summary of the model structure is reported. Now that we know how to calculate a weighted average of model weights, we can evaluate predictions with the resulting model. One issue is that we don¡¯t know how many models are appropriate to combine in order to achieve good performance. We can address this by evaluating model weight averaging ensembles with the last n models and vary n to see how many models results in good performance. The evaluate_n_members() function below will create a new model from a given number of loaded models. Each model is given an equal weight in contributing to the final model, then the model_weight_ensemble() function is called to create the final model that is then evaluated on the test dataset. Importantly, the list of loaded models is reversed first to ensure that the last n models in the training run are used, which we would assume might have better performance on average. We can then evaluate models created from different numbers of the last n models saved from the training run from the last 1-model to the last 10 models. In addition to evaluating the combined final model, we can also evaluate each saved standalone model on the test dataset to compare performance. The collected scores can be plotted, with blue dots for the accuracy of the single saved models and the orange line for the test accuracy for the model that combines the weights the last n models. Tying all of this together, the complete example is listed below. Running the example first loads the 10 saved models. The performance of each individually saved model is reported as well as an ensemble model with weights averaged from all models up to and including each model, working backward from the end of the training run. The results show that the best test accuracy was about 81.4% achieved by the last two models. We can see that the test accuracy of the model weight ensemble levels out the performance and performs just as well. Your specific results will vary based on the models saved during the previous section. A line plot is also created showing the test accuracy of each single model (blue dots) and the performance of the model weight ensemble (orange line). We can see that averaging the model weights does level out the performance of the final model and performs at least as well as the final model of the run. Line Plot of Single Model Test Performance (blue dots) and Model Weight Ensemble Test Performance (orange line) We can update the example and evaluate a linearly decreasing weighting of the model weights in the ensemble. The weights can be calculated as follows: This can be used instead of the equal weights in the evaluate_n_members() function. The complete example is listed below. Running the example reports the performance of each single model again, and this time the test accuracy of each average model weight ensemble with a linearly decreasing contribution of models. We can see that, at least in this case, the ensemble achieves a small bump in performance above any standalone model to about 81.5% accuracy. The line plot shows the bump in performance and shows a more stable performance in terms of test accuracy over the different sized ensembles created, as compared to the use of an evenly weighted ensemble. Line Plot of Single Model Test Performance (blue dots) and Model Weight Ensemble Test Performance (orange line) With a Linear Decay We can also experiment with an exponential decay of the contribution of models. This requires that a decay rate (alpha) is specified. The example below creates weights for an exponential decay with a decrease rate of 2. The complete example with an exponential decay for the contribution of models to the average weights in the ensemble model is listed below. Running the example shows a small improvement in performance much like the use of a linear decay in the weighted average of the saved models. The line plot of the test accuracy scores shows the stronger stabilizing effect of using the exponential decay instead of the linear or equal weighting of models. Line Plot of Single Model Test Performance (blue dots) and Model Weight Ensemble Test Performance (orange line) With an Exponential Decay This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to combine the weights from multiple different models into a single model for making predictions. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Better Deep Learning It provides self-study tutorials on topics like: weight decay, batch normalization, dropout, model stacking and much more¡¦ Skip the Academics. Just Results. Click to learn more. Thanks for the article Jason!
Just wanted to understand how is this averaging of weights more effective than having an adaptive learning rate in optimizer? In effect, the two should be equivalent. Do you mean one model with adaptive learning rate vs an ensemble of models? The ensemble will still reduce the variance and may have the benefit of lifting skill over a single solution. It really depends on the complexity of the problem. Comment  Name (required)  Email (will not be published) (required)  Website"
