"site","date","headline","url_address","text"
"vidhya",2018-11-29,"Tutorial on Text Classification (NLP) using ULMFiT and fastai Library in Python","https://www.analyticsvidhya.com/blog/2018/11/tutorial-text-classification-ulmfit-fastai-library/","Natural Language Processing (NLP) needs no introduction in today¡¯s world. It¡¯s one of the most important fields of study and research, and has seen a phenomenal rise in interest in the last decade. The basics of NLP are widely known and easy to grasp. But things start to get tricky when the text data becomes huge and unstructured. That¡¯s where deep learning becomes so pivotal. Yes, I¡¯m talking about deep learning for NLP tasks <U+2013> a still relatively less trodden path. DL has proven its usefulness in computer vision tasks like image detection, classification and segmentation, but NLP applications like text generation and classification have long been considered fit for traditional ML techniques. Source: Tryolabs And deep learning has certainly made a very positive impact in NLP, as you¡¯ll see in this article. We will focus on the concept of transfer learning and how we can leverage it in NLP to build incredibly accurate models using the popular fastai library. I will introduce you to the ULMFiT framework as well in the process. Note- This article assumes basic familiarity with neural networks, deep learning and transfer learning. If you are new to deep learning, I would strongly recommend reading the following articles first: If you are a beginner in NLP, check out this video course<U+00A0>with 3 real life projects. I praised deep learning in the introduction, and deservedly so. However, everything comes at a price, and deep learning is no different. The biggest challenge in deep learning is the massive data requirements for training the models. It is difficult to find datasets of such huge sizes, and it is way too costly to prepare such datasets. It¡¯s simply not possible for most organizations to come up with them. Another obstacle is the high cost of GPUs needed to run advanced deep learning algorithms. Thankfully, we can use pre-trained state-of-the-art deep learning models and tweak them to work for us. This is known as transfer learning. It is not as resource intensive as training a deep learning model from scratch and produces decent results even on small amounts of training data. This concept will be expanded upon later in the article when we implement our learning on quite a small dataset. Pre-trained models help data scientists start off on a new problem by providing an existing framework they can leverage. You don¡¯t always have to build a model from scratch, especially when someone else has already put in their hard work and effort! And these pre-trained models have proven to be truly effective and useful in the field of computer vision (check out this article to see our pick of the top 10 pre-trained models in CV). Their success is popularly attributed to the Imagenet dataset. It has over 14 million labeled images with over 1 million images also accompanying bounding boxes. This dataset was first published in 2009 and has since become one of the most sought-after image datasets ever. It led to several breakthroughs in deep learning research for computer vision, with transfer learning being one of them. However, in NLP, transfer learning has not been as successful (as compared to computer vision, anyway). Of course we have pre-trained word embeddings like word2vec, GloVe, and fastText, but they are primarily used to initialize only the first layer of a neural network. The rest of the model still needs to be trained from scratch and it requires a huge number of examples to produce a good performance. What do we really need in this case? Like the aforementioned computer vision models, we require a pre-trained model for NLP which can be fine-tuned and used on different text datasets. One of the contenders for pre-trained natural language models is the Universal Language Model Fine-tuning for Text Classification, or ULMFiT (Imagenet dataset [cs.CL]). How does it work? How widespread are it¡¯s applications? How can we make it work in Python? In the rest of this article, we will put ULMFiT to the test by solving a text classification problem and check how well it performs. Proposed by fast.ai¡¯s Jeremy Howard and NUI Galway Insight Center¡¯s Sebastian Ruder, ULMFiT is essentially a method to enable transfer learning for any NLP task and achieve great results. All this, without having to train models from scratch. That got your attention, didn¡¯t it? ULMFiT achieves state-of-the-art result using novel techniques like: This method involves fine-tuning a pre-trained language model (LM), trained on the<U+00A0>Wikitext 103 dataset, to a new dataset in such a manner that it does not forget what it previously learned. Language modeling can be considered a counterpart of Imagenet for NLP. It captures general properties of a language and provides an enormous amount of data which can be fed to other downstream NLP tasks. That is why Language modeling has been chosen as the source task for ULMFiT. I highly encourage you to go through the original ULMFiT<U+00A0>paper<U+00A0>to understand more about how it works, the way Jeremy and Sebastian went about deriving it, and parse through other interesting details. Alright, enough theoretical concepts <U+2013> let¡¯s get our hands dirty by implementing ULMFiT on a dataset and see what the hype is all about. Our objective here is to fine-tune a pre-trained model and use it for text classification on a new dataset. We will implement ULMFiT in this process. The interesting thing here is that this new data is quite small in size (<1000 labeled instances). A neural network model trained from scratch would overfit on such a small dataset. Hence, I would like to see whether ULMFiT does a great job at this task as promised in the paper. Dataset: We will use the 20 Newsgroup dataset available in sklearn.datasets. As the name suggests, it includes text documents from 20 different newsgroups. We will perform the python implementation on Google<U+00A0>Colab instead of our local machines. If you have never worked on colab before, then consider this a bonus! Colab, or Google Colaboratory, is a free cloud service for running Python. One of the best things about it is that it provides GPUs and TPUs for free and hence, it is pretty handy for training deep learning models. So, it doesn¡¯t matter even if you have a system with pretty ordinary hardware specs <U+2013> as long as you have a steady internet connection, you are good to go. The only other requirement is that you must have a Google account. Let¡¯s get started! First, sign in to your Google account. Then select ¡®NEW PYTHON 3 NOTEBOOK¡¯. This notebook is similar to your typical Jupyter Notebook, so you won¡¯t have much trouble working on it if you are familiar with the Jupyter environment. A Colab notebook looks something like the screenshot below: Then go to Runtime, select Change runtime type, then select GPU as the hardware accelerator to utilise GPU for free. Most of the popular libraries like pandas, numpy, matplotlib, nltk, and<U+00A0>keras, come preinstalled with Colab. However, 2 libraries, PyTorch and fastai v1 (which we need in this exercise), will need to be installed manually. So, let¡¯s load them into our Colab environment: Import the dataset which we downloaded earlier. Let¡¯s create a dataframe consisting of the text documents and their corresponding labels (newsgroup names). (11314, 2) We¡¯ll convert this into a binary classification problem by selecting only 2 out of the 20 labels present in the dataset. We will select labels 1 and 10 which correspond to ¡®comp.graphics¡¯ and ¡®rec.sport.hockey¡¯, respectively. Let¡¯s have a quick look at the target distribution. The distribution looks pretty even. Accuracy would be a good evaluation metric to use in this case. It¡¯s always a good practice to feed clean data to your models, especially when the data comes in the form of unstructured text. Let¡¯s clean our text by retaining only alphabets and removing everything else. Now, we will get rid of the stopwords from our text data. If you have never used stopwords before, then you will have to download them from the nltk package as I¡¯ve shown below: Now let¡¯s split our cleaned dataset into training and validation sets in a 60:40 ratio. Perfect! Before proceeding further, we¡¯ll need to prepare our data for the language model and for the classification model separately. The good news? This can be done quite easily using the fastai library: We can use the data_lm object we created earlier to fine-tune a pre-trained language model. We can create a learner object, ¡®learn¡¯, that will directly create a model, download the pre-trained weights, and be ready for fine-tuning: The one cycle and cyclic momentum allows the model to be trained on higher learning rates and converge faster. The one cycle policy provides some form of regularisation. We won¡¯t go into the depth of how this works as this article is about learning the implementation. However, if you wish to know more about one cycle policy, then feel free to refer to this excellent paper by Leslie Smith <U+2013> ¡°A disciplined approach to neural network hyper-parameters: Part 1 <U+2014> learning rate, batch size, momentum, and weight decay¡±. Total time: 00:09 We will save this encoder to use it for classification later. Let¡¯s now use the data_clas object we created earlier to build a classifier with our fine-tuned encoder. We will again try to fit our model. Total time: 00:32 Wow! We got a whopping increase in the accuracy and even the validation loss is far less than the training loss. It is a pretty outstanding performance on a small dataset. You can even get the predictions for the validation set out of the learner object by using the below code: With the emergence of methods like ULMFiT, we are moving towards more generalizable NLP systems. These models would be able to perform multiple tasks at once. Moreover, these models would not be limited just to the English language, but to several other languages spoken across the globe. We also have upcoming techniques like ELMo, a new word embedding technique, and BERT, a new language representation model designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. These techniques have already achieved state-of-the-art results on many NLP tasks. Hence, the golden period for NLP has just arrived and it is here to stay. I hope you found this article helpful. However, there are still a lot more things to explore in ULMFiT using the fastai library which I encourage you guys to go after. If you have any recommendations/suggestions, then feel free to let me know in the comments section below. Also, try to use ULMFiT on different problems and domains of your choice and see how the results pan out. Code: You can find the complete code here. Thanks for reading and happy learning! Nice article. Thanks for sharing. really such a nice article Thanks Ashwin! Nice tutorial. I just walked through it, but I wondered why you removed stop words? I think there is a belief in NLP that it¡¯s always good to remove stop words, but this is often not true. I tried re-running the tutorial but skipped the remove stop words part and I got a 2.4% increase in accuracy. I thought you might want to try that and see if you see the same increase. I¡¯m Glad you liked this tutorial. Yes, you are right that removing stop words does not always help. However, it might work for another dataset and that is why I have included it in this article. Hey, I was able to run succesfully on Google collab. But I am not able to run same code with required library installed on my local machine. It gives following error for line below
` learn = language_model_learner(data_lm, pretrained_model=URLs.WT103, drop_mult=0.7)` `
Traceback (most recent call last):
File ¡°transfer_learning_classification_nlp_rir_classification.py¡±, line 114, in
learn = language_model_learner(data_lm, pretrained_model=URLs.WT103, drop_mult=0.7)
File ¡°/home/abhay/mml/venv-nerapi/lib/python3.6/site-packages/fastai/text/learner.py¡±, line 135, in language_model_learner
model_path = untar_data(pretrained_model, data=False)
File ¡°/home/abhay/mml/venv-nerapi/lib/python3.6/site-packages/fastai/datasets.py¡±, line 108, in untar_data
tarfile.open(fname, ¡®r:gz¡¯).extractall(dest.parent)
File ¡°/home/abhay/mml/venv-nerapi/lib/python3.6/tarfile.py¡±, line 1587, in open
return func(name, filemode, fileobj, **kwargs)
File ¡°/home/abhay/mml/venv-nerapi/lib/python3.6/tarfile.py¡±, line 1641, in gzopen
t = cls.taropen(name, mode, fileobj, **kwargs)
File ¡°/home/abhay/mml/venv-nerapi/lib/python3.6/tarfile.py¡±, line 1617, in taropen
return cls(name, mode, fileobj, **kwargs)
File ¡°/home/abhay/mml/venv-nerapi/lib/python3.6/tarfile.py¡±, line 1480, in __init__
self.firstmember = self.next()
File ¡°/home/abhay/mml/venv-nerapi/lib/python3.6/tarfile.py¡±, line 2310, in next
raise ReadError(¡°empty file¡±)
tarfile.ReadError: empty file"
"vidhya",2018-11-28,"Highlights from DataHack Summit 2018 <U+2013> a Truly Overwhelming and Resounding Success!","https://www.analyticsvidhya.com/blog/2018/11/highlights-from-datahack-summit-2018-a-truly-overwhelming-and-resounding-success/","What do you do when you have to improve upon the best? When you need to deliver what has never been done before? And you need to deliver it at a scale which differentiates itself because of the scale. These are some questions we had when we started thinking about DataHack Summit 2018. We promised you an experience like never before <U+2013> where we would bring together people, machines, and their collaborative experience. A chance to see artificial intelligence in a way no other conference in India has even shown. With more than 1,200 attendees from various diverse industries and domains (more than 400 organizations), DataHack Summit 2018 was an unqualified success. This year¡¯s conference<U+00A0>was even bigger than last year, from the speaker line up and the enriching content, to the massive sprawling venue. There were more than 50+ power talks and hack sessions from the best industry leaders, thought leaders, practitioners, data scientists, and folks from all sorts of data related roles. The venue, NIMHANS Convention Centre, was bigger and better than last year, with three massive auditoriums, all jam packed with data science professionals eager to learn from the best in the business. We would like to thank all our sponsors for making DataHack Summit an unparalleled success. And this was just about the first 2 days! 9 stimulating workshops (yes, 9!) were conducted on topics ranging from Applied Machine Learning to Computer Vision using PyTorch and we received an overwhelmingly positive response on them. Our aim of curating and delivering only the best data science knowledge to our community all culminated and reflected in our content at DataHack Summit 2018. But enough talk <U+2013> here are a few awesome highlights from the blockbuster conference! Kunal Jain, the man behind Analytics Vidhya, kicked things off on Day 1 as he set the tone for entire conference with a superbly eloquent opening speech. He spoke about the significance of DataHack Summit and what lay in store for all the attendees. Kunal also spoke about the importance of ethics in AI later in the day, a very relevant and timely talk on a sensitive subject. Ronald van Loon, Director at Adversitement and a well-respected thought leader, was the keynote speaker on day 1 of DataHack Summit 2018. He spoke about the future of data in the digital enterprise and kept the audience enraptured throughout his 60-minute talk. Wondering how to get your data science career started? Then this panel discussion was the place to be! Featuring Ronald van Loon, Dr. Sarabjot Singh Anand, Rohit Pandharkar, Charanpreet Singh, and moderated by Kunal Jain, a range of topics were pondered upon, including how to make a career switch from an entirely different domain. Throughout the conference, auditorium 3 saw the most love from the community. EVERY SINGLE talk and hack session was jam-packed as the audience flocked to audi 3 <U+2013> as you can see in the image above! We have such a wonderful community that is so eager to learn new things, and that¡¯s what keeps us working so hard to make DataHack Summit a fulfilling experience for everyone. Recognize this person? Of course you do <U+2013> it¡¯s none other than Tarry Singh! He was one of the most sought-after speakers at the conference and he brought his relentless work ethic and supreme energy to the stage. Tarry was the keynote speaker on day 2, as he spoke about the different nuances of deep learning. He was also part of the panel discussion on GANs and conducted a very successful workshop for CxO¡¯s on how to make the leap from a business executive to an AI leader. Reinforcement Learning was a prevalent topic throughout the summit with eminent personalities like Professor Balaraman Ravindran and Xander Steenbrugge lending their voice to this crucial subject. In fact, Xander even took a live hack session on RL, showing how you can build your own intelligent agent that can play ATARI games! The workshops we hosted just added to the uniqueness of DataHack Summit 2018. 9 interactive and fully sold out workshops were held on the following topics: Business Executive to AI leader <U+2013> A CXO¡¯s Invite-Only Roundtable Hands-On Workshop Giving our community the chance to network with fellow professionals is something DataHack Summit excels at <U+2013> and this year was no different. There were plenty of chances to connect with thought leaders, industry veterans and even intermediate level folks <U+2013> whether it was during lunch, tea, or in-between sessions. We are proud to offer our community the chance to enhance their careers. There were also a number of interactive booths at the venue, set up by Intel, IBM, H2O.ai, Praxis, and Great Learning. Attendees had a chance to interact with them and find out more about their offerings. Check out this interaction between a young data scientist and a robot <U+2013> truly creating a place WHERE HUMANS MEET ARTIFICIAL INTELLIGENCE The Startup Showcase track, introduced for the first time, was a massive success with Rice Inc., Empower Energy, and woroxogo brandishing their awesome machine learning-powered services."
