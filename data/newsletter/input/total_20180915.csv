"","site","date","headline","url_address","text","keyword"
"1","kaggle",2018-09-14,"Winner Interview | Particle Tracking Challenge first runner-up, Pei-Lien Chou","http://blog.kaggle.com/2018/09/14/pei-lien-chou/","What does it take to get almost to the top? Meet Pei-Lien Chou, the worthy runner-up in our recent MLTrack Particle Tracking Challenge. We invited him to tell us about how he placed so well in this challenge. In this contest, Kagglers were challenged to build an algorithm that would quickly reconstruct particle tracks from 3D points left in the silicon detectors. This was part one of a two-phase challenge. In the accuracy phase, which ran from May to August 13th 2018, we focused on the highest score, irrespective of the evaluation time. The second phase is an official NIPS competition (Montreal, December 2018) focused on the balance between accuracy and algorithm speed. For more on the second phase, see the contest post here. For tips from second-place winner Pei-Lien Chow, read on! I hold a Bachelor¡¯s degree in Mathematics and a Master¡¯s degree in Electronic Engineering. I¡¯ve been an engineer in image-based deep learning since last year. I joined Kaggle about 1.5 years ago to practice deep learning, and it helped a lot in my day job. I got a top 1% in my first competition, and won in the next. It is really exciting to be in Kaggle competitions. I did not pay attention at first, because the competition was not image-based, although I did experiment with some point cloud methods during this competition. But when I realized that the organizer was CERN, <U+00A0>the people who are making black holes, I joined for sure. <U+0001F642> My approach started from a naive idea. I wanted to build a model which could map all of the tracks (the model output) to the detector hits (the model input) for each event, just like we use DL for other problems. The output can be easily represented by NxN matrix if an event has N hits (usually N is around 100k), and Mij = 1 if hit-i and hit-j are in the same track, otherwise 0. But the model size was too large, so I split it into minimum units: input a pair of two hits and output their relationship (Fig.1). Unlike the real ¡°connect the dots¡± game which only connects adjoining dots, I connect all the dots if they belong to the same track for robustness. Now, I'm ready to start working in this competition. First, I used hit location (x, y, z) as my input, and easily got an accuracy of 0.99 by training on 10 events. But I quickly discovered that this was not good enough to reconstruct tracks. The problem is that even if the false positive rate = 0.01, for a given hit, the false positive pair count = 0.01*100k = 1000, and the true positive pairs are around 10 (the true average length of tracks). But we need the overlap to be larger than 50% both on truth and the reconstructed one to start getting a score. I got a 0.2 local score on my first try, which was the same as public kernels at that time. I was guessing that maybe 0.6 would win, and hoping that was possible by my approach. God knows! I tried so many methods, and I did improve much more than I expected. Finally, I got an average of 80 false positive pairs for a given hit at 0.97 TPR, and only 6 false positive pairs¡¯ probability are larger than the mean of true positive pairs. So far I have a not-so-precise NxN relationship matrix, but it is enough to get good tracks if I use all of them. Reconstruct: Find N tracks Merge and extend Other work
I added an z-axis constraint and ensemble of two models in the end, and got a 0.003 improvement.
I also tried to apply PointNet to find the track on the predicted candidates and track refining. Both performed well but not better. 
Fig.3 An example of reconstruction of an event with 6 hits. 
Fig.6 An example of the determination of merging priority Fig.4 The seeds (large circles) and their corresponding candidates (of matching colors) in x-y plane. It's clear that the seeds are in a track. 
Fig.5 The diameter of each hit is in direct proportion to the sum of predicted probability seeding by the nine truth hits (in red). Here is a kernel for reference I call this process as endless loop, and it is far from my own original idea. Nevertheless, I was very happy when I passed 0.9 in the end. <U+0001F642> You know, I have to train on 5k events and apply hard negative mining. And for every test event, I have to predict 100k*100k pairs, reconstruct 100k tracks (actually 800k+ in the winning solution), merge and extend them to 10k tracks. So the run time is an astronomical number. To reproduce all this work might take several months on one computer. <U+0001F641> In my opinion, it depends on whether the target can be well described. If so, then the rule-based method should be better. In other words, a clustering approach can get 0.8 in this competition, so applying DL is asking for trouble. But also having fun. <U+0001F642> Join Kaggle now (if you haven't already) and just get started. Pei-Lien Chou is an engineering team lead in image-based deep learning. He has 12 years of experience in the video surveillance industry. He holds a Bachelor¡¯s in Mathematics at National Taiwan University and a Master¡¯s in Electronic Engineering specialized in speech signal processing at National Tsing Hua University.","Keyword(freq): track(8), pair(5), hit(4), dot(3), bachelor(2), candidate(2), event(2), master(2), mathematic(2), method(2)"
"2","datacamp",2018-09-10,"Data Science at Stitch Fix (Transcript)","https://www.datacamp.com/community/blog/data-science-stitch-fix","Here is a link to the podcast. Hugo:                Hi there, Eric. And welcome to DataFramed. Eric:              Hi Hugo, thanks for having me. Hugo:                It's a great pleasure to have you on the show and I'm really excited to be talking about data science at Stitch Fix, how it's woven into the very fabric of what Stitch Fix does, your role of Chief algorithms officer. But before that, I want to find out a bit about what you do. And I want to approach it in a slightly different perspective. I'd first like to find out what your colleagues, people you work with, think that you do. Eric:               Colleagues. So people at work, they would probably say we solve some problems and then I guess go beyond solving but implementing a solution as well. Hugo:                 Absolutely. And they probably think you work with computers or something as well, technical stuff? Eric:               Oh, sure. They think that there's a lot of math and exotic code that we do. Sometimes their imagination is beyond where we actually are, thinking they're far more exotic than what we really do. Hugo:                 So what do you actually do? Eric:                 Well, I think the best characterization of it is applied micro economics. Well, it's my own bias because that's what I studied as an undergrad. But many of our problems are straight out of textbooks and even the data looks like it's straight out of textbooks, which is really fun to work with if you're a data scientist. Things that look Gaussian curves actually do behave like Gaussian curves. That's pretty rare, at least in my experience in the real world, at least for things that are not so bound by physics, like height and weight. More social properties tend to have different distributions, but by golly, we found some Gaussians at Stitch Fix. Eric:                 So it is fun to work in that way where the stuff you learned right out of school is applicable. And it's like, wow, I thought that was just theory but here it is, I can actually implement this and make a difference. So when I say micro economics, I think you can think things like just making a business more efficient by changing the way we do various processes to leverage data. Hugo:                 So that's the micro economics part. What about the applied part? What happens in your day to day or the people you work with that makes it so applied and how do you approach this? Eric:                 Yeah, I think the big difference is Stitch Fix has sort of stumbled upon the data that's been long missing from retail. It's a nuance of our business model. I can tell you a bit more about how Stitch Fix works in a moment. But, the main thing to know is that our clients aren't picking out the clothes for themselves. We have a styling service that does that for them. And so, what happens is they provide a lot of valuable information to us, things like their age, and height and weight, and so forth. This data reveals all sorts of things about how to run your business better. This was always guesswork for other retailers, like how to do things like set prices, how to buy inventory, how to set the levels of supply. Eric:                 All these things were just guesswork and now they're actually really possible in practice, and by golly, all that theory we learned in college actually works or at least most of it. You can actually frame problems as you would in theory and then you can use statistics and computer science to actually solve them and find relationships and then implement things to leverage that new knowledge that you just unearthed. And so this is what we call algorithms, automated decision making, things we put into production to run a certain business process. Hugo:                And so, soon we'll be getting into exactly what type of business problems you solve and how you think about building types of algorithms you use. But before that, I just want to find a bit more out about your background. Maybe you could tell us a bit about how you got into data science originally? Eric:                 Well, sure. Yeah, I think I was lucky enough to be interested in all the right things. My undergrad was economics, specifically micro economics. I was always intrigued by this, what I called a noble way to compete. If you can run a company more efficient, you make things better for both the company and the consumers. And so I always called it this noble way to compete. I naively thought that businesses leveraged this stuff, that they were optimized for these things. This was in 1995, it was perhaps the cusp of the information revolution, it was in full swing by that point, meaning data storage was becoming available and cheap and compute resources were becoming affordable, that companies can actually start doing some of these things, not just being theory. Eric:                 And so, I graduated with an undergrad degree in economics and I somehow talked my way into getting a job as a statistician. We worked on various problems to make them more efficient leveraging data. But they're mostly in the vein of analyses, meaning I perform some analysis and that would kind of advise as what we should do, we should change these prices or move to those locations. So it wasn't anything automated about it. It was really just, output the results of an analyses. Eric:                 What I learned was that you need to do more than that. I later got a job as a consultant doing this type of work, work for many of the large consumer packaged goods companies like Coke and Chlorox and Pepsi and so forth. And we would help them with certain things, again, performing analyses, perhaps how to do a pricing strategy across all your markets, what to price your products at and all the various sizes and so forth in order to optimize revenue or margin, whatever the objective function is. We would deliver those results in probably PowerPoint and they would say that's super helpful, this is great, this is going to help us really improve our numbers. As a consultant, you kind of move on to your next project that you worked very much project based. Eric:                 Out of curiosity, I'd follow up with those clients like a month later and I'd call them up and say, hey, how'd you do with that pricing stuff we told you about? They would respond with, well, we never did it because we didn't know where in the system to make the changes. I'm like, oh my god, you got to be kidding me. And I happened to work for a consulting firm. We also had a software division that made the systems. And so, I knew where to, how to help them with this intellect. Let me come over and I'll show you where to change the prices and all that kind of stuff and how to actually implement algorithms to do this automatically. Hugo:                 And I think that would implement there is key, right? That the challenge there was one of implementation. After a PowerPoint slide, how do you then build it back into your business? Eric:                 Exactly. This is the part that was just missing from companies, like you could show them what to do but you couldn't make them do it. They didn't know what to do next. Back then, we didn't even call it data science, we called it data mining was the term. The output was just PowerPoint you shared with them. But that wasn't enough because people didn't know what to do with it. To make it really compelling, to actually make a difference because the analysis alone doesn't generate the value, you actually have to implement it to get the value. So to make it practical, you had to get it integrated with systems. That took more computer science skills than anything else. You need to know how to code and get in there. Eric:                 But even that wasn't enough. You also had to work with their business teams to really embrace this and adopt it, meaning they had to have conversations about why the prices are being set the way they are and why they're going to be dynamic and not static. Even there needed to be narratives created so that they can work with the retailers they sold to because retailers would get very nervous about would you undercut my private label price, right? They didn't like that, so you have to teach them about how to do these things. Eric:                 And so, I learned that it's far more than the statistics alone or even if you back up even further economic framing, how to frame this problem as a model in this case, we're probably if I recall, we're linear regression, and how to train those models took statistics. But it also required that all the tools of implementation, getting it done, integrating with systems and also giving the business context so you can get adoption both from the company and the company's partners. Eric:                 That's where it really kind of led into what is now called data science, I believe. Again, that term didn't exist back then. And I recall, this was my foray into data sciences, just kind of this organic growth of wanting to make a difference. But I remember when I decided to go back to graduate school, I had to write a statement of purpose for Stanford University. I remember we didn't have the term data science back then. So I didn't know exactly how to tell them what I'm passionate about. But I wrote down something that went a little bit like this. Listen, I know how to write code but I'm not native to any one language. I speak statistics but I have an obvious accent. I know how to operate a business, but I certainly don't identify as a business person. So what am I? I left it dangling like that but I implied that I wanted to combine those things to create some new discipline. Eric:                 And then years later, the word data science was coined kind of simultaneously by Jeff Hammerbacher, Hillary Mason and DJ Patil. And I go well, I think that's it, I think that's the word that I've been trying to come up with. So I globbed on to that several years ago back when I was with Netflix and I named my team that. It was called Data Science and Engineering was the name of the team. Eric:                 But I was glad somebody came up with a word that captured that because statistics didn't do it and computer science didn't do it and business didn't do it, but it was some amalgam of the three. Hugo:                 Exactly. And something else you spoke to in there was the fact that a lot of people when speaking about data science, they think of it as moving from data to insight, right? But I think a point that was a throughline in what you were just saying is that we also need the intersection of data science and decision science. It's not enough to go from data to insight necessarily, but you need to make informed business decisions, informed from the data science, right? Eric:                 Yeah, that's absolutely correct. You'll find that a heavy weighting at Stitch Fix towards what we call algorithms. So my team consists of primarily three types of people. There's data scientists that are developing the algorithms. We have a data platform team that builds a lot of the infrastructure and tooling that can make the data scientists more efficient. Then we have the analytics engineering team. The analytics engineering team is both newer. They're only about five whereas the data science and data platform team combined are about 100. And that's a very different weighting than you'll find at most companies. Eric:                 The reason for that is the insights are very important. We need to keep the company informed. But it's mostly to do just that, keep them informed. So our analytic systems, you know, the dashboards and reports that our business users use, they are important, but they shouldn't be made or shouldn't be used for a lot of decision making. Probably at the high level, it's okay, but for detailed strategic things, I really get nervous about it. There's a lot of confounding factors that we've revealed at Stitch Fix. These are the same confounding factors that have always plagued all companies. But I think the only difference here is we've actually found them here. We have the data to reveal them. And so now, we can no longer play by the rules of ""ignorance is bliss"". We actually know better and we have to be more responsible. Eric:                 So we do have an analytics team but it's much smaller than the data science team. The reason for that is because there's usually a lot more quantifiable value of putting things into action as opposed to informing other humans. And then, there's a disconnect, you may inform a human but what do they do about it. The problem is twofold is one, you need to somehow, if you're informing a human, it's hard to get them back into the loop of a system to take action. But second, there's usually confounding factors that the human is not going to be able to diagnose and probably take into account. Eric:                 And so sometimes, you're better off with a machine only route, meaning let's have the machines automate this, they can control for those things and then since they know that action to take, let's automate it. Other cases, you want that human decision making in the loop, right? There's something more that a human can bring to the table, whether it's values based decision making or ambient information or even the ability to improvise. Those are all qualities that are in the purview of humans, and sometimes we want to incorporate those things. Eric:                 So we do have a myriad of solutions here. Some we call MTM or machine to machine, others we call MTH, machine to human. We leverage a mix of them including sometimes humans in the loop of the algorithms, meaning they're part of the algorithm as opposed to just digesting the output. Hugo:                 Yeah, and I'm really excited to discuss the details of how you consider humans in the loop, particularly in the way you think about styling at Stitch Fix. One of the really interesting things about your work at Stitch Fix for me and for our listeners I think is that as we said earlier, that data science is inextricably woven into the fabric of Stitch Fix and that Stitch Fix is a company that leverages data science for part of its strategic differentiation. I was just wondering what this means to you and how does it differ from the use of data science in other companies. Eric:                 When data science or anything for that matter is part of your differentiation, it really takes a more prominent role. So, the way to think about this is we are a retailer. Stitch Fix, if you were to define the business model, it'd be in the class of retail. That is because we buy merchandise at wholesale and we sell it at a retail price and the difference is called margin. So that's the exact same business model that has existed for hundreds of years. Of course, the way we do things is pretty different, right? The fact our clients are not picking out the clothing themselves. So they're signing up for this service to do that. That changes the game. I already mentioned earlier that means this delegation of responsibility and data from the consumer to us, they say, well, here, if you're going to do this for me, I'm going to have to tell you about myself. Height, weight, age and all that kind of stuff comes over to us and now we're responsible for picking up the clothes for them. Eric:                 All right, so we have the transference of information that used to be once private to the consumer is now we hold in order to act on behalf of that consumer. So that's the big difference that shifts the data. But from a company standpoint, with respect to differentiation, it also changes the game. We set this company up from the very beginning, or, you know, the credit goes to our founder and CEO Katrina Lake, this was her brainchild. And she said from the very beginning, well, this is going to be a different type of service. We're going to have to differentiate very differently. Most retailers will pick some of the more common ways to differentiate such as price, having better pricing than your competitors will give you an advantage. Eric:                 But we said we're not going to do that, we're going to be a full price model. We don't do any sales or promotions or markdowns, that type of thing. We don't mark the close up any higher than anybody would do the standard markups. But we won't discount them either, that we believe it sort of cheapens the brand, it's not what the experience is supposed to be. At Stich Fix, is supposed about getting relevant clothes, not the cheapest clothes. So, we're not going to differentiate on price. Eric:                 Another great way to differentiate yourself as a retailer is brand. If you think of a company like Prada, it's amazing what they have achieved. They can charge 10 times more for their products than comparable products just because it says the word Prada on it. That is an amazing place to be in. But that's not what we're going to be setting out to do. Nobody's going to pay anything extra because the clothes are coming from Stitch Fix. Eric:                 Other ways would be fast shipping. Amazon's wonderful at that. You want something fast, you should go to Amazon. If you need same day or even next day delivery, they're wonderful at this. That's not the value proposition of Stitch Fix. Those are very traditional means of differentiation, price, brand, fast shipping. And we said well, we're not going to have those. What are we going to have then? And we said, well, we got this rich data. Why don't we be the best in the world at data science meaning getting relevant clothes into the hands of our clients and running an efficient business. That's something we can differentiate on. Eric:                 So, that's one of our advantages, we have others, but that is the one certainly relevant to me and why I ended up joining Stitch Fix over six years ago was wow, that puts me in a wonderful position to have every incentive to be the best in the world at what I do. And so I get to try these crazy ideas that just might work because if they do work they're going to have an outsized return on this company. And so, that's exciting place to be if you're a data scientist is anywhere where what you do really matters to the company. Hugo:                 Absolutely. And it sounds that data is so central to what you do with Stitch Fix. So, what I'd like to do now is kind of go through the Stitch Fix client experience and identify the roles data science plays in each step. But before we do that, perhaps you could just give me a quick overview of how the client experience works. Eric:                 The model is this: you come to us when you need clothes. It's an e-commerce site but minus the shopping. There's no browse pages or product pages or search boxes on stitchfix.com or on the app. There's no shopping in the app or on stitchfix.com. Instead, you're going to go and put in your preferences. The things I mentioned before, age, height, weight and sizes, preferences for fit, preferences for style, etc. And then you're going to let the service pick out things on your behalf and send them to your doorstep before you even see them. Eric:                 The experience is you get a curated set of merchandise to try on in the privacy of your own home, which is a much better experience than going to the mall. So you get to try them on the privacy on home with your own shoes and wardrobe. You get feedback from a loved one because it's pretty hard to drag somebody to the mall these days. You don't have to make a snap decision. You can take them off, try them on again tomorrow before you make a decision. You don't need to decide there in a dressing room. Eric:                 So a much better experience, and the value proposition to the consumer is obviously convenience. You don't go to the mall, or even if you're shopping online these days, it's no picnic because you have to digest so much information about ratings and reviews and search and filters and all that, that it could take several hours to buy clothes online as well, especially if you're not clear on what it is you want or even should want. Eric:                 So, that's the convenience property of Stitch Fix is it comes to your home and it's curated for you and you don't need to go anywhere. But the more compelling value proposition I think is these two other things. One is what we call discovery. Precisely because you're not picking up the clothes yourself, you're open to more or you're open to try new things. So for example, we hear this feedback a lot from our clients: well, I would have never picked this out for myself. I saw something like that in the store, I just didn't think it was for me. But you send it to me, you compelled me to try it on and turns out I loved it. I don't know what I was thinking. This is definitely for me, it's like my favorite thing ever, right? Eric:                 So this is very consistent feedback we get that we do open our clients eyes to things that they wouldn't have found on their own. So that's the discovery aspect. Eric:                 And finally is one of confidence. It's a little more subtle of a value proposition but it certainly resonates with clients who have tried the service and after they get going, on their third or fourth shipment, they just have to realize this value proposition of confidence. My narrative goes something like this. When we're picking out clothes for ourselves, we're never quite sure, is this thing currently in style? Am I wearing it correctly? Is it appropriate for my age or my body type? We harbor these subtle insecurities. But when something comes from Stitch Fix we know all that stuff has been taken into account and you can check those boxes off and you don't worry about it. And so it does instill this confidence in you that yes, this is the appropriate thing for you. Eric:                 So, those three things combined are very different than what you're doing when you're shopping at other e-commerce sites or going to malls or even other shopping experiences where more commodity products, you know exactly what you want, you just go get it. This is different, where you want help with this, you want somebody to tell you what would be appropriate for you or to suggest things for you or what is currently in style. And so, it's a very different value proposition to the consumer. Hugo:                 That's fantastic. I presume there's some sort of trade off between sending out to a particular customer 100% of things that work for them but also wanting to get feedback on stuff that doesn't work, right? Eric:                 Oh, that's right. It's very important that we get both the good and the bad, right? In fact, you could argue you learn more by introducing more variation like that. If you're sending out only things that people love, you're not going to learn as much about them. I use a story from my Netflix days. In Netflix, when you hired a new hire that you introduce them to the company by sending out an email, one little quirky bit is you would put what their favorite movie is. So many people would say my favorite movie is Shawshank Redemption. And that doesn't tell much about the person because everybody's favorite movie is Shawshank Redemption. And so, that doesn't reveal much about your preferences or who you are. Eric:                 But if you were to say, well, I really hate Quentin Tarantino. Okay, now we're talking, I just learned something about you that I can use, right? And so, sometimes what you don't like is as important as what you do like. It can reveal some of those preferences and that allows us to learn much better over time. Hugo:                 I like that example. And of course, finding out that someone hated Shawshank Redemption would tell you even more about them. Eric:                 That's true. That would tell they're an iconoclast. They don't Like what everybody else does. Hugo:                 Exactly. So, I'd love to jump in and have you walk me through the client experience and just let us know about how data science influences each step. Eric:                 As a client, it's not ostensible to how much you get to enjoy the benefits of algorithms because a lot of it, almost all of it is behind the scenes. But, if you go through the normal experience of filling out that style profile and getting your fix, that's what we call a shipment, it's called a fix, you're touched by several algorithms through the process. Eric:                 So, first of all, we use a phrase at Stitch Fix: ""there's an algorithm for that"". Like I said, nearly every business processes can be augmented or enhanced or even operated by an algorithm, given the data sets we have here. And so, we've found places for algorithms in nearly every function of the company. So, one of the first things that's going to happen when you request a fix from Stitch Fix is, well, we got to match you to one of our many warehouses. We have a lot of warehouses throughout the country that we store our inventory at. And so there's an algorithm that does that. It matches you based on the inventory and its relevance to you. It also tries to minimize the shipping time to get it to you and that kind of thing. So there's an algorithm for that. Eric:                 Then it's going to match you to a stylist, a human stylist. We'll get to this in a moment but we do combine machines and humans in our styling algorithm. But which of those humans we need to pick is important. For the machines, just pick any of them, they're homogenous. But humans are far more heterogeneous. It matters which one we pick for you. So, we have an algorithm that will pick the stylist with the highest chance of success for you. Hugo:                 And is this some sort of recommender system of some sort. Is that how you tend to think about it? Eric:                 Sure. The end result to the consumer will be a set of recommendations that they get in the flesh. They're shipped to their door and they get to choose from. Behind the scenes where using machine algorithms combined with human judgment to pick those that set out for the- Hugo:                 I actually meant to pick the stylist out. Eric:                 Oh, the stylist. It's a little stronger than a recommendation, it's going to happen. So, we do assign this stylist algorithmically to the client. Now there is some overriding that could happen if the stylist feels that there is a ... different styles that would be a better one. But those are rare. Generally they'll follow the algorithmic assignments, because the algorithms take into account so much. There's things like, well first of all, capacity constraints. Styles only have so much time they can work, we allow them to work very flexible hours. And so they tell us how many hours they're going to work this week. And so that's that's a capacity constraint, how many clients we can assign to that stylist. Eric:                 But more importantly, relationships matter. This may be an existing client that has been working with a certain stylist for several years. If they've had a successful relationship, we'll try to keep them together. But there are also other things that you need to consider such as affinities between the stylist and the client. We have to learn these one by one such as what is the revealed style of the client as revealed through our algorithms and what type of clothes do our stylists like to send, right? So if we can match those up, we'll have a better chance of success. Eric:                 And so, all these things are taken into account by our what we call the smart fix assignment algorithm or SFA as we call it internally. Some of the ways that they're matching, there's a whole suite of logic underneath it that matches clients to stylist. And then once they've been paired up, then we need to perform the styling process which is that's more where we're picking out the clothes for the person. That's the piece that is really a hybrid model between machines and humans. The two are collaborating. So machines take on all the things that have to do with rote calculations, which would include all the various algorithm techniques that you can think of. Things like collaborative filtering and mixed effect models and neural networks, etc. Machines will do all those things to recommend merchandise over to the stylist, and she's presented with all the algorithmic output rank ordered in terms of relevancy. Eric:                 But we won't stop there. The stylist isn't going to just take the top five from the algorithm. She's going to layer in some judgment because humans have access to a little bit more information. They can leverage ambient things such as a magazine, a fashion magazine sitting on the coffee table, or observing who wore what to the Oscars. These are all things that are ambient and unavailable to the client. Other things that you need to leverage are more nuanced context. Imagine a client writing in a note, we allow them to request using text, add a little bit more information to what they're looking for. Imagine a client requesting something like well, I need some clothes to wear to my ex-boyfriend's wedding. All right, that changes the context. The meaning to that is going to be better digested by a human. A human can empathize with what that really means to this person, right? Even our best natural language processing algorithms are not going to get to the level of empathy that a fellow human can. Eric:                 So we need to instill that into our recommendations for this client, right? And so, we leverage humans for that. That goes to our final thing, which is relationships. We want to have human to human contact so that we can foster a relationship. People treat humans a lot differently than they do machines. If you've ever tried chatting with a chat bot, you're typically pretty rude to it. You don't really answer all that synchronously. You give one word responses. You don't really check your spelling and so forth. But with a human on the other side, you're going to write a little bit more thoughtfully and you're going to engage more. So, we need that in all of our fixes. And so, that's how we're trying to leverage the machines for all the calculations, leveraging all the data and hidden relationships. But humans for curation, empathy, relationships, and even the ability to improvise occasionally. Eric:                 Those things are in the purview of humans and probably always will be. And we need them both. So we've figured out a way to combine them in our processes. Hugo:                 That's really, really good. I love the example you gave, because I was wondering, having humans in the loop for interpreting nuance language is incredibly important. But I'm sure you have a whole bunch of natural language processing stuff to deal with the less nuance typed of notes people can make. I suppose I was also thinking you probably have a whole bunch of image analysis or computer vision algorithms to deal with, photos of clothing and that type of stuff as well. Eric:                 Oh yeah, we have an algorithm that we've experimented with that pulls down Pinterest pins. A lot of our clients make us Pinterest pin boards because it's much easier to express in an image of what kinds of things they like than rather than describe them with words. So they pin a bunch of blouses and dresses, jeans, that kind of thing to a Pinterest pin board and we're able to digest them with machines and train with computer vision algorithms to figure out how to strip out the backgrounds and that kind of stuff. And then find very similar items that we own in our inventory that might be good recommendations for these clients. And of course, our human stylists also look at those results and they're also varying their judgment on them. And so, it's a good collaboration between humans and machines. Hugo:                 Fantastic. So Stitch Fix is also placed at the forefront of data driven fashion design. I'm wondering what data driven fashion design as a concept and practice means to you and Stitch Fix's approach to it in general? Eric:                 Well, we do use algorithms for designing some of our clothes. It's the vast minority of the clothes, but we do have a program called Hybrid Designs that leverages algorithms for designing the next set of fashion styles. So this was the brainchild of one of our data scientists, a fellow named Daragh, who tinkered with this. He tinkered with a form of genetic algorithm where he's going to take all the clothes that we've ever sent in our history and he's going to look for gaps in the market: What clothing should exist but doesn't? To do this, he has to have an audience in mind. So say younger females that are looking for cardigans. And perhaps there isn't anything suitable for the younger females in the class of cardigans. Eric:                 So what he'll do is say, well, we should really provide that and if there's no vendor that's going to make that, we'll design it ourselves. And so, the process for designing it is by looking in the past at what has been successful and we're talking millions and millions and millions of points of feedback. We're able to decompose the garments into elements, say, a collar type and a sleeve length and a color, a pattern etc. And so there's a couple dozen of these elements, each taking on between two and 30 values that can hold, like patterns could be plaid or they can be houndstooth, etc. Eric:                 So taking all that data, quantifying each thing, separating and then recombining them. And again, this is where it's a bit generous to say they're a form of genetic algorithm. It's more or less the same process that Mother Nature uses for evolution by natural selection. That is really two properties, recombination and mutation. So, in nature, we recombine the genes of two parents to create a child. In clothing, what we're doing is combining elements from several parents to create something new. Say, we take a silhouette from one garment and the sleeves from another, maybe a collar from a third and a pattern from a fourth. And then we recombine them to create something that has never existed. Eric:                 So we are recombining elements of clothing. So we may take a silhouette from one and a sleeve type from another and a collar from a third and recombine them to create something that has never existed before. And then we might also introduce random mutations, say a new pattern or a new color that might enhance this, the applicability of the style. And so, we have the algorithms doing the search cost. What they're going to do, I mentioned before, there's several elements, each with multiple values, and if you do the multiplication, that results in literally trillions of combinations that you could evaluate. Eric:                 So, what we do is ask our machines to traverse that search space and come back to us with say the top dozen or so designs that would be most likely to succeed with our target audience. So they come back to us after they do their calculations and they present these designs and then we do let our human designers augment them. Let's say, well, they loved the pattern but maybe make it a little less symmetrical or change the color of red a little bit. Those types of things. Eric:                 The result is a final design that we actually have manufactured and we send them out to our clients and they have been wonderfully successful. We started with just three blouses where we did small units, we're talking like 1000 each. They did so well that we created a dozen more and then we did another dozen more. We've done up to 100 design so far. I think it surprised us all with how well they've done. It was a bit of an experiment by this person Daragh a data scientists that, I don't know that any of us had all that much confidence in it, it just seemed like a neat thing to try so we tried it out and to our surprise, it really works well. So we'll keep expanding this program. Hugo:                 That's really exciting. If I recall correctly, Daragh actually wrote a really fantastic post on it on your blog in I think probably summer 2016 or something like that. Eric:                 That's right. There's a lot of great blog posts out there on Multithreaded, that's our tech site, multithreaded.stitchfix.com. And so, there is a good blog post on using algorithms to design clothes. Hugo:                 Yep. And we'll definitely link to those in the show notes. So we spoke briefly about the role of humans in the loop with respect to data driven decision making at Stitch Fix. Some of the points you made which I find really attractive, you know, the ability to improvise, empathy and relationship building. That's definitely a huge part of what you're doing at Stitch Fix. I was wondering how you feel about the roles of humans in the loop in general in data science and as we move forward? Eric:                 I think humans have a lot of value to add to processes, right? Again, there's a lot of things that, they have access to information that machines don't, such as values, what do we care about. Sometimes that doesn't manifest itself in the data. Or even smaller decisions, such as, we have an algorithm that actually helps to buy the clothes and they might be saying, we got to go buy 10,000 units from this vendor. A human might be able to look at that and say, well, that's true. That performs well for us, but that vendor is unethical. We don't like the way they do their business. We're not going to do it. And so, that's something that wouldn't be available to the data and the machines, yet the human will very much enhance that decision in this case not to buy it. Eric:                 So humans do add a lot of value, but they also need to be mitigated. Us humans, we're not good with math. We're not good with statistics. We kind of lack this in our genetic code through the way we evolved, you know, back in our hunter gatherer days, we were almost rewarded for using heuristics. We don't have to take in so much data, we just make a knee jerk decision on things. This story I like to tell is one of them, imagine some ancient hunter gatherer that was very astute and liked to make measured decisions. If that person came across like a saber tooth lion and he said well, I wonder if that's the man eating type, is it a carnivore, is it angry right now. Yeah, that guy got killed, right? But the guy that made a knee jerk decision and saw Sabretooth, I'm out, he survived at a higher rate. Eric:                 And so, the snap judgment decision making was rewarded and therefore manifest in our genes. The older part of our brain, the limbic system is filled with this type of knee jerk reactions and type of decision making where it's more quick, emotional response, almost involuntary versus the neocortex, the newer part of the brain. This is the part where you would use it to do math in your head. And as we all know, we're pretty bad at it. Like even ask me to hold 10 numbers in my head, I probably can't do it. Hugo:                 And as you said, our statistical intuition is perhaps even worse also. Eric:                 Oh, it's horrible. We just don't, we can't do the calculations in our head. We have recency bias and confirmation bias, every bias, all the cognitive biases from Kahneman and Tversky, we're guilty of them all. Again, there was a reward system for that and we survived at a higher rate for having those biases. But they no longer serve us well in the context of business when we have more data available to us to make better decisions. So we almost can't trust our guts sometimes. We like to make up narratives that are not true. We say, oh, this went up because I did that. But we don't have any evidence that supports that. And so often that we want something to be true that we fall victim to it, we conclude that it is true. It must be true because I did this thing and then it happened. Eric:                 And if we take a more disciplined approach using data, it can often reveal how wrong we are. And so one of the things, of course, like any modern company, we do a lot of A/B testing at Stitch Fix. It's amazing how often our intuition fails us. We do things, like we collect what I call a priori judgments. Before we launch an A/B test, we collect from employees, hey, what do you think is going to happen with this test? And well, everybody will write them down, we record them in a Google form and then we run the experiment. And once we have results we compare to what we thought would happen and we're often wrong, just plain wrong, like not even close. Eric:                 But this is new. This is a new practice, only in the last 10 or 15 years of doing these randomized control trials or A/B testing. That hadn't been done prior to 15 years ago, not in most domains anyways. To see our success rate or how successful our intuition is, which the answer is pretty bad, it's not great, it makes me think back to how many decisions were made for hundreds of years that were really horrible but people never knew. We had the benefit of ""ignorance is bliss"". You make a decision, you do it, you'll never know the outcome. Eric:                 But today, with A/B testing, we can test all these things and find out, wow, we're pretty wrong about stuff. And so, it just emboldens us to leverage data science when you're making decisions to make sure that the outcome of our decisions is having an impact that we thought it would. Or even, we no longer, I no longer get too attached to the outcomes of making sure it's the way I wanted it to go, because even when you're wrong, you get to learn. An A/B test where you learn not to do something is as valuable as one where you should do something. Hugo:                 Yeah, absolutely. I know, of course, the browser based interface is incredibly important. But also, is the experience of receiving clothes. I do think to improve the client experience, I'm sure you need to think about the future of fashion. And essentially, this is a prediction problem of thinking about how the future of fashion will evolve and your role in that and whether data science can be used for that or if that's more of a human task. Eric:                 Yeah, that's a great question. So, forward looking on what's fashion going to be six months or nine months or 12 months out from now, it turns out that existing data isn't particularly valuable. It has some merits but it's not enough to really make that decision of what might be interesting to people six months or a year out. That tends to be more of a human task. So we have human buyers that are actually buying the new things, the things that we'll have in our inventory six months from now. Eric:                 This has more to do with all those things that I said that humans are great at. Understanding social mores, leveraging ambient information like who wore what to the Oscars. These are things that humans are great at and we relegate that task to them because we don't have, it's not much that our machines can help with in those things. And so it's a human task and they have to take their leap of faith on buying things. And what we can do is once they've bought something and they get it into our inventory, the machines can take over from there and decide, well, should we rebuy, should we get more of this stuff? Eric:                 And so, we've divided the work, it's another place we're combining machines and humans is in merchandise buying. The work is roughly divided between the algorithmic rebuying of things or once we know something works to go rebuy it, it's done algorithmically. But the exploring of new things is still a human endeavor. And so, the world is divided, or the world of buying at Stitch Fix is divided into explore, exploit. Humans for exploring, algorithms for exploiting. And so, it's a nice balance where, again, each resource humans and machines are doing what they're great at. Hugo:                 That sounds like some sort of human in the loop reinforcement learning challenge as well, right, framed in that fashion. Eric:                 That's right. The nice thing about humans and machines together in the same system is they get to learn from each other. We're not training our machines to act like humans, but we are training them on what works. So we do get some interaction and some feedback between humans and machines. In some of our internal interfaces, you'll have these little thumbs up and thumbs down things where our internal workers can click on and say this was a good recommendation, this thing wasn't. And so they do get some responses from them. And of course, the machines inform the humans the outcomes of their decision. through not just A/B testing, but all the outcomes of their decisions and in terms of metrics and performance of the things they bought in the past. Hugo:                 Yeah. And an aspect of this practice and philosophy that I find really attractive also is what is essentially an axiom for you at Stitch Fix, to allow humans and machines to do what they do best because they're very different, they're orthogonal in so many ways. Eric:                 Yeah, this is a very different perspective on AI. The narrative out there in the world is that AI is going to take over the world and leave us humans without jobs. It's quite the opposite at Stitch Fix. Where AI is intruding into jobs is actually making lives better for our humans, meaning they get to seed the rote tasks over to machines. These are all things you don't really like to do anyways. A lot of rote calculations and rote processes, let's give that to machines and let's leverage humans for doing the things that only they can do and the things that they do better such as improvising and relating to other humans and the ability to appreciate aesthetics. These are the things we like doing. And we get to do more of it, because we're giving away parts of the job to machines. Hugo:                 Absolutely. So I think that really leads well into my next question is, which is really another prediction problem that none of us have a concrete answer to yet. But I'm wondering your views on what the future of data science looks like. Eric:                 Well, I think there's still so much to do at integrating it into processes. It's kind of we opened with it from that story I told from the consumer packaged goods company. This was 15, 20 years ago, where it was hard to even get them to change their prices. When you find an optimal price, how to get them to implement it. And that's still an issue in a lot of companies. Integration into what were formerly human processes, it can be a challenge. There's not a lot of robust APIs for things. We do have them for things like Google and Facebook, but there isn't an API for say buying clothes. What we need to do is we need to get those humans in the loop since they're the ones that are going to go reach out to vendors and purchase orders to buy their clothes. We need to get them on board. That means trust. This can't be black box algorithms. You're going to have to provide some transparency in how they work so that they can feel good about the decisions they're making. Eric:                 It turns out that has a lot of implications for how you design systems and how you design the processes within an organization. So, you really have to take time to do this very diligently. There's a lot of interesting things that emerge out of introducing algorithms, such as you're going to force clarity on objective functions. For human processes, you can be kind of ambiguous on things. But machines need very clear objective functions. They need very clear constraints. No longer can you fudge things, you're going to have to very clearly provide instructions on what to do in every case. And sometimes it's going to be uncomfortable. You may be familiar with the trolley car problem where, I think the narrative goes something like if you're a trolley car operator and you're rolling down the tracks, and you have your choice, the brakes are out, right? So now you have your choice. I'm either going to kill that one person on that track or I can switch tracks and kill a whole group of people. Which one do you do because neither is a pleasant outcome? I've seen this come up in the context of autonomous vehicles. They have to be told something, what to do in this situation. And it's a very uncomfortable thing that as humans don't like to really have to talk about. We don't come up with those contingency plans. We just pray that we're never in that situation. Eric:                 But if you're going to cede something to an algorithm, you're going to have to be clear with what you're going to do. Now, that's a very morbid example that we fortunately do not have to face at Stitch Fix. But we may have to come across a far less uncomfortable situation. Something like we have one more unit of this dress and we have a thousand clients that like it. Who should get it. And so, you do have to be explicit with those instructions, you have to have some way to break the tie. For humans, we can be ambiguous. We can do the first one, we can do the one we like best, whatever. But you're going to have to code something up if you want an algorithm to operate on that and be clear. Eric:                 So that forces a lot of very interesting conversations that were kind of hand-waved away by either ambiguity or ignorance is bliss. It's actually interesting when you have to address these things to put in a system. You're going to have to get very explicit. Hugo:                 I think something we do when we develop algorithms or APIs, we do encode certain biases and having humans to be able to help us figure out what those systematic biases are as well is incredibly important. Eric:                 Oh, absolutely. Like figuring out what things we shouldn't be allowing these algorithms to do, that type of stuff? Hugo:                 Exactly. Eric:                 Yeah, I do get a lot of questions about what is known as the filter bubble out there. If your algorithms are getting really good at what works, won't they only send things that work? My response to that is that you can code them to do what you need to do. You can code them to take risks if you need to. There's lots of literature out there on exploration bandits and so forth or even just taking the upper bound of confidence interval to encourage risk taking on the part of machines. So you can train them to do whatever you need to do so that they don't narrow into niches or bubbles that you can bounce them out of them when you need to. Eric:             So I think most of that is addressable. And of course, there's the more nefarious bias in algorithms, which again, we're mostly removed from at Stitch Fix since we're not doing financial loans for people or anything. We're picking out clothes for folks. Hugo:               Yeah, and it sounds as though anecdotally at least you're doing a very good job from the stories of people responding and being like, hey, this is something I would have never thought of wearing. But you sent it to me and it actually works, so thank you. Eric:             Good. Yes, it's always good to hear those stories. Hugo:               So, we've dove into a number of different algorithms that you use it at Stich Fix. I'm wondering on a kind of a personal professional level, what one of your favorite data science-y techniques or methodologies is? Eric:                 Great question. Personally, I'm less attracted to techniques. My favorite thing is framing. I love the art of framing a problem. When somebody comes to you and says god, here's the dilemma, we want to do this but we don't know about that. Usually these are couched as the need for some algorithm with uncertainty. So, there's lots of outcomes out there. Some don't have any uncertainty such as a sorting algorithm. That's usually not where we play. We love the algorithms with uncertainty. Those usually warrant a different framing. And so usually you get to do some real math, no numbers at all, just algebra. Eric:                 It's an amazing feeling you get when you know you've nailed it, when it feels right. Wow, I think this captures all the phenomenon, all the relationships between things. Then you really get a nice little tickling feeling when you can use the math to do some intuition such as check boundary conditions, things like that. And it's just a wonderful feeling to know that I think I've captured this right. And then of course you have more work to do with empirically validating model. But that art of framing is really probably the thing that gets me going the most. Eric:                 Now, if you ask me to pick a technique, I think one of my favorites is principal component analysis or PCA. The reason I love it is because, first of all, it's an unsupervised technique which means I can just throw the data in there and see what happens. But what I like about it is that again, maybe it's back to this paradigm of machines and humans working together, is that the machines take on all the rote calculations, finding the eigenvectors. That's a machine task that takes millions, if not, depending on your data set, billions of rote calculations to do. Eric:                 But once it returns its results to me, it returns kind of the various components, it takes some human intuition, something perhaps I'm good at to name and label these components. Like, what is this direction it found it at and what is that thing mean? It's up to human interpretation to assign a label to that thing. I just love that interplay between machines and humans in that context. Hugo:               That's awesome. I love that one of the things you love about PCA is that a tells you immediately what explains the most variation, particularly in the framing of you're working with and at Stitch Fix where human bodies and measurements of bodies are so wild and various, and there are so many ways in which we can move, and thinking about it in those terms is really cool. Eric:                 Absolutely. I love what it reveals and it's usually not intuitive. I mean, you got a lot of dimensions you can throw in there. Things like age and price and style and fit. It's almost always a surprise of what the direction explains the most variation is. It's usually not what you think. Hugo:               Yeah, absolutely. So Eric, as a closing question, I'm wondering if you have any final calls to action for our listeners out there. Eric:                 Yeah, I'd like to offer up two. First of all, for consumers that don't like to shop, check out Stitch Fix. We serve men's, women's, and more recently, we've launched our kids line. So just in time for back to school. It's great way if you hate shopping to go find things that you probably wouldn't have found on your own and it will change the way you shop. Eric:                 Now, also, I'll give a plug out there for data scientists. We have a nearly ideal environment for practicing data science and we're always looking for folks. We do what we call the full stack data scientist. They have to be pretty diverse in skill sets, we make you do all the ETL yourself but of course the model framing and the model selection and training and all that kind of stuff. You do though your own product management, project management, all that kind of stuff. So you have to be a full stack data scientist. You have to be curious and you have to be passionate. And if you are interested, go check out multithreaded.stitchfix.com and you can see some open roles we have there. Hugo:               Absolutely. And you actually have a really great blog post from a few years ago now called Advice for Data Scientists on where to work, which spells out a lot of different ways to think about where to work and actually says, when you feel the way you do and the way you've just explained why Stitch Fix would be such a great place. Eric:                 Yep. Yes, it's a good read too because data scientists do have a lot of choices these days in where they want to work. So, make a good one, work somewhere where what you do is going to matter, you get to work with great data, and you get to have greenfield opportunities, things that nobody's done before. Those are the big three we like to talk about. Hugo:               Absolutely. Thank you, Eric, for coming on the show. It's been such a great chat. Eric:                 My pleasure, Hugo.","Keyword(freq): machine(30), algorithm(29), clothe(22), client(13), decision(9), process(9), calculation(7), company(7), scientist(7), economic(6)"
"3","mastery",2018-09-14,"How to Develop a Reusable Framework to Spot-Check Algorithms in Python","https://machinelearningmastery.com/spot-check-machine-learning-algorithms-in-python/","Spot-checking algorithms is a technique in applied machine learning designed to quickly and objectively provide a first set of results on a new predictive modeling problem. Unlike grid searching and other types of algorithm tuning that seek the optimal algorithm or optimal configuration for an algorithm, spot-checking is intended to evaluate a diverse set of algorithms rapidly and provide a rough first-cut result. This first cut result may be used to get an idea if a problem or problem representation is indeed predictable, and if so, the types of algorithms that may be worth investigating further for the problem. Spot-checking is an approach to help overcome the ¡°hard problem¡± of applied machine learning and encourage you to clearly think about the higher-order search problem being performed in any machine learning project. In this tutorial, you will discover the usefulness of spot-checking algorithms on a new predictive modeling problem and how to develop a standard framework for spot-checking algorithms in python for classification and regression problems. After completing this tutorial, you will know: Let¡¯s get started. How to Develop a Reusable Framework for Spot-Check Algorithms in PythonPhoto by Jeff Turner, some rights reserved. This tutorial is divided into five parts; they are: We cannot know beforehand what algorithms will perform well on a given predictive modeling problem. This is the hard part of applied machine learning that can only be resolved via systematic experimentation. Spot-checking is an approach to this problem. It involves rapidly testing a large suite of diverse machine learning algorithms on a problem in order to quickly discover what algorithms might work and where to focus attention. Spot-checking may require that you work with a small sample of your dataset in order to turn around results quickly. Finally, the results from spot checking are a jumping-off point. A starting point. They suggest where to focus attention on the problem, not what the best algorithm might be. The process is designed to shake you out of typical thinking and analysis and instead focus on results. You can learn more about spot-checking in the post: Now that we know what spot-checking is, let¡¯s look at how we can systematically perform spot-checking in Python. In this section we will build a framework for a script that can be used for spot-checking machine learning algorithms on a classification or regression problem. There are four parts to the framework that we need to develop; they are: Let¡¯s take a look at each in turn. The first step of the framework is to load the data. The function must be implemented for a given problem and be specialized to that problem. It will likely involve loading data from one or more CSV files. We will call this function load_data(); it will take no arguments and return the inputs (X) and outputs (y) for the prediction problem. The next step is to define the models to evaluate on the predictive modeling problem. The models defined will be specific to the type predictive modeling problem, e.g. classification or regression. The defined models should be diverse, including a mixture of: Each model should be a given a good chance to perform well on the problem. This might be mean providing a few variations of the model with different common or well known configurations that perform well on average. We will call this function define_models(). It will return a dictionary of model names mapped to scikit-learn model object. The name should be short, like ¡®svm¡® and may include a configuration detail, e.g. ¡®knn-7¡¯. The function will also take a dictionary as an optional argument; if not provided, a new dictionary is created and populated. If a dictionary is provided, models are added to it. This is to add flexibility if you would like to have multiple functions for defining models, or add a large number of models of a specific type with different configurations. The idea is not to grid search model parameters; that can come later. Instead, each model should be given an opportunity to perform well (i.e. not optimally). This might mean trying many combinations of parameters in some cases, e.g. in the case of gradient boosting. The next step is the evaluation of the defined models on the loaded dataset. The scikit-learn library provides the ability to pipeline models during evaluation. This allows the data to be transformed prior to being used to fit a model, and this is done in a correct way such that the transforms are prepared on the training data and applied to the test data. We can define a function that prepares a given model prior to evaluation to allow specific transforms to be used during the spot checking process. They will be performed in a blanket way to all models. This can be useful to perform operations such as standardization, normalization, and feature selection. We will define a function named make_pipeline() that takes a defined model and returns a pipeline. Below is an example of preparing a pipeline that will first standardize the input data, then normalize it prior to fitting the model. This function can be expanded to add other transforms, or simplified to return the provided model with no transforms. Now we need to evaluate a prepared model. We will use a standard of evaluating models using k-fold cross-validation. The evaluation of each defined model will result in a list of results. This is because 10 different versions of the model will have been fit and evaluated, resulting in a list of k scores. We will define a function named evaluate_model() that will take the data, a defined model, a number of folds, and a performance metric used to evaluate the results. It will return the list of scores. The function calls make_pipeline() for the defined model to prepare any data transforms required, then calls the cross_val_score() scikit-learn function. Importantly, the n_jobs<U+00A0>argument is set to -1 to allow the model evaluations to occur in parallel, harnessing as many cores as you have available on your hardware. It is possible for the evaluation of a model to fail with an exception. I have seen this especially in the case of some<U+00A0>models from the statsmodels library. It is also possible for the evaluation of a model to result in a lot of warning messages. I have seen this especially in the case of using XGBoost models. We do not care about exceptions or warnings when spot checking. We only want to know what does work and what works well. Therefore, we can trap exceptions and ignore all warnings when evaluating each model. The function named robust_evaluate_model() implements this behavior. The evaluate_model() is called in a way that traps exceptions and ignores warnings. If an exception occurs and no result was possible for a given model, a None result is returned. Finally, we can define the top-level function for evaluating the list of defined models. We will define a function named evaluate_models() that takes the dictionary of models as an argument and returns a dictionary of model names to lists of results. The number of folds in the cross-validation process can be specified by an optional argument that defaults to 10. The metric calculated on the predictions from the model can also be specified by an optional argument and defaults to classification accuracy. For a full list of supported metrics, see this list: Any None results are skipped and not added to the dictionary of results. Importantly, we provide some verbose output, summarizing the mean and standard deviation of each model after it was evaluated. This is helpful if the spot checking process on your dataset takes minutes to hours. Note that if for some reason you want to see warnings and errors, you can update the evaluate_models() to call the evaluate_model() function directly, by-passing the robust error handling. I find this useful when testing out new methods or method configurations that fail silently. Finally, we can evaluate the results. Really, we only want to know what algorithms performed well. Two useful ways to summarize the results are: The line summaries are quick and precise, although assume a well behaving Gaussian distribution, which may not be reasonable. The box and whisker plots assume no distribution and provide a visual way to directly compare the distribution of scores across models in terms of median performance and spread of scores. We will define a function named summarize_results() that takes the dictionary of results, prints the summary of results, and creates a boxplot image that is saved to file. The function takes an argument to specify if the evaluation score is maximizing, which by default is True. The number of results to summarize can also be provided as an optional parameter, which defaults to 10. The function first orders the scores before printing the summary and creating the box and whisker plot. Now that we have specialized a framework for spot-checking algorithms in Python, let¡¯s look at how we can apply it to a classification problem. We will generate a binary classification problem using the make_classification() function. The function will generate 1,000 samples with 20 variables, with some redundant variables and two classes. As a classification problem, we will try a suite of classification algorithms, specifically: I tried LDA and QDA, but they sadly crashed down in the C-code somewhere. Further, I added multiple configurations for a few of the algorithms like Ridge, kNN, and SVM in order to give them a good chance on the problem. The full define_models() function is listed below. That¡¯s it; we are now ready to spot check algorithms on the problem. The complete example is listed below. Running the example prints one line per evaluated model, ending with a summary of the top 10 performing algorithms on the problem. We can see that ensembles of decision trees performed the best for this problem. This suggests a few things: A box and whisker plot is also created to summarize the results of the top 10 well performing algorithms. The plot shows the elevation of the methods comprised of ensembles of decision trees. The plot enforces the notion that further attention on these methods would be a good idea. Boxplot of top 10 Spot-Checking Algorithms on a Classification Problem If this were a real classification problem, I would follow-up with further spot checks, such as: Next, we will see how we can apply the framework to a regression problem. We can explore the same framework for regression predictive modeling problems with only very minor changes. We can use the make_regression() function to generate a contrived regression problem with 1,000 examples and 50 features, some of them redundant. The defined load_dataset() function is listed below. We can then specify a get_models() function that defines a suite of regression methods. Scikit-learn does offer a wide range of linear regression methods, which is excellent. Not all of them may be required on your problem. I would recommend a minimum of linear regression and elastic net, the latter with a good suite of alpha and lambda parameters. Nevertheless, we will test the full suite of methods on this problem, including: The full get_models() function is listed below. By default, the framework uses classification accuracy as the method for evaluating model predictions. This does not make sense for regression, and we can change this something more meaningful for regression, such as mean squared error. We can do this by passing the metric=¡¯neg_mean_squared_error¡¯ argument when calling evaluate_models() function. Note that by default scikit-learn inverts error scores so that that are maximizing instead of minimizing. This is why the mean squared error is negative and will have a negative sign when summarized. Because the score is inverted, we can continue to assume that we are maximizing scores in the summarize_results() function and do not need to specify maximize=False as we might expect when using an error metric. The complete code example is listed below. Running the example summarizes the performance of each model evaluated, then prints the performance of the top 10 well performing algorithms. We can see that many of the linear algorithms perhaps found the same optimal solution on this problem. Notably those methods that performed well use regularization as a type of feature selection, allowing them to zoom in on the optimal solution. This would suggest the importance of feature selection when modeling this problem and that linear methods would be the area to focus, at least for now. Reviewing the printed scores of evaluated models also shows how poorly nonlinear and ensemble algorithms performed on this problem. A box and whisker plot is created, not really adding value to the analysis of results in this case. Boxplot of top 10 Spot-Checking Algorithms on a Regression Problem In this section, we explore some handy extensions of the spot check framework. I find myself using XGBoost and gradient boosting a lot for straight-forward classification and regression problems. As such, I like to use a course grid across standard configuration parameters of the method when spot checking. Below is a function to do this that can be used directly in the spot checking framework. By default, the function will use XGBoost models, but can use the sklearn gradient boosting model if the use_xgb argument to the function is set to False. Again, we are not trying to optimally tune GBM on the problem, only very quickly find an area in the configuration space that may be worth investigating further. This function can be used directly on classification and regression problems with only a minor change from ¡°XGBClassifier¡± to ¡°XGBRegressor¡± and ¡°GradientBoostingClassifier¡± to ¡°GradientBoostingRegressor¡°. For example: To make this concrete, below is the binary classification example updated to also define XGBoost models. Running the example shows that indeed some XGBoost models perform well on the problem. Boxplot of top 10 Spot-Checking Algorithms on a Classification Problem with XGBoost The above results also highlight the noisy nature of the evaluations, e.g. the results of extra trees in this run are different from the run above (0.858 vs 0.869). We are using k-fold cross-validation to produce a population of scores, but the population is small and the calculated mean will be noisy. This is fine as long as we take the spot-check results as a starting point and not definitive results of an algorithm on the problem. This is hard to do; it takes discipline in the practitioner. Alternately, you may want to adapt the framework such that the model evaluation scheme better matches the model evaluation scheme you intend to use for your specific problem. For example, when evaluating stochastic algorithms like bagged or boosted decision trees, it is a good idea to run each experiment multiple times on the same train/test sets (called repeats) in order to account for the stochastic nature of the learning algorithm. We can update the evaluate_model() function to repeat the evaluation of a given model n-times, with a different split of the data each time, then return all scores. For example, three repeats of 10-fold cross-validation will result in 30 scores from each to calculate a mean performance of a model. Alternately, you may prefer to calculate a mean score from each k-fold cross-validation run, then calculate a grand mean of all runs, as described in: We can then update the robust_evaluate_model() function to pass down the repeats argument and the evaluate_models() function to define a default, such as 3. A complete example of the binary classification example with three repeats of model evaluation is listed below. Running the example produces a more robust estimate of the scores. There will still be some variance in the reported means, but less than a single run of k-fold cross-validation. The number of repeats may be increased to further reduce this variance, at the cost of longer run times, and perhaps against the intent of spot checking. I am a big fan of avoiding assumptions and recommendations for data representations prior to fitting models. Instead, I like to also spot-check multiple representations and transforms of input data, which I refer to as views. I explain this more in the post: We can update the framework to spot-check multiple different representations for each model. One way to do this is to update the evaluate_models() function so that we can provide a list of make_pipeline() functions that can be used for each defined model. The chosen pipeline function can then be passed along down to the robust_evaluate_model() function and to the evaluate_model() function where it can be used. We can then define a bunch of different pipeline functions; for example: Then create a list of these function names that can be provided to the evaluate_models() function. The complete example of the classification case updated to spot check pipeline transforms is listed below. Running the example shows that we differentiate the results for each pipeline by adding the pipeline number to the beginning of the algorithm description name, e.g. ¡®0rf¡® means RF with the first pipeline, which is no transforms. The ensembles of trees algorithms perform well on this problem, and these algorithms are invariant to data scaling. This means that their results on each pipeline will be similar (or the same) and in turn they will crowd out other algorithms in the top-10 list. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered the usefulness of spot-checking algorithms on a new predictive modeling problem and how to develop a standard framework for spot-checking algorithms in python for classification and regression problems. Specifically, you learned: Have you used this framework or do you have some further suggestions to improve it?
Let me know in the comments. Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of scikit-learn code Discover how in my new Ebook:Machine Learning Mastery With Python Covers self-study tutorials and end-to-end projects like:Loading data, visualization, modeling, tuning, and much more¡¦ Skip the Academics. Just Results. Click to learn more. hi Jason,
have you tried using the autoML tools such as autoKeras & Auto-sklearn packages?
(I¡¯ve also used AutoWeka in the weka package <U+2013> very limited, and Rapidminer Auto-Modelling). I generally don¡¯t use them. Have you? Is this possible in R? Yes, with the caret package. I even have examples on the blog, search ¡°spot check¡±. This is truly one of the best resources for ML on the web Thanks, I¡¯m happy it helps. Hello Sir,  Awesome post on spot checking algorithm. This is really a great one <U+2013> one stop shop! I have couple of basic questions here <U+2013>  1) How the similar approach along with pipeline can be applied to different Keras models -sequential, functional and KerasRegressor/KerasClassifier (with layers/dropouts/optimizers etc.)?  2) Do we need to reset states/layers in evaluate models prior to build/apply pipeline? 3) Post evaluation and finalizing the model, do we need to rebuild model, compile, fit to predict the test dataset or use the existing compiled model to fit and predict? Will predicted values/scores vary for each runs due to different weight initializations? Thanking you in anticipation for your response. Neural nets might be too big/slow for this type of framework and you may have to grid search manually. Regardless of model, you should redefine and refit for each eval. A finalized model is refit on all available data. Good well rounded post.
Would really appreciate similar spot checking for (not too deep) neural nets framework in keras or tensorflow. Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning.","Keyword(freq): algorithm(29), result(23), model(21), score(13), method(8), transform(8), problem(5), repeat(5), tree(5), configuration(4)"
"4","mastery",2018-09-12,"A Gentle Introduction to a Standard Human Activity Recognition Problem","https://machinelearningmastery.com/how-to-load-and-explore-a-standard-human-activity-recognition-problem/","Human activity recognition is the problem of classifying sequences of accelerometer data recorded by specialized harnesses or smart phones into known well-defined movements. It is a challenging problem given the large number of observations produced each second, the temporal nature of the observations, and the lack of a clear way to relate accelerometer data to known movements. Classical approaches to the problem involve hand crafting features from the time series data based on fixed-size windows and training machine learning models, such as ensembles of decision trees. The difficulty is that this feature engineering requires deep expertise in the field. Recently, deep learning methods such as recurrent neural networks and one-dimensional convolutional neural networks or CNNs have been shown to provide state-of-the-art results on challenging activity recognition tasks with little or no data feature engineering. In this tutorial, you will discover a standard human activity recognition dataset for time series classification and how to explore the dataset prior to modeling. After completing this tutorial, you will know: Let¡¯s get started. A Gentle Introduction to a Standard Human Activity Recognition ProblemPhoto by tgraham, some rights reserved. This tutorial is divided into 8 parts; they are: Human Activity Recognition or HAR for short, is the problem of predicting what a person is doing based on a trace of their movement using sensors. Movements are often normal indoor activities such as standing, sitting, jumping, and going up stairs. Sensors are often located on the subject such as a smartphone or vest and often record accelerometer data in three dimensions (x, y, z). The idea is that once the subject¡¯s activity is recognized and known, an intelligent computer system can then offer assistance. It is a challenging problem because there is no clear analytical way to relate the sensor data to specific actions in a general way. It is technically challenging because of the large volume of sensor data collected (e.g. tens or hundreds of observations per second) and the classical use of hand crafted features and heuristics from this data in developing predictive models. More recently, deep learning methods have been achieving success on HAR problems given their ability to automatically learn higher-order features. Sensor-based activity recognition seeks the profound high-level knowledge about human activities from multitudes of low-level sensor readings. Conventional pattern recognition approaches have made tremendous progress in the past years. However, those methods often heavily rely on heuristic hand-crafted feature extraction, which could hinder their generalization performance. [¡¦] Recently, the recent advancement of deep learning makes it possible to perform automatic high-level feature extraction thus achieves promising performance in many areas. <U+2014> Deep Learning for Sensor-based Activity Recognition: A Survey Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course The dataset ¡°Activity Recognition from Single Chest-Mounted Accelerometer Data Set¡± was collected and made available by Casale, Pujol, et al. from the University of Barcelona in Spain. It is freely available from the UCI Machine Learning repository: The dataset was described and used as the basis for a sequence classification model in their 2011 paper ¡°Human Activity Recognition from Accelerometer Data Using a Wearable Device¡°. The dataset is comprised of uncalibrated accelerometer data from 15 different subjects, each performing 7 activities. Each subject wore a custom-developed chest-mounted accelerometer and data was collected at 52Hz (52 observations per second). Photographs of the custom chest-mounted systems worn by each subject.Taken from ¡°Human Activity Recognition from Accelerometer Data Using a Wearable Device¡±. The seven activities were performed and recorded by each subject in a continuous sequence. The specific activities performed were: The paper used a subset of the data, specifically 14 subjects and 5 activities. It is not immediately clear why the other 2 activities (2 and 6) were not used. Data have been collected from fourteen testers, three women and eleven men with age between 27 and 35. [¡¦] The data set collected is composed by 33 minutes of walking up/down stairs, 82 minutes of walking, 115 minutes of talking, 44 minutes of staying standing and 86 minutes of working at computer. <U+2014> Human Activity Recognition from Accelerometer Data Using a Wearable Device, 2011. The focus of the paper was the development of hand-crafted features from the data, and the developing of machine learning algorithms. Data was split into windows of one second of observations (52) with a 50% overlap between windows. We extract features from data using windows of 52 samples, corresponding to 1 second of accelerometer data, with 50% of overlapping between windows. From each window, we propose to extract the following features: root mean squared value of integration of acceleration in a window, and mean value of Minmax sums. [¡¦] Still, in order to complete the set of features we add features that have proved to be useful for human activity recognition like: mean value, standard deviation, skewness, kurtosis, correlation between each pairwise of accelerometer axis(not including magnitude), energy of coefficients of seven level wavelet decomposition. In this way, we obtain a 319-dimensional feature vector. <U+2014> Human Activity Recognition from Accelerometer Data Using a Wearable Device, 2011. A suite of machine learning models were fit and evaluated using 5-fold cross-validation and achieved an accuracy of 94%. Confusion Matrix of Random Forest evaluated on the dataset.Taken from ¡°Human Activity Recognition from Accelerometer Data Using a Wearable Device¡±. The dataset can be downloaded directly from the UCI Machine Learning Repository. Unzip the dataset into a new directory called ¡®HAR¡®. The directory contains a list of CSV files, one per subject (1-15) and a readme file. Each file contains 5 columns, the row number, x, y, and z accelerometer readings and a class number from 0 to 7, where class 0 means ¡°no activity¡± and classes 1-7 correspond to the activities listed in the previous section. For example, below are the first 5 rows from the file ¡°1.csv¡°: To get started, we can load each file as a single NumPy array and drop the first column. The function below named load_dataset() will load all CSV files in the HAR directory, drop the first column and return a list of 15 NumPy arrays. The complete example is listed below. Running the example loads all of the data and prints the number of loaded subjects. Note, the files in the directory are navigated in file-order, which may not be the same as the subject order, e.g. 10.csv comes before 2.csv in file order. I believe this does not matter in this tutorial. Now that we know how to load the data, we can explore it with some visualizations. A good first visualization is to plot the data for a single subject. We can create a figure with a plot for each variable for a given subject, including the x, y, and z accelerometer data, and the associated class class values. The function plot_subject() below will plot the data for a given subject. We can tie this together with the data loading in the previous section and plot the data for the first loaded subject. Running the example creates a line plot for each variable for the first loaded subject. We can see some very large movement in the beginning of the sequence that may be an outlier or unusual behavior that could be removed. We can also see that the subject performed some actions multiple times. For example, a closer look at the plot of the class variable (bottom plot) suggests the subject performed activities in the following order, 1, 2, 0, 3, 0, 4, 3, 5, 3, 6, 7. Note that activity 3 was performed twice. Line plots of x, y, z and class for the first loaded subject. We can re-run this code and plot the second loaded subject (which might be 10.csv). Running the example creates a similar plot. We can see more detail, suggesting that the large outlier seen at the beginning of the previous plot might be washing out values from that subject¡¯s trace. We see as similar sequence of activities, with activity 3 occurring twice again. We can also see that some activities are performed for much longer than others. This may impact the ability of a model to discriminate between the activities, e.g. activity 3 (standing) for both subjects has very little data relative to the other activities performed. Line plots of x, y, z and class for the second loaded subject. The previous section raised a question around how long or how many observations we have for each activity across all of the subjects. This may be important if there is a lot more data for one activity than another, suggesting that less-well-represented activities may be harder to model. We can investigate this by grouping all observations by activity and subject, and plot the distributions. This will give an idea of how long each subject spends performing each activity over the course of their trace. First, we can group the activities for each subject. We can do this by creating a dictionary for each subject and store all trace data by activity. The group_by_activity() function below will perform this grouping for each subject. Next, we can calculate the total duration of each activity for each subject. We know that the accelerometer data was recorded at 52Hz, so we can divide the lengths of each trace per activity by 52 in order to summarize the durations in seconds. The function below named plot_durations() will calculate the durations for each activity per subject and plot the results as a boxplot. The box-and-whisker plot is a useful way to summarize the 15 durations per activity as it describes the spread of the durations without assuming a distribution. The complete example of plotting the distribution of activity durations is listed below. Running the example plots the distribution of activity durations per subject. We can see that there is relatively fewer observations for activities 0 (no activity), 2 (standing up, walking and going up/down stairs), 5 (going up/down stairs) and 6 (walking and talking). This might suggest why activities 2 and 6 were excluded from the experiments in the original paper. We can also see that each subject spent a lot of time on activity 1 (standing Up, walking and going up/down stairs) and activity 7 (talking while standing). These activities may be over-represented. There may be benefit in preparing model data that undersamples these activities or over samples the other activities. Boxplot of the distribution of activity durations per subject Next, it may be interesting to look at the trace data for each subject. One approach would be to plot all traces for a single subject on a single graph, then line up all graphs vertically. This will allow for a comparison across subjects and across traces within a subject. The function below named plot_subjects() will plot the accelerometer data for each of the 15 subjects on a separate plot. The trace for each of the x, y, and z data are plotted as orange, green and blue respectively. The complete example is listed below. Running the example creates a figure with 15 plots. We are looking at the general rather than specific trends across the traces. Each subject may have a different full trace length, so direct comparisons by the x-axis may not be reasonable (e.g. performing similar activities at the same time). We are not really concerned with this aspect of the problem anyway. Line plots of accelerometer trace data for all 15 subjects. The traces seem to have the same general scale, but the amplitude differences between the subjects suggest that per-subject rescaling of the data might make more sense than cross-subject scaling. This might make sense for training data, but may be methodologically challenging for scaling the data for a test subject. It would require or assume that the entire trace would be available prior to predicting activities. This would be fine for offline use of a model, but not-online use of the model. It also suggests, that online use of a model might be a lot easier with pre-calibrated trace data (e.g. data coming in at a fixed scale). The point in the previous section about the possibility of markedly different scales across the different subjects might introduce challenges in modeling this dataset. We can explore this by plotting a histogram of the distribution of observations for each axis of accelerometer data. As with the previous section, we can create a plot for each subject, then align the plots for all subjects vertically with the same x-axis to help spot obvious differences in the spreads. The updated plot_subjects() function for plotting histograms instead of line plots is listed below. The hist() function is used to create a histogram for each axis of accelerometer data, and a large number of bins (100) is used to help spread out the data in the plot. The subplots also all share the same x-axis to aid in the comparison. The complete example is listed below Running the example creates a single figure with 15 plots, one for each subject, and 3 histograms on each plot for each of the 3 axis of accelerometer data. The three colors blue, orange and green represent the x, y and z axises. This plot suggest that the distribution of each axis of accelerometer is Gaussian or really close to Gaussian. This may help with simple outlier detection and removal along each axis of the accelerometer data. The plot really helps to show both the relationship between the distributions within a subject and differences in the distributions between the subjects. Within each subject, a common pattern is for the x (blue) and z (green) are grouped together to the left and y data (orange) is separate to the right. The distribution of y is often sharper where as the distributions of x and z are flatter. Across subjects, we can see a general clustering of values around 2,000 (whatever the units are), although with a lot of spread. This marked difference in distributions does suggest the need to at least standardize (shift to zero mean and unit variance) the data per axis and per subject before any cross-subject modeling is performed. Histograms of accelerometer data for each subject In this section we will explore some ideas and approaches for data preparation and modeling for this problem based on the above exploration of the dataset. These may help in both modeling this dataset in particular, but also human activity recognition and even time series classification problems in general. There are many ways to frame the data as a predictive modeling problem, although all center around the idea of time series classification. Two main approaches to consider are: The latter, cross-subject, is more desirable, but the former may also be interesting if the goal is to deeply understand a given subject, e.g. a personalized model within a home. Two main approaches for framing the data during modeling include: The former approach may be less realistic in terms of the actual use of the model, but may be an easier problem to model. The latter was the framing of the problem used in the original paper, where 1-second windows were prepared with a 50% overlap. I fail to see the benefit of the overlap in the framing of the problem, other than doubling the size of the training dataset, that may benefit deep neural networks. In fact, I expect it may result in an overfit model. The data suggest some preparation schemes that may be helpful during modeling: As noted in a previous section, the standardization of data per-subject does introduce methodological issues, and may be used regardless, defended as the need for calibrated observations from the original hardware system. I would recommend exploring this problem using neural networks. Unlike the approach used in the paper that used feature engineering and domain-specific hand-crafted features, it would be useful and general to model the raw data directly (downsampled or otherwise). First, I¡¯d recommend discovering a baseline in performance using a robust method such as random forest or gradient boosting machines. Then explore neural network approaches specifically suited to time series classification problems. Two types of neural network architectures that might be appropriate are: A third is a hybrid of the two: CNNs are able to extract features from input sequences, such as windows of input accelerometer data. RNNs, such as LSTMs are able to learn from long sequences of input data directly, and learn long-term relationships in the data. I would expect there is little causal relationship in the sequence data, other than each subject looks like they are performing the same artificial sequence of actions, which we would not want to learn. Naively, this might suggest that CNNs would be a better fit for predicting the activity given a sequence of observed accelerometer data. One-dimensional CNNs have been widely used for this type of problem, with one channel for each axis of the accelerometer data. A good simple starting point would be to fit a CNN model on windows of sequence data directly. This is the approach described in the 2014 paper titled ¡°Convolutional Neural Networks for Human Activity Recognition using Mobile Sensors¡°, and made clearer with the figure below taken from the paper. Example of 1D CNN Architecture for Human Activity RecognitionTaken from ¡°Convolutional Neural Networks for Human Activity Recognition using Mobile Sensors¡±. A CNN LSTM could be used where the CNN learns a representation for a sub-sequence of observations, then the LSTM learns across these subsequences. For example, a CNN could distill one second of accelerometer data, this could then be repeated for 30 seconds, to provide 30 time steps of CNN-interpreted data to the LSTM. I¡¯d expect all three approaches may be interesting on this problem and problems like it. I don¡¯t expect overlapping of the windows would be useful, and in fact might result in minor misleading results if portions of the trace data is available in both train and test dataset during cross-validation. Nevertheless, it would increase the amount of training data. I think it is important that the data are exposed to the model while maintaining the temporal ordering of the observations. It is possible that multiple windows from a single activity will look similar for a given subject and a random shuffling and separation of windows to train an test data may lead to misleading results. I would not recommend randomly shuffled k-fold cross-validation was was used in the original paper. I expect that this would lead to optimistic results, with one-second windows of data from all across each the traces of the 15 subjects mixed together for training and testing. Perhaps a fair evaluation of models in this data would be to use leave-one-out cross validation or LOOCV by subject. This is where a model is fit on the first 14 subjects and evaluated on all windows of the 15th subject. This process is repeated where each subject is given a chance to be used as the hold-out test dataset. The segmentation of the dataset by subject avoids any issues related to the temporal ordering of individual windows during model evaluation as all windows will are guaranteed new/unseen data. If you explore any of these modeling ideas, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered a standard human activity recognition dataset for time series classification. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning.","Keyword(freq): activity(22), subject(17), window(15), observation(12), feature(10), plot(8), approach(7), duration(7), network(6), result(6)"
"5","mastery",2018-09-10,"Indoor Movement Time Series Classification with Machine Learning Algorithms","https://machinelearningmastery.com/indoor-movement-time-series-classification-with-machine-learning-algorithms/","Indoor movement prediction involves using wireless sensor strength data to predict the location and motion of subjects within a building. It is a challenging problem as there is no direct analytical model to translate the variable length traces of signal strength data from multiple sensors into user behavior. The ¡®indoor user movement¡® dataset is a standard and freely available time series classification problem. In this tutorial, you will discover the indoor movement prediction time series classification problem and how to engineer features and evaluate machine learning algorithms for the problem. After completing this tutorial, you will know: Let¡¯s get started. Indoor Movement Time Series Classification with Machine Learning AlgorithmsPhoto by Nola Tularosa, some rights reserved. This tutorial is divided into five parts; they are: The ¡®indoor user movement¡® prediction problem involves determining whether an individual has moved between rooms based on the change in signal strength measured by wireless detectors in the environment. The dataset was collected and made available by Davide Bacciu, et al. from the University of Pisa in Italy and first described in their 2011 paper ¡°Predicting User Movements in Heterogeneous Indoor Environments by Reservoir Computing¡± as a dataset for exploring a methodology that seems like recurrent neural networks called ¡®reservoir computing.¡¯ The problem is a special case of the more generic problem of predicting indoor user localization and movement patterns. Data was collected by positioning four wireless sensors in the environment and one on the subject. The subject moved through the environment while the four wireless sensors detected and recorded a time series of sensor strength. The result is a dataset comprised of variable length time series with four variates describing trajectory through a well-defined static environment, and the classification of whether the movement led to the subject changing rooms in the environment. It is a challenging problem because there is no obvious and generic way to relate signal strength data to subject location in an environment. The relationship between the RSS and the location of the tracked object cannot be easily formulated into an analytical model, as it strongly depends on the characteristics of the environment as well as on the wireless devices involved. I <U+2014> Predicting User Movements in Heterogeneous Indoor Environments by Reservoir Computing, 2011. The data was collected under controlled experimental conditions. Sensors were placed in three pairs of two connected rooms containing typical office furniture. Two sensors were placed in the corners of each of the two rooms and the subject walked one of six predefined paths through the rooms. Predictions are made at a point along each path that may or may not lead to a change of room. The cartoon below makes this clear, showing the sensor locations (A1-A4), the six possible paths that may be walked, and the two points (M) where a prediction will be made. Overview of two rooms, sensor locations and the 6 pre-defined paths.Taken from ¡°Predicting User Movements in Heterogeneous Indoor Environments by Reservoir Computing.¡± Three datasets were collected from the three pairs of two rooms in which the paths were walked and sensor measurements taken, referred to as Dataset 1, Dataset 2, and Dataset 3. The table below, taken from the paper, summarizes the number of paths walked in each of the three datasets, the total number of room changes and non-room-changes (class label), and the lengths of the time series inputs. Summary of sensor data collected from the three pairs of two rooms.Taken from ¡°Predicting User Movements in Heterogeneous Indoor Environments by Reservoir Computing.¡± Technically, the data is comprised of multivariate time series inputs and a classification output and may be described as a time series classification problem. The RSS values from the four anchors are organized into sequences of varying length corresponding to trajectory measurements from the starting point until marker M. A target classification label is associated to each input sequence to indicate whether the user is about to change its location (room) or not. <U+2014> Predicting User Movements in Heterogeneous Indoor Environments by Reservoir Computing, 2011. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course The dataset is freely available from the UCI Machine Learning Repository: The data can be downloaded as a .zip file that contains the following salient files: The provided data is already normalized. Specifically, each input variable is normalized into the range [-1,1] per dataset (pair of rooms), and the output class variable is marked -1 for no transition between rooms and +1 for a transition through the rooms. [¡¦] put data comprises time series of 4 dimensional RSS measurements (NU = 4) corresponding to the 4 anchors [¡¦] normalized in the range [<U+2212>1, 1] independently for each dataset <U+2014> Predicting User Movements in Heterogeneous Indoor Environments by Reservoir Computing, 2011. The scaling of data by dataset may (or may not) introduce additional challenges when combining observations across datasets if the pre-normalized distributions differ greatly. The time series for one trace in a given trace file are provided in temporal order, where one row records the observations for a single time step. The data is recorded at 8Hz, meaning that one second of clock time elapses for eight time steps in the data. Below is an example of a trace, taken from ¡®dataset/MovementAAL_RSS_1.csv¡®, which has the output target ¡®1¡¯ (a room transition occurred), from group 1 (the first pair of rooms) and is the path 1 (a straight shot from left to right between the rooms). The datasets were used in two specific ways (experimental settings or ES) to evaluate predictive models on the problem, designated ES1 and ES2, as described in the first paper. The ES1 case evaluates a model to generalize movement within two pairs of known rooms, that is, rooms with known geometry. The ES2 case attempts to generalize movement from two rooms to a third unseen room: a harder problem. The 2011 paper, reports performance of about 95% classification accuracy on ES1 and about 89% on ES2, which after some testing of a suite of algorithms myself is very impressive. In this section, we will load the data into memory and explore it with summarization and visualization to help better understand how the problem might be modeled. First, download the dataset and unzip the downloaded archive into your current working directory. The targets, groups, and path files can be loaded directly as Pandas DataFrames. The signal strength traces are stored in separate files in the dataset/ directory. These can be loaded by iterating over all files in the directory and loading the sequences as directly. Because each sequence has a variable length (variable number of rows), we can store the NumPy array for each trace in a list. We can tie all of this together into a function named load_dataset() and load the data into memory. The complete example is listed below. Running the example loads the data and shows that 314 traces were correctly loaded from disk along with their associated outputs (targets as -1 or +1), dataset number, (group as 1, 2 or 3) and path number (path as 1-6). We can now take a closer look at the loaded data to better understand or confirm our understanding of the problem. We know from the paper that the dataset is reasonably balanced in terms of the two classes. We can confirm this by summarizing the class breakdown of all observations. Next, we can review the distribution of the sensor strength values for each of the four anchor points by plotting a histogram of the raw values. This requires that we create one array with all rows of observations so that we can plot the distribution of each column. The vstack() NumPy function will do this job for us. Finally, another interesting aspect to look at is the distribution of the length of the traces. We can summarize this distribution using a histogram. Putting this all together, the complete example of loading and summarizing the data is listed below. Running the example first summarizes the class distribution for the observations. The results confirm our expectations of the full dataset being nearly perfectly balanced in terms of observations of both class outcomes. Next, a histogram of the sensor strength for each anchor point is created, summarizing the data distributions. We can see that the distributions for each variable are close to normal showing Gaussian-like shapes. We can also see perhaps too many observations around -1. This might indicate a generic ¡°no strength¡± observation that could be marked or even filtered out from the sequences. It might be interesting to investigate whether the distributions change by path type or even dataset number. Histograms for the sensor strength values for each anchor point Finally, a histogram of the sequence lengths is created. We can see clusters of sequences with lengths around 25, 40, and 60. We can also see that if we wanted to trim long sequences that a maximum length of around 70 time steps might be appropriate. The smallest length appears to be 19. Histogram of sensor strength sequence lengths We are working with time series data, so it is important that we actually review some examples of the sequences. We can group traces by their path and plot an example of one trace for each path. The expectation is that traces for different paths may look different in some way. We can also plot each series from one trace along with the trend predicted by a linear regression model. This will make any trends in the series obvious. We can fit a linear regression for a given series using the lstsq() NumPy Function. The function regress() below takes a series as a single variable, fits a linear regression model via least squares, and predicts the output for each time step returning a sequence that captures the trend in the data. We can use the function to plot the trend for the time series for each variable in a single trace. Tying all of this together, the complete example is listed below. Running the example creates a plot containing six figures, one for each of the six paths. A given figure shows the line plots of a single trace with the four variables of the trace, one for each anchor point. Perhaps the chosen traces are representative of each path, perhaps not. We can see some clear differences with regards to: Ideally, if these changes in behavior are predictive, a predictive model must extract these features, or be presented with a summary of these features as input. Line plots of one trace (4 variables) for each of the six paths. A second plot is created showing the line plots for the four series in a single trace along with the trend lines. We can see that, at least for this trace, there is a clear trend in the sensor strength data as the user moves around the environment. This may suggest the opportunity to make the data stationary prior to modeling or using the trend for each series in a trace (observations or coefficients) as inputs to a predictive model. Line plots for the time series in a single trace with trend lines There are many ways to fit and evaluate a model on this data. Classification accuracy seems like a good first-cut evaluation metric given the balance of the classes. More nuance can be sought in the future by predicting probabilities and exploring thresholds on an ROC curve. I see two main themes in using this data: The ES1 and ES2 cases described in the paper and summarized above explore these questions and provide a useful starting point. First, we must partition the loaded traces and targets into the three groups. In the case of ES1, we can use k-fold cross-validation where k=5 to use the same ratio from the paper and the repeated evaluation provides some robustness to the evaluation. We can use the cross_val_score() function from scikit-learn to evaluate a model and then calculate the mean and standard deviation of the scores. In the case of ES2, we can fit the model on datasets 1 and 2 and test model skill on dataset 3 directly. There is flexibility in how the input data is framed for the prediction problem. Two approaches come to mind: Both are interesting approaches. As a first pass, we will prepare the more traditional fixed-length vector input via manual feature engineering. Below are some ideas on features that could be included in the vector: Additionally, data scaling is probably not required of the raw values as the data has already been scaled to the range -1 to 1. Scaling may be required if new features are added with different units. Some of the variables do show some trend, suggesting that perhaps a differencing of the variables may help in teasing out a signal. The distribution of each variable is nearly Gaussian, so some algorithms may benefit from standardization, or perhaps even a Box-Cox transform. In this section, we will spot-check the default configuration for a suite of standard machine learning algorithms with different sets of engineered features. Spot-checking is a useful technique to flush out quickly whether there is any signal to be learned in the mapping between inputs and outputs with engineered features as most of the tested methods will pick something up. The method can also suggest methods that might be worth further investigating. A downside is that each method is not given its best chance (configuration) to show what it can do on the problem, meaning any methods that are further investigated will be biased by the first results. In these tests, we will look at a suite of six different types of algorithms, specifically: We will test the default configurations of these methods on features that focus on the end of the time series variables as they are likely the most predictive of whether a room transition will occur or not. The last n observations are likely to be predictive of whether the movement leads to a transition in rooms. The smallest number of time steps in the trace data is 19, therefore, we will use n=19 as a starting point. The function below named create_dataset() will create a fixed-length vector using the last n observations from each trace in a flat one-dimensional vector, then add the target as the last element of the vector. This flattening of the trace data is required for simple machine learning algorithms. We can load the dataset as before and sort it into the datasets 1, 2, and 3 as described in the ¡°Model Evaluation¡± section. We can then call the create_dataset() function to create the datasets required for the ES1 and ES2 cases, specifically ES1 combines datasets 1 and 2, whereas ES2 uses datasets 1 and 2 as a training set and dataset 3 as a test set. The complete example is listed below. Running the example creates three new CSV files, specifically ¡®es1.csv¡®, ¡®es2_train.csv¡®, and ¡®es2_test.csv¡® for the ES1 and ES2 cases respectively. The shapes of these datasets are also summarized. Next, we can evaluate models on the ES1 dataset. After some testing, it appears that standardizing the dataset results in better model skill for those methods that rely on distance values (KNN and SVM) and generally has no effect on other methods. Therefore a Pipeline is used to evaluate each algorithm that first standardizes the dataset. The complete example of spot checking algorithms on the new dataset is listed below. Running the example prints the estimated performance of each algorithm, including the mean and standard deviation over 5-fold cross-validation. The results suggest SVM might be worth looking at in more detail at 58% accuracy. The results are also presented as box-and-whisker plots showing the distribution of scores. Again, SVM appears to have good average performance and tight variance. Spot-check Algorithms on ES1 with last 19 observations We can pad each trace to a fixed length. This will then provide the flexibility to include more of the prior n observations in each sequence. The choice of n must also be balanced with the increase in padded values added to shorter sequences that in turn may negatively impact the performance of the model on those sequences. We can pad each sequence by adding the 0.0 value to the beginning of each variable sequence until a maximum length, e.g. 200 time steps, is reached. We can do this using the pad() NumPy function. The updated version of the create_dataset() function with padding support is below. We will try n=25 to include 25 of the last observations in each sequence in each vector. This value was found with a little trial and error, although you may want to explore whether other configurations result in better skill. Running the script again with the new function creates updated CSV files. Again, re-running the spot-check script on the data results in a small lift in model skill for SVM and also suggests that KNN might be worth investigating further. The box plots for KNN and SVM show good performance and relatively tight standard deviations. Spot-check Algorithms on ES1 with last 25 observations We can update the spot-check to grid search a suite of k values for the KNN algorithm to see if the skill of the model can be further improved with a little tuning. The complete example is listed below. Running the example prints the mean and standard deviation of the accuracy with k values from 1 to 21. We can see that a k=7 results in the best skill of 62.872%. The box and whisker plots of accuracy scores for k values show that k values around seven, such as five and six, also produce stable and well-performing models on the dataset. Spot-check KNN neighbors on ES1 with last 25 observations Now that we have some idea of a representation (n=25) and a model (KNN, k=7) that have some skill over a random prediction, we can test the approach on the harder ES2 dataset. Each model is trained on the combination of dataset 1 and 2, then evaluated on dataset 3. The k-fold cross-validation procedure is not used, so we would expect the scores to be noisy. The complete spot checking of algorithms for ES2 is listed below. Running the example reports the model accuracy on the ES2 scenario. We can see that KNN does well and that the KNN with seven neighbors found to perform well on ES1 also performs well on ES2. A bar chart of the accuracy scores helps to make the relative difference in performance between the methods clearer. Bar chart of model accuracy on ES2 The chosen representation and model configurations do have skill over a naive prediction with 50% accuracy. Further tuning may result in models with better skill, and we are a long way from the 95% and 89% accuracy reported in the paper on ES1 and ES2 respectively. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered the indoor movement prediction time series classification problem and how to engineer features and evaluate machine learning algorithms for the problem. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hi Jason.
Many thanks for this great and helpful tutorial.
I went through the paper of Davide Bacciu, et al. (2011) ¡°Predicting User Movements in Heterogeneous Indoor Environments by Reservoir Computing¡±. His approach consisting in Reservoir Computing and LI-ESN seems impressively efficient.
Do you see any opporunity to implement such approach in Python?
All the best,
Remi Not at this stage. I¡¯m skeptical of the ability to reproduce the results in most papers. Hello Jason, Great tutorials on Multivariate Time Series and LSTM; really enjoyed reading them and learned a lot. I am new to the field (still taking online deep learning courses) and have a general question (I better say I need a general suggestion). Suppose I have a multivariate time series data, and I want to build a classifier model (three-class classification with values 0, 1, or 2). What is the best approach to tackle this problem? LSTM or multi-channel CNN? And have you written any other tutorials? Thanks I recommend testing a suite of methods in order to discover what works best for your specific dataset. Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning.","Keyword(freq): room(17), observation(15), value(12), algorithm(10), dataset(10), feature(9), result(9), method(8), path(8), sequence(8)"
"6","vidhya",2018-09-14,"Heroes of Deep Learning: Top Takeaways for Aspiring Data Scientists from Andrew Ng¡¯s Interview Series","https://www.analyticsvidhya.com/blog/2018/09/heroes-deep-learning-top-takeaways-andrew-ng-interview-series/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 Andrew Ng is the most recognizable personality of the modern deep learning world. His machine learning course is cited as the starting point for anyone looking to understand the math behind algorithms. But even the great Andrew Ng looks up to and takes inspiration from other experts. In this amazing and in-depth video series, he has interviewed some of the most eminent personalities in the world of deep learning (eight heroes, to be precise). The interviews span the length and breadth of deep learning, including topics like backpropogation, GANs, transfer learning, etc. Even artificial intelligence crops up in between conversations. But don¡¯t worry if these terms sound overwhelming, we have listed down the key takeaways from each interview just for you. Source: Forbes The ¡°heroes¡± Andrew Ng has interviewed are: What a stellar cast of experts! Now it¡¯s time to dive in and look at the top takeaways from each video. Geoffrey Hinton is best known for his work on artificial neural networks (ANNs). His contributions in the field of deep learning are the main reason behind the success of the field and he is often called the ¡°Godfather of Deep Learning¡± (with good reason). His research on the backpropagation algorithm brought about a drastic change in the performance of deep learning models. Ian Goodfellow is a rockstar in the deep learning space and is currently working as a research scientist at Google Brain. He is best known for his invention of generative adversarial networks (GANs). His book on ¡°Deep Learning¡± covers a broad range of topics like mathematical and conceptual backgrounds and deep learning techniques used in the industry, which can be a good starting point for any deep learning enthusiast. We strongly recommend reading that book, it¡¯s free! Yoshua Bengio is a computer scientist, well known for his work on ANN and deep learning. He is the co-founder of Element AI, a Montreal-based business incubator that seeks to transform AI research into real-world business applications. Pieter Abbeel is the Director of the UC Berkeley Robot Learning Lab. His work in reinforcement learning is often cited by scholars as the best in the modern era. He has previously worked in a senior role at OpenAI. Yuanquing Lin is the Director at the Institute of Deep Learning at Baidu. He has a background in mathematics and physics, and holds a Ph.D in machine learning. A word of caution <U+2013> the English might be a little hard to understand in the video as it¡¯s not Mr.<U+00A0>Yuanquing¡¯s first language. Andrej Karpathy is the director of artificial intelligence and Autopilot Vision at Tesla. Like Pieter Abbeel, Andrej previously worked at OpenAI, but as a research scientist. He is a widely considered and cited as a leading expert in the field of computer vision, especially image recognition (though of course he¡¯s an expert in quite a lot of deep learning areas). This is one the most intriguing videos in the series! Ruslan Salakhutdinov is the director of AI Research at Apple and is known as the developer of Bayesian Program Learning. His areas of specialization are many, but are listed as probabilistic graphical models, large-scale optimization, and of course, deep learning. Here¡¯s a fun fact <U+2013> his doctoral adviser? None other than Geoffrey Hinton! Yann LeCun is the founding father of convolutional nets. He is currently the Chief AI Scientist and VP at Facebook. He is a professor, researcher, and R&D manager with academic and industry experience in AI, machine learning,<U+00A0>deep learning, computer vision, intelligent data analysis, data mining, data compression, digital library<U+00A0>systems, and robotics. And that¡¯s just scraping the surface of what this expert is capable of. This is easily the most fascinating interview series on YouTube concerning deep learning. There is SO MUCH to learn from each of these seven heroes. If you haven¡¯t seen these videos before, we¡¯re glad you stopped by because this will feel like hitting the jackpot. Andrew Ng is a wonderful interviewer and him conversing with other experts feels like a dream. Grab your pen and notebook because there¡¯s a whole host of things for you to learn.","Keyword(freq): expert(3), hero(3), gan(2), model(2), network(2), takeaway(2), topic(2), video(2), algorithm(1), ann(1)"
"7","vidhya",2018-09-13,"How Machine Learning Algorithms & Hardware Power Apple¡¯s Latest Watch and iPhones","https://www.analyticsvidhya.com/blog/2018/09/how-machine-learning-hardware-and-algorithms-power-apples-latest-watch-and-iphones/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 This is a great time to be a data scientist <U+2013> all the top tech giants are integrating machine learning into their flagship products and the demand for such professionals is at an all-time high. And it¡¯s only going to get better! Apple has been a major advocate of machine learning, and has packed it¡¯s products with features like FaceID, Augmented Reality, Animoji, Healthcare sensors, etc. While watching Apple¡¯s keynote event yesterday, I couldn¡¯t help but wonder at the new chip technology they have developed that uses the power of machine learning algorithms. In this article, we¡¯ll check out some of the ways Apple has used machine learning to enrich the user experience. And believe me, some of the numbers you¡¯ll see will blow your mind. And if you¡¯re already itching to get started with building your first ML models on an iPhone using Apple¡¯s CoreML, check out this excellent article. Source: The Verge Designed in-house by Apple¡¯s developers, the A12 chip features an even more advanced neural engine than last year (when the neural engine made it¡¯s official debut inside the A11 chip). The A11 chip powers the iPhone X, 8, and 8 Plus so you can imagine why the A12 has created quite a stir in the machine learning community. The A12 is using features as small as 7 nanometers as compared to 10 in the A11, which explains the acceleration in speed. And did you really think Apple would let the event slide without mentioning battery life? The A12 chip has a smart compute system that automatically recognizes which tasks should run on the primary part of the chip, which ones should be sent to the GPU, and which ones should be delegated to the neural engine. Source: Apple Insider The neural engine¡¯s key functions are two-fold: This year¡¯s engine has eight cores which is how the chip can perform 5 trillion operations per second. Last year¡¯s version had two cores and could go up to 600 billion operations per second. It¡¯s a nice microcosm of how rapidly technology is evolving in front of our eyes. And the neural engine can do even more.. It will help iPhone users take better pictures (how much better can you get every year?!). When you press the shutter button, the neural network identifies the kind of scene in the lens, and makes a clear distinction between any object in the image and the background. So next time you take a photograph, just remember how quick the neural network must be, to do all this in a matter of milliseconds. You can learn all about object detection and computer vision algorithms in our ¡®Computer Vision using Deep Learning¡® course! It¡¯s a comprehensive offering and an invaluable addition to your machine learning skillset. The Apple Watch Series 4 feels like a health monitoring device more than at any point since it¡¯s debut four years back. Of course all the excitement is around the watch¡¯s design and how it¡¯s 35% bigger than last year¡¯s product. But let¡¯s step out of that limelight and look at one of the more intriguing features <U+2013> new health sensors. The Watch comes with an electrocardiogram (ECG) sensor. Why is this important, you ask? Well for starters, it¡¯s the first smartwatch to pack in this feature. But more importantly, the sensor measure not just your heart¡¯s rate, but also it¡¯s rhythm. This helps monitor any irregular rhythm and the Watch immediately alerts you in case of any impending danger. These sensors have been approved by the FDA and the American Heart Association. Further, these the Series 4 watches are integrated with an improved accelerometer and gyroscope. This will help the sensors in detecting if the wearer has fallen over. Once a person has fallen over and shown no sign of movement for 60 seconds, the device sends out an emergency call to up to five (pre-defined) emergency contacts simultaneously. I¡¯m sure you must have guessed by now what¡¯s behind all these updates? Yes, it¡¯s machine learning. Healthcare, as I mentioned in this article, is ripe for taking in machine learning terms. There are billions of data points at play, and combining ML with domain expertise is where the jackpot lies. I¡¯m glad to see companies like Apple utilizing it, albeit in their own products. The competition between the likes of Apple, Google, and others is heating up and artificial intelligence and machine learning could be the key to winning the battle. Hardware is critical here <U+2013> as it gets significant upgrades each year, more and more complex algorithms can be built in. Fascinated by all this and looking for a way to get started with data science? Try out our ¡®Introduction to Data Science¡® course today! We will help you take your first steps into this awesome new world. You couldn¡¯t have picked a better time to get into data science, honestly. A quick glance at Apple¡¯s official job postings shows more than 400 openings for machine learning related positions. The question then remains whether there are enough experienced people to fulfill that demand.","Keyword(freq): apple(4), sensor(4), algorithm(3), feature(3), product(3), core(2), operation(2), billion(1), company(1), contact(1)"
"8","vidhya",2018-09-13,"A Gentle Introduction to Handling a Non-Stationary Time Series in Python","https://www.analyticsvidhya.com/blog/2018/09/non-stationary-time-series-python/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 What do these applications have in common: predicting the electricity consumption of a household for the next three months, estimating traffic on roads at certain periods, and predicting the price at which a stock will trade on the New York Stock Exchange? They all fall under the concept of time series data! You cannot accurately predict any of these results without the ¡®time¡¯ component. And as more and more data is generated in the world around us, time series forecasting keeps becoming an ever more critical technique for a data scientist to master. But time series is a complex topic with multiple facets at play simultaneously. For starters, making the time series stationary is critical if we want the forecasting model to work well. Why? Because most of the data you collect will have non-stationary trends. And if the spikes are erratic how can you be sure the model will work properly? The focus of this article is on the methods for checking stationarity in time series data. This article assumes that the reader is familiar with time series, ARIMA, and the concept of stationarity. Below are some references to brush up on the basics: ¡®Stationarity¡¯ is one of the most important concepts you will come across when working with time series data. A stationary series is one in which the properties <U+2013> mean, variance and covariance, do not vary with time. Let us understand this using an intuitive example. Consider the three plots shown below: The three examples shown above represent non-stationary time series. Now look at a fourth plot: In this case, the mean, variance and covariance are constant with time. This is what a stationary time series looks like. Think about this for a second <U+2013> predicting future values using which of the above plots would be easier? The fourth plot, right? Most statistical models require the series to be stationary to make effective and precise predictions. So to summarize, a stationary time series is the one for which the properties (namely mean, variance and covariance) do not depend on time. In the next section we will cover various methods to check if the given series is stationary or not. In this and the next few sections, I will be introducing methods to check the stationarity of time series data and the techniques required to deal with any non-stationary series. I have also provided the python code for applying each technique.<U+00A0>You can download the dataset we¡¯ll be using from this link: AirPassengers. Before we go ahead and analyze our dataset, let¡¯s load and preprocess<U+00A0>the data first. Looks like we are good to go! The next step is to determine whether a given series is stationary or not and deal with it accordingly. This section looks at some common methods which we can use to perform this check. Consider the plots we used in the previous section. We were able to identify the series in which mean and variance were changing with time, simply by looking at each plot. Similarly, we can plot the data and determine if the properties of the series are changing with time or not. Although its very clear that we have a trend (varying mean) in the above series, this visual approach might not always give accurate results. It is better to confirm the observations using some statistical tests. Instead of going for the visual test, we can use statistical tests like the unit root stationary tests. Unit root indicates that the statistical properties of a given series are not constant with time, which is the condition for stationary time series. Here is the mathematics explanation of the same : Suppose we have a time series : yt = a*yt-1 + ¥å t where yt is the value at the time instant t and ¥å t is the error term. In order to calculate yt we need the value of yt-1, which is : yt-1 = a*yt-2 + ¥å t-1 If we do that for all observations, the value of yt will come out to be: yt = an*yt-n + ¥Ò¥åt-i*ai If the value of a is 1 (unit) in the above equation, then the predictions will be equal to the yt-n and sum of all errors from t-n to t, which means that the variance will increase with time. This is knows as unit root in a time series. We know that for a stationary time series, the variance must not be a function of time. The unit root tests check the presence of unit root in the series by checking if value of a=1. Below are the two of the most commonly used unit root stationary tests: The Dickey Fuller test is one of the most popular statistical tests. It can be used to determine the presence of unit root in the series, and hence help us understand if the series is stationary or not. The null and alternate hypothesis of this test are: Null Hypothesis: The series has a unit root (value of a =1) Alternate Hypothesis: The series has no unit root. If we fail to reject the null hypothesis, we can say that the series is non-stationary. This means that the series can be linear or difference stationary (we will understand more about difference stationary in the next section). Python code: Results of ADF test: The ADF tests gives the following results <U+2013> test statistic, p value and the critical value at 1%, 5% , and 10% confidence intervals. The results of our test for this particular series are: Test for stationarity: If the test statistic is less than the critical value, we can reject the null hypothesis (aka the series is stationary). When the test statistic is greater than the critical value, we fail to reject the null hypothesis (which means the series is not stationary). In our above example, the test statistic > critical value, which implies that the series is not stationary. This confirms our original observation which we initially saw in the visual test. KPSS is another test for checking the stationarity of a time series (slightly less popular than the Dickey Fuller test). The null and alternate hypothesis for the KPSS test are opposite that of the ADF test, which often creates confusion. The authors of the KPSS test have defined the null hypothesis as the process is trend stationary, to an alternate hypothesis of a unit root series. We will understand the trend stationarity in detail in the next section. For now, let¡¯s focus on the implementation and see the results of the KPSS test. Null Hypothesis: The process is trend stationary. Alternate Hypothesis: The series has a unit root (series is not stationary). Python code: Results of KPSS test: Following are the results of the KPSS test <U+2013> Test statistic, p-value, and the critical value at 1%, 2.5%, <U+00A0>5%, and 10% confidence intervals. For the air passengers dataset, here are the results: Test for stationarity: If the test statistic is greater than the critical value, we reject the null hypothesis (series is not stationary). If the test statistic is less than the critical value, if fail to reject the null hypothesis (series is stationary). For the air passenger data, the value of the test statistic is greater than the critical value at all confidence intervals, and hence we can say that the series is not stationary. I usually perform both the statistical tests before I prepare a model for my time series data. It once happened that both the tests showed contradictory results. One of the tests showed that the series is stationary while the other showed that the series is not! I got stuck at this part for hours, trying to figure out how is this possible. As it turns out, there are more than one type of stationarity. So in summary, the ADF test has an alternate hypothesis of linear or difference stationary, while the KPSS test identifies trend-stationarity in a series. Let us understand the different types of stationarities and how to interpret the results of the above tests. It¡¯s always better to apply both the tests, so that we are sure that the series is truly stationary. Let us look at the possible outcomes of applying these stationary tests. Now that we are familiar with the concept of stationarity and its different types, we can finally move on to actually making our series stationary. Always keep in mind that in order to use time series forecasting models, it is necessary to convert any non-stationary series to a stationary series first. In this method, we compute the difference of consecutive terms in the series. Differencing is typically performed to get rid of the varying mean. Mathematically, differencing can be written as: yt¡® = yt <U+2013> y(t-1) where yt is the value at a time t Applying differencing on our series and plotting the results: In seasonal differencing, instead of calculating the difference between consecutive values, we calculate the difference between an observation and a previous observation from the same season. For example, an observation taken on a Monday will be subtracted from an observation taken on the previous Monday. Mathematically it can be written as: yt¡® = yt <U+2013> y(t-n) Transformations are used to stabilize the non-constant variance of a series. Common transformation methods include power transform, square root, and log transform. Let¡¯s do a quick log transform and differencing on our air passenger dataset: As you can see, this plot is a significant improvement over the previous plots. You can use square root or power transformation on the series and see if they come up with better results. Feel free to share your findings in the comments section below! In this article we covered different methods that can be used to check the stationarity of a time series. But the buck doesn¡¯t stop here. The next step is to apply a forecasting model on the series we obtained. You can refer to the following article to build such a model: Beginner¡¯s Guide to Time Series Forecast. You can connect with me in the comments section below if you have any questions or feedback on this article. Great post. For differencing, you can also use the diff method on pandas Series and DataFrame objects.","Keyword(freq): result(13), test(13), method(6), plot(4), property(4), interval(3), comment(2), model(2), observation(2), prediction(2)"
"9","vidhya",2018-09-11,"Artificial Intelligence, Machine Learning and Big Data <U+2013> A Comprehensive Report","https://www.analyticsvidhya.com/blog/2018/09/artificial-intelligence-machine-learning-and-big-data-a-comprehensive-report/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 Artificial Intelligence and Machine Learning are the hottest jobs in the industry right now. 2018 has seen an even bigger leap in interest in these fields and it is expected to grow exponentially in the next five years! For instance, did you know that more than 50,000 positions related to Data and Analytics are currently vacant in India? We are excited to release a comprehensive report together with Great Learning on how AI, ML and Big Data are changing and evolving the world around us. Additionally, this report aims to provide an overview of the kind of career opportunities available in these fields right now, and the different roles we might see in the future. The aim behind creating this report is to provide our Data Science community with the context of changes happening at a macro level, and how they can best prepare for these upcoming changes. So, if you are already a Data Science professional or want to get into Data Science, we expect this report to be useful in providing you a context and preparing you for the future. <U+2013> Kunal Jain, Founder and CEO, Analytics Vidhya Wondering what¡¯s in the report and if you should download it? Check out what all is included below: There are a whole host of amazing statistics and insights in the report that will blow your mind. For example<U+00A0><U+2013> there are over 10 lakh registered companies in India. A survey by Gartner shows that around 75% of these companies are either already investing or are planning to invest in the field of Big Data. Let¡¯s go through some of the most intriguing patterns and insights from this report. An oft-asked question I¡¯ve seen is <U+2013> which industries have the most job opportunities? Here¡¯s your answer: Data science isn¡¯t confined to one narrow field. Its reach spans across domains and applications. The banking and finance sector is clearly the biggest market for data science professionals. 44% of all jobs were created in this domain in 2017. Yes, 44%! E-commerce and healthcare have also emerged as promising areas for data science professionals. Python or R? Ah, that question again. But now we have a concrete answer! The below graph, using the job postings from Indeed.com, shows a neat analysis of the data science skills in demand these days: The most number of jobs listed contain SQL as a requirement. But Python has truly become a universally popular tool and is eating up that ground with incredible speed. These languages are followed by Java, Hadoop (especially for data engineers, a very necessary role), and R. SAS is a bit further behind Tableau round-up our top 10. This gives you a really good idea of what the industry demand is in today¡¯s market. It might be time to buckle up and upskill your existing skillset! What¡¯s in store for all the aspiring data scientists, engineers, and analysts? The below chart depicts the expected number of jobs by 2020 in various industries: Data science jobs in healthcare are expected to soar. Agriculture, transportation and aviation are also expected to integrate a lot of data science tasks soon (the transformation is well under way). Cyber security, lagging a touch behind at the moment, should see significant investment. It¡¯s a field ripe for data science and we expect to see professionals moving in that direction in the next couple of years. There is a whole lot more in this report. We have an entire section dedicated to understanding the different roles in data science, their expected skills, etc. So what are you waiting for?<U+00A0>Get your hands on the report right now!","Keyword(freq): job(5), professional(3), analytics(2), change(2), company(2), engineer(2), industry(2), insight(2), opportunity(2), role(2)"
"10","vidhya",2018-09-11,"Deep Learning Tutorial to Calculate the Screen Time of Actors in any Video (with Python codes)","https://www.analyticsvidhya.com/blog/2018/09/deep-learning-video-classification-python/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 When I started my deep learning journey, one of the first things I learned was image classification. It¡¯s such a fascinating part of the computer vision fraternity and I was completely immersed in it! But I have a curious mind and once I had a handle on image classification, I wondered if I could transfer that learning to videos. Was there a way to build a model that automatically identified specific people in a given video at a particular time interval? Turns out, there was and I¡¯m excited to share my approach with you! Source: Coastline Automation Now to give you some context on the problem we¡¯ll be solving, keep in mind that screen time is extremely important for an actor. It is directly related to the<U+00A0>money he/she gets. Just to give you a sense of this commission, did you know that Robert Downey Jr. Downey picked up $10 million for just 15 minutes of screen time in ¡°Spider-Man Homecoming¡±? Incredible. How cool would it be if we could take any video and calculate the screen time of any actor present in it? In this article, I will help you understand how to use deep learning on video data. To do this, we will be working with videos from the popular TOM and JERRY cartoon series. The aim is to calculate the screen time of both TOM and JERRY in any given video. Sounds interesting? Read on then! Note: This article assumes you have a prior knowledge of image classification using deep learning. If not, I recommend going through this article which will help you get a grasp of the basics of deep learning and image classification. Ever heard of a flip book? If you haven¡¯t, you¡¯re missing out! Check out the one below: (Source: giphy.com) We have a different image on each page of the book, and as we flip these pages, we get an animation of a shark dancing. You could even call it a kind of video. The visualization gets better the<U+00A0> faster we flip the pages. In other words, this visual is a collection of different images arranged in a particular order. Similarly, videos are nothing but a collection of a set of images. These images are called frames and can be combined to get the original video. So, a problem related to video data is not that different from an image classification or an object detection problem. There is just one extra step of extracting frames from the video. Remember, our challenge here is to calculate the screen time of both Tom and Jerry from a given video. Let me first summarize the steps we will follow in this article to crack this problem: Believe me, just following these steps will help you in solving many such video related problems in deep learning. Time to get our Python hats on now, and dig into this challenge. Let us start with importing all the necessary libraries. Go ahead and install the below libraries in case you haven¡¯t already: Now we will load the video and convert it into frames. You can download the video used for this example from this link. We will first capture the video from the given directory using the VideoCapture() function, and then we¡¯ll extract frames from the video and save them as an image using the imwrite() function. Let¡¯s code it: Done! Once this process is complete, ¡®Done!¡¯ will be printed on the screen as confirmation that the frames have been created. Let us try to visualize an image (frame). We will first read the image using the imread() function of matplotlib, and then plot it using the imshow() function. Getting excited, yet? This is the first frame from the video. We have extracted one frame for each second, from the entire duration of the video. Since the duration of the video is 4:58 minutes (298 seconds), we now have 298 images in total. Our task is to identify which image has TOM, and which image has JERRY. If our extracted images would have been similar to the ones present in the popular Imagenet dataset, this challenge could have been a breeze. How? We could simply have used models pre-trained on that Imagenet data and achieved a high accuracy score! But then where¡¯s the fun in that? We have cartoon images so it¡¯ll be very difficult (if not impossible) for any pre-trained model to identify TOM and JERRY in a given video. So how do we go about handling this? A possible solution is to manually give labels to a few of the images and train the model on them. Once the model has learned the patterns, we can use it to make predictions on a previously unseen set of images. Keep in mind that there could be frames when neither TOM nor JERRY are present. So, we will treat it as a multi-class classification problem. The classes which I have defined are: Don¡¯t worry, I have labelled all the images so you don¡¯t have to! Go ahead and download the mapping.csv file which contains each image name and their corresponding class (0 or 1 or 2). The mapping file contains two columns: Our next step is to read the images which we will do based on their names, aka, the Image_ID column. Tada! We now have the images with us. Remember, we need two things to train our model: Since there are three classes, we will one hot encode them using the to_categorical() function of keras.utils. We will be using a VGG16 pretrained model which takes an input image of shape (224 X 224 X 3). Since our images are in a different size, we need to reshape all of them. We will use the resize() function of skimage.transform to do this. All the images have been reshaped to 224 X 224 X 3. But before passing any input to the model, we must preprocess it as per the model¡¯s requirement. Otherwise, the model will not perform well enough. Use the preprocess_input() function of<U+00A0>keras.applications.vgg16 to perform this step. We also need a validation set to check the performance of the model on unseen images. We will make use of the train_test_split() function of the sklearn.model_selection module to randomly divide images into training and validation set. The next step is to build our model. As mentioned, we shall be using the VGG16 pretrained model for this task. Let us first import the required libraries to build the model: We will now load the VGG16 pretrained model and store it as base_model: We will make predictions using this model for X_train and X_valid, get the features, and then use those features to retrain the model. The shape of X_train and X_valid is (208, 7, 7, 512), (90, 7, 7, 512) respectively. In order to pass it to our neural network, we have to reshape it to 1-D. We will now preprocess the images and make them zero-centered which helps the model to converge faster. Finally, we will build our model. This step can be divided into 3 sub-steps: Let¡¯s check the summary of the model using the summary() function: We have a hidden layer with 1,024 neurons and an output layer with 3 neurons (since we have 3 classes to predict). Now we will compile our model: In the final step, we will fit the model and simultaneously also check its performance on the unseen images, i.e., validation images: We can see it is performing really well on the training as well as the validation images. We got an accuracy of around 85% on unseen images. And this is how we train a model on video data to get predictions for each frame. In the next section, we will try to calculate the screen time of TOM and JERRY in a new video. First, download the video we¡¯ll be using in this section from here. Once done, go ahead and load the video and extract frames from it. We will follow the same steps as we did above: Done! After extracting the frames from the new video, we will now load the test.csv file which contains the names of each extracted frame. Download the test.csv file and load it: Next, we will import the images for testing and then reshape them as per the requirements of the aforementioned pretrained model: We need to make changes to these images similar to the ones we did for the training images. We will preprocess the images, use the<U+00A0>base_model.predict() function to extract features from these images using the VGG16 pretrained model, reshape these images to 1-D form, and make them zero-centered: Since we have trained the model previously, we will make use of that model to make prediction for these images. Recall that Class ¡®1¡¯ represents the presence of JERRY, while Class ¡®2¡¯ represents the presence of TOM. We shall make use of the above predictions to calculate the screen time of both these legendary characters: And there you go! We have the total screen time of both TOM and JERRY in the given video. I tried and tested many things for this challenge <U+2013> some worked exceedingly well, while some ended up flat. In this section, I will elaborate a bit on some of the difficulties I faced, and then how I tackled them. After that, I have provided the entire code for the final model which gave me the best accuracy. First, I tried using the pretrained model without removing the top layer. The results were not satisfactory. The possible reason could be that these are the cartoon images and our pretrained model was trained on actual images and hence it was not able to classify these cartoon images. To tackle this problem, i retrained the pretrain model using few labelled images and the results were better from the previous results. Even after training on the labelled images, the accuracy was not satisfactory. The model was not able to perform well on the training images itself. So, i tried to increase the number of layers. Increasing the number of layers proved to be a good solution to increase the training accuracy but there was no sync between training and validation accuracy. The model was overfitting and its performance on the unseen data was not satisfactory. So I added a Dropout layer after every Dense layer and then there was good sync between training and validation accuracy. I noticed that the classes are imbalanced. TOM had more screen time so the predictions were dominated by it and most of the frames were predicted as TOM. To overcome this and make the classes balanced, i used compute_class_weight() function of sklearn.utils.class_weight module. It assigned higher weights to the classes with lower value counts as compared to the classes with higher value counts. I also used Model Checkpointing to save the best model, i.e. the model which produced lowest validation loss and then used that model to make the final predictions. I will summarize all the above mentioned steps and will give the final code now. The actual classes for the testing images can be found in testing.csv file. Done! Done! We got an accuracy of around 88% on the validation data and 64% on the test data using this model. One possible reason for getting a low accuracy on test data could be a lack of training data. As the model does not have much knowledge of cartoon images like TOM and JERRY, we must feed it more images during the training process. My advice would be to extract more frames from different TOM and JERRY videos, label them accordingly, and use them for training the model. Once the model has seen a plethora of images of these two characters, there¡¯s a good chance it will lead to a better classification result. Such models can help us in various fields: These are just a few examples where this technique can be used. You can come up with many more such applications on your own! Feel free to share your thoughts and feedback in the comments section below. Very interesting Hi Abdul, Glad you found it useful! Good job Pulkit!
I think it is a useful project too.
But the limit is the fact that we have generate each time images from a movie and label them. We have so to build a new model for each actor. Perhaps it is good to think now on automatic models, which are generalizable on any movie (autolabelled). Hi, As per my knowledge, I don¡¯t think there are pretrained models trained on the faces of actors (correct me if I am wrong). So, we need to give labels for training the model. As seen in this project, labeling only few images can produce good results. Will look forward and try to automate these labeling part. If you find some insights related to this, please share it here. It would be helpful to take this forward. I really enjoyed this project. Thanks for your work and sharing it! I am not sure, but it looks as if a fourth category of both Tom and Jerry being in a frame is overlooked? Hi,","Keyword(freq): image(39), frame(10), prediction(6), model(5), step(5), result(4), video(4), feature(3), library(3), application(2)"
