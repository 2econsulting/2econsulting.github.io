"site","date","headline","url_address","text"
"vidhya",2018-10-18,"Deep Learning in the Trenches: Understanding Inception Network from Scratch","https://www.analyticsvidhya.com/blog/2018/10/understanding-inception-network-from-scratch/","

FLASH SALE: Course On Introduction To Data Science | Use COUPON CODE: FLASH25 for 25% Discount| Work on 4 real-life projects |
Buy Now 

 Deep learning is rapidly gaining steam as more and more research papers emerge from around the world. These papers undoubtedly contain a ton of information, but they can often be difficult to parse through. And to understand them, you might have to go through that paper multiple number of times (and perhaps even other dependent papers!). This is truly a daunting task for non-academicians like us. Personally, I find the task of going through a research paper, interpreting the crux behind it, and implementing the code as an important skill every deep learning enthusiast and practitioner should possess. Practically implementing research ideas brings out the thought process of the author and also helps transform those ideas into real-world industry applications. So in this article (and the subsequent series of articles) my motive of writing is two-fold: This article assumes that you have a good grasp on the basics of deep learning. In case you don¡¯t, or simply need a refresher, check out the below articles first and then head back here pronto: This article focuses on the paper ¡°Going deeper with convolutions¡± from which the hallmark idea of inception network came out. Inception network was once considered a state-of-the-art deep learning architecture (or model) for solving image recognition and detection problems. It put forward a breakthrough performance on the ImageNet Visual Recognition Challenge (in 2014), which is a reputed platform for benchmarking image recognition and detection algorithms. Along with this, it set off a ton of research in the creation of new deep learning architectures with innovative and impactful ideas. We will go through the main ideas and suggestions propounded in the aforementioned paper and try to grasp the techniques within. In the words of the author: ¡°In this paper, we will focus on an efficient deep neural network architecture for computer vision, code named Inception, which derives its name from (¡¦) the famous ¡°we need to go deeper¡± internet meme.¡± That does sound intriguing, doesn¡¯t it? Well, read on then! There¡¯s a simple but powerful way of creating better deep learning models. You can just make a bigger model, either in terms of deepness, i.e., number of layers, or the number of neurons in each layer. But as you can imagine, this can often create complications: A solution for this, as the paper suggests, is to move on to sparsely connected network architectures which will replace fully connected network architectures, especially inside convolutional layers. This idea can be conceptualized in the below images: Densely connected architecture Sparsely connected architecture This paper proposes a new idea of creating deep architectures. This approach lets you maintain the ¡°computational budget¡±, while increasing the depth and width of the network. Sounds too good to be true! This is how the conceptualized idea looks: Let us look at the proposed architecture in a bit more detail. The paper proposes a new type of architecture <U+2013> GoogLeNet or Inception v1. It is basically a convolutional neural network (CNN) which is 27 layers deep. Below is the model summary: Notice in the above image that there is a layer called inception layer. This is actually the main idea behind the paper¡¯s approach. The inception layer is the core concept of a sparsely connected architecture. Idea of an Inception module Let me explain in a bit more detail what an inception layer is all about. Taking an excerpt from the paper: ¡°(Inception Layer) is a combination of all those layers (namely, 1¡¿1 Convolutional layer, 3¡¿3 Convolutional layer, 5¡¿5 Convolutional layer) with their output filter banks concatenated into a single output vector forming the input of the next stage.¡± Along with the above-mentioned layers, there are two major add-ons in the original inception layer: Inception Layer To understand the importance of the inception layer¡¯s structure, the author calls on the Hebbian principle from human learning. This says that ¡°neurons that fire together, wire together¡±. The author suggests that when creating a subsequent layer in a deep learning model, one should pay attention to the learnings of the previous layer. Suppose, for example, a layer in our deep learning model has learned to focus on individual parts of a face. The next layer of the network would probably focus on the overall face in the image to identify the different objects present there. Now to actually do this, the layer should have the appropriate filter sizes to detect different objects. This is where the inception layer comes to the fore. It allows the internal layers to pick and choose which filter size will be relevant to learn the required information. So even if the size of the face in the image is different (as seen in the images below), the layer works accordingly to recognize the face. For the first image, it would probably take a higher filter size, while it¡¯ll take a lower one for the second image. The overall architecture, with all the specifications, looks like this: Note that this architecture came about largely due to the authors participating in an image recognition and detection challenge. Hence there are quite a few ¡°bells and whistles¡± which they have explained in the paper. These include: Among this, the auxiliary training done by the authors is quite interesting and novel in nature. So we will focus on that for now. The details for the rest of the techniques can be taken from the paper itself, or in the implementation which we will see below. To prevent the middle part of the network from ¡°dying out¡±, the authors introduced two auxiliary classifiers (the purple boxes in the image). They essentially applied softmax to the outputs of two of the inception modules, and computed an auxiliary loss over the same labels. The total loss function is a weighted sum of the auxiliary loss and the real loss. The weight value used in the paper was 0.3 for each auxiliary loss. Now that you have understood the architecture of GoogLeNet and the intuition behind it, it¡¯s time to power up Python and implement our learnings using Keras! We will use the CIFAR-10 dataset for this purpose. CIFAR-10 is a popular image classification dataset. It consists of 60,000 images of 10 classes (each class is represented as a row in the above image). The dataset is divided into 50,000 training images and 10,000 test images. Note that you must have the required libraries installed to implement the code we will see in this section. This includes Keras and TensorFlow (as a back-end for Keras). You can refer to the official installation guide<U+00A0>in case you don¡¯t have Keras already installed on your machine. Now that we have dealt with the prerequisites, we can finally start coding the theory we covered in the earlier sections. The first thing we need to do is import all the necessary libraries and modules which we will use throughout the code. Our model gave an impressive 80%+ accuracy on the validation set, which proves that this model architecture is truly worth checking out! This was a really enjoyable article to write and I hope you found it equally useful. Inception v1 was the focal point on this article, wherein I explained the nitty gritty of what this framework is about and demonstrated how to implement it from scratch in Keras."
"vidhya",2018-10-17,"Must Read Books<U+00A0>for Beginners on Machine Learning and Artificial Intelligence","https://www.analyticsvidhya.com/blog/2018/10/read-books-for-beginners-machine-learning-artificial-intelligence/","

FLASH SALE: Course On Introduction To Data Science | Use COUPON CODE: FLASH25 for 25% Discount| Work on 4 real-life projects |
Buy Now 

 This article was originally published on October 25, 2015, and updated on October 17, 2018. Machine Learning has granted incredible power to humans. The power to run tasks in an automated manner, the power to make our lives comfortable,<U+00A0>the power to improve things continuously by studying decisions at a large scale, and the power to create species who think better than humans. This list can go on and on. Still sceptical about AI and ML? Read what Google¡¯s CEO Mr. Sundar Pichai had to say all the way back in 2015: ¡®Machine learning is a core, transformative way by which we¡¯re rethinking everything we¡¯re doing. We¡¯re thoughtfully applying it across all our products, be it search, ads, YouTube, or Play. We¡¯re in the early days, but you¡¯ll see us in a systematic way think about how we can apply machine learning to all these areas.¡¯ <U+2013> Sundar Pichai, CEO, Google Those who know of these advancements are keen to master this<U+00A0>concept, including us at Analytics Vidhya. When we started with this mission, we found various forms of digitized study material. They seemed promising and comprehensive, yet lacked a perspective. Our curiosity didn¡¯t let us rest for long and we resorted to books. When Elon Musk, one of the busiest men on the planet, was asked about his secret of success, he replied, ¡®I used to read books. A LOT¡¯. Later, Kimbal Musk, Elon¡¯s brother, said, ¡®He would even complete two books in a day¡¯. In this article, we¡¯ve listed some of the must-read books on Machine Learning and Artificial Intelligence. These books are in no particular rank or order. The motive of this article is not to promote any particular book, but to make you aware of a world which<U+00A0>exists beyond video tutorials, blogs and podcasts. I encourage you to check out these 10 Free E-books on Machine Learning as well which are a great starting point (or a refresher) for anyone in this field. We had to start with a book by the great Andrew Ng. It is still a work in progress, but several chapters have been released and can be downloaded FOR FREE today. This book will help the reader get up to speed with building AI systems. It will effectively<U+00A0>teach you how to make the various decisions required with organizing a machine learning project. There¡¯s no better person to start off this list, in our opinion. You can sign up on the site to receive updates as soon as a new chapter is released. Programming Collective Intelligence, PCI as it is popularly known, is one of the best books to start learning machine learning.<U+00A0>If there is one book to choose on machine learning <U+2013> it is this one. I haven¡¯t met a data scientist yet who has read this book and does not recommend to keep it on your bookshelf. A lot of them have re-read this book multiple times. The book was written long before data science and machine learning acquired the cult status they have today <U+2013> but the topics and chapters are entirely relevant even today! Some of the topics covered in the book are collaborative filtering techniques, search engine features, Bayesian filtering and Support vector machines.<U+00A0>If you don¡¯t have a copy of this book <U+2013> order it as soon as you finish reading this article!<U+00A0>The book uses Python to deliver machine learning in a fascinating manner. This book is written by Drew Conway and John Myles White. It is majorly based on data analysis in R. This book is best suited for beginners having a basic knowledge and grasp of R. It covers the use of advanced R in data wrangling. It has interesting case studies which will help you to understand the importance of using machine learning algorithms. After you¡¯ve read the above books, you are good to dive into the world of machine learning. And this is a great introductory book to start your journey. It<U+00A0>provides a nice overview of ML theorems<U+00A0>with pseudocode summaries of their algorithms. Apart from case studies,<U+00A0>Tom has used basic examples to help you understand these algorithms easily. Most of the experts you ask in this field never fail to mention this book which helped them at the start of their careers. It¡¯s such a well-written and explained book that we feel it should be made mandatory in every machine learning course! This is quite a popular book. It was written by Trevor Hastie, Robert Tibshirani and Jerome Friedman. This book aptly explains various machine learning algorithms mathematically from a statistical perspective. It provides a powerful world created by statistics and machine learning. This books lays emphasis on mathematical derivations to define the underlying logic behind an algorithm. Keep in mind that you need to have a rudimentary understanding of linear algebra before picking this up. There¡¯s a beginner friendly version of these concepts in a book by some of the same authors, called ¡®Introduction to Statistical Learning¡¯. Make sure you check that out if this one is too complex for you right now. Free PDF Link: Download This book is written by<U+00A0>Yaser Abu Mostafa, Malik Magdon-Ismail and Hsuan-Tien Lin. It provides a perfect introduction to machine learning.<U+00A0>This book prepares you to understand complex areas of machine learning. Yaser, a very popular and brilliant professor, has provided ¡®to the point¡¯ explanations instead of lengthy and go-around explanations. If you choose this book, I¡¯d suggest you to refer to online tutorials of Yaser Abu Mostafa as well. They¡¯re awesome. Free PDF Link: Download This book is written by Christopher M Bishop. This book serves as a excellent reference<U+00A0>for students keen to understand the use of statistical techniques in machine learning and pattern recognition. This books assumes the knowledge of linear algebra and multivariate calculus. It provides a comprehensive<U+00A0>introduction to statistical pattern recognition techniques using practice exercises. Free PDF Link: Download Folks interested in getting into Natural Language Processing (NLP) should read this book. It¡¯s written in a lucid and clear manner with extremely well-presented codes in Python. Readers are given access to well-annotated datasets to analyse and deal with unstructured data, linguistic structure in text, among other NLP things. The authors of this book are Steven Bird, Ewan Klein, and Edward Loper. A ML superteam! Who better to learn AI from than the great Peter Norvig? You have to take a course from Norvig to understand his style of teaching. But once you do, you will remember it for a long, long time. This book is written by Stuart Russell and Peter Norvig. It is best suited<U+00A0>for people new to A.I. More than just providing an overview of artificial intelligence,<U+00A0>this book<U+00A0>thoroughly covers subjects from search algorithms and reducing problems to search problems, working with logic, planning, and more advanced topics in AI, such as reasoning with partial observability, machine learning and language processing. Make it the first book on A.I in your book shelf. Free PDF Link: Download This book is written by Jeff Heaton. It teaches basic artificial intelligence algorithms such as<U+00A0>dimensionality, distance metrics, clustering, error calculation, hill climbing, Nelder Mead, and linear regression. It explains these algorithms using interesting examples and cases. Needless to say, this book requires good commands over mathematics. Otherwise, you¡¯ll have tough time deciphering the equations. Another one by Peter Norvig! This book teaches advanced common lisp techniques to build major A.I systems. It delves deep into the practical aspects of A.I and teaches its readers the method to build and debug robust practical programs. It also demonstrates superior programming style and essential AI concepts. I¡¯d recommend reading this book, if you are serious about a career in A.I specially. This book is written by Nils J Nilsson. After reading the above 3 books, you¡¯d like something which could challenge your mind. Here¡¯s what you are looking for. This books covers topics such as<U+00A0>Neural networks, genetic programming, computer vision, heuristic search, knowledge representation and reasoning, Bayes networks and explains them with great ease. I wouldn¡¯t recommend this book for a beginner. However, it¡¯s a must read for advanced level user. Nick Bostrom has authored (or co-authored) over 200 publications, including this book called Superintelligence. Most of the world is enthralled and captivated by what AI can do and it¡¯s potential to change the world. But how many of us stop to think about how AI will affect our society? Are we considering the human aspect at all when building AI products and services? If not, we really should. In this thought-provoking book, Nick Bostrom lays down a future scenario where machines reach the superintelligent stage and deliberately or accidentally lead to the extinction of humans. This might sound like a sci-fi movie plot, but the way Mr. Bostrom has laid down his arguments and the thinking behind them will definitely sway you and make you take him seriously. We consider this a must-read for everyone working in the AI space. Similar to the above idea propounded by Nick Bostrom, Ray Kurzweil¡¯s ¡®Singularity is Near¡¯ delves into the thick depths of superintelligent machines. It is a slightly long read, but well worth it in the end. The way Mr. Ray has described the Singularity is breathtaking and will make you stop in your tracks. Singularity, as<U+00A0> Ray Kurzweil has described it, is the point where humans and the intelligence of machines will merge. Once this happens, machines will be far more intelligent than all of the human species combined. It¡¯s NOT science fiction but a truly poignant description of what might happen in the future if we aren¡¯t careful with what and how we work with AI. When Stephan Hawking endorses a book, one sits up and listens. This book by Max Tegmark is an international bestseller and deals with the topic of superintelligence. Some of the basic questions this book asks (and answers) are (taken from Amazon¡¯s summary): How can we grow our prosperity through automation, without leaving people lacking income or purpose? How can we ensure that future AI systems do what we want without crashing, malfunctioning or getting hacked? Should we fear an arms race in lethal autonomous weapons? Will AI help life flourish as never before, or will machines eventually outsmart us at all tasks, and even, perhaps, replace us altogether? This is one of our favorite books in this list. Can there be just one algorithm that deals with all the aspects of technology? Instead of building AI products for specific functions, can we build one single algorithm for all functions? This thought is quite similar to what Albert Einstein spent the latter years of his life trying to discover. Pedro Domingos is a masterful writer, and he deals with the intricacies of his subject extremely well. Make sure you add this to your reading list! Disclosure:<U+00A0>The Amazon links in this article are affiliate links. If you buy a book through this link, we would get paid through Amazon. This is one of the ways for us to cover our costs while we continue to create these awesome articles. Further, the list reflects our recommendation based on content of book and is no way influenced by the commission. This is just the tip of the iceberg. Books are a wonderful source of knowledge for anyone willing to learn from them. This collection spans various aspects of AI and ML <U+2013> from the mathematics and statistics side to the intangible factors like ethics and impact of society. All of these should be considered together when working on an AI and ML project. Having said that, there is truly no substitute for experience. Once you¡¯ve devoured all these books can provide, always apply your learning to real-world problems and challenges. And as always, if you have any questions or suggestions for us on this article, feel free to share them in the comments section below. We look forward to connecting with you! Really good. It would be great if the pdf links are shared.. Thanks. I¡¯ll compile the links and share them with you shortly. Please paste the link of some of their legal pdf¡¯s uploaded by their authors. I¡¯ll compile the links and share them with you shortly. may be in a google drive. Please mail me the links of the pdfs of these books and any other data science related books materials which you have.
It would be great and a treasure house if these are uploaded in a drive and granted access. Loved your suggestion. I¡¯m working on it. I¡¯ll share the link as soon as pdf links are compiled. I second DK Samuel¡¯s comment. A reputed site like AV should be sharing only those (free) PDFs which are made available by the authors or publishers (e.g. ESL by Hastie et al.). Agree with your suggestion. I¡¯ve already mentioned this part in the end notes section. Great article! By the way, there are actually 3 authors for Learning from Data, not only Prof. Yaser Abu Mostafa. You forgot to mention the other 2 authors. I missed them! My Bad. Thank you so much for highlighting this error, I would have never known otherwise. Appreciate it.
Added now. Quite helpful information on Machine Learning. Do share the links to PDFs. Hi. Thanks for this useful post, especially for beginers. It¡¯ll be great if you can share those pdfs. Quite fascinating list, please share the links for legal pdf¡¯s uploaded by their authors.. And would it be possible to compile the list of books on Business/Data Analytics. Thanks for this great list! Could you share the pdfs please? I think some of the more honourable mentions are Kevin Murphy¡¯s Machine Learning, Pattern Classification by Duda, Hart, et, al and Hal Daume¡¯s A course in Machine learning. Hello all, I have added the pdf download links below their respective books in this article. Keep Learning Machine Learning and AI. <U+0001F642> I¡¯d recommend ¡°An Introduction to Statistical Learning with Applications in R¡± by James, Witten, Hastie, Tibshirani before ¡°The Elements of Statistical Learning.¡± I wouldn¡¯t consider the latter an intro book. What do you think about Applied Predictive Modeling? After you study machine learning by yourself by reading book, you might need to study some practical exercises to be better. I would recommend to take the machine leaning courses that I took, because it covered almost everything that I wanted. http://www.thedevmasters.com/machine-learning-in-python/ One great ML book that is not in your list: ML A probabilistic perspective by Kevin P Murphy I am beginners with the basic knowledge on python and want to work more on ML. I do not know the right direction. Please share me ur thoughts as a beginners This are the indeed the best resource for machine learning. A well structured article which clearly explains the relevance of each book. Boy, this site is serious about giving people information. Good going boys. Cheers. The download links for most of them are not working. Can you kindly share. Thanks"
"vidhya",2018-10-16,"An NLP Approach to Mining Online Reviews using Topic Modeling (with Python codes)","https://www.analyticsvidhya.com/blog/2018/10/mining-online-reviews-topic-modeling-lda/","

FLASH SALE: Course On Introduction To Data Science | Use COUPON CODE: FLASH25 for 25% Discount| Work on 4 real-life projects |
Buy Now 

 E-commerce has revolutionized the way we shop. That phone you¡¯ve been saving up to buy for months? It¡¯s just a search and a few clicks away.<U+00A0>Items are delivered within a matter of days (sometimes even the next day!). For online retailers, there are no constraints related to inventory management or space management They can sell as many different products as they want. Brick and mortar stores can keep only a limited number of products due to the finite space they have available. I remember when I used to place orders for books at my local bookstore, and it used to take over a week for the book to arrive. It seems like a story from the ancient times now! Source: http://www.yeebaplay.com.br But online shopping comes with its own caveats. One of the biggest challenges is verifying the authenticity of a product. Is it as good as advertised on the e-commerce site? Will the product last more than a year? Are the reviews given by other customers really true or are they false advertising? These are important questions customers need to ask before splurging their money. This is a great place to experiment and apply Natural Language Processing (NLP) techniques.<U+00A0>This article will help you understand the significance of harnessing online product reviews with the help of Topic Modeling. Please go through the below articles in case you need a quick refresher on Topic Modeling: A few days back, I took the e-commerce plunge and purchased a smartphone online. It was well within my budget, and it had an above decent rating of 4.5 out of 5. Unfortunately, it turned out to be a bad decision as the battery backup was well below par. I didn¡¯t go through the reviews of the product and made a hasty decision to buy it based on its ratings alone. And I know I¡¯m not the only one out there who made this mistake! Ratings alone do not give a complete picture of the products we wish to purchase, as I found to my detriment. So, as a precautionary measure, I always advise people to read a product¡¯s reviews before deciding whether to buy it or not. But then an interesting problem comes up. What if the number of reviews is in the hundreds or thousands? It¡¯s just not feasible to go through all those reviews, right? And this is where natural language processing comes up trumps. A problem statement is the seed from which your analysis blooms. Therefore, it is really important to have a solid, clear and well-defined problem statement. How we can analyze a large number of online reviews using Natural Language Processing (NLP)? Let¡¯s define this problem. Online product reviews are a great source of information for consumers. From the sellers¡¯ point of view, online reviews can be used to gauge the consumers¡¯ feedback on the products or services they are selling. However, since these online reviews are quite often overwhelming in terms of numbers and information, an intelligent system, capable of finding key insights (topics) from these reviews, will be of great help for both the consumers and the sellers. This system will serve two purposes: To solve this task, we will use the concept of Topic Modeling (LDA) on Amazon Automotive Review data. You can download it from this<U+00A0>link. Similar datasets for other categories of products can be found here. As the name suggests, Topic Modeling is a process to automatically identify topics present in a text object and to derive hidden patterns exhibited by a text corpus. Topic Models are very useful for multiple purposes, including: A good topic model, when trained on some<U+00A0>text about the stock market, should result in topics like ¡°bid¡±, ¡°trading¡±, ¡°dividend¡±, ¡°exchange¡±, etc. The below image illustrates how a typical topic model works: In our case, instead of text documents, we have thousands of online product reviews for the items listed under the ¡®Automotive¡¯ category. Our aim here is to extract a certain number of groups of important words from the reviews. These groups of words are basically the topics which would help in ascertaining what the consumers are actually talking about in the reviews. In this section, we¡¯ll power up our Jupyter notebooks (or any other IDE you use for Python!). Here we¡¯ll work on the problem statement defined above to extract useful topics from our online reviews dataset using the concept of Latent Dirichlet Allocation (LDA). Note: As I mentioned in the introduction, I highly recommend going through this article to understand what LDA is and how it works. Let¡¯s first load all the necessary libraries: To import the data, first extract the data to your working directory and then use the<U+00A0>read_json( ) function of pandas to read it into a pandas dataframe. As you can see, the data contains the following columns: For the scope of our analysis and this article, we will be using only the reviews column, i.e.,<U+00A0>reviewText. Data preprocessing and cleaning<U+00A0>is an important step before any text mining task, in this step, we will remove the punctuations, stopwords and normalize the reviews as much as possible. After every preprocessing step, it is a good practice to check the most frequent words in the data. Therefore, let¡¯s define a function that would plot a bar graph of n most frequent words in the data. Let¡¯s try this function and find out which are the most common words in our reviews dataset. Most common words are ¡®the¡¯, ¡®and¡¯, ¡®to¡¯, so on and so forth. These words are not so important for our task and they do not tell any story. We¡¯ have to get rid of these kinds of words. Before that let¡¯s remove the punctuations and numbers from our text data. Let¡¯s try to remove the stopwords<U+00A0>and short words (<2 letters) from the reviews. Let¡¯s again plot the most frequent words and see if the more significant words have come out. We can see some improvement here. Terms like ¡®battery¡¯, ¡®price¡¯, ¡®product¡¯, ¡®oil¡¯ have come up which are quite relevant for the Automotive category. However, we still have neutral terms like ¡®the¡¯, ¡®this¡¯, ¡®much¡¯, ¡®they¡¯ which are not that relevant. To further remove noise from the text we can use lemmatization from the<U+00A0>spaCy library. It reduces any given word to its base form thereby reducing multiple forms of a word to a single word. Let¡¯s tokenize the reviews and then lemmatize the tokenized reviews. As you can see, we have not just lemmatized the words but also filtered only nouns and adjectives. Let¡¯s de-tokenize the lemmatized reviews and plot the most common words. It seems that now most frequent terms in our data are relevant. We can now go ahead and start building our topic model. We will start by creating the term dictionary of our corpus, where every unique term is assigned an index Then we will convert the list of reviews (reviews_2) into a Document Term Matrix using the dictionary prepared above. The code above will take a while. Please note that I have specified the number of topics as 7 for this model using the num_topics parameter. You can specify any number of topics using the same parameter. Let¡¯s print out the topics that our LDA model has learned. The fourth topic Topic 3<U+00A0>has terms like ¡®towel¡¯, ¡®clean¡¯, ¡®wax¡¯, ¡®water¡¯, indicating that the topic is very much related to car-wash. Similarly, Topic 6<U+00A0>seems to be about the overall value of the product as it has<U+00A0>terms like ¡®price¡¯, ¡®quality¡¯, and ¡®worth¡¯. To visualize our topics in a 2-dimensional space we will use the pyLDAvis library. This visualization is interactive in nature and displays topics along with the most relevant words. Apart from topic modeling, there are many other NLP methods as well which are used for analyzing and understanding online reviews. Some of them are listed below: Information retrieval saves us from the labor of going through product reviews one by one. It gives us a fair idea of what other consumers are talking about the product. However, it does not tell us whether the reviews are positive, neutral, or negative. This becomes an extension of the problem of information retrieval where we don¡¯t just have to extract the topics, but also determine the sentiment. This is an interesting task which we will cover in the next article. Topic modeling is one of the most popular NLP techniques with several real-world applications such as dimensionality reduction, text summarization, recommendation engine, etc.. The purpose of this article was to demonstrate the application of LDA on a raw, crowd-generated text data. I encourage you to implement the code on other datasets and share your findings. If you have any<U+00A0>suggestion, doubt, or anything else that you wish to share regarding topic modeling, then please feel free to use the comments section below. Full code is available<U+00A0>here. Thanks man Thanks pratik. That was a nice article to read. Is there any techniques to find out the synonyms and antonyms in NLP? Hi Gokul, You can use nltk library and WordNet database to find synonym/antonym of a word. Thanks!"
"vidhya",2018-10-14,"DataHack Radio #12: Exploring the Nuts and Bolts of Natural Language Processing with Sebastian Ruder","https://www.analyticsvidhya.com/blog/2018/10/datahack-radio-podcast-nlp-research-sebastian-ruder/","

FLASH SALE: Course On Introduction To Data Science | Use COUPON CODE: FLASH25 for 25% Discount| Work on 4 real-life projects |
Buy Now 

 There¡¯s text everywhere around us, from digital sources like social media to physical objects like books and print media. The amount of text data being generated every day is mind boggling and yet we¡¯re not even close to harnessing the full power of natural language processing. I see a ton of aspiring data scientists interested in this field, but they often turn away daunted by the challenges NLP presents. It¡¯s such a niche line of work, and we at Analytics Vidhya would love to see more of our community actively participate in ground-breaking work in this field. So we thought what better way to do this than get an NLP expert on our DataHack Radio podcast? Yes, we¡¯ve got none other than Sebastian Ruder in Episode 12! This podcast is a knowledge goldmine for NLP enthusiasts, so make sure you tune in. It¡¯s catapulted to the top of my machine learning recommended podcasts. I strongly feel every aspiring and established data science professional should take the time to hear Sebastian talk about the diverse and often complex NLP domain. If you¡¯re looking for a place to get started with NLP, you¡¯ve landed at the right place! Check out Analytics Vidhya¡¯s Natural Language Processing using Python course and enrol yourself TODAY! Subscribe to DataHack Radio today and listen to this, as well as all previous episodes, on any of the below platforms: Sebastian¡¯s background is in computational linguistics, which is essentially a combination of computer science and linguistics. His interest in mathematics and languages piqued in high school and he carved a career out of that. He completed his graduation in Germany in this field as well, and has been immersed in the research side of things ever since. This is a very niche field so there are a lot of things Sebastian had to pick up from scratch and learn on his own. Quite an inspiring story for folks looking to transition into machine learning <U+2013> a healthy dose of passion added to tons of dedication and an inquisitive mind. He is currently working at Aylien as a research scientist and also pursuing a Ph.D. from the National University of Ireland in, you guessed it, Natural Language Processing.<U+00A0>Relationship extraction, named entity recognition and sentiment analysis are some of the areas where Sebastian has worked during his initial Ph.D. years. There¡¯s a certain bias in the machine learning world when it comes to NLP. When someone mentions text, the first language that pops into our mind is English. How difficult is it to transfer a NLP model between languages? Next to impossible, as it turns out. Sebastian explained this with an example between his native tongue German and English. German is more syntactically richer than English, while in the latter you can go a long way with techniques like tokenization and building models on the word-level. In German, you need to be more careful of how words are composed. This is important in computational linguistics as hierarchy on words matters. And of course, a common difference is how the words as used. Because of this, there are different rules in place for individual languages which is what makes working with text so challenging. For sentiment analysis, Sebastian mentioned that the primary example is looking at different categories of product reviews. These are usually already well-defined and getting training data is comparatively easier. Apart from this, social media data (especially Twitter) is popularly used to mine text and extract sentiments. Other sources that are referred to include print media, like digital newspaper articles, magazines, blogs, among others. If you¡¯re applying deep learning techniques, then scanned images of text can also be used for training your model. Sebastian recently co-authored a fascinating research paper with the great Jeremy Howard called ¡®Universal Language Model Language Fine-tuning¡¯ (ULMFiT). The paper made waves in the NLP community as soon as it was released, and the techniques are available in the fastai library. ULMFiT is an effective transfer learning method that can be applied to almost any NLP task. At the time of the release, it outperformed six state-of-the-art text classification tasks. Sebastian and Jeremy have done all the hard work for you <U+2013> they have released pretrained models that you can plug into your existing project and generate incredibly accurate text classification results. You can read the paper in full here. The most prominent challenge in most machine learning applications is first getting the data you need to train the model, and then find the right amount of computational resources to actually do the training. This has often proved to be a step too far for a lot of project. So Sebastian introduced us to the idea of increased sample efficiency wherein you can train models without having to collect millions of text data points. In addition to this, the trained model should not overfit on this relatively smaller sample, and should generalize well. Another challenge, which we touched on earlier, is the lack of datasets in non-English languages. The majority of data, and subsequently algorithms, are from English origins. We should seriously think about democratizing data from other languages to reduce this gap and eliminate the current state of bias."
