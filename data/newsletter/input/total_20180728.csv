"","site","date","headline","url_address","text","keyword"
"1","kaggle",2018-07-26,"Data Notes: Winning Solutions of Kaggle Competitions","http://blog.kaggle.com/2018/07/26/data-notes-winning-solutions-of-kaggle-competitions/","Winners, tumors, and avocados: Enjoy these new, intriguing, and overlooked datasets and kernels. 1.<U+00A0><U+0001F3C6> Winning Solutions of Kaggle Competitions (link) 2.<U+00A0><U+0001F3E5> Tumor Diagnosis - Neural Net from First Principles (link) 3.<U+00A0><U+0001F494> Cardiomegaly Pretrained VGG16 (link) 4.<U+00A0><U+0001F637> Xception Model for Chest X-Rays (link) 5.<U+00A0><U+0001F951> Predicting Prices of Avocados (link) 6.<U+00A0><U+0001F4F1> Visual Analysis of Apps on AppleStore (link) 7.<U+00A0><U+0001F6B2> The Beautiful Tour de France (link) 8.<U+00A0><U+0001F4F8> Dataset: NIH DeepLesion Subset (link) 9.<U+00A0><U+0001F4C8> Dataset: Financial Tweets (link) 10.<U+00A0><U+0001F913> Dataset: Face Detection in Images (link) Interested in financial data? Here's an overview of methods for working with a time series! Copyright ¨Ï 2018 Kaggle, All rights reserved.","Keyword(freq): avocado(2), app(1), competition(1), dataset(1), data(1), image(1), kernel(1), method(1), price(1), principle(1)"
"2","datacamp",2018-07-25,"New Project: Visualizing Inequalities in Life Expectancy","https://www.datacamp.com/community/blog/project-visualizing-inequalities-life-expectancy","Here is the project link. Do women live longer than men? How long? Does it happen everywhere? Is life expectancy increasing? Everywhere? Which is the country with the lowest life expectancy? Which is the one with the highest? In this Project, you will answer all these questions by manipulating and visualizing United Nations life expectancy data using ggplot2. We recommend that you have completed Introduction to the Tidyverse and Chapter 2 of Cleaning Data in R prior to starting this Project. The dataset can be found here and contains the average life expectancies of men and women by country (in years). It covers four periods: 1985-1990, 1990-1995, 1995-2000, and 2000-2005.","Keyword(freq): data(2), man(2), woman(2), expectancy(1), nation(1), period(1), question(1), NA(NA), NA(NA), NA(NA)"
"3","datacamp",2018-07-25,"Data Science at Doctors without Borders (Transcript)","https://www.datacamp.com/community/blog/data-science-doctors-without-borders","Here is a link to the podcast. Hugo:    Hi there Derek, and welcome to DataFramed. Derek:    Hi. How are you? Hugo:    I'm very well. And it's great to have you on the show. How are you? Derek:    I'm doing great. I'm excited to be on the show today. Hugo:    This is super exciting. I'm in Sydney, Australia at the moment. Whereabouts are you? Derek:    I am speaking to you from Southern Myanmar in a place called Dawei. Hugo:    And are you in an office there or whereabouts are you? Derek:    Currently in Dawei, I'm actually in an office. I'm in an office that's kind of attached to a health clinic here in Dawei. I work with Doctors Without Borders and we have been running an HIV clinic out of Dawei for the last 18 years. So that's kind of where I'm speaking to you from for the moment. Hugo:    Okay, great. What else are you doing in Myanmar with Doctors Without Borders? Derek:    Currently we have two projects. We have the HIV clinic in Dawei which has been operational for about 18 years, but we also recently have opened up a project in Northern Myanmar in Nagaland in a town called Lahai, and by comparison, that's only been up and running for about a year now. We don't necessarily do a lot of HIV up there, but we're doing more of supporting the health infrastructure up there and doing more general health support. So very, very different than what we do in Dawei. Hugo:    So these are two very different projects that you're involved with. I'd love for you to abstract from that a little bit and let me know, or tell us a bit more about your general role at MSF. For listeners out there, we'll be referring to Doctors Without Borders as Doctors Without Borders, Medecins Sans Frontieres, and MSF which is an acronym, interchangeably. Hopefully you can stick along with that. Hugo: So Derek, yes. Tell us more generally about your role at MSF. Derek:    I'm actually currently the epidemiologist here. Basically, I help out with a lot of operational research and a lot of monitoring and evaluation. In Dawei, where I am right now, we're doing a bit of operational research on Hepatitis C. We are currently trying to treat people for Hepatitis C in Dawei. Recently, the price for treatment has dropped considerably from tens of thousands of dollars for treatment down to a few hundred dollars to treat people. And so here, we're trying to sort of scale up the treatment for Hepatitis C because it's more prevalent here than in other places in the world.
    In Lahai, I do operational research as well but I do a lot of monitoring and evaluation so the projects of our doctors who go to Lahe township villages, we're kind of just monitoring the routine data that they collect there. But we also have recently done a big baseline health survey that assesses ... it's township-wide representative and it assesses the health of the several different villages there. Hugo:    Fantastic. And that's something that we're going to get into a lot later in this conversation. But before that, you said you worked as an epidemiologist which of course, involves a lot of statistics and thinking about data management, data provenance, all of these types of things. More generally, a lot of things which intersect with data science. And I'm wondering how you think about data science, how you got into it originally, and what your path has been to end up in Myanmar working for MSF. Derek:    I kind of have an interesting path to data science. Originally, I was actually a biochemistry major way, way, way, way back in undergrad decades ago. I guess I kind of realized it very early and ... I got lucky, and realized that working in a lab, while very challenging and super rewarding, is not very social. There were internships that I worked in where I used to run kind of like a mass spec machine. A mass spectrometer. And back in the day, these things were huge, like the size of a car. Basically, I would just be sitting in a basement and the only people I would see would be occasionally my supervisor, maybe one other person, and that would be for 8 to 10 hours a day. I was like, I can't do this. I have to do something a little more adventurous.
    And so I kind of got into public health, but I still wanted to do a lot of science. I still wanted to do a lot of research. And so epidemiology, it's a great mix. You take a lot of data, and you get to collect a lot of data, which is really fun, but you get to do that kind of in the field, so to speak. You don't necessarily have to be going to the Northern parts of remote areas to do it. You can do it in a clinic. You can do it with just the data that people collect. And then you analyze it, and then you look for patterns or outcomes, or how things are associated with one another. It's like puzzle work, mixed with being a little bit of Indiana Jones. It's great. I love it. I'm super, super lucky to be able to do this. Hugo:    For sure. And how did you actually get involved in MSF originally? Derek:    Initially, I was in grad school and I used to do a lot of HIV work, and STDs in general. I ended up getting very lucky and getting a chance to go to Malawi to look at ... Initially, I was looking at using an HIV drug called tenofovir as a first line HIV drug and for people who know about antiretrovirals and HIV drugs, that's how old I am. Tenofovir became a first line drug in 2010 in Malawi so that's a long time ago. So I just kind of got into that. I was like, hey, this is really great. I can do data science. I can do my own surveys. Collect my own data. Analyze it. Use a lot of statistics. But then get to travel and get to meet really interesting, really crazy people. It was really great.
    From there, when I finished school, I was thinking, well, what to do now? And so, I met somebody who actually worked on ... MSF has refugee boats in the Mediterranean, off of Italy, and he worked on one of the boats helping refugees. Basically, the boats that would come from Northern Africa with two, three, five hundred people, trying to get to Europe. MSF has these boats that kind of help. Basically, just bring people in to make sure that they're safe, they don't die, they don't drown. And he said it was the most intense experience he's ever had. And I was like, ""I'll join MSF. I'll see if I can look at the data and things they collect.""
    And it turns out that, because MSF has operations all over the world, they need a lot of help with data science because that knowledge that they gain helps formulate and helps shape their policies. Much in the same way that when you work in a lab, that you publish a paper, people are like, ""Oh, this molecule is related to another molecule. We're going to go with this."" And that helps shape science. You get to help shape health policy of sorts. Hugo:    For sure. And I love that you mentioned the data collection process is something that you're very passionate about and involved in, and that's something we'll get to, particularly with respect to the baseline health assessment you've been doing in Lahai township because I think a lot of working data scientists and a lot of our listeners, when they think about data science and data collection, they think clicks, and browser-based stuff, and stuff that's put in a database as opposed to getting there on the ground into remote areas and conducting the types of surveys you do. That's a little teaser for where we're headed.
    But before we get there, I think a lot of people have a sense of what Doctors Without Borders does, but I'd love a brief rundown from you on the work that Doctors Without Borders does from your position on the ground. Derek:    Doctors Without Borders basically is a kind of an impartial and neutral NGO that provides healthcare to anybody, particularly in humanitarian crises. Be that natural disasters like earthquakes or typhoons, or more like manmade disasters like in war zones. They provide humanitarian relief. And so what Doctors Without Borders does is they go to these areas and they basically provide humanitarian aid. Most of the projects for example that I'm a part of, tend to be very short term. Before I came to Myanmar, I was actually in a refugee camp in Northern Uganda doing actually a big household survey on mosquito nets, but in the refugee camp itself. So we were just living there and doing work there but it was short. I was only there for about four months. Whereas compared to a lot of development work where they can be in an area, for example like Red Cross or USAID, there's plenty of development NGO's, that can be in an area for years, for decades. So that's kind of what MSF does, is provide humanitarian aid to places in crises. Hugo:    Great. Given that context around what is essentially a mission statement of MSF, what are the biggest challenges that the organization faces that data science and analysis and analytics can help to solve? Derek:    Basically, getting to know, I would say, in my experience, getting to know the context of where a health clinic works, and how do you take that knowledge and then inform the policies that bring your decisions forward. Because MSF works in so many different areas and so many different countries, it's critical that they know about the context that they're working in, and every place is different. When I was in Uganda, that is completely different than what I'm doing here now at Myanmar. The people who help decide what to do in terms of projects, and in terms of policies, they need as much information as they can get.
    And so as an epidemiologist, my job is to basically do monitoring and evaluation of some of these projects, and then putting together internal reports that people can read and be then like, oh, okay, these people that are taking Hepatitis C drugs, for example, maybe 25% of them don't complete their treatment. What are we going to do about it? Or in the case of like when I was in Uganda, we were looking at passing out mosquito nets to people in the refugee camp but the problem with that also is that a lot of times, people will tend to misuse the nets. A lot of times you'll use it for fishing, and I didn't know this. I actually didn't know this until I started working there. Until we started collecting the data to look at this. The nets are really strong, and they're just perfect for catching fish. And they're already in a little basket type shape and you just throw it in the water and then they're actually really good for fishing, which is a total misuse of the nets.
    The survey that we did there, helped us form these educational campaigns. So we would actually go to different parts of the camp and be like, ""Hey guys. Don't go fishing with this. This is more for mosquitoes."" And so it really helped get people to understand to use the nets better, and that's the purpose of the data. So you get the data, and that actually helps shape what everybody is going to be doing. So it's cool in the sense that if you stick around a project long enough, you actually get to see your results be translated into action which is something that I know when I used to work in the States, sometimes you can publish a paper, and you just never see the results. And then it gets really depressing when you realize that only like three people have cited your paper and you're just like, ""Oh, okay."" Like what's the point? Hugo:    Yeah. As I said  before, it really seems like data science in this form is so different to what people think of when they think of tech data science for example. Maybe you could speak to the types of differences that are I think dominant in your mind for the respectives. Derek:    Yeah. It's different. It's funny because at its core, the statistics in study design are very, very similar and very much kind of the same. Which I think is very fascinating that you can take your survival analysis or your logistic regression or your cluster designs and you can apply it to schools in England or you can apply it to villages in Northern Myanmar. But at the same time, the data ... I'm getting used to finding messy data, and it's smaller data. Nowadays, people are used to having these gigabyte size datasets with hundreds and thousands and thousands of observations. Sometimes, like here, a lot of the times you might only get a couple hundred people and so you're not actually working with big data. And when you have problems with your variables, like if you have a lot of missing variables for example, you might have to start getting creative and do things like imputation, or you might have to drop a question in general, just be like, ""Oh, it didn't quite work out."" And so it's very different than using larger datasets. Hugo:    And even the data collection process is very different, right? In terms of what you do when you're running surveys, whether it be with pen and paper and then putting them into your spreadsheet or computer program or database later. Derek:    Yeah, the collecting the data, I love it. That's the fun part. The best part of it is when you actually get to go to the field. For example in Lahai, we actually got to take motorcycles and go from village to village. It was kind of like off-roading through the mountains in Northern Myanmar to go to village to village, and then you do these household based surveys when you get there. And so you're right there to collect the data and see how it's collected and see what questions work, what questions don't. It gives you a better feel of where the data comes from. It's nice actually that you get to actually see the birth of your data. That sounds sappy but it's kind of like how your data is generated- Hugo:    No, that's a wonderful- Derek:    Yeah. Hugo:    Yeah. Derek:    Probably the most nerdy thing I've ever said. Hugo:    You understand the data provenance in that sense and you know what all your units are. It's not like you're being handed a CSV or pinging an API where you may not actually have the correct assumptions about your data. Derek:    Exactly, exactly. It's interesting. In MSF, I've done surveys where we use electronic means of data collection. I've used both Epi Info on a smart phone and a program called Dharma. It's a service that's actually started by an ex-MSF staff. She worked in the Ebola outbreak. Not the one that's currently going on but the one a couple years ago, and was like, ""We need better data collection tools. We need almost real-time information about what's going on because things, and outbreaks like Ebola, change so fast."" They were actually an epidemiologist and they actually ended up just creating this program that allowed for, not only data collection, but it also projected just trends and statistics on the dashboard. It was really good.
    But at the same time also, like in Lahai, because our teams are out for a week at a time, and it was very rural and rugged, we resorted to paper based surveys which are horrible. I think from here on out I think I'm just going to have to go with the electronic data collection route. Because with paper based surveys, there's no checks and balances. So if you're interviewing somebody and they say their age is really 10 years old, but you put down 100, there's no little automatic check that will be like ""beep,"" like, oh, that doesn't make any sense. Hugo:    No ability to test your data. Derek:    And then also, you have to enter the data which takes double the amount of time. So not only are you collecting the data, but then you have to get somebody to enter the data which takes forever. So electronic data, I'm glad MSF is adopting smartphone tech for entering and collecting data. It makes things a lot easier. Hugo:    For sure. But as you said, if you're in a region where you may not have access to electricity for a certain number of days, there are only so many battery packs you can carry, right? Derek:    Yeah, yeah. That's the problem. It was decided finally that because we're spending about a week in the field at a time ... So the Lahai survey went over the course of about two months, and people would go out for a week, come back for a day or two to rest, and then we would go out again for a week. But these are areas with no electricity, no cellphone coverage for the vast majority of it, until you start getting to the border of India. And then India has some pretty great cellphone coverage, or at least in that region. But you can't charge your battery pack or your phone and then, if you actually drop phone, or something happens to your phone, you lose a whole week of data out of a two week survey process. And it would be very hard to go back and redo it. So we decided to go with paper surveys and then we just had waterproof folders.
    So it has its pros and cons. I tend to be a little bit more tech oriented and I was like, ""But we could've had solar chargers or we could've had four battery packs. We could've gone out for two days at a time."" But in the end, we ended up doing paper. Hugo:    For sure. And you very kindly sent me through the study protocol for this baseline health assessment in Lahai township and there were so ... My eyes were stuck to this PDF while I was reading it. There were so many interesting things in there, particularly with respect to the data collection process. One thing that stood out to me was that it actually said that the local Naga dialect is a non-written language. So I presume you had translators there who were speaking Naga to the locals and then writing down the data in a different language or ... Can you just give me the rundown on that? Derek:    It's interesting and it actually comes up a lot when we do surveys in remote areas. Particularly areas with several different languages or different dialects. What ended up happening is we recruited ... we had about 34 people for this study between the drivers and the data collectors. About 12 data collectors and the rest were drivers because we had to carry supplies on motorbikes. We actually needed quite a bit of motorbikes to just carry tents and to carry food and to carry water and things. The data collectors themselves were actually recruited locally and they speak Burmese, they speak Myanmar, but when you get to parts of Nagaland, like you were saying, they speak Naga, Nagamese, which is like an umbrella term for a lot of local village languages, and a lot of these languages are not written down.
    The paper surveys themselves were translated into Burmese. The people we hired, they spoke Burmese but they also spoke their own village dialect. And so they'd be speaking in their own dialect but then translating it back to Burmese. Usually that causes a lot of problems in surveys, particularly if you're asking about complex things or behavioral questions. But this survey was a lot of health questions and so a lot of was things like, have you been coughing for over two weeks? Yes, no. Is there blood in your cough? Yes or no. So we didn't have to worry too much about the translational differences but that does become a problem. That definitely does become quite a problem if you start asking more sensitive questions. In a lot of refugee camps, if you start asking people about why did they flee? Why did they run away? That's actually quite a different question and it's open ended. And that requires that you actually have the proper translator or people speaking the same language as the person you're interviewing otherwise your data gets to be a little funky and not representative of what you're doing. Hugo:    Now I'd like to kind of step back a bit and talk about the baseline health assessment project in Lahai township as a whole. Maybe you can give us the rundown as to the motivation behind it and how it played out in practice. Derek:    Yeah. Hugo:    And what is it? Derek:    When MSF decides to open up a new project, they first do what's called an exploratory mission. They'll have a couple doctors and a couple logistics people. They'll go to an area and they'll do a quick assessment to see if there's any sort of glaring health needs. In Lahai for example, they went up there and they found that the access to healthcare was incredibly poor. Most people couldn't access a health clinic if they wanted to. And at the same time, there was a lot of just basic infectious diseases that would go untreated. There was this idea to, okay, now that we know that there's this general need, we need to be more specific. So usually what happens is, after an exploratory mission, then do a baseline health assessment which is a much more formal scientific health assessment of the needs in a particular area. That's kind of where the epidemiology comes into play because you end up designing a survey to collect the data for that area. And then what type of information you want to collect.
    So the baseline health assessment had a couple different parts. There's your basic demographics. There's health seeking behavior like where do you go if you are sick? How often do you go to a doctor? But then there was also assessments on malnutrition for children. We assess the nutritional status of children under five years old and then we also tried to assess the vaccination status. And so there's a couple different components to this survey that was pretty long, but it's a very cursory, very general overview.
    One of the things we found in this for example, is that there's a lot of respiratory illness. And because this is all self-reported, just things that they mention, we try to get to what do they think it is but people don't go to a hospital and there's just no way to really tell what it is unless you go to a doctor. So we ask all these questions about it but in the end it all becomes still fairly basic information captured in a very specific kind of scientific way.
    For example, for this particular baseline survey, the villages in Naga, in Lahai, they're spread out and in the mountains so it's very clustered. So a village might be an hour and a half drive away from the next village but really, it would only be about 30 kilometers away. But it just takes you a long time because you're literally driving on a dirt path. The problem is, within the Lahai township, there's about 107 villages. How do you sample enough villages to represent Lahai township and then within those villages, how do you sample enough houses to make sure you're actually capturing the information in the village? So you end up with this two stage cluster design to your survey which is pretty neat actually. It's different than health surveys I've done before.
    In Uganda, it's a big refugee camp and it was split into ... Well, where we were in Northern Uganda, the refugee camp was split into six different parts, and we almost pretty much did sort of random sampling in each of the parts. So we didn't do as much of a clustered survey design. It was a little easier to do. That's kind of how the data collection process happens with that.
    It was kind of neat because it actually ... we mixed a little bit of old tech and new tech to this. The WHO, the World Health Organization, has recommendations for cluster designs, how to sample it, dating all the way back to the 70s where, basically when you get to a village and you're trying to randomly select villages, you throw a pen up in the air and then wherever it lands, where that pen is pointing, that's the house you go to because they didn't have all these smartphones and all this tech.
    Now what we did, it's kind of nuts, that you can actually get satellite imagery of different villages in Lahai township and so we put that into QGIS, drew the borders around the villages, and randomly dropped points into that picture. Then we're like, ""Okay, that point is closer to this house. That's the house you're going to go to."" And so instead of doing the pen method from back in the 70s, we actually did this GIS way of selecting the households within the village. So it was kind of cool actually. It was pretty neat. I got to work on some GIS work which was pretty good. Hugo:    That's really cool. How many people are there in Lahai township and how many villages and how many people did you end up interviewing over what time scale? I've just asked you four questions actually so that's far too much, but just trying to get a general idea of quantities here. Derek:    Lahai township is one of three parts of the Nagaland area that's in Myanmar. The majority of Nagaland is actually over the Indian border but there's three sections within the Myanmar side and Lahai township is one of them. Lahai township itself, it's in the mountains and there's not a lot of people that live there. It's about 120,000 people. Lahai town is kind of the center of it and that's about 3,000 people. There's about 107 villages that are in the township and these villages will move every couple of years. Something we actually had to do, that we were told to do beforehand is when we would select a village, we actually had to send a team out there to make sure that village was still there. And it happened once, we selected 30 villages to represent this township, it's called ground-truthing, where you just go out there and you're like, ""Okay, does this village exist? Is there enough houses to do this survey?""
    It happened once where a village relocated 10-15 kilometers away because they had water problems. The stream that was providing water to this village dried up a year or two ago and so everybody just sort of migrated, pretty much to find water. That's kind of the context of Lahai is that it's mountainous and it's sparsely populated. The baseline survey itself took about two months. We did it in 30 villages and we chose those villages based on population size, or as close to population size as we could get. So we had basically kind of the size of population in the village in a Myanmar census and then the chance of the village being selected was weighted on the size. For example, Lahai town itself was chosen twice because it's up 3,000 people whereas the next biggest town we went to had about 700 people. So we chose the 30 villages and then within each of those villages, we did 30 households. Hugo:    So that's around 900 you surveyed in total? Derek:    Yeah, about 900 houses. The average household size came out to be about seven people. The interquartile range was between five and nine people per household. There's a common practice of intergenerational living, like kind of joint families. So you'll have younger children with the mom and the dad and then you'll have the grandparents and sometimes you also have aunts and uncles that live in a house as well. Household sizes can actually be quite big. It ended up being a little over 5,000 people that we would represent. We didn't interview all 5,000. You just took one member of the household and then just asked questions about everybody to that one person. That way, not everybody had to be present. Hugo:    How do you choose which member of the household? Derek:    It actually came down to a little bit of cultural acceptance. Usually it was the father of the household or the male figure. A lot of times, though, people do a lot of agricultural work and so if it was in the middle of the day, a lot of people would already be out in the field. What ended up actually happening more often than not, was a lot of times we did get a lot of female household heads that would answer for the survey. We also got a lot of grandparents that answered for the survey. The only requirement really to be considered a head of household, was that you had to be over 18 and you had to have been able to answer questions for everybody in the household. That was the only inclusion criteria to be deemed the head person to answer the survey. Hugo:    How are the insights that are gained from this baseline health assessment turned into actionables and deliverables for MSF? Derek:    Currently, actually, because we found a lot of respiratory illness with this survey, we're actually helping to put in what's called a GeneXpert machine at Lahai township hospital and so it's to help test for tuberculosis. The old school way of testing for TB would be to hack up your lungs and then you spit your sputum onto a slide and then you stain it, then you put it on a slide and then you have to have a trained health professional to look at that slide and be like, ""Oh, that's your microbacteria. You have TB."" Which is not very sensitive or specific. It's got a sensitivity & specificity in the 50s or 60s so it's pretty crap. But with this technology, the GeneXpert machine, it's highly accurate and it basically can give you test results in a couple hours. But it's expensive and it requires a bit of a constant electrical supply and we wouldn't have put it in unless we knew that there was a high amount of respiratory illness within the area. Hugo:    And is this type of actionable developed from the insights gained from the health assessment similar to other projects at MSF? For example, your work in Uganda at the refugee camp? Derek:    It's pretty similar. A lot of the operational research MSF does, it's all geared towards doing actionable results. One of the surveys we did was on mosquito net use because there's a lot of malaria at the time. And Malaria is cyclical. So when the rainy season came about, that's when you would see spikes of malaria. We did this assessment on net use right before the rainy season and be like, ""Hey, does everybody have bed nets? Or how do you use bed nets?"" The results from that kind of led to mass mosquito net distribution, and also the educational campaign to make sure you use mosquito nets right.
    A couple months after the survey, we ended up actually passing out a whole bunch of mosquito nets. It ended up being close to 13, 14 thousand mosquito nets for the area. It's a great way to see your data in action, and it's great. That's probably the number one thing I like about doing epidemiology for MSF is that if you're lucky, and you stick around long enough, you get to see your results turn into something actionable. Hugo:    So Derek, something I've been thinking a lot about recently is how we don't necessarily have good models for ... or we haven't settled on a global for how data scientists, statisticians, statistical modelers, are embedded in organizational structures in businesses. So I'm wondering how data science is embedded in the organizational structure of MSF. Derek:    Data has always been there in forms of like internal reports, but the operational research aspect of it is actually somewhat new. And it's funny because you can kind of look at ... like in the early days, MSF never really published a lot of scientific data but then all of a sudden there's actually kind of became two big data hubs in MSF. So there's Epicentre in Paris, which is a big repository of a lot of the data that MSF collects, and they also provide a lot of support when it comes to designing various studies, just in terms of study design. And then there's The Mason Unit in England which does the same thing. The Epicentre tends to focus a lot of the French speaking countries and The Mason Unit tends to take a lot of the English speaking countries. But these are two big data repositories so MSF has actually become quite serious in the last five to seven years. More like seven years, on how data is pretty much the best way to inform your policy decisions. It's hard to argue with the numbers.
    And before, especially with a lot of the work that MSF does, a lot of can be a little controversial, and a lot of it can be quite risky. So when you have the hard science and the hard numbers in data there, it adds a lot of weight to your policy decisions. For example, with the Ebola crisis that happened a couple years ago, it was very important to have real-time data of where cases were clustering because that's where you would send your health workers. Not just your doctors but educational people. Like your health promoters to be like, ""Oh, if somebody is bleeding, bring them to a doctor but don't touch the blood. It's a good way to prevent the transmission of Ebola."" But you have to do that fairly fast because Ebola, the incubation time is only a handful of days and then the mortality rate of it, it actually would kill people in about two weeks time. So you had to act really fast. And so the data that you collected was the best way to inform your decisions, otherwise you'll just be arguing over what you heard from people in the village at the time and it would take forever to do something. Hugo:    Something you mentioned earlier was that you see more and more abilities for tech to be used in the work MSF does and the type of surveys you've been doing. What else is there in the future of data science at MSF which isn't there or hasn't been discussed in this conversation? Derek:    Something that I kind of want to see be used a little more, particularly when it comes to health promotion, and something that I'm kind of playing with a little bit is network analysis. So social network analysis. Particularly for health promotion, in addition to outbreak, outbreak epidemiology. That's kind of what most people think about when they think about a network analysis and infectious disease, but it's also a great way to find who in the community has the most influence, and who do you really want to target with your health promotion behaviors.
    So if you're trying to get people to, like something fun or something silly like brush your teeth more, who do you really want to talk to the most? Do you want to talk to the children? Do you want to talk to the moms? Or maybe it's the grandparents that happen to have the most sway. By doing a network analysis, you can see who has the most connections and then you can see the strengths of those connections, and rather than target an entire village and be like, ""Hey, everybody. Brush your teeth."" And have a big rock concert on it, you can just target a handful of people knowing that they would just spread the message to a good chunk of the community. Hugo:    So it almost hurts me to say this but what we're looking for are influencers, right? This is influencer culture. Derek:    Yeah, yeah. Exactly. Instead of taking the influencers of like fashion and stuff, you can kind of give it a little bit of a health bend and be like, ""All right. Maybe if the cool guy were to brush his teeth in public, maybe he has the biggest influence."" But it's getting to know who has these influences in a way that it's really hard to do with traditional data collection methods. So you can find, with a lot of odds ratios and p-values and you can find the strength and magnitude of associations for one variable and another, but you can't really tell, okay, that's just the relation between those things but what outside of it influences those factors as well? Hugo:    For sure. I like the idea that you're thinking about network theory and network analysis in this respect because as you were saying, network analysis is thought about a lot in terms of infectious epidemiology but we know that it has a huge role to play in even non-infectious epidemiology. I think one of the common examples is, people don't contract obesity from each other but if you have a network and you're connected to more people with obesity then you've got a higher likelihood of having it yourself at some point. Derek:    Oh, exactly. That's actually a really great example of it. Obesity and dietary habits are greatly influenced by your friends and network. I can definitely speak from experience. I used to smoke cigarettes for about six years, seven years, and all my friends smoked cigarettes. And it wasn't really until I moved from ... I grew up in Boston, but moved from Boston to Philadelphia and just got an entirely new friend circle where nobody smoked and everybody was healthy and I was like, ""Huh, maybe I should quit smoking."" But I mean, if I had never changed my friend circle, I'd still be smoking two packs a day. Hugo:    So, we haven't talked much about the technical stuff that we love so much. You did mention that for sampling you do a two stage cluster sampling methodology. We¡¯ve also been discussing network analysis. But I'm wondering what's one of your favorite data sciencey techniques or methodologies. Just something you love to do when playing with data. Derek:    That's a great question. That's really good. I don't want to sound lame and kind of basic but logistic regression. Hugo:    Right on. Derek:    That's super simplistic but odds ratios are great because everybody understands them, they're easy to calculate, and you'd be surprised how much data you can get with a binary response. Like, do you like mangos? Yes, no. Like, do you use condoms? Yes, no. Are you an injection drug user? Did you go to a doctor last week? Yes, no. Anything that is kind of binary. And then by extension, you can do multivariate logistic regression type.
    You can do it with different levels of categories as well to get different odds. But that works out really well actually. Not only collect and analyze data but when you present it, so if data science is really going to lead the way in helping to change health policy in this case, you have to be able to communicate your results to people who might not be as like minded. So while I didn't get into coding about R and all this data management and things and it's great, and I love it. I know the project coordinator for Lahai, for example, that doesn't like it at all. Unless you can put it in a graph, their attention span kind of wanes after like three minutes. And so, logistic regression is probably a great way to communicate a lot of results, in my experience. Hugo:    I agree. I agree completely. I've said this time and time again on the podcast, but for people who are non-technical, you can show them that a 10% increase in this feature results in this probability in the outcome. And in that sense, it's interpretable which gives you massive gains. Derek:    Exactly. That, kind of what you just said, is a great way to just explain it. It's short, easy to understand. Just something like that. The odds of this risk factor increasing your risk of contracting HIV are such and such. However, kind of behind the scenes, is the way you sample, the way you collect that data to make sure your analysis is correct. And that gets to be quite complicated. For example, the two stage cluster design thing, using randomly assigned GPS points, that's somewhat complicated and it takes a little bit to do. But in the end, you try to distill it down to things that everybody can kind of digest. Hugo:    So Derek, my last question for you is, do you have a final call to action for all our listeners out there? Derek:    Yeah, yeah. I guess I'm a little different than a lot of people that come on this podcast in the sense that I don't always use massive data sets or I'm not always crunching out numbers in very clean data, but at the same time, I think people should get into data science and get into more field work out there. There's definitely a place for people to get out of the office, collect their own data, analyze it, and then actually have actionable results. Don't let the fear sitting in a cubicle somewhere just pumping out code dissuade you from doing data science. Data science is for everybody. Hugo:    Fantastic, Derek. It's been such a pleasure having you on the show. Derek:    This is great. I had a real fun time. Really good.","Keyword(freq): data(79), village(18), survey(14), doctor(11), net(11), result(10), border(9), question(9), project(7), term(5)"
"4","datacamp",2018-07-25,"Chatbots, Conversational Software & Data Science (Transcript)","https://www.datacamp.com/community/blog/chatbots-conversational-software-data-science","Here is a link to the podcast. Hugo:    Hi there Alan and welcome to DataFramed. Alan:    Hey, it's great to be here. Hugo:    It's great to have you on the show. I want to jump in and discuss your OG Media post from April 2016, which really got the ball rolling for everything you're working in now. You opened this post with the following statement, ""We don't know how to build conversational software yet."" I feel like you meant ""we"" as a society and community of tool builders. Conversational software includes ChatBots. I'm wondering now, two odd years later, is it still the case that we don't know how to build these types of things yet? Alan:    I would say probably yes, I still agree with that statement, but we definitely made some progress. It's still very much early days for building great natural language interfaces for computers. Hugo:    Tell me a bit about the progress you've seen in the past two years. Alan:    Maybe a bit of context as well where that blog post came from. We built a few SlackBots because Slack was one of the first platforms that opened for this. Alex, my co-founder and I had ... Actually we had a couple of chatbots that companies were paying for. They were both around actually making data science more accessible, turning natural language queries into SQL and then running those queries on a database. Anyway, we had some experience building these things and looked around and thought, ""Wow. There really aren't any great developer tools for how to build conversational software."" Then we saw that Facebook Messenger platform was about to open up and that was right around this time. I thought, this is just, this is not going to go well. Hugo:    What you mean by Facebook Messenger platform opening up is opening up in order to have conversational software in it? Alan:    Exactly. Letting developers build chatbots and that anyone with a Facebook account could then chat to. We thought, ""Well, we have now quite a bit of experience. We know it¡¯s really hard to do this well and these are not going to be great experiences for people."" I think that is actually pretty much how it panned out. There's a lot of hard work involved in getting things working well even in a narrow domain. Doing something with just kind of open ended conversation is definitely nowhere in sight. Alan:    I think we've made some good progress in terms of libraries and tools that people can use that make it easier to build something that does work, even though we've definitely not cracked this problem by any means. Hugo:    What are the most pressing things that we haven't cracked with respect to building conversational software? Alan:    There are a number of things. The two things you need for building conversational software ... The first is understanding messages that people are sending to you. That's called NLU, or natural language understanding. What that usually means is classifying a short piece of text as belonging to one of N intents. Those are class labels for things like a greeting or saying good bye or asking for some specific things that your chatbot can do. Hugo:    Like looking for a hotel or something like that? Alan:    Exactly. Like, ""I'm looking for a hotel."" Then, the other part is pulling out some structured data. That's usually called entities. I'm looking for a hotel in San Francisco and then knowing that San Francisco is what you want to use in your query. That's NLU. I think it's fine to call it that as so long as you remember it's absolutely not true. The computer doesn't understand anything. It's just able to classify text into one of these buckets and then pull out some entities. Hugo:    Yeah. This is a large concern in general with terms like artificial intelligence, that kind of anthropomorphism involved in nomenclature and the way we name stuff. It's dangerous, especially when it permeates common language. Alan:    Definitely. Definitely. I think there are lots of complications and limitations to that, even that simple model of the intention of what a short message means, being a universal single label. This always means this. You can only represent its meaning by a single one hot vector. That's obviously a very limited model of understanding what people can express. It's a good starting point and lets you build on top of that, but of course, it's not ... When we think about the future, I think that's not how we're going to represent the meaning of short messages.Hugo:    We have NLU as being a huge challenge. What was the other one you were going to speak to? Alan:    Yeah, the other one then is dialogue management. If somebody says ""yes"" in the middle of the conversation, the way you respond, of course, depends on the context. It depends on what happened before. It's not enough to just map each of these intents or each of the outputs of your NLU system to the same action. You need to always build up some kind of statefulness. Alan:    Then, the question is how do you handle that in a way that doesn't break and is actually maintainable? That's a big part of what we've been working on for the last couple of years. That's, actually, I think the biggest part that was really missing back in 2016 was a reasonable way of dealing with that complexity because the way that people were doing it, basically manually writing a lot of rules, just doesn't work and it doesn't scale. It causes a lot of headaches. Hugo:    Right. I like this idea of scaling. Because as you say, writing a bunch of rules is, I suppose, the most bare-bones naive way to think about writing conversational software. I imagine you can have a set of nested ""if-elses"" to try and deal with everything, or a subset of everything. Then, as soon as a new use case comes up, you then need to nest them even further. This is something which definitively does not scale. Alan:    Right. Exactly. If you nest them deep enough, then it counts as deep learning.Hugo:    That's hilarious. Alan:    In general, you have ... I've tried this plenty of times. Even building a relative simple bot that it was a banking chatbot that we just built for fun to see ¡°How badly does this work if you really just do it with rules?¡±You could do a couple of things like check your balance and transfer some cash to people and everything. I think it came out at over 500 rules just for doing this simple stuff. Of course, then when you want to update something or something goes wrong when you add a new rule, it clashes with the old ones. Then, you go, ""Oh, no."" Then, you have to go and try to reason about all these rules and figure out why it is that something broke or that something clashed. Alan:    There's an asymmetry there, which is interesting, because in the middle of conversation ... The kind of cliche example by now is what do we want? chatbots. When do we want them? Sorry, I didn't understand your request. Hugo:    Okay. That's provided a great teaser into a lot of the through lines that we're going to talk about with respect to NLU, scaling, a lot of different use cases of these types of chatbots and conversational software. Before all that, I'd like to find out a bit about you and Rasa. Maybe you can tell me a bit about yourself, what you do, and what Rasa does and what Rasa is. Alan:    Yeah, Rasa is two different things. Rasa is a company. It's a start-up. It's also a pair of open source libraries. There's Rasa NLU, which does language understanding, so parsing short messages. Then, there's Rasa Core, which does dialogue management. I'm a maintainer of both of those libraries. The aim of them is really to expand chatbots and conversational software beyond the answering simple questions, FAQ style, one input, one output kind of ... Turning it into a real conversation and building that in a way that scales.Alan:    Where I see us as a company and also where we come from was around 2016, when we said, ""Okay, nobody knows how to do this."" Actually there's a lot of research on how to use machine learning to overcome some of these problems. There's a lot of great papers written on it, but there weren't any libraries that developers could use to actually implement those ideas. Where we see our role is really in that big gap between arXiv and github, let's say. Something that's actually well maintained, has lots of tests, has people responding to issues, has support, gets updated regularly, and ... We do a lot of applied research in this field and we publish papers. We work together with universities, but it's always very strictly applied. Then, the primary output is always to put out some new code that people can do that does something better that they couldn't do before.Alan:    Where I see us as a company and also where we come from was around 2016, when we said, ""Okay, nobody knows how to do this."" Actually there's a lot of research on how to use machine learning to overcome some of these problems. There's a lot of great papers written on it, but there weren't any libraries that developers could use to actually implement those ideas. Where we see our role is really in that big gap between arXiv and github, let's say. Something that's actually well maintained, has lots of tests, has people responding to issues, has support, gets updated regularly, and ... We do a lot of applied research in this field and we publish papers. We work together with universities, but it's always very strictly applied. Then, the primary output is always to put out some new code that people can do that does something better that they couldn't do before.Alan:    One recent example was we completely changed how we do intent classification. We shipped a new model, which threw all the old assumptions out the window and just said, ""Okay, now we're going to learn word embeddings in a supervised way for this task."" That lets us do things like building up hierarchical representations of meaning, understanding that a message can contain multiple meanings, because sometimes people just say multiple things. We do a lot of that. Then, the primary output is a piece of code people can use. Then, if we write up a paper, that's a nice component.Hugo:    This speaks to the open source side of Rasa. You said that Rasa is two things?Alan:    Yeah, so the other side is a company. The first year that we operated, we basically did a bunch of consulting work on top of the open source. That was really great, because building stuff with it yourself, it keeps you really honest about its limitations, helps you understand your customers. Then, after we did that for a year, that was obviously very nice and we bootstrapped the company, then we said, ""Actually, we think this is scalable product we can build here for an enterprise version."" We talked to a lot of these big companies that we've been consulting for. There was a clear need for an enterprise package for more features and a different product. We thought, ""Okay, that's something we want to take a bet on."" For the last six months now, we've stopped all the consulting work. We raised some venture capital and really just went full on building out the enterprise version.Alan:    Then, still all the machine learning stuff goes into the open source. A big part of what we believe in is you can't build up a competitive advantage by having secret implementations of algorithms lurking around. The machine learning stuff needs to be open and needs to be tweakable. People need to be able to play with it. There's just so much nonsense around in the AI space that it's better to just say, ""No fairy dust. There's no magic. It's just stats. Go look at the code. All the machine learning's open source. You can see exactly what it does. You can tweak it for your own purposes.""Hugo:    Yeah. I'm sure all the different interplays between your company and your open source development are really exciting. For example, correct me if I'm wrong, but you've recently hired two machine learning researchers. Alan:    Yeah. We're only 10 full-time people, but two of those are full-time on ML research. The measure that we really care about is how quickly can we take a new idea, like this little trick actually works, and then put it in production. The great thing about having the open source community is that whenever we have a new idea or something that kind of works, there are just thousands of people who are just ready to check out the master branch and see what it does on their data sets. That feedback loop is really awesome. You can't do that if you build things that are closed source product. You don't get that kind of insight. Hugo:    That's really awesome. I suppose we should say also that it's a Python library, right? Alan:    It is. It's written in Python, but also we obviously work a lot with large companies. Our main enterprise customers are Fortune 500. They're, let's say, not mostly on Python. They mostly write in .NET or Java or C#. Then, there's a large chatbot developer community that uses JavaScript. What we did was we made sure that the libraries, even though they're written in Python, you can use them without using any Python. You can consume everything over http API. That means that you don't have to be running a Python stack yourself to use these libraries. You can spin them up in docker containers and use them and deploy them to production without having to actually write in Python yourself. Hugo:    That's really cool. I may be putting the cart before the horse, or the chatbot before the company to extend the analogy into absurdity, but there is a fantastic DataCamp course that you've created and I facilitated last year on building conversational software with Python and using Rasa. There's also a lovely interplay, for those people who know a bit about the Python data science landscape, a lovely interplay between Rasa, scikit-learn, and SpaCy, for example. Alan:    Exactly. There's no point in reinventing the wheel. A lot of really great libraries out there for doing different things ... For example, in Rasa Core, which is the dialogue manager, you can plug in different back ends to implement your model in and actually do the machine learning part. One way you can think about Rasa Core is it does all the hard work to get that conversation into the kind of XY pair format that you think about when you think about machine learning. Then, you can plug in whatever classifier you like. Of course, we have some good ones implemented already. You can implement your stuff in TensorFlow or Keras. Actually, I think the Keras API was a big inspiration for that. Just saying, ""Okay, can we just abstract over all the things that aren't important for understanding the problem and really just present the API that makes sense to you?"" Then, we don't need to build our own autodiff library, because why should we? You can use TensorFlow for that. That also runs on GPU, et cetera.Alan:    Similarly for doing things like part of speech tagging. There's a lot of things you can do with SpaCy and it makes more sense to build on top of libraries like that. You can choose different back ends you want to use and implement some lower level functionality for both Rasa NLU and Rasa Core. Hugo:    That's fantastic. I want to in a second to think about what type of use cases, which verticals and industries, you see most interested in conversational software. Before that, I just want to step back a bit. What's a good working definition of a chatbot or conversational software? Are they the same thing? That's a relatively ill-formed question, but maybe you can speak to that. Alan:    Yeah. I'd say that the things that we're interested in are not just strictly chatbots. Some people would call some of these things virtual assistants. I think that voice is extremely important. Anything where you interact with the computer through natural language is something that we're interested in. I would call all of that conversational software. Hugo:    That's different to just a chatbot, right? Alan:    I think chatbots mean different things. In some groups of people, a chatbot strictly means actually something that does chit chat, can't actually do anything for you, it's just there having an interesting conversation. That's actually less what we're interested in. Small talk is not something that we really focus on. I'm more interested in purposeful conversations that actually do something. Then, there's another kind of definition that says a chatbot is on these mobile messaging apps, right? Facebook Messenger or Slack or WhatsApp or Telegram or any kind of application that lives inside of one of those apps, even whether it's chat or button-based or it shows you a little web view. All of those are also chatbots. It's ill-defined as most terms are. Hugo:    This may be a relatively naive question, but I think naive questions can give us a lot of insight. Why should we care about chatbots or conversational software?Alan:    I think it's a great question. I think one of the main reasons I'm really excited about it is that it makes computers usable by people who aren't experts. That can be on a very banal level or it can be on a more sophisticated level. For example, my grandfather never used a computer in his life. He knew what the word ""internet"" was, but he never knew what it meant and he never had an experience of it. Then, I think about my parents, who are to some degree computer-literate, but also certainly not the same as my generation. Then, I think of how quickly technology is progressing.Alan:    If we all believe that tech goes exponentially fast, then if my grandfather was behind, imagine how far behind we're going to be when we're old. I think a big step that we need to take is how do we, rather than have the need for computer literacy, how does the computer adapt to deal with how we already think about things? It's sometimes for really stuff. Just asking a simple question and getting an answer, rather than having to open up a document and find where it is or something. Then, sometimes it's for more sophisticated things. I always like to think how many Google searches do you think there are every year for, ""How do I do X in software Y?"" Then, imagine that you didn't have to Google that. You just had to say to the software, ""How do I do X?""Alan:    Photoshop is something I know is very powerful. I have no idea how to use it, but I could express some of the simple things I'd like to get done. ""Add a blue border around this image"" or something. Why can't I just engage with computers by saying what I want and just having it happen? I think it's powerful in a lot of different ways, especially for making tech more inclusive. It's really important.Hugo:    No, that's great. Because that also speaks to a very personal, individual power. Not the type of power when you're working with corporations or businesses achieving business goals, but really on the ground empowering. Alan:    Yeah. Yeah, absolutely. Hugo:    Which verticals and industries are currently most interested in using conversation software? Alan:    My sample is definitely biased, because I work on Rasa. The companies that we know about and approach us are the ones that find us important enough that they want to invest their own engineers into building this out. They realize it's strategically important. They want to build up these capabilities. I'm sure there are other industries where it's really relevant, but the considerations are different. They don't necessarily want to run the tech or own the tech in-house. Where we see the strongest pull from industries in financial services, so insurance and banking, and another one, which is automotive. I think that makes a lot of sense. I remember when I got the Amazon Echo. I had it for about six months. I thought it was cool. It was fun, but wasn't a game-changer. Alan:    Then, I went to a conference in the south of Spain. I rented a car and I had to drive three hours from the airport. I remember just driving on the motorway in a foreign country with my phone clipped to the dashboard. I was tapping on it change the map and to change the song on Spotify. I thought, ""This is so dangerous. Why can't I just talk to the car and say, 'Play this song' the same was I was used to doing with the Echo at home?"" I think automotive is a really valid use case, especially for voice interactions. Hugo:    For sure. That's very clear where the value can be delivered. The example you just gave was a paradigm, I think. How about in insurance and banking? What kind of value can conversational software deliver there? Alan:    I think it's on different levels. If you think about what is your main interface to your bank, to the data that you have with the bank, and how do you engaged with it? Some banks have really nice mobile apps and you can just do everything there. One other interface that a lot of people fall back to is just going to the branch and talking to a human. Why do you do that? Because you have questions, right? You can't ask a question of an app generally. You can do the things that were suggested to you that the developers decided to implement, but you can't say the things that you don't know or ask for the things that haven't been implemented. Asking for advice, understanding things a bit more in depth. Those are all things where there's a lot of value to be added, especially around insurance where the actual product that you buy, the policy, can be very complicated and hard to understand. Talking to a human is also tedious. You have to take time out of your day to engage with them.Alan:    If you have a 24/7 agent that you can chat with, they can answer most of your questions, it's actually really pretty powerful. Hugo:    Having some sort of intelligent conversational software for banking in that sense, for questions and FAQs and that type of stuff, would stop me from becoming infuriated when I call the bank. You end up in one of those graphs that directs you downwards and you just keep shouting representative like 15 times. Press 0 for whatever, because it makes no sense. I also have the added challenge, maybe you can speak to this, that I'm an Australian living in the United States most of the year round. I have to put on a fake American accent to be understood by the automated telephone system of Bank of America. Alan:    I can definitely understand that. I've seen a lot of that also. My girlfriend's from Scotland. Her accent can be misunderstood both by humans and by software. Also, non-native English speakers understanding language. Understanding speech, there's been some great successes, but it's certainly not a solved problem or commodity by any means.Alan:    It's not something where we try and compete. You've got to focus on something and it's not something that we do. It's a certainly really interesting problem. Hugo:    Hey, I remember you telling me some time ago about an insurance chatbot, but it wasn't voice. It was an SMS chatbot. Maybe you could tell us a bit about these types of use cases. Alan:    Yeah. That, actually, was a really interesting one. It's a large insurance company in Switzerland]. A really big company. It's 160 years old. They wanted to do something to engage people whose house insurance policies were about to expire. These are five year policies and they run out. They have a large customer base. Because employing people in Switzerland is very expensive, it actually wasn't literally worth their time to have an agent call up all these people and ask if they want to renew their policy. What they did was they built a chatbot, actually with Rasa, and it went out over SMS and engaged with these people. It said, ""Hey, your policy is due to expire. Is your living situation still the same?"" It would ask them questions. Has anything changed? Maybe you moved. Maybe you got a dog. Maybe you got a more expensive car. Or whatever it is. If the person wanted to renew and they collected all the data they needed, they'd say, ""Okay, here's your quote. Is that cool?"" If they agreed to it, it would just actually finalize the policy and the policy would be in the post with them within a couple of days.Alan:    It's really end-to-end automated. That's also what I mean by conversations that really do something. It's nice to answer questions, but it's a lot more powerful ... You can say, ""Okay, it's now done."" It's on the way. I think one of the interesting things is it challenged a lot of assumptions around what chatbots are and what they're useful for. People think about it firstly as a customer service thing and saving costs, which is certainly relevant. If you can actually increase revenue, that's really compelling. This was actively reaching out to these people over SMS. It was a 30% conversion rate, which is really astonishing in terms of getting people to buy a new five-year policy.Alan:    The other thing is that people think about chatbots and they think about generation Z and messaging apps, when actually the first person to buy an insurance policy over this chatbot was a 55 year old Swiss lady. That's all really interesting. I think it also speaks to the fact that it does make tech more accessible for a larger group of people. You can just speak to these systems. You let your customers speak to you how they think about the problem, rather than forcing them into your paradigm, which is this software that you've built for them to  with your company. Hugo:    Exactly. You've spoken to some really interesting use cases in insurance, banking, the automotive industry. Are there any other industries that aren't as interested in conversational software as you think they should be? Alan:    That's a really good question. I am not sure I have a good answer to it. I have personal frustrations with things like telecom companies getting your internet set up in your flat and all that kind of stuff, where it could certainly be a lot more useful if you could have a 24/7 automated support that you chat to and get things done. I'm not sure if I can think of off-the-shelf of examples of industries that are really neglecting this. We see, actually, a really big rise of companies reaching out telling us that they're using Rasa and sometimes asking about the enterprise or sometimes just have some questions. It's actually much more diverse than the use cases I listed. Definitely the dominant ones are things like financial services and automotive. Hugo:    Okay. Great. As you mentioned, expressed your frustrations with calling up telcos for example, I actually remembered government, I think is probably a great example. Alan:    Oh, yeah. Hugo:    The IRS and the tax system. I actually ended up ... I mentioned my frustration with those telephone trees that you get sent down, like, ""Press 1 for whatever"". I ended up, somehow, when I called the IRS several years ago, on a loop. I thought maybe you wouldn't be allowed loops in these graphs, but I ended up on a ... I was about to swear. I ended up on a loop in one of these conversations. I ended up hanging up. There's definitely room for a lot more aspects of conversational software in these types of places. Alan:    Yeah. I think when the experience of someone who's in a phone tree is, ""How do I most quickly get to speak to a human?"" It's not, ""How do I most quickly get the thing done?"" Just because we have such low expectations of these things actually ... Because they're mostly just pre-qualifying what you're problem is to send you to the correct person. Or that when the person talks to you, they already know half the information that they need and they don't have to collect that. It's really optimization on their part. It's really not optimization on the point of you as a customer getting your problem solved quickly. Hugo:    For sure. I think this has provided a lot of insight for our listeners into conversational software, where it's being used, the ins and outs. I'm sure a lot of them are eager to hear about where they can get started. I think both technical and non-technical listeners alike would be interested in hearing about how they can get started with conversational software. Maybe you can give a few pointers? Alan:    Yeah, definitely. Also partly because there was this big boom when Facebook Messenger opened up, there are actually some really nice online tools that you can just point and click and build a little prototype of a chatbot. Dialogue Flow is one example. It's owned by Google. Chat Fuel is another. You don't have to really write any code. You can design a prototype of your chatbot. You don't have to set up a server or anything. You can very quickly get something you can try out and just get a feel for how something like that would work. Alan:    Then, if you want to go beyond that, there's lots of good resources on understanding the tech behind it. Building something you can really maintain and scale. There's, of course, the DataCamp course that we created that really covers the fundamentals. What are you really doing? What are you really going on? Demystifying this concept. Which if you've never worked in NLP or anything, it seems really bizarre and almost impossible. How do you build computer programs and understand language? It seems so inconceivably hard. We go into a lot of fundamentals there.Alan:    Then, if you want to build more advanced things, you can either then build everything from the ground up, which is always an interesting, especially for a learning experience, an interesting approach. Or you can use something like Rasa, an open source library, where a lot of the heavy-lifting is done for you. You can then get started, build something out, then really tweak parameters to get better performance on your data set. Give it to real people and iterate. I'd tell you the best way, if you want to tinker, build a chatbot first. If you want to actually solve a problem, and you actually understand what people do, then actually the best way is not to have a chatbot, but just pretend to be a chatbot yourself. Just say that it's a bot and ask people to talk to it. Then, answer them yourself.Alan:    If people don't like the experience with your brain behind it, then they're definitely not going to like the experience with an artificial machine learning system behind it. It's a great way to validate if what you're doing actually makes sense and really get some inspiration for all the things you didn't think about.Alan:    One mantra that we have, and it's really one of the principles that we use to build our products, is that real conversations are more important than hypothetical ones. I'm less interested in giving people tools to design hypothetical situations and think about all possible conversations that people can have. It's more important to look at real conversations that people do have and learning from them. That's what Rasa Core is all about is learning to have conversations from real data. Hugo:    Absolutely. There's so much in there. I want to recap a couple of the takeaways. The first takeaway for me, of course, is take this DataCamp course. I do think it provides a wonderful introduction. In the first chapter, you get to build a chatbot, which is based around one of the early chatbots, the ELIZA chatbot, which is incredible. You do a bunch of natural language understanding. Then, you get to build a virtual assistant. It's a personal assistant that helps you plan trips, which is really cool. I think you've got a lot of insight into this course into everything we've been talking about.Hugo:    The second takeaway, I think, is you need to think about what the purpose of your chatbot is. For example, a lot of people, I think have a misconception that chatbots need to sound like humans, for example. The question is if you've got a virtual assistant who's going to help you build a trip, do you care whether it sounds like a human at all? Or do you want it to do the job you want it to do, right? Alan:    Yeah, I do have to agree with you. I do think there's a big, important topic as well around the design and writing good copy and being empathetic and doing things like active listening. That's a whole orthogonal set of problems kind of to what we're really tackling. It is important. Of course, yeah, the question is actually do you solve the problem? Do you actually do something for people?Alan:    I end up using, at least a couple of times a month, some product that I just completely hate, but I have to use it, because it's the only thing that gets the job done that I need. I think also in start-ups, if you think about product market fit, you almost want to put buyers in people's way. You almost want to have your first version of your product be really crappy. Maybe even intentionally so, just to see people persevere, because it really solves a problem for them. Hugo:    What are the most common misconceptions surrounding conversational software that you want to correct? Alan:    I think the first one is really artificial intelligence, maybe even artificial general intelligence. That you need to wait for DeepMind or OpenAI to solve these grand problems. Then, you'll be able to download a brain from the internet. That's going to magically solve all your problems. You can build stuff now with the techniques that exist. You've just got to put in a bit of work. You don't need a degree in statistics or computer science or anything like that to build these things. You can definitely self-teach. You can build something that's really useful and adds a lot of value with what's out there now. You shouldn't even try, I think, to build a do everything kind of system.Alan:    I think another one is that only Facebook, Google, Amazon, Apple have the tech to build conversational AI. It's just not true. We see that a lot. There's also some academic papers published, where they compare Rasa to some of these closed-source tools from Google and Microsoft. It actually does very well, because there's no fairy dust. I don't want to pitch Rasa too much, but the way Microsoft and IBM think about this problem is we have magical algorithms in the cloud. Upload your data here and we will turn it into gold. I think that's just nonsense. You can build a lot of great things, even better things, with opensource tools because you have full control. You can tweak things. You can customize things for your use case. It's definitely not something that's only in the domain of the big tech companies.Alan:    Then, another one that we already spoke to a little bit is that this generational bias that it's only something that's fun and light and for Millennials. It's actually something for the Boomers and the older generations as well. It's a really important piece of tech now. Hugo:    One thing you mentioned was this idea of product market fit. I want to zoom in on where Rasa Core came from and how it emerged. I want to approach it from, I suppose, the idea that we discussed at the start of this conversation about needing to have conversational software that scales. Maybe you could tell us about the landscape before Rasa Core and then how Rasa Core emerged in relation to the idea of scaling this software.Alan:    Yeah, definitely. Before Rasa Core came out, there was some nice cloud APIs for doing the NLU part. You send it a sentence and it sends you back its interpretation of what that means, what's the intent, what entities are in there. Then, the question is what do you do in response to that? You have things like yes and no. Of course, the response of that depends on what happened before. What do you do? You write you some rules. You write out a rule for, ""Okay, if the last thing we asked was this and the person says yes, then proceed down this direction. If the last thing the person said was no, then go down this path."" That's great. Then, you say, ""Okay, we now need to maintain that state over multiple turns in a conversation."" You don't just build point A to point B. You build a state machine.Alan:    You say, ""Okay, this person is currently in this state."" Now, they've said this. I've moved into this state, where they're now ready to make a purchase or they're asking about this topic or whatever. That, of course, works to an extent. Then, you deploy this and you give it to people. You ask them a yes or no question. One example is, ""Should we just send that to your home address?"" You've maybe built a branch for yes and you've built a branch for no. Then, the person responds, ""Oh, what home address do you have on record for me?"" There's always an edge case that you haven't thought of. Which is fine. Of course, all software has edge cases. Then the question is how do you deal with that? If that's an important thing that lots of people say, you need to answer that to them. You can either then add another nested if statement in your logic, or you can add another state to your state machine, which then is explaining some deeper information. Then, probably you can get into that state from multiple different other states. You have one use states, but then you have order N squared ways of getting in and out, because you have all the other states.Alan:    That very, very quickly becomes unmanageable. You have this bag of rules about how people navigate this state space. Then, whenever you want to change something, it clashes with old rules or breaks something else. The thing is that mid-conversation, it's absolutely trivial to know if a chat box said the wrong thing. You can give that to a four year old and say, ""No, that's nonsense. That doesn't make sense."" It's really hard to then figure out from that big state machine why did that happen? Why did that go wrong? Reversing the logic. Alan:    We said, ""Okay, why don't we do it completely differently?"" We don't build out that state machine at all and we just say, ""Okay, here's a conversation that went wrong."" Say what should have happened instead. Add that to your training data. Then, train the machine learning model that learns how to have these conversations. Rather than having a fixed, solid representation of this state machine, you learn a continuous fuzzy representation. A continuous vector that represents that state. You learn that such that you can do these conversations that you've had, you've seen. Then, you can measure how well it can generalize some of these patterns.Alan:    There's a recent paper that we just finished. I'll make public very soon. Where we actually study how well you can take these general patterns, like answering a clarification question like, ""Oh, what address do you have on file for me?"" Then, reusing that in different context and even different domains. That's what we've always wanted to do with Rasa Core is say, ""Throw away your state machine. Don't try and anticipate and write rules for every possible conversation."" Because it's basically impossible to build a flow chart and reason about every conversation that everybody could have, because it's just a combinatorially big space. Don't do that. Just have real conversations and learn from that. That's where Rasa Core came from.Alan:    There was lots of research on doing machine learning-based dialogue management. We certainly didn't embed them. We had to do things a little bit differently from how the academic world was doing it. They were doing a lot of work on reinforcement learning. There were some technical reasons why that didn't make sense for people getting started with Rasa Core. The short version is basically you have to implement a reward function. That's not a trivial thing to do. We said, ""Okay, we don't go for reinforcement learning. We do everything supervised learning."" We let people build up these conversations interactively by talking to the system. That's what we've been doing with Rasa Core is taking some of the ideas from the literature. The ones that we think are most applicable.Alan:    Then, straying from the literature, where we think it makes sense to. Then, building a library that lets people who don't have a PhD in statistical dialogue systems actually build machine learning-based dialogue and not have to build these unmaintainable state machines. Hugo:    That's great. This movement from state machines to the importance of real conversations and machine learning. Implementations in machine learning algorithms, which learn for more and more conversations and more training data, real data is incredible. So what happens next? What is the future of conversational software look like, thinking about it in these terms.Alan:    That's really interesting. I think for developers, it's a really cool time to be in this. We engaged a lot with our community. That's probably my favorite thing about open source is we have literally thousands and thousands of developers who use our software. We have over 100 contributors to the code base, who are just people using it for their own purposes and contribute back. This is a real sense of excitement. People are building and inventing new things. I kind of think about it like being a web developer in the 90s. If you build websites now or you have friends in development, it's well understood what you have to do. None of that's really been invented yet for conversational AI.Alan:    I mean, some of the kind of early versions of it are there. As a developer, this is a green field. I get to invent a lot of stuff. That's really fun. There's still lots of challenges. It's not, by any means, a solved problem. That's why we invest so heavily in research and breaking through the limitations of what we see currently in our own libraries and then what people do in academia. Pushing beyond that and shipping it into the libraries.Alan:    On the consumer side, the future is lots of little magical moments, where something just works. I had a great one literally yesterday. I was on Google Analytics. It's not something I look at very often. Maybe once every few months, because I was working on our documentation. I wanted to know how many people view our documentation on a mobile device. I'm not an expert in Google Analytics, so I would have to go and build some filter. They have this cool feature where you can just ask a question. I literally just typed into the box, ""How many of our users at nlu.rasa.com, our documentation, are a mobile client?"" It just answered, ""6%."" I thought, ""Amazing. I didn't have to do anything and it worked the first time.Alan:    Those magical moments are really cool, especially fun because that was one of the first use cases we worked on in early 2016 that we actually had some paying customers for. Then, to see that deployed at scale in the wild was really exciting. I think a lot of really magical moments that we'll all experience over the following years, but we won't have anytime soon is the do everything magical assistant that replaces a human butler or something. Hugo:    Yeah. It's great to hear your experience as a user as well, as someone who develops a lot of these things also. You spoke to the future challenge, or current or future challenge, of scaling to multiple use cases. Are there any other big challenges facing conversational software development? Alan:    Yes, I do think the biggest one is, ""Okay, you have something which works in a narrow domain. How do you extend it to more domains? We recently saw this demo from Google Duplex. Primarily very, very impressive, text-to-speech, the speech synthesis. Also a very nice functioning dial-up system that can handle quite a bit of complexity. They're very open in their post about the software. It only works because it's very limited. It works on restaurants and hairdresser appointments. Then, the question is, ""Okay. How do you build the next 100 use cases?"" If you look at Amazon, I think there's 6,000 engineers working on Alexa. That's a big effort.Alan:    Then, the other part is really not just around the technical challenge of building it, but it's also the conceptual challenge for programmers to build software, where the core logic is learn from data. That's very different from calling an image-recognition API. You send it a picture and it tells you that there's an apple in the picture and then you have statements like, ""If apple say apple"" or something. The way the conversations go is learned from real conversations that people have had and that you've checked and annotated and fixed and learned from. Alan:    Managing that training data become a way of programming. A lot of product management needs to be rented there in terms of how do you actually do that? That's really interesting. That's obviously something that we spent a lot of time thinking about. I think we have some cool things in the pipeline as well. I'll have some great things to show there in the future. Hugo:     Great. I look forward to it, hearing about those future developments. Alan, my last question is there is do you have a final call to action for all our listeners out there? Alan:    Yeah. All of Rasa is open source so go check it out. Rasa.com is the website. If you search ""Rasa Core"" or ""Rasa NLU"", Google should show you that the documentation, the GitHub repos, and build something. Try it out. Let us know how it goes.Alan:    I'm really curious. People always come up with infinitely creative use cases. Yeah, if you have any problems, let us know.Hugo:    Fantastic. Alan, it's been an absolute pleasure having you on the show.","Keyword(freq): conversation(15), data(14), chatbot(13), library(11), case(10), rule(9), question(8), company(7), developer(6), industry(6)"
"5","mastery",2018-07-27,"How to Configure the Number of Layers and Nodes in a Neural Network","https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/","Artificial neural networks have two main hyperparameters that control the architecture or topology of the network: the number of layers and the number of nodes in each hidden layer. You must specify values for these parameters when configuring your network. The most reliable way to configure these hyperparameters for your specific predictive modeling problem is via systematic experimentation with a robust test harness. This can be a tough pill to swallow for beginners to the field of machine learning, looking for an analytical way to calculate the optimal number of layers and nodes, or easy rules of thumb to follow. In this post, you will discover the roles of layers and nodes and how to approach the configuration of a multilayer perceptron neural network for your predictive modeling problem. After reading this post, you will know: Let¡¯s get started. How to Configure the Number of Layers and Nodes in a Neural NetworkPhoto by Ryan, some rights reserved. This post is divided into four sections; they are: A node, also called a neuron or Perceptron, is a computational unit that has one or more weighted input connections, a transfer function that combines the inputs in some way, and an output connection. Nodes are then organized into layers to comprise a network. A single-layer artificial neural network, also called a single-layer, has a single layer of nodes, as its name suggests. Each node in the single layer connects directly to an input variable and contributes to an output variable. Single-layer networks have just one layer of active units. Inputs connect directly to the outputs through a single layer of weights. The outputs do not interact, so a network with N outputs can be treated as N separate single-output networks. <U+2014> Page 15, Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999. A single-layer network can be extended to a multiple-layer network, referred to as a Multilayer Perceptron. A Multilayer Perceptron, or MLP for sort, is an artificial neural network with more than a single layer. It has an input layer that connects to the input variables, one or more hidden layers, and an output layer that produces the output variables. The standard multilayer perceptron (MLP) is a cascade of single-layer perceptrons. There is a layer of input nodes, a layer of output nodes, and one or more intermediate layers. The interior layers are sometimes called ¡°hidden layers¡± because they are not directly observable from the systems inputs and outputs. <U+2014> Page 31, Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999. We can summarize the types of layers in an MLP as follows: Finally, there are terms used to describe the shape and capability of a neural network; for example: Traditionally, there is some disagreement about how to count the number of layers. The disagreement centers around whether or not the input layer is counted. There is an argument to suggest it should not be counted because the inputs are not active; they are simply the input variables. We will use this convention; this is also the convention recommended in the book ¡°Neural Smithing¡°. Therefore, an MLP that has an input layer, one hidden layer, and one output layer is a 2-layer MLP. The structure of an MLP can be summarized using a simple notation. This convenient notation summarizes both the number of layers and the number of nodes in each layer. The number of nodes in each layer is specified as an integer, in order from the input layer to the output layer, with the size of each layer separated by a forward-slash character (¡°/¡±). For example, a network with two variables in the input layer, one hidden layer with eight nodes, and an output layer with one node would be described using the notation: 2/8/1. I recommend using this notation when describing the layers and their size for a Multilayer Perceptron neural network. Before we look at how many layers to specify, it is important to think about why we would want to have multiple layers. A single-layer neural network can only be used to represent linearly separable functions. This means very simple problems where, say, the two classes in a classification problem can be neatly separated by a line. If your problem is relatively simple, perhaps a single layer network would be sufficient. Most problems that we are interested in solving are not linearly separable. A Multilayer Perceptron can be used to represent convex regions. This means that in effect, they can learn to draw shapes around examples in some high-dimensional space that can separate and classify them, overcoming the limitation of linear separability. In fact, there is a theoretical finding by Lippmann in the 1987 paper ¡°An introduction to computing with neural nets¡± that shows that an MLP with two hidden layers is sufficient for creating classification regions of any desired shape. This is instructive, although it should be noted that no indication of how many nodes to use in each layer or how to learn the weights is given. A further theoretical finding and proof has shown that MLPs are universal approximators. That with one hidden layer, an MLP can approximate any function that we require. Specifically, the universal approximation theorem states that a feedforward network with a linear output layer and at least one hidden layer with any ¡°squashing¡± activation function (such as the logistic sigmoid activation function) can approximate any Borel measurable function from one finite-dimensional space to another with any desired non-zero amount of error, provided that the network is given enough hidden units. <U+2014> Page 198, Deep Learning, 2016. This is an often-cited theoretical finding and there is a ton of literature on it. In practice, we again have no idea how many nodes to use in the single hidden layer for a given problem nor how to learn or set their weights effectively. Further, many counterexamples have been presented of functions that cannot directly be learned via a single one-hidden-layer MLP or require an infinite number of nodes. Even for those functions that can be learned via a sufficiently large one-hidden-layer MLP, it can be more efficient to learn it with two (or more) hidden layers. Since a single sufficiently large hidden layer is adequate for approximation of most functions, why would anyone ever use more? One reason hangs on the words ¡°sufficiently large¡±. Although a single hidden layer is optimal for some functions, there are others for which a single-hidden-layer-solution is very inefficient compared to solutions with more layers. <U+2014> Page 38, Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999. With the preamble of MLPs out of the way, let¡¯s get down to your real question. How many layers should you use in your Multilayer Perceptron and how many nodes per layer? In this section, we will enumerate five approaches to solving this problem. In general, when I¡¯m asked how many layers and nodes to use for an MLP, I often reply: I don¡¯t know. Use systematic experimentation to discover what works best for your specific dataset. I still stand by this answer. In general, you cannot analytically calculate the number of layers or the number of nodes to use per layer in an artificial neural network to address a specific real-world predictive modeling problem. The number of layers and the number of nodes in each layer are model hyperparameters that you must specify. You are likely to be the first person to attempt to address your specific problem with a neural network. No one has solved it before you. Therefore, no one can tell you the answer of how to configure the network. You must discover the answer using a robust test harness and controlled experiments. For example, see the post: Regardless of the heuristics you might encounter, all answers will come back to the need for careful experimentation to see what works best for your specific dataset. The network can be configured via intuition. For example, you may have an intuition that a deep network is required to address a specific predictive modeling problem. A deep model provides a hierarchy of layers that build up increasing levels of abstraction from the space of the input variables to the output variables. Given an understanding of the problem domain, we may believe that a deep hierarchical model is required to sufficiently solve the prediction problem. In which case, we may choose a network configuration that has many layers of depth. Choosing a deep model encodes a very general belief that the function we want to learn should involve composition of several simpler functions. This can be interpreted from a representation learning point of view as saying that we believe the learning problem consists of discovering a set of underlying factors of variation that can in turn be described in terms of other, simpler underlying factors of variation. <U+2014> Page 201, Deep Learning, 2016. This intuition can come from experience with the domain, experience with modeling problems with neural networks, or some mixture of the two. In my experience, intuitions are often invalidated via experiments. In their important textbook on deep learning, Goodfellow, Bengio, and Courville highlight that empirically, on problems of interest, deep neural networks appear to perform better. Specifically, they state the choice of using deep neural networks as a statistical argument in cases where depth may be intuitively beneficial. Empirically, greater depth does seem to result in better generalization for a wide variety of tasks. [¡¦] This suggests that using deep architectures does indeed express a useful prior over the space of functions the model learns. <U+2014> Page 201, Deep Learning, 2016. We may use this argument to suggest that using deep networks, those with many layers, may be a heuristic approach to configuring networks for challenging predictive modeling problems. This is similar to the advice for starting with Random Forest and Stochastic Gradient Boosting on a predictive modeling problem with tabular data to quickly get an idea of an upper-bound on model skill prior to testing other methods. A simple, but perhaps time consuming approach, is to leverage findings reported in the literature. Find research papers that describe the use of MLPs on instances of prediction problems similar in some way to your problem. Note the configuration of the networks used in those papers and use them as a starting point for the configurations to test on your problem. Transferability of model hyperparameters that result in skillful models from one problem to another is a challenging open problem and the reason why model hyperparameter configuration is more art than science. Nevertheless, the network layers and number of nodes used on related problems is a good starting point for testing ideas. Design an automated search to test different network configurations. You can seed the search with ideas from literature and intuition. Some popular search strategies include: This can be challenging with large models, large datasets and combinations of the two. Some ideas to reduce or manage the computational burden include: I recommend being systematic if time and resources permit. I have seen countless heuristics of how to estimate the number of layers and either the total number of neurons or the number of neurons per layer. I do not want to enumerate them; I¡¯m skeptical that they add practical value beyond the special cases on which they are demonstrated. If this area is interesting to you, perhaps start with ¡°Section 4.4 Capacity versus Size¡± in the book ¡°Neural Smithing¡°. It summarizes a ton of findings in this area. The book is dated from 1999, so there are another nearly 20 years of ideas to wade through in this area if you¡¯re up for it. Also, see some of the discussions linked in the Further Reading section (below). Did I miss your favorite method for configuring a neural network? Or do you know a good reference on the topic?
Let me know in the comments below. This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered the role of layers and nodes and how to configure a multilayer perceptron neural network. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of Python Discover how in my new Ebook: Deep Learning With Python It covers self-study tutorials and end-to-end projects on topics like:Multilayer Perceptrons,<U+00A0>Convolutional Nets and<U+00A0>Recurrent Neural Nets, and more¡¦ Skip the Academics. Just<U+00A0>Results. Click to<U+00A0>learn more. Thanks for the blog post.
There is indeed a large number of recent research going to answer this question automatically, dubbed (neural) architecture search. Here a list of papers which I maintain:https://www.automl.org/automl/literature-on-neural-architecture-search/ Thanks. Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning.","Keyword(freq): layer(28), node(20), network(12), function(7), problem(7), variable(6), hyperparameter(4), idea(4), input(4), output(4)"
"6","mastery",2018-07-25,"How to Calculate McNemar¡¯s Test to Compare Two Machine Learning Classifiers","https://machinelearningmastery.com/mcnemars-test-for-machine-learning/","The choice of a statistical hypothesis test is a challenging open problem for interpreting machine learning results. In his widely cited 1998 paper, Thomas Dietterich recommended the McNemar¡¯s test in those cases where it is expensive or impractical to train multiple copies of classifier models. This describes the current situation with deep learning models that are both very large and are trained and evaluated on large datasets, often requiring days or weeks to train a single model. In this tutorial, you will discover how to use the McNemar¡¯s statistical hypothesis test to compare machine learning classifier models on a single test dataset. After completing this tutorial, you will know: Let¡¯s get started. How to Calculate McNemar¡¯s Test for Two Machine Learning ClassifiersPhoto by Mark Kao, some rights reserved. This tutorial is divided into five parts; they are: Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In his important and widely cited 1998 paper on the use of statistical hypothesis tests to compare classifiers titled ¡°Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms¡°, Thomas Dietterich recommends the use of the McNemar¡¯s test. Specifically, the test is recommended in those cases where the algorithms that are being compared can only be evaluated once, e.g. on one test set, as opposed to repeated evaluations via a resampling technique, such as k-fold cross-validation. For algorithms that can be executed only once, McNemar¡¯s test is the only test with acceptable Type I error. <U+2014> Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithm, 1998. Specifically, Dietterich¡¯s study was concerned with the evaluation of different statistical hypothesis tests, some operating upon the results from resampling methods. The concern of the study was low Type I error, that is, the statistical test reporting an effect when in fact no effect was present (false positive). Statistical tests that can compare models based on a single test set is an important consideration for modern machine learning, specifically in the field of deep learning. Deep learning models are often large and operate on very large datasets. Together, these factors can mean that the training of a model can take days or even weeks on fast modern hardware. This precludes the practical use of resampling methods to compare models and suggests the need to use a test that can operate on the results of evaluating trained models on a single test dataset. The McNemar¡¯s test may be a suitable test for evaluating these large and slow-to-train deep learning models. The McNemar¡¯s test operates upon a contingency table. Before we dive into the test, let¡¯s take a moment to understand how the contingency table for two classifiers is calculated. A contingency table is a tabulation or count of two categorical variables. In the case of the McNemar¡¯s test, we are interested in binary variables correct/incorrect or yes/no for a control and a treatment or two cases. This is called a 2¡¿2 contingency table. The contingency table may not be intuitive at first glance. Let¡¯s make it concrete with a worked example. Consider that we have two trained classifiers. Each classifier makes binary class prediction for each of the 10 examples in a test dataset. The predictions are evaluated and determined to be correct or incorrect. We can then summarize these results in a table, as follows: We can see that Classifier1 got 6 correct, or an accuracy of 60%, and Classifier2 got 5 correct, or 50% accuracy on the test set. The table can now be reduced to a contingency table. The contingency table relies on the fact that both classifiers were trained on exactly the same training data and evaluated on exactly the same test data instances. The contingency table has the following structure: In the case of the first cell in the table, we must sum the total number of test instances that Classifier1 got correct and Classifier2 got correct. For example, the first instance that both classifiers predicted correctly was instance number 5. The total number of instances that both classifiers predicted correctly was 4. Another more programmatic way to think about this is to sum each combination of Yes/No in the results table above. The results organized into a contingency table are as follows: McNemar¡¯s test is a paired nonparametric or distribution-free statistical hypothesis test. It is also less intuitive than some other statistical hypothesis tests. The McNemar¡¯s test is checking if the disagreements between two cases match. Technically, this is referred to as the homogeneity of the contingency table (specifically the marginal homogeneity). Therefore, the McNemar¡¯s test is a type of homogeneity test for contingency tables. The test is widely used in medicine to compare the effect of a treatment against a control. In terms of comparing two binary classification algorithms, the test is commenting on whether the two models disagree in the same way (or not). It is not commenting on whether one model is more or less accurate or error prone than another. This is clear when we look at how the statistic is calculated. The McNemar¡¯s test statistic is calculated as: Where Yes/No is the count of test instances that Classifier1 got correct and Classifier2 got incorrect, and No/Yes is the count of test instances that Classifier1 got incorrect and Classifier2 got correct. This calculation of the test statistic assumes that each cell in the contingency table used in the calculation has a count of at least 25. The test statistic has a Chi-Squared distribution with 1 degree of freedom. We can see that only two elements of the contingency table are used, specifically that the Yes/Yes and No/No elements are not used in the calculation of the test statistic. As such, we can see that the statistic is reporting on the different correct or incorrect predictions between the two models, not the accuracy or error rates. This is important to understand when making claims about the finding of the statistic. The default assumption, or null hypothesis, of the test is that the two cases disagree to the same amount. If the null hypothesis is rejected, it suggests that there is evidence to suggest that the cases disagree in different ways, that the disagreements are skewed. Given the selection of a significance level, the p-value calculated by the test can be interpreted as follows: It is important to take a moment to clearly understand how to interpret the result of the test in the context of two machine learning classifier models. The two terms used in the calculation of the McNemar¡¯s Test capture the errors made by both models. Specifically, the No/Yes and Yes/No cells in the contingency table. The test checks if there is a significant difference between the counts in these two cells. That is all. If these cells have counts that are similar, it shows us that both models make errors in much the same proportion, just on different instances of the test set. In this case, the result of the test would not be significant and the null hypothesis would not be rejected. Under the null hypothesis, the two algorithms should have the same error rate ¡¦ <U+2014> Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithm, 1998. If these cells have counts that are not similar, it shows that both models not only make different errors, but in fact have a different relative proportion of errors on the test set. In this case, the result of the test would be significant and we would reject the null hypothesis. So we may reject the null hypothesis in favor of the hypothesis that the two algorithms have different performance when trained on the particular training <U+2014> Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithm, 1998. We can summarize this as follows: After performing the test and finding a significant result, it may be useful to report an effect statistical measure in order to quantify the finding. For example, a natural choice would be to report the odds ratios, or the contingency table itself, although both of these assume a sophisticated reader. It may be useful to report the difference in error between the two classifiers on the test set. In this case, be careful with your claims as the significant test does not report on the difference in error between the models, only the relative difference in the proportion of error between the models. Finally, in using the McNemar¡¯s test, Dietterich highlights two important limitations that must be considered. They are: Generally, model behavior varies based on the specific training data used to fit the model. This is due to both the interaction of the model with specific training instances and the use of randomness during learning. Fitting the model on multiple different training datasets and evaluating the skill, as is done with resampling methods, provides a way to measure the variance of the model. The test is appropriate if the sources of variability are small. Hence, McNemar¡¯s test should only be applied if we believe these sources of variability are small. <U+2014> Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithm, 1998. The two classifiers are evaluated on a single test set, and the test set is expected to be smaller than the training set. This is different from hypothesis tests that make use of resampling methods as more, if not all, of the dataset is made available as a test set during evaluation (which introduces its own problems from a statistical perspective). This provides less of an opportunity to compare the performance of the models. It requires that the test set is an appropriately representative of the domain, often meaning that the test dataset is large. The McNemar¡¯s test can be implemented in Python using the mcnemar() Statsmodels function. The function takes the contingency table as an argument and returns the calculated test statistic and p-value. There are two ways to use the statistic depending on the amount of data. If there is a cell in the table that is used in the calculation of the test statistic that has a count of less than 25, then a modified version of the test is used that calculates an exact p-value using a binomial distribution. This is the default usage of the test: Alternately, if all cells used in the calculation of the test statistic in the contingency table have a value of 25 or more, then the standard calculation of the test can be used. We can calculate the McNemar¡¯s on the example contingency table described above. This contingency table has a small count in both the disagreement cells and as such the exact method must be used. The complete example is listed below. Running the example calculates the statistic and p-value on the contingency table and prints the results. We can see that the test strongly confirms that there is very little difference in the disagreements between the two cases. The null hypothesis not rejected. As we are using the test to compare classifiers, we state that there is no statistically significant difference in the disagreements between the two models. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to use the McNemar¡¯s test statistical hypothesis test to compare machine learning classifier models on a single test dataset. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦by writing lines of code in python Discover how in my new Ebook:Statistical Methods for Machine Learning It provides self-study tutorials on topics like:Hypothesis Tests, Correlation, Nonparametric Stats, Resampling, and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hi Jason, Thanks for this nice post. Any practical python post about 5¡¿2 CV + paired t-test coming soon? Best,
Elie Good question. 5¡¿2 is straight forward with sklearn. I do have a post on how to code the t-test from scratch scheduled. It can be modified with the suggestions from:https://machinelearningmastery.com/statistical-significance-tests-for-comparing-machine-learning-algorithms/ Thanks Jason. I found a nice Kaggle kernel treating 5¡¿2 CV t-test: https://www.kaggle.com/ogrellier/parameter-tuning-5-x-2-fold-cv-statistical-test Nice. Thank you Jason.
I¡¯ve learnt alot from you I¡¯m glad to hear that. Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning.","Keyword(freq): model(19), mcnemar(16), test(11), classifier(9), result(8), algorithm(7), case(7), cell(6), method(5), data(4)"
"7","mastery",2018-07-23,"When to Use MLP, CNN, and RNN Neural Networks","https://machinelearningmastery.com/when-to-use-mlp-cnn-and-rnn-neural-networks/","What neural network is appropriate for your predictive modeling problem? It can be difficult for a beginner to the field of deep learning to know what type of network to use. There are so many types of networks to choose from and new methods being published and discussed every day. To make things worse, most neural networks are flexible enough that they work (make a prediction) even when used with the wrong type of data or prediction problem. In this post, you will discover the suggested use for the three main classes of artificial neural networks. After reading this post, you will know: Let¡¯s get started. When to Use MLP, CNN, and RNN Neural NetworksPhoto by PRODAVID S. FERRY III,DDS, some rights reserved. This post is divided into five sections; they are: Deep learning is the application of artificial neural networks using modern hardware. It allows the development, training, and use of neural networks that are much larger (more layers) than was previously thought possible. There are thousands of types of specific neural networks proposed by researchers as modifications or tweaks to existing models. Sometimes wholly new approaches. As a practitioner, I recommend waiting until a model emerges as generally applicable. It is hard to tease out the signal of what works well generally from the noise of the vast number of publications released daily or weekly. There are three classes of artificial neural networks that I recommend that you focus on in general. They are: These three classes of networks provide a lot of flexibility and have proven themselves over decades to be useful and reliable in a wide range of problems. They also have many subtypes to help specialize them to the quirks of different framings of prediction problems and different datasets. Now that we know what networks to focus on, let¡¯s look at when we can use each class of neural network. Multilayer Perceptrons, or MLPs for short, are the classical type of neural network. They are comprised of one or more layers of neurons. Data is fed to the input layer, there may be one or more hidden layers providing levels of abstraction, and predictions are made on the output layer, also called the visible layer. For more details on the MLP, see the post: Model of a Simple Network MLPs are suitable for classification prediction problems where inputs are assigned a class or label. They are also suitable for regression prediction problems where a real-valued quantity is predicted given a set of inputs. Data is often provided in a tabular format, such as you would see in a CSV file or a spreadsheet. Use MLPs For: They are very flexible and can be used generally to learn a mapping from inputs to outputs. This flexibility allows them to be applied to other types of data. For example, the pixels of an image can be reduced down to one long row of data and fed into a MLP. The words of a document can also be reduced to one long row of data and fed to a MLP. Even the lag observations for a time series prediction problem can be reduced to a long row of data and fed to a MLP. As such, if your data is in a form other than a tabular dataset, such as an image, document, or time series, I would recommend at least testing an MLP on your problem. The results can be used as a baseline point of comparison to confirm that other models that may appear better suited add value. Try MLPs On: Convolutional Neural Networks, or CNNs, were designed to map image data to an output variable. They have proven so effective that they are the go-to method for any type of prediction problem involving image data as an input. For more details on CNNs, see the post: The benefit of using CNNs is their ability to develop an internal representation of a two-dimensional image. This allows the model to learn position and scale in variant structures in the data, which is important when working with images. Use CNNs For: More generally, CNNs work well with data that has a spatial relationship. The CNN input is traditionally two-dimensional, a field or matrix, but can also be changed to be one-dimensional, allowing it to develop an internal representation of a one-dimensional sequence. This allows the CNN to be used more generally on other types of data that has a spatial relationship. For example, there is an order relationship between words in a document of text. There is an ordered relationship in the time steps of a time series. Although not specifically developed for non-image data, CNNs achieve state-of-the-art results on problems such as document classification used in sentiment analysis and related problems. Try CNNs On: Recurrent Neural Networks, or RNNs, were designed to work with sequence prediction problems. Sequence prediction problems come in many forms and are best described by the types of inputs and outputs supported. Some examples of sequence prediction problems include: The Many-to-Many problem is often referred to as sequence-to-sequence, or seq2seq for short. For more details on the types of sequence prediction problems, see the post: Recurrent neural networks were traditionally difficult to train. The Long Short-Term Memory, or LSTM, network is perhaps the most successful RNN because it overcomes the problems of training a recurrent network and in turn has been used on a wide range of applications. For more details on RNNs, see the post: RNNs in general and LSTMs in particular have received the most success when working with sequences of words and paragraphs, generally called natural language processing. This includes both sequences of text and sequences of spoken language represented as a time series. They are also used as generative models that require a sequence output, not only with text, but on applications such as generating handwriting. Use RNNs For: Recurrent neural networks are not appropriate for tabular datasets as you would see in a CSV file or spreadsheet. They are also not appropriate for image data input. Don¡¯t Use RNNs For: RNNs and LSTMs have been tested on time series forecasting problems, but the results have been poor, to say the least. Autoregression methods, even linear methods often perform much better. LSTMs are often outperformed by simple MLPs applied on the same data. For more on this topic, see the post: Nevertheless, it remains an active area. Perhaps Try RNNs on: A CNN or RNN model is rarely used alone. These types of networks are used as layers in a broader model that also has one or more MLP layers. Technically, these are a hybrid type of neural network architecture. Perhaps the most interesting work comes from the mixing of the different types of networks together into hybrid models. For example, consider a model that uses a stack of layers with a CNN on the input, LSTM in the middle, and MLP at the output. A model like this can read a sequence of image inputs, such as a video, and generate a prediction. This is called a CNN LSTM architecture. The network types can also be stacked in specific architectures to unlock new capabilities, such as the reusable image recognition models that use very deep CNN and MLP networks that can be added to a new LSTM model and used for captioning photos. Also, the encoder-decoder LSTM networks that can be used to have input and output sequences of differing lengths. It is important to think clearly about what you and your stakeholders require from the project first, then seek out a network architecture (or develop one) that meets your specific project needs. For a good framework to help you think about your data and prediction problems, see the post: This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered the suggested use for the three main classes of artificial neural networks. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of Python Discover how in my new Ebook: Deep Learning With Python It covers self-study tutorials and end-to-end projects on topics like:Multilayer Perceptrons,<U+00A0>Convolutional Nets and<U+00A0>Recurrent Neural Nets, and more¡¦ Skip the Academics. Just<U+00A0>Results. Click to<U+00A0>learn more. Very nice article on neural networks.  I love to work on data using neural networks. The human brain is clearly the baseline for many computer programs and artificial intelligence approaches. Artificial neural networks algorithm are focused on replicating the thought and reasoning patterns of the human brain which makes it an intriguing algorithm to use. Thanks. Hi Dr. Brownlee, Firstly, thanks for all your posts, they¡¯ve been a useful reference for me since I began getting involved with ML problems about a year ago. Right now I¡¯m working with the problem of audio classification using conventional and neural network approaches. I¡¯m actually using the three NNs you mention above. The idea of a hybrid model fascinates me but 1. I don¡¯t know if it can work properly for my audio problem and 2. I don¡¯t have any experience designing these hybrid models.  I appreciate any advice on this! Try it and see how it goes. What hybrid do you want to try? I have many on the blog, a good start might be here:https://machinelearningmastery.com/keras-functional-api-deep-learning/ Thanks   jason it is useful I¡¯m glad to hear that. Thank you Dr. Jason.
Your tutorials have given me an inroad to ML and Data Mining.
Thank you I¡¯m glad to hear that. Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning.","Keyword(freq): network(21), data(17), problem(14), type(9), layer(6), model(6), rnn(6), cnn(5), input(5), detail(4)"
"8","vidhya",2018-07-27,"Top 10 Pretrained Models to get you Started with Deep Learning (Part 1 <U+2013> Computer Vision)","https://www.analyticsvidhya.com/blog/2018/07/top-10-pretrained-models-get-started-deep-learning-part-1-computer-vision/","Pretrained models are a wonderful source of help for people looking to learn an algorithm or try out an existing framework. Due to time restrictions or computational restraints, it¡¯s not always possible to build a model from scratch which is why pretrained models exist! You can use a pretrained model as a benchmark to either improve the existing model, or test your own model against it. The potential and<U+00A0>possibilities are vast. Source: Facebook AI In this article, we will look at various pretrained models in Keras that have applications in computer vision. Why Keras? First, because I believe it¡¯s a great library for people starting out with neural networks. Second, I wanted to stick with one framework throughout the article. This will help you in moving from one model to the next without having to worry about frameworks. I encourage you to try each model out on your own machine, understand how it works, and how you can improve or tweak the internal parameters. We have segmented this topic into a series of articles. Part II will focus on Natural Language Processing (NLP) and Part III will look at Audio and Speech models. The aim is to get you up and running in these fields with existing solutions which will fast track your learning process. Object detection is one of the most common applications in the field of computer vision. It has applications in all walks of life, from self-driving cars to counting the number of people in a crowd. This section deals with pretrained models that can be used for detecting objects. You can also check out the below articles to get familiar with this topic: Mask R-CNN is a flexible framework developed for the purpose of object instance segmentation. This pretrained model is an implementation of this Mask R-CNN technique on Python and Keras. It generates bounding boxes and segmentation masks for each instance of an object in a given image (like the one shown above). This GitHub repository features a plethora of resources to get you started. It includes the source code of Mask R-CNN, the training code and pretrained weights for MS COCO, Jupyter notebooks to visualize each step of the detection pipeline, among other things. YOLO is an ultra popular object detection framework for deep learning applications. This repository contains implementations of YOLOv2 in Keras. While the developers have tested the framework on all sorts of object images <U+2013> like kangaroo detection, self-driving car, red blood cell detection, etc., they have released the pretrained model for raccoon detection. You can download the raccoon dataset here and get started with this pretrained model now! The dataset<U+00A0>consists of 200 images (160-training, 40-validation). You can download the pretrained weights for the entire model here. According to the developers, these weights can be used for an object detector for one class. As the name suggests, MobileNet is an architecture designed for mobile devices. It has been built by none other than Google. This particular model, which we have linked above, comes with pretrained weights on the popular ImageNet database (it¡¯s a database containing millions of images belonging to more than 20,000 classes). As you can see above, the applications of MobileNet are not just limited to object detection but span a variety of computer vision tasks <U+2013> like facial attributes, landmark recognition, finegrain classification, etc. If you were given a few hundred images of tomatoes, how would you classify them <U+2013> say defective/non-defective, or ripe/unripe? When it comes to deep learning, the go-to technique for this problem is image processing. In this classification problem, we have to<U+00A0>identify whether the tomato in the given image is grown or unripe using a pretrained Keras VGG16 model. The model was trained on 390 images of grown and unripe tomatoes from the ImageNet dataset and was tested on 18 different validation images of tomatoes.<U+00A0>The overall result on these validation images is given below: There are numerous ways of classifying a vehicle <U+2013> by it¡¯s body style, number of doors, open or closed roof, number of seats, etc. In this particular problem, we have to classify the images of cars into various classes. These classes include make, model, year, e.g. 2012 Tesla Model S. To develop this model, the car dataset from Stanford was used which contains<U+00A0>16,185 images of 196 classes of cars. The model was trained using pretrained VGG16, VGG19 and InceptionV3 models. The VGG network is characterized by its simplicity, using only 3¡¿3 convolutional layers stacked on top of each other in increasing depth. The 16 and 19 stand for the number of weight layers in the network. As the dataset is small, the simplest model, i.e. VGG16, was the most accurate. Training the VGG16 network gave an accuracy of 66.11% on the cross validation dataset.<U+00A0>More complex models like InceptionV3 were less accurate due to bias/variance issues. Facial recognition is all the rage in the deep learning community. More and more techniques and models are being developed at a remarkable pace to design facial recognition technology. Its applications span a wide range of tasks <U+2013> phone unlocking, crowd detection, sentiment analysis by analyzing the face, among other things. Face regeneration on the other hand, is the generation of a 3D modelled face from a closeup image of a face. The creation of 3D structured objects from mere two dimensional information is another thought out problem in the industry. The applications of face regeneration are vast in the film and gaming industry. Various CGI models can be automated thus saving tons of time and money in the process. This section of our article deals with pretrained models for these two domains. Creating a facial recognition model from scratch is a daunting task. You need to find, collect and then annotate a ton of images to have any hope of building a decent model. Hence using a pretrained model in this domain makes a lot of sense. VGG-Face is a<U+00A0>dataset that contains 2,622 unique identities with more than two million faces. This pretrained model<U+00A0>has been designed through the following method: This is a really cool implementation of deep learning. You can infer from the above image how this model works in order to reconstruct the facial features into a 3 dimensional space. This pretrained model was originally developed using Torch and then transferred to Keras. Semantic image segmentation is the task of assigning a semantic label to every single pixel in an image. These labels can be ¡°sky¡±, ¡°car¡±, ¡°road¡±, ¡°giraffe¡±, etc. What this technique does is it finds the outlines of objects and thus places restrictions on the accuracy requirements (this is what separates it from image level classification which has a much looser accuracy requirement). Deeplabv3 is Google¡¯s latest semantic image segmentation model. It was originally created using TensorFlow and has now been implemented using Keras. This GitHub repository also has code for how to get labels, how to use this pretrained model with custom number of classes, and of course how to trail your own model. This model attempts to address the problem of image segmentation of surgical instruments in a robot-assisted surgery scenario. The problem is further divided into two parts, which are as follows: This pretrained model is based on the U-Net network architecture and is further improved by using state-of-the-art semantic segmentation neural networks known as LinkNet and TernausNet. The model was trained on 8 ¡¿ 225-frame sequences of high resolution stereo camera images. Remember those games where you were given images and had to come up with captions? That¡¯s basically what image captioning is. It uses a combination of NLP and Computer Vision to produce the captions.<U+00A0>This task has been a challenging one for a long time as it requires huge datasets with unbiased images and scenarios. Given all these constraints, the algorithm must be generalized for any given image. A lot of businesses are leveraging this technique nowadays but how can you go about using it?<U+00A0>The solution lies in converting a given input image into a short and meaningful description. The encoder-decoder framework is widely used for this task. The image encoder is a convolutional neural network (CNN). This is a<U+00A0>VGG 16 pretrained model on the MS COCO dataset where the decoder is a long short-term memory (LSTM) network predicting the captions for the given image. For detailed explanation and walk through it¡¯s recommended that you follow up with our article on Automated Image Captioning.","Keyword(freq): image(13), model(10), application(7), kera(5), weight(4), caption(3), car(3), object(3), tomato(3), article(2)"
"9","vidhya",2018-07-27,"Infographic <U+2013> 13 Common Mistakes Amateur Data Scientists Make and How to Avoid Them","https://www.analyticsvidhya.com/blog/2018/07/infographic-common-mistakes-amateur-data-scientists-make-how-avoid-them/","The path to becoming a data scientist is one riddled with traps and challenges. If you¡¯re not careful, you might just fall into one of them! There are so many resources out there which aim to help aspiring data scientists become experts, but most of them are half-baked efforts that leave a gaping hole in your data scientist journey. This infographic lists down 13 common mistakes we have seen fresher data scientists make, and we even porvide tips on how to avoid them. To get a more comprehensive overview of this infographic, and to get free resources on how to avoid each mistake, head over to our popular article <U+2013> 13 Mistakes Amateur Data Scientists Make and How to Avoid Them.","Keyword(freq): data(4), scientist(3), mistake(2), resource(2), challenge(1), effort(1), expert(1), list(1), tip(1), trap(1)"
"10","vidhya",2018-07-24,"Top Highlights from AV¡¯s Record-Breaking Weekend Online Hackathon","https://www.analyticsvidhya.com/blog/2018/07/top-highlights-from-avs-record-breaking-weekend-online-hackathon/","What an eventful and intense hackathon weekend! Our recently concluded McKinsey Analytics Online Hackathon saw an overwhelming number of people participate and compete to win a NIPS conference ticket + $5,000, as well as interview opportunities with McKinsey Analytics. The entries kept on rolling as the weekend progressed, leading to a thrilling deadline finish. The hackathon saw over 5,000 data scientists and aspiring data science enthusiasts vying for a spot at the top of the leaderboard. As parameters were tweaked, and models were finalized, the number of submissions soared to over 11,000! A huge shout out to all who participated and made this a wonderfully competitive hackathon. Check out a few of the comments from the participants: You can check out the leaderboard standings<U+00A0>here. Those who were not able to participate missed out on one of the most exciting hackathons Analytics Vidhya has ever hosted. But don¡¯t worry, we have plenty in store for you ahead. Head over to our DataHack platform and check out upcoming events and practice problems. The problem statement posed by McKinsey Analytics was a fascinating one. The participants were tasked with building a model for an insurance company<U+00A0>for predicting the propensity to pay renewal premium and build an incentive plan for its agents to maximize the net revenue (i.e. renewals <U+2013> incentives given to collect the renewals) collected from the policies post their issuance. The participants were provided information about<U+00A0>past transactions from the policy holders along with their demographics.<U+00A0>Given this information, the the challenge was to predict the propensity of renewal collection and create an incentive plan for agents (at policy level) to maximise the net revenues from these policies. The final solutions will be evaluated on 2 criteria: There were some amazing approaches that went into this hackathon. We have shared a few of them in this section to help you understand how to structure and compete in these hackathons.","Keyword(freq): analytics(4), participant(3), agent(2), data(2), hackathon(2), policy(2), approach(1), comment(1), criterion(1), demographic(1)"
"11","vidhya",2018-07-22,"DataHack Radio Episode #5: Building High Performance Data Science teams with Kiran R","https://www.analyticsvidhya.com/blog/2018/07/datahack-radio-building-data-science-teams-kiran-r/","Very rarely do you come across someone who brings all of the following skills to the table: What do you do when you get some one with these skills? Well <U+2013> you ask them to be your guest on DataHack Radio and be a speaker on DataHack Summit and you let them do the talking! Ladies and Gentleman <U+2013> today we have a guest like this on the show <U+2013> please give a round of applause to Kiran R,<U+00A0>Director of the Data Sciences Center of Excellence at VMware. He has also agreed to do a talk at DataHack Summit 2018 this year <U+2013> so stay tuned. Kiran is a computer science engineer from MSRIT and a post-graduate from the Indian Institute of Management. Kozhikode (IIM-K). He brings more than<U+00A0>14 years of experience building high performance teams for several leading organizations. He is currently leading the Data Sciences Center of Excellence at VMware. Kiran is one of the few Grandmasters on Kaggle who has made his mark in the overall top 10 data scientists there. He is also a winner of the KDD 2014 Data Mining Cup. He is very passionate about data science, as you will discover in his conversation with Kunal. This is a MUST-LISTEN podcast that spans the length and breadth of the various components in the world of data science. This article is essentially a highlight reel of this incredibly knowledge-rich podcast. Happy listening!  You can subscribe to DataHack Radio or listen to previous episodes on any of the below platforms:  Kiran¡¯s first job was at Motorola as a software engineer writing C/C++ code. After spending a year there, he decided to pursue higher studies and completed his MBA from IIM Kozhikode in 2006. He took up an offer from Dell as an Analytics Manager and worked on various tools like SQL, Access and SAS (to build propensity models). His brilliant work ethic and keen innovative mind led him to win Dell¡¯s India Innovator of the Year award in 2012, handed to him directly by Michael Dell! He set up his own proprietorship under the name ¡®Chaotic Experiments¡¯ which he used while participating in several Kaggle competitions. He regularly finished in the top 10 in various competitions and achieved his highest rank of 7 till date during this period. He also did some freelance work in data mining at the time. From there, Kiran had stints at Amazon and Flipkart. He has been in his current role at VMware since the last four years. His role involves managing a team of data scientists that work across various domains including marketing, pricing and strategy. One of the more fascinating things about Kiran¡¯s career arc has been his choice to stay in a technical role even after getting his management degree. He was determined not to lose touch with his technical skills and carved out a path for himself accordingly. Kiran started off his Kaggle journey back when there weren¡¯t too many data science ¡°enthusiasts¡± around. The way he used to approach these problems was to build a diverse set of models <U+2013> a linear model was always a good start, random forest, gradient boosting and then an ensemble model. He made a lot of acquaintances through the platform and these competitions always pushed him to get out of his comfort zone and learn new techniques and build frameworks. This is something he stressed on, because most data scientist jobs restrict you to only those tools and techniques which the business can support. Platforms like Kaggle and Analytics Vidhya allow you to spread your wings. ¡°If you have a hammer and only one tool, everything looks like a nail to you.¡± Participating and learning through competitions helped Kiran a lot when it came to his professional job. There is obviously a difference between the two <U+2013> like in the industry, you won¡¯t get a ready-made dataset and you will need to convert the business problem into a data mining problem. But competitions and platforms like Analytics Vidhya help you become an expert in techniques and tools, and finding out why and how things work (or don¡¯t work). Concepts like overfitting can really help you in the industry and competitions also expose you to problems from various domains, which is an added advantage. Kiran also used his experience to tell us that most problems in the industry are classification based. Regression and segmentation are used as well, but pale in comparison to classification problems. Some of the most widely used techniques are logistic regression and tree-based models. Kiran¡¯s experience has mostly been in the B2C space (with the last 4 years in the B2B sector). He uses the following parameters to classify the data science work companies do: Kiran used these pointers to tell us his take on how Amazon, Flipkart and Dell leverage data science. His insights are fascinating and will enrich your knowledge of how data science works in the real-world. He also elaborated on how his current organization, VMware, uses data science. He went on tell us the difference between B2B and B2C, and how it¡¯s easier to test your models and results in a B2B space than a B2C space (primarily because of the longer purchase cycle the former has). Kiran¡¯s role is divided into 2 parts <U+2013> 90% of it is a functional role, which is to lead the Data Sciences Center of Excellence. The other 10% is to play a sight leader role for enterprise information management (BI, Analytics, and Data Science). His typical day involves a lot of operational activities, tons of people activities, project deliveries, data science technique brainstorming, interacting with stakeholders, among other things. He has a very hands-on leadership style which helps him juggle and manage these various aspects on a daily basis. Kiran believes hiring is the most important part of team building, because if you have the right people, the job will get done. When he hires people for his team, there are a few critical areas he looks at: I highly recommend listening to the podcast to understand the details of these pointers which Kiran has elaborated on in detail. This will give you a brilliant insight into what an industry leader looks for when hiring a data scientist. One thing that has stood out for Kiran is the pace at which data science has grown in recent years. The interest in this field is extremely high right now and he sees this continuing in the future. Advancement in the type of models we make will increase and the time it takes to train models will decrease, according to Kiran. This will of course be helped by far better computational power (GPUs could well get cheaper) and we should see deep learning being used more and more on structured data. Another change Kiran predicts in the near future is the increasing pressure on business leaders to adopt AI and data mining. As more and more techniques are developed and become interpretable, businesses will be forced into situations where they have the leverage AI to gain any competitive edge they can get. Most of the proprietor software, like SAS, will go away according to Kiran. Open source languages like R and Python have already eaten away into the lead SAS had and will continue to gain popularity in the coming years. Open source is the driving force behind machine learning and more and more researchers and organizations are realizing this. Kiran firmly believes one needs to be good in programming to make a career in machine learning. If you can¡¯t do programming, you will have an almighty struggle to get into this field. If you¡¯re new to this field, Kiran¡¯s advice is to pick up Python and get very familiar with it. You should also eventually learn more than one language which will help expand your skillset.","Keyword(freq): data(21), competition(6), model(6), technique(5), analytics(4), kiran(4), problem(4), platform(3), science(3), skill(3)"
