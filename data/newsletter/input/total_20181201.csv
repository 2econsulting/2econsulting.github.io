"","site","date","headline","url_address","text","keyword"
"1","kaggle",2018-11-29,"Designing a Self-Learning Tic-Tac-Toe Player","http://blog.kaggle.com/2018/11/29/designing-a-self-learning-tic-tac-toe-player/","YOLO, tuberculosis, and candy: <U+00A0>Enjoy these new, intriguing and overlooked datasets and kernels 1.<U+00A0><U+0001F916> Designing a Self-Learning Tic-Tac-Toe Player (Link) 2.<U+0001F4F8> Yolo v3 Object Detection in Tensorflow (Link) 3.<U+0001F691> Tuberculosis (TB) Analyzer + Web App (Link) 4.<U+0001F1F0> Comparing Kaggle and StackOverflow Communities (Link) 5.<U+0001F4DA><U+00A0>The MOOC Wars: Kaggle's Perspective (Link) 6.<U+0001F4C8><U+00A0>Time Series Analysis - Artificial Neural Networks (Link) 7.<U+0001F36D> Candy Forecast - Simple, Exponential, ARIMA (Link) 8.<U+0001F1EF><U+0001F1F5> Dataset: Global Map Japan Data (Link) 9.<U+0001F1E7><U+0001F1F7> Dataset: Marketing Funnel by Olist (Link) 10.<U+0001F1FA><U+0001F1F8> Dataset: Kansas City Barbeque Society Competition Results (Link) <U+200B><U+200B><U+200B><U+200B> Working with categorical data? Try embeddings! Copyright ¨Ï 2018 Kaggle, All rights reserved.","Keyword(freq): community(1), dataset(1), embedding(1), kernel(1), network(1), result(1), right(1), war(1), NA(NA), NA(NA)"
"2","kaggle",2018-11-27,"LogicAI and Kaggle team up on Kaggle Days events in 2019 and beyond!","http://blog.kaggle.com/2018/11/27/kaggle-days/","Last year, a group of Kaggle fans from LogicAI had the idea of an offline event dedicated to serving the data science community. Their efforts were so inspirational that we reached out and asked if they¡¯d like to shape future ¡°Kaggle Days¡± events together. After the success of the first event in Warsaw last year, we¡¯ve decided to officially partner with LogicAI for additional Kaggle Days events and meetups<U+2014>the first of which will be held in January 2019, in Paris! Why Kaggle Days? The Kaggle community has over 2 million of users from all over the world. As a fully remote team ourselves, we know the importance of getting together in person on a regular basis. Events like Kaggle Days can deepen relations among existing Kagglers, and entice more data scientists to engage with us in the future.  What is Kaggle Days?Kaggle days is a two-event event where data science enthusiasts can talk to each other face to face, exchange knowledge, and compete together.In Warsaw last May, over 100 participants from 30 countries took part in presentations and workshops held by 13 Grandmasters and Masters. Participants were divided up into teams to take part in an 11-hour offline competition!Who can come to Kaggle Days?Anyone who loves data science or wants to learn more is welcome to attend Kaggle Days. To learn more about the upcoming Paris event, visit kaggledays.com/registration Call for SpeakersWant to speak at Kaggle Days? We're taking applications from Kaggle Masters and Grandmasters, as well as anyone in the Data Science community with a unique message to share. If this is you, please tell us more <U+00A0>at: hello@kaggledays.com Local MeetupsIn addition to the tentpole two-day events, the Kaggle Days team is also inviting the Data Science community to host smaller, more frequent Kaggle Days Meetups in their hometowns. If you¡¯re interested in hosting a meetup in your area, please apply here: Become an Organizer<U+00A0> Stay tuned for more information about Kaggle Days Paris and let us know where you¡¯d like to see these events take place in the future! Email us: hello@kaggledays.com Team Kaggle + LogicAI","Keyword(freq): event(5), grandmaster(2), logicai(2), master(2), participant(2), application(1), effort(1), enthusiast(1), fan(1), hometown(1)"
"3","datacamp",2018-11-29,"Request for Proposal: Topical Projects for January 2019","https://www.datacamp.com/community/blog/topical-projects","The first task in the Explore 538's Halloween Candy Rankings project. Sounds like fun, right? Well, it is! And the good news is you can get in on this too and have your project launch in January 2019. As explained in a previous blog post (How is content created at DataCamp?), DataCamp outsources our instructors and uses an in-house Content Team to guide instructors through the creation process. The benefits of being a DataCamp project instructor are many, as explained in this job posting. It takes 10-24 hours over 4-8 weeks to create most DataCamp projects, depending on the experience of the instructor, if the project is being converted from another format (e.g., blog post), etc., which is why this request for proposals was sent out in November. Are you interested in creating a topical project for January 2019? Great! If you'd like more information on DataCamp projects, scroll to the bottom of this post. If you'd like more information on becoming a DataCamp instructor, check out these FAQs for creating DataCamp projects and these FAQs for creating DataCamp content in general. We did some work for you already, using DataCamp's internal brainpower to generate potential topical project ideas. Here they are, verbatim: The Super Bowl is February 3, 2019 for this NFL season. January is the month of started (and failed?) New Year's resolutions. War and Peace will be published 150 years ago come 2019. It is regarded as a central work of world literature and one of Leo Tolstoy's finest literary achievements. Chinese New Year 2019 is February 5. 2019 is the Year of the Pig. DataCamp projects are where students can apply several of skills they learned in DataCamp courses in real-world, end-to-end data analyses. If you're a DataCamp subscriber, take a look at our ever-growing project library, otherwise, take one of our free projects below. If you want more, this article outlines projects in detail and the pedagogical advantages of the product. Instructor FAQs:","Keyword(freq): project(6), faq(2), instructor(2), achievement(1), advantage(1), analysis(1), benefit(1), hour(1), idea(1), outsource(1)"
"4","datacamp",2018-11-26,"Cathy O'Neil discusses the current lack of fairness in artificial intelligence and much more.","https://www.datacamp.com/community/blog/weapons-math-destruction","Hugo Bowne-Anderson, the host of DataFramed, the DataCamp podcast, recently interviewed Cathy O'Neil, author of the blog mathbabe.org and several books on data science, including Weapons of Math Destruction. Here is the podcast link. Hugo:               Hi there, Cathy, and welcome to DataFramed. Cathy:              Thank you. I'm glad to be here. Hugo:               It's such a great pleasure to have you on the show and I'm really excited to be here to talk about a lot of different things surrounding data science, and ethics, and algorithmic bias, and all of these types of things. But before we get into the nitty-gritty, I'd love to know a bit about you. Perhaps you can start off by telling us what you do and what you're known for in the data community. Cathy:              Well, I'm a data science consultant and I started a company to audit algorithms, but I guess I've been a data scientist for almost as long as there's been that title. And actually, I would argue that I was a data scientist before that title existed, because I worked as a quant in finance starting in 2007. And I think of that as a data science job, even though other people might not agree. I mean, and the reason I'm waffling is because when I entered data science maybe I would say in 2011, a large question in my mind was, ""To what extent is this a thing?"" I wrote a book actually called Doing Data Science just to explore that question. I co-authored it with Rachel Schutt. The idea of that book was, what is data science? Is it a thing? Is it new? Is it important? Is it powerful? Is it too powerful? Things like that. So what am I known for? I think I'm known for being a gadfly, for sort of calling out bullshit, for possibly ... People think of me as overly negative about the field. I think of myself as the antidote to Kool-Aid. Hugo:               Well yeah, I think a lot of the work you've done as kind of a restoring or restorative force to a lot of what's happening at blinding speed in tech, in data science, in how algorithms are becoming more and more a part of our daily lives. Cathy:              Yeah. That sounds nice. Thank you. Hugo:               You're welcome. And I like kind of the way you motivated your book that you co-wrote Doing Data Science in terms of exploring what data science is, because you actually have a nice working definition in there which is something along the lines of like a data savvy, quantitatively minded coding literate problem solver. And that's how I like to think of data science work in general as well. Cathy:              Yeah. You know, I've actually kind of discarded that definition in preference for a new definition, which I just came up with like a couple months ago, but I'm into. And this is a way of distinguishing the new part of data science from the old part of data science. So it's not really a definition of data science per se, but it is what it a definition of what I worry about in data science, if you will, which is that data science doesn't just predict the future, it causes the future. So that distinguishes it from astronomers. Astronomers use a lot of quantitative techniques. They use tons of data. They're not new, so are they data scientists? In the first definition that you just told me from my book, probably yes, but in the second no, because the point is that they can tell us when Halley's Comet is coming back, but they're not going to affect when Halley's comet is coming back. And that's the thing that data science, or I should say data scientists need to understand about what they're doing. For the most part, they are not just sort of predicting but they're causing. And that's where it gets these society wide feedback loops that we need to start worrying about. Hugo:               Agreed completely. And I look forward to delving into these feedback loops. And this idea of a feedback loop in data science work in algorithms and modeling is one of the key ingredients of what you call a Weapon of Math Destruction, which I really look forward to getting back to. But I like the idea that you've moved on from a quasi-definition you had in your book Doing Data Science. Because the question, I mean, Doing Data Science was written five or six years ago now. Is that right? Cathy:              Yeah. 2012 I think. Hugo:               Yeah. Right. So I'm wondering, looking back on that, if you were to rewrite it or do it again, what do you think is worth talking about now that you couldn't see then? Cathy:              Well to be clear each chapter was a different lecture in a class at Columbia. It was taken from those lectures including a lecture I gave about finance, including a lecture that we had the data scientists from Square speak. We had people that were probably not considered data scientists but statisticians speaking, et cetera. So it was a grab bag, and in that way it was actually really cool because it was all over the place and broad, and we could see how sort of these techniques were broadly applicable, various techniques, and we could also go into networks in one chapter and time series in another. And that was neat because we could have a survey if you will, of stuff. But it wasn't meant to be a deep dive in any given direction. Cathy:              If I rewrote it now, I would probably, if I kept with that survey approach, I would be surveying a totally different world because we have very different kinds of things going on now. I guess we also have some sort of through streams, like we have some things that are still happening that were happening then that we've emphasize more. I think in particular I would spend a lot more time on recommendation engines, although we do have a chapter on recommendation engines, from the former CEO of Hunch, I believe, to try to understand a person by 20 questions and then sort of recommend what kind of iPhone they should buy or something like that. But nowadays, I'd spend a lot more time exploring things like to what extent do the youtube recommendations radicalize our youth? Hugo:               That's interesting because I think what that does is it puts data science and data science work, as we've been discussing already, into a broader societal context, and assesses and communicates around the impact of all the work that happens in data science. So I think that provides a nice segue into a lot of the work you've done, which culminated in your book, Weapons of Math Destruction, that I'd like to spend a bit of time on. So could you tell me kind of the basic ingredients of what a weapon of math destruction actually is? Cathy:              Sure. A weapon of math destruction is a kind of algorithm that I feel we're not worrying enough about. It's important, and it's secret, and it's destructive. Those are the three characteristics. By important, I mean it's widespread, it's scaled, it's used on a lot of people for important decisions. I usually think of the categories of decisions in the following, like financial decisions, so it could be a credit card or insurance or housing. Or livelihood decisions like do you get a job? Do you keep your job? Are you good at your job? Do you get a raise? Or liberty. So how are you policed, how are you sentenced to prison? How are you given parole? Your actual liberty. And then the fourth category would be information. So how are you fed information? How is your environment online and particular informed through algorithms and what kind of long-term effects are those having on different parts of the population? So those are the four categories. They're important. One of the things that I absolutely insist on when we talk about weapons of math destruction or algorithms or regulation in particular is that we really focus in on important algorithms. There's just too many algorithms to worry about. So we have to sort of triage and think about which ones actually matter to people. Cathy:              And then the second thing is that they're secret. Almost all of these are secret. Even people don't even know that they exist, never mind understand how they work. And then finally, they're making important secret decisions about people's lives and they fuck up. They make mistakes, and it's destructive for that individual who doesn't get the opportunity or the job or the credit card or the housing opportunity or they get imprisoned too long. So it's destructive for them. But as an observation, and this goes back to the feedback loop thing, it's not just destructive for an individual, but it actually sort of undermines the original goal of the algorithm and sort of creates a destructive feedback loop on the level of society. Hugo:               Yeah. And a point you make in your book, which we may get to, is that they can also feed into each other and exacerbate conditions that already exist in society such as being unfair on already underrepresented groups. So before we get there though, could you provide, I mean you've provided a nice kind of framework of the different buckets of these algorithms and WMDs and where they fall, but could you provide a few concrete examples of what you consider to be the most harmful WMDs? Cathy:              Yeah. So I'll give you a few. And I choose these in part because they're horrible, but also because they all fail in totally different ways. I want to make the point that there's not one solution to this problem. So the first one comes from the world of teaching, public school teaching. So there was this thing called the Value Added Model for teachers which was used to fire a bunch of teachers, and unfairly because it turned out it was not much better than a random number generator. Didn't contain a lot of information about a specific teacher. And in instances where it did seem to be sort of an extreme value, it was manipulated by previous teachers cheating. So you couldn't really control your numbers, but if your a previous teacher cheated, then your number would go down. So it was like this crazy system. Hugo:               Yeah. Because if I remember correctly, the baseline is set by where your students were in the previous year or who taught them right? Cathy:              Yeah. The idea was like how well did your students do relative to their expected performance in a standardized test? And it was a very noisy question in terms of statistics. Unless the previous teacher in the previous year had cheated on those kids' tests, and those kids did extremely well relative to what they actually understood, which would force them of course to do extremely badly the next year, even if you're a good teacher. So it would look really bad for you. But long story short, it was normally speaking, when there wasn't cheating involved, just a terrible statistically non robust model. And yet it was being used to fire people. So that's the first example. The next example is this example from hiring, which is a story about Kyle Beam, this young man who noticed in a personality test that he had to take to get a job that he failed. He noticed some of the questions were exactly the same questions that he had been given in a mental health assessment when he was being treated for bipolar disorder. So that was an embedded illegal mental health assessment, that is illegal under the Americans with Disability Act, which makes it illegal for any kind of health exam, including a mental health exam to be administered as part of a hiring process. So that's another example, and I should add that it wasn't just one job. Kyle ended up taking seven different versions of this test, I should say he ended up taking the same exact test seven different times when he applied to seven different chain stores, all of them in the Atlanta, Georgia area. So he wasn't just precluded from that one job, he was precluded from almost any minimum wage work in the area. Cathy:              And it wasn't just him, it was anybody who would have failed that mental health assessment, which is to say a vast community of people with mental health status. So that's a great example of the feedback loop I was mentioning. Because of the scale of this Kronos test, it wasn't just destructive for the individual, but it was undermining the exact goal of personality tests and also undermining the overall goal of the ADA, which is to avoid the systematic filtering out of subpopulations. And so that's the second example. And the third example I would give is what we call recidivism risk algorithms in the criminal justice system where you have basically questionnaires that end up with a score for recidivism risk that is handed to a judge and being told to the judge this is so objective, scientific measurement of somebody's risk of recidivism, recidivism being the likelihood of being arrested after leaving prison. Cathy:              And the problem with that, well there's lots of problems with that, but the very immediate problem with that is that the questions on the questionnaire are almost entirely proxies for race and class. So they ask questions like, ""Did you grow up in a high crime neighborhood?"" I mean, you grew up in a high crime neighborhood if you're a poor black person, fact. That's almost the definition of high crime neighborhood. That's where the police are sent to arrest people, historically from the broken windows policy, the theory of policing, to the present day. And by the way I should add, in part that has been propagated by another algorithm which is predictive policing. So you're being asked all these proxies for poverty, proxies for race and class. Other questions are like, ""Are you a member of a gang? Do you have mental health problems? Do you have addiction problems?"" Cathy:              A lot of this kind of information is only held against you if you are poor, and richer people, white people get treated, they don't get punished for this kind of thing. So long story short, it's basically a test to see how poor you are and how minority you are. And then if you're score is higher, which it is if you are poor and if you're black, then you get sent to prison for longer. Now I should say as toxic as that algorithm is, and as obvious as it is that it creates negative feedback loops, one of the things that sort of the jury is still out on is whether that is actually that different from what we have already. We have already a racist and classist system, not to mention judges. And we have evidence for that. Cathy:              And the idea was we're going to get better. We're going to be more scientific, we're going to be more objective. It's not at all clear that kind of scoring system would do so. Nor is it clear by the way, because there's been lots of, not lots, but there's been some amount of testing since my book came out about how judges actually use these scoring systems. It's not clear that they use them the way that they're intended. And there's all sorts of evidence now that judge either ignore them or they ignore them in certain cases, but listen to them in other cases. For example, they ignore them in black courtrooms and they use them in white courtrooms. So they actually keep a lot of people, especially if they're being used for pretrial detention considerations, they'll let white people out of incarceration pretrial, but then they're going to ignore them in urban districts where they are going to keep black people incarcerated before trial. Cathy:              Long story short, there's also a lot of questions around how they're actually being used, but it's a great example of a weapon of math destruction just created as if the nature of algorithms will make things more fair. I guess going to your earlier point, no algorithm is perfect, and we couldn't expect that to be perfect. The reason these sort of society wide just destructive feedback loops get propagated, get created by these algorithms isn't just because they're imperfect. It's because they're being used, as I said in that example, but more broadly they're more funneling people in different classes and for different genders or races or different mental health status or disability status. They're funneling them onto a path which they were sort of 'already on' depending on their demographics. Hugo:               Yeah, and I think speaking to your point of the fact that these algorithms might not be creating new biases, I mean they may as well, but that they're encoding societal biases and keeping people on a path they may have been on already, I think something distinct from that is that they're actually scalable as well, right? Cathy:              Yeah, right. So we shouldn't be surprised of course now that we say it out loud, they're propagating past practices, they're automating the status quo, they're just doing what was done in the past and acting like, ""Since this happened in the past, in a pattern, we should predict that it will happen in the future."" But the way they're being actually utilized, it means not just that we predict it will happen, but we're going to cause it to happen. If you are more likely to pay back a loan, you're more likely to get a loan. So the people who are who are deemed less likely are going to be cut out of the system. They're not going to be offered a credit card. And since all the algorithms work in concert and similarly to each other, this becomes a rule and it's highly scaled, even if it's not the exact same algorithm, which it was in the case of Kyle Beam with the Kronos algorithm, same exact algorithm being used. But even if it isn't, the fact is data scientists do their job similarly across different companies in the same industry, so online credit decisioning is going to be based on similar kinds of demographic questions. Hugo:               I also think, I mean there are a lot of different avenues we can take here, and for people who want more after this conversation, I highly recommend Cathy's book Weapons of Math Destruction. Something I'd like to focus on is that in all of these models, the value added model for teaching, the hiring model, these models to predict recidivism rate, one really important aspect of these is that they're not interpretable. We can't tell why they make the predictions they do. The fact that they're black box in that sense. And the relationship between this inability to interpret them, the inability of a teacher to go and say why have you given me this rating? And they're pointed to the algorithm, and the fact that this combined with the scalability really makes en masse lack of accountability and lack of fairness, correct? Cathy:              Yeah. I mean it's exactly right. And I talked about that as in fact a characteristic of a weapon of mass destruction, that it's secret. And that's a really important part of it because when you have something that's important and secret, it's almost always going to be destructive. A good data science model has a feedback loops and it incorporates its mistakes, but there's no reason for their mistakes to be incorporated when we don't alert people to them. So that's this sort of lack of accountability is a real problem for the model. But it's also obviously a real problem for the people who are scored incorrectly because they have no appeal. There's no due process. And to that point, there were six teachers in Houston that won a lawsuit. They were fired based on their value added model scores. They sued and won and the judge found that their due process rights had been violated, and I'm sort of sitting around waiting for that to happen in every other example that I have mentioned, but also in lots and lots of other examples that are similar where you have this secret important decision made about you. Why is that okay? Hugo:               So this is a retroactive, I suppose, I don't want to use the word solution, but a way of dealing with what has happened. I agree that action needs to be taken across the board. I'm wondering what some viable solutions are to stop this happening in future. And I love the fact that we opened this conversation with you telling us that you work in consulting now, in particular, in algorithmic audits, and I'm wondering if that will be a part of the solution going forward and what else we can do as a data science community to make sure that we're accountable? Cathy:              I mean, yes. So there's two different approaches and one of them is transparency and one of them is auditability. And honestly I think we need to consider both very carefully. We have to think about what it means for something to be transparent. Certainly it wouldn't be very useful to hand over the source code to teachers to tell them, ""This is how you're being evaluated, here are the coefficients that we've trained on this data."" No, that would not be useful, so we need to understand what we mean by transparency. And I sort of worked out a kind of idea that I think is worth a try. It's kind of a sensitivity analysis. I mean that's the technical term, but really what it looks like is first confirm that the data that you have about me is correct. Next what if something had changed a little bit, what if this kid had gotten a slightly better score? What if that kid hadn't been in my class? What if I had another kid? What if I'd been teaching a different school? What if I'd been teaching in a different classroom in my school? What if I'd had, 30 kids instead of 20, how would my score change? Cathy:              And it's not going to prove everything. It would catch obvious errors, it would catch obvious instabilities, which actually that algorithm in particular had. So if you found out that your score would go from bad to good based on one small change, then you would know that this is a bogus algorithm. So that's one idea at the level of transparency, but I would insist on suggesting that you really don't know whether an algorithm is fair, just knowing how your own score works. Even if you really, really understood your own score, you wouldn't know if it's fair. Cathy:              Fairness is a statistical concept. It's a notion that we need to understand at an aggregate level. So I am pushing for the idea of auditing as just as important as transparency really, to ask the questions along the lines of for whom does this algorithm fail? Does this fail more often for black people or white people, does it fail more often for women than for men, et cetera. And that's a question you cannot get to just by understanding your own score or whether your own data is correct or incorrect. It's a question that has to be asked at a much higher level with much more access. Now to your point that I myself have an algorithmic auditing company, I do. But guess what, it doesn't have that many customers, sadly. And it's a result of the fact that algorithms essentially don't have that much scrutiny. There's not much leverage to convinced somebody to audit their algorithms. Cathy:              I have some clients and those clients are great and I love them. They are clients who really do want to know whether their algorithm is working as intended and they want to know either for their own sake because money's on the line or their reputation's on the line, or for some third party on behalf of some third party, like the investors or their customers or the public at large. They want to know whether it's working. What I really started my company for though is to audit algorithms that I think are generally speaking, the algorithms that companies don't want to have audited, if you see where I'm going with this. It's those algorithms that are profiting from racism or profiting from bypassing the Americans with Disability Act. Those are the very algorithms that I want to be auditing, but I don't have those clients yet, and I don't have them because we're still living in a sort of plausible deniability situation with respect to algorithms. Hugo:               So it may not currently be within these companies interests to be audited, right? So where do you see these incentives coming from? I can imagine the endgame could be legislators catching up with technology. Another thing we currently have is that data scientists and the community as a whole are in relative positions of being able to make requests to their own company. So you could imagine we're having this conversation now around checklists versus oaths versus codes of conduct within the data science community as a whole. And you could imagine algorithmic audits becoming part of a checklist or an oath or a code of conduct. So I'm wondering, where you see the incentives for companies in late stage capitalism coming from? Cathy:              Yeah, I mean I know there's a lot of really ethical data scientists out there and I love them all, but I don't expect their power to be sufficient to get their company that they work for to start worrying about this, in general. So I think it has to come from fear honestly, and that's either fear of federal regulators, not holding my breath for that to happen. Or fear of litigation, so that essentially their compliance officer says you have to do this or else we're taking on too much risk and we're going to get screwed. Just in this example, Kyle Beam was applying to work at Kroger's grocery store when he got redlighted by that Kronos algorithm. So Kroger's grocery store was licensing the Kronos algorithm with a license agreement that said they wouldn't understand the algorithm that Kronos had built, but they had this indemnification clause, extra contract on top of their licensing agreement that said if there's any problem with this algorithm, Kronos would pay for the problem. So they would take on the risk. Cathy:              But Kronos is not a very big company. It was working with seven huge companies just in the Atlanta, Georgia area taking on the risk, which is stupid because honestly the fair hiring law, the ADA, the onus is on the large company, not on some small data vendor. So when Kyle's father, who's a lawyer, sued, he filed a class action lawsuit, seven class action lawsuits against every one of those large companies. Those large companies are on the hook for the settlement if it ends up as a settlement. And Kronos is going to go bankrupt very, very quickly if that ends up being settled for lots of money. Cathy:              So it's just one example, but it's I think a very important example to demonstrate the fact that the company's using these algorithms for HR or what have you, and that's often the framework, the setup is that some small company builds the algorithm and then licenses to some large either company or government agency in the case of predictive policing, recidivism or, for that matter, teacher evaluation. And they can't just offshore the risk because it's those large companies that are going to be on the hook for the lawsuits. Right now, the world is that those large companies do not see the risk. They do not acknowledge the risk. And so far they've gotten away with it. Hugo:               Your discussion of Kronos there really reminded me something that really surprised me when reading Weapons of Math Destruction was how, I mean I knew about a lot of these cases, but about a lot of the data vendors and small companies that build these models, I'd heard of hardly any of them. That kind of shocked me with respect to how much impact they are having and can have, in the future, on society. Cathy:              Yeah. You know, it's this kind of funny thing where we as a society are waking up, and that's a very important thing. The public itself is starting to say, ""Hey wait, algorithms aren't necessarily fair."" But how do we know that? It's because we use Google search and we use Facebook and we see these, I would say, consumer facing algorithms, one by one, on a daily basis. And so we see the flaws of those things and we see the longer term sort of societal effects of being outraged by the news we see on Facebook everyday. Those happen to be sort of obvious examples of problematic algorithms, but they also have it to be like some of the hardest, biggest, most complex algorithms out there. I would not know actually how to go about auditing them. Cathy:              Let me put it this way, there'd be a thousand different ways to audit them, and you'd have to sort of think really hard about each way of how to set up a test. Whereas just asking whether specific personality tests or application filter which is also used, algorithm that filters out applications for jobs, whether that is legal is a much more finite doable question, but because of the nature of those algorithms, we may send in an application for a job, we don't even know our application is being filtered by an algorithm, so how is the public going to find out it's wrong or their application was wrongly classified? It's completely hidden from our view, and I would say that most of the algorithms that are having strong effects on our lives, college admissions offices all use algorithms too, we don't know about them. We can't complain if they go wrong because we're never made aware of them. And yet those are the ones that desperately need to be audited. Hugo:               And so in terms of where people can find out more about these types of algorithms and the challenges we're facing as a society, I know for example the recidivism work, ProPublica has done a lot of great work on that. I follow Data And Society and AI Now Institute, but I'm wondering do you have any suggestions for where people can read more widely about what's happening now? Cathy:              I mean the good news is that there's lots and lots of people thinking about this. The bad news is ProPublica, AI Now, any kind of sort of outside group, even with the best intentions, doesn't have access to these algorithms. That's a large part of why I did not go that route. I'm not an academic. I don't have the goal of having a sort of think tank that audits algorithms from the outside because you can't. You literally can't audit algorithms that are HR algorithms from the outside. You have to be invited in. So that's why I started a company that theoretically anyway could be invited in to audit an algorithm. But then the problem I still have, in spite of the fact that I'm willing to sign a non-disclosure agreement, is that nobody wants my services because of this plausible deniability issue. Literally there are people that I have talked to that want my services, but then their corporate lawyers come on the phone and they say, ""What if you find a problem with our algorithm that we don't know how to fix? And then later on when somebody sues us, in discovery it's found that we knew there was a problem with this algorithm? That's no good. We can't use your services. Goodbye."" Hugo:               So I'd like to move slightly and just think about the broader context of data science and the data revolution, and I'm wondering what other important challenges you think we are facing with respect to the amount of data there is, data privacy, and all the work that's happening. Cathy:              I mean, I'd say the biggest problem is that, we live in a putatively free society, and we're having a lot of problems thinking about how to deal with this in a large part because it doesn't give way to that many individual stories. I think I've found a few stories like the Kyle Beam story, et cetera, found some teachers who were fired unfairly by the value added model, but the way our policy making works in this country is like they need to find victims and the people get outraged and then they complain and then the policymakers pass laws. And the nature of this statistical harm is different, and it's harder to measure and so it's harder to imagine laws being passed. And that's the best case scenario when you live in a society that actually cares. I guess the best best case scenario might be happening in Europe where they actually do pass laws. Although I think it's much more focused on privacy and less focused on this kind of algorithmic discrimination. But in terms of what I worry about the most, I'm looking at places like China with their social credit score, which are intrinsically not trying to be fair, they are just sort of explicitly trying to nudge people or strong arm people really into behaving well, and they're social control mechanisms and they're going to be very, very successful. Hugo:               So what lessons do you think we can take from history to help approach this issue? And I mean in particular from previous technological revolutions such as the industrial revolution, but there may be others. Cathy:              Well I mean, so there's lots of different answers to that. One of them is it took us a while to catch up with pollution because it was like, who in particular is harmed by a polluted river? So it's kind of an external, what is it called? Externality. And so we have externalities here which we are not keeping track of in the same kind of way. And it took us a while to actually care enough about our environment to worry about how chemicals change it. But we ended up doing that and in large part because of the book Silent Spring, but other things as well. And then another example I like to give is sort of if you think about the exciting new invention called the car, people were super excited about the car, but it was also really dangerous. And over time, we have kept track of car related deaths, and we have lowered them quite a bit because of inventions like the seatbelt and the crash test dummies, et cetera. Cathy:              And we started paying attention to what makes something safer. Not to say that they're totally safe because they're not, they're still not. But we have traded the convenience for the risk. I feel like best case scenario in our future interactions with algorithms, we're going to be doing a similar kind of trade where we need algorithms, they're so efficient and convenient, but we have to be aware of that risk. And the first step of that is to sort of measure the deaths. We measured car deaths, car related deaths. We need to measure algorithmic related harm, and that goes back to the point I was making at least twice already, which is that we are not aware currently of the harm because it's invisible to us. And so when I talk to policy makers, which I do, I beg them not to regulate algorithms by saying, ""Here's how you have to make an algorithm,"" because I think that would be possibly too restrictive, but regulate algorithms in saying, ""Tell us how this is going wrong, measure your harm, show us who's getting harmed."" That's the very first step. And understanding how to make things safer. Hugo:               And I think this speaks also to have a greater general cultural contextual challenge we're facing is in that as part of the political cycle, the amount of deaths incurred in a society forms of fundamental part. In a lot of respects, in America and other countries, but the amount of unfairness and poverty isn't necessarily something that's discussed in the same framework. Right? Cathy:              Can you say that again? Hugo:               Yeah. So deaths are something which are immediately quantifiable and are able to brought to legislators and politicians as part of the political cycle, whereas the amount of poverty isn't necessarily something that is as interesting in the news cycle and the political cycle. Cathy:              Yeah, that's a good point. It's harder to quantify inequality than it is to quantify deaths. And that goes back to the question of what does our political system respond to, if anything? I mean right now it's just a complete shit show, but even in the best of times it responds better to stories of cruelty and death than it does to silent mistakes that nevertheless cost people real opportunities. So it's hard to measure what does an opportunity loss cost to not getting a particular job or not being, here's another one that's even relevant to current lawsuits going on with Facebook, not being shown an ad for a job that you might have wanted because Facebook's getting in trouble for showing ads only to young people. And so it's like age issue, or only to men, so women don't get to see STEM related job ads. And so how much harm is that for a given person? It's not obviously the most harmful thing that's ever happened to someone. So it's not as exciting at a policy level, but if it happened systemically which it does, it's a problem for society. Hugo:               Yeah. And that speaks to another really important point in terms of accountability and transparency that you can be shown stuff in your online experience and I'm shown something totally different and legislators are shown something entirely different. And this type of targeting is a relatively new phenomenon. Cathy:              That's right. I mean that's one of the reasons it's so hard to pin down is... it's going to my earlier point if you get to see what you get to see, but that's not a statistical statement about what people get to see. Flipping that in the other direction, it is an amazing tool for predatory actions like payday lending or for- profit colleges, it's like they can't believe how lucky they got. They used to have a lot of trouble locating their victims, desperate poor people, but now they couldn't be happier because they've got this system and it's called the internet that finds them for them, cheaply, and en masse and scaling and in a way that's exceedingly easy to scale. So it's a fantasy come true for those kinds of bad actors. But then the question becomes how do we even keep track of that? If they are actually going after those people that are in sort of a very real way, voiceless, and don't have the political capital to make their problems a priority. Hugo:               So I've got one final question for you, Cathy. We're in the business of data science education here at DataCamp. Because of that, a lot of our listeners will be the data analysts and data scientists of the future. And I'd like to know what you'd like to see them do in their practice that isn't happening yet. Cathy:              I just wrote a paper, it's not out yet, but it will be out pretty soon about ethics and artificial intelligence with a philosopher named Hanna Gunn. It's called The Ethical Matrix. I actually don't know what it's called, but I think it's something along the lines of The Ethical Matrix. At least it introduces this concept of an ethical matrix, and it's a very simple idea. The idea is to broaden our definition of what it means for an algorithm to work. So when you ask somebody, ""Does this algorithm work?"" They always say yes, and then you say, ""What do you mean?"" And then it's like, ""Oh, it's efficient."" And so you're like, ""But beyond that does it work?"" And that's when it becomes like, what do you mean? And then even if they want to go there, they're like is that an infinitely complicated question that I don't know how to attack? Cathy:              So the idea of this ethical matrix is sort of to give a rubric to address this question, and it's something that I claim we should do before we start building an algorithm, that we should do with everyone who is involved. And so to that point, the first step in building an ethical matrix is to understand who the stakeholders are, and to get those stakeholders involved in the construction of the matrix, and to embed the values of the stakeholders in a balanced way relative to their concerns. So the rows are the stakeholders, the columns are the concerns, and then you go through each cell of the matrix and try to decide are these stakeholders at high risk for this concern to go very, very wrong. Cathy:              It's basically as simple as that. But our theory is that if this becomes part of the yoga of building a data driven algorithm, then it will theoretically at least help us consider much more broadly what it means for an algorithm to work, what it means for it to have long term negative consequences. Things to monitor for making sure that they're not going wrong, et cetera. And it will bring us from the narrow point of view, it's working because it's working for me and I'm making money, which is the one by one ethical matrix, the stakeholder is me or my company and the only concern is profit. Broaden that out to look at all the people that we're affecting, including maybe the environment. Look at all of the concerns they might have: fairness, transparency, false positives, false negatives, and consider all of these and balance their concerns explicitly. Hugo:               Well, I for one am really excited about reading this paper and we'll include a link in the show notes to it as well. When it's out. Cathy:              When it's out. Great, cool. Hugo:               Fantastic. Look, Cathy, thank you so much for coming on the show. I've enjoyed this conversation so much. Cathy:              Great. Thank you for having me. Thanks Hugo.","Keyword(freq): algorithm(36), company(10), krono(9), question(9), scientist(8), death(7), teacher(6), decision(5), loop(5), problem(5)"
"5","mastery",2018-11-30,"How to Reduce Generalization Error in Deep Neural Networks With Activity Regularization in Keras","https://machinelearningmastery.com/how-to-reduce-generalization-error-in-deep-neural-networks-with-activity-regularization-in-keras/","Activity regularization provides an approach to encourage a neural network to learn sparse features or internal representations of raw observations. It is common to seek sparse learned representations in autoencoders, called sparse autoencoders, and in encoder-decoder models, although the approach can also be used generally to reduce overfitting and improve a model¡¯s ability to generalize to new observations. In this tutorial, you will discover the Keras API for adding activity regularization to deep learning neural network models. After completing this tutorial, you will know: Let¡¯s get started. How to Reduce Generalization Error in Deep Neural Networks With Activity Regularization in KerasPhoto by Johan Neven, some rights reserved. This tutorial is divided into three parts; they are: Keras supports activity regularization. There are three different regularization techniques supported, each provided as a class in the keras.regularizers module: Each of the l1 and l2 regularizers takes a single hyperparameter that controls the amount that each activity contributes to the sum. The l1_l2 regularizer takes two hyperparameters, one for each of the l1 and l2 methods. The regularizer class must be imported and then instantiated; for example: Activity regularization is specified on a layer in Keras. This can be achieved by setting the activity_regularizer argument on the layer to an instantiated and configured regularizer class. The regularizer is applied to the output of the layer, but you have control over what the ¡°output¡± of the layer actually means. Specifically, you have flexibility as to whether the layer output means that the regularization is applied before or after the ¡®activation¡® function. For example, you can specify the function and the regularization on the layer, in which case activation regularization is applied to the output of the activation function, in this case, relu. Alternately, you can specify a linear activation function (the default, that does not perform any transform) which means that the activation regularization is applied on the raw outputs, then, the activation function can be added as a subsequent layer. The latter is probably the preferred usage of activation regularization as described in ¡°Deep Sparse Rectifier Neural Networks¡± in order to allow the model to learn to take activations to a true zero value in conjunction with the rectified linear activation function. Nevertheless, the two possible uses of activation regularization may be explored in order to discover what works best for your specific model and dataset. Let¡¯s take a look at how activity regularization can be used with some common layer types. The example below sets l1 norm activity regularization on a Dense fully connected layer. The example below sets l1 norm activity regularization on a Conv2D convolutional layer. The example below sets l1 norm activity regularization on an LSTM recurrent layer. Now that we know how to use the activity regularization API, let¡¯s look at a worked example. In this section, we will demonstrate how to use activity regularization to reduce overfitting of an MLP on a simple binary classification problem. Although activity regularization is most often used to encourage sparse learned representations in autoencoder and encoder-decoder models, it can also be used directly within normal neural networks to achieve the same effect and improve the generalization of the model. This example provides a template for applying activity regularization to your own neural network for classification and regression problems. We will use a standard binary classification problem that defines two two-dimensional concentric circles of observations, one circle for each class. Each observation has two input variables with the same scale and a class output value of either 0 or 1. This dataset is called the ¡°circles¡± dataset because of the shape of the observations in each class when plotted. We can use the make_circles() function to generate observations from this problem. We will add noise to the data and seed the random number generator so that the same samples are generated each time the code is run. We can plot the dataset where the two variables are taken as x and y coordinates on a graph and the class value is taken as the color of the observation. The complete example of generating the dataset and plotting it is listed below. Running the example creates a scatter plot showing the concentric circles shape of the observations in each class. We can see the noise in the dispersal of the points making the circles less obvious. Scatter Plot of Circles Dataset with Color Showing the Class Value of Each Sample This is a good test problem because the classes cannot be separated by a line, e.g. are not linearly separable, requiring a nonlinear method such as a neural network to address. We have only generated 100 samples, which is small for a neural network, providing the opportunity to overfit the training dataset and have higher error on the test dataset: a good case for using regularization. Further, the samples have noise, giving the model an opportunity to learn aspects of the samples that don¡¯t generalize. We can develop an MLP model to address this binary classification problem. The model will have one hidden layer with more nodes that may be required to solve this problem, providing an opportunity to overfit. We will also train the model for longer than is required to ensure the model overfits. Before we define the model, we will split the dataset into train and test sets, using 30 examples to train the model and 70 to evaluate the fit model¡¯s performance. Next, we can define the model. The hidden layer uses 500 nodes and the rectified linear activation function. A sigmoid activation function is used in the output layer in order to predict class values of 0 or 1. The model is optimized using the binary cross entropy loss function, suitable for binary classification problems and the efficient Adam version of gradient descent. The defined model is then fit on the training data for 4,000 epochs and the default batch size of 32. We will also use the test dataset as a validation dataset. We can evaluate the performance of the model on the test dataset and report the result. Finally, we will plot the performance of the model on both the train and test set each epoch. If the model does indeed overfit the training dataset, we would expect the line plot of accuracy on the training set to continue to increase and the test set to rise and then fall again as the model learns statistical noise in the training dataset. We can tie all of these pieces together, the complete example is listed below. Running the example reports the model performance on the train and test datasets. We can see that the model has better performance on the training dataset than the test dataset, one possible sign of overfitting. Your specific results may vary given the stochastic nature of the neural network and the training algorithm. Because the model is severely overfit, we generally would not expect much, if any, variance in the accuracy across repeated runs of the model on the same dataset. A figure is created showing line plots of the model accuracy on the train and test sets. We can see the expected shape of an overfit model where test accuracy increases to a point and then begins to decrease again. Line Plots of Accuracy on Train and Test Datasets While Training Showing an Overfit We can update the example to use activation regularization. There are a few different regularization methods to choose from, but it is probably a good idea to use the most common, which is the L1 vector norm. This regularization has the effect of encouraging a sparse representation (lots of zeros), which is supported by the rectified linear activation function that permits true zero values. We can do this by using the keras.regularizers.l1 class in Keras. We will configure the layer to use the linear activation function so that we can regularize the raw outputs, then add a relu activation layer after the regularized outputs of the layer. We will set the regularization hyperparameter to 1E-4 or 0.0001, found with a little trial and error. The complete updated example with the L1 norm constraint is listed below: Running the example reports the model performance on the train and test datasets. We can see that activity regularization resulted in a slight drop in accuracy on the training dataset down from 100% to 96% and a lift in accuracy on the test set up from 78% to 82%. Reviewing the line plot of train and test accuracy, we can see that it no longer appears that the model has overfit the training dataset. Model accuracy on both the train and test sets continues to increase to a plateau. Line Plots of Accuracy on Train and Test Datasets While Training With Activity Regularization For completeness, we can compare results to a version of the model where activity regularization is applied after the relu activation function. The complete example is listed below. Running the example reports the model performance on the train and test datasets. We can see that, at least on this problem and with this model, activation regularization after the activation function did not improve generalization error; in fact, it made it worse. Reviewing the line plot of train and test accuracy, we can see that indeed the model still shows the signs of having overfit the training dataset. Line Plots of Accuracy on Train and Test Datasets While Training With Activity Regularization, Still Overfit This suggests that it may be worth experimenting with both approaches for implementing activity regularization with your own dataset, to confirm that you are getting the most out of the method. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered the Keras API for adding activity regularization to deep learning neural network models. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. train_acc = model.evaluate(trainX, trainy, verbose=0)
_, test_acc = model.evaluate(testX, testy, verbose=0)
print(¡®Train: %.3f, Test: %.3f¡¯ % (train_acc, test_acc)) <<< Does not work here is the error TypeError: must be real number, not list The example does work. Ensure your libraries are up to date and that you¡¯re using Python 3.
Ensure that you¡¯re running the example from the command line. Does that help? Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): dataset(6), model(6), observation(6), set(6), circle(5), kera(5), plot(4), sample(4), network(3), output(3)"
"6","mastery",2018-11-28,"Activation Regularization for Reducing Generalization Error in Deep Learning Neural Networks","https://machinelearningmastery.com/activation-regularization-for-reducing-generalization-error-in-deep-learning-neural-networks/","Deep learning models are capable of automatically learning a rich internal representation from raw input data. This is called feature or representation learning. Better learned representations, in turn, can lead to better insights into the domain, e.g. via visualization of learned features, and to better predictive models that make use of the learned features. A problem with learned features is that they can be too specialized to the training data, or overfit, and not generalize well to new examples. Large values in the learned representation can be a sign of the representation being overfit. Activity or representation regularization provides a technique to encourage the learned representations, the output or activation of the hidden layer or layers of the network, to stay small and sparse. In this post, you will discover activation regularization as a technique to improve the generalization of learned features in neural networks. After reading this post, you will know: Let¡¯s get started. Activation Regularization for Reducing Generalization Error in Deep Learning Neural NetworksPhoto by Nicholas A. Tonelli, some rights reserved. This tutorial is divided into five parts; they are: Deep learning models are able to perform feature learning. That is, during the training of the network, the model will automatically extract the salient features from the input patterns or ¡°learn features.¡± These features may be used in the network in order to predict a quantity for regression or predict a class value for classification. These internal representations are tangible things. The output of a hidden layer within the network represent the learned features by the model at that point in the network. There is a field of study focused on the efficient and effective automatic learning of features, often investigated by having a network reduce an input to a small learned feature before using a second network to reconstruct the original input from the learned feature. Models of this type are called auto-encoders, or encoder-decoders, and their learned features can be useful to learn more about the domain (e.g. via visualization) and in predictive models. The learned features, or ¡°encoded inputs,¡± must be large enough to capture the salient features of the input but also focused enough to not over-fit the specific examples in the training dataset. As such, there is a tension between the expressiveness and the generalization of the learned features. More importantly, when the dimension of the code in an encoder-decoder architecture is larger than the input, it is necessary to limit the amount of information carried by the code, lest the encoder-decoder may simply learn the identity function in a trivial way and produce uninteresting features. <U+2014> Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition, 2007. In the same way that large weights in the network can signify an unstable and overfit model, large output values in the learned features can signify the same problems. It is desirable to have small values in the learned features, e.g. small outputs or activations from the encoder network. The loss function of the network can be updated to penalize models in proportion to the magnitude of their activation. This is similar to ¡°weight regularization¡± where the loss function is updated to penalize the model in proportion to the magnitude of the weights. The output of a layer is referred to as its ¡®activation,¡¯ as such, this form of penalty or regularization is referred to as ¡®activation regularization.¡¯ ¡¦ place a penalty on the activations of the units in a neural network, encouraging their activations to be sparse. <U+2014> Page 254, Deep Learning, 2016. The output of an encoder or, generally, the output of a hidden layer in a neural network may be considered the representation of the problem at that point in the model. As such, this type of penalty may also be referred to as ¡®representation regularization.¡¯ The desire to have small activations or even very few activations with mostly zero values is also called a desire for sparsity. As such, this type of penalty is also referred to as ¡®sparse feature learning.¡¯ One way to limit the information content of an overcomplete code is to make it sparse. <U+2014> Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition, 2007. The encouragement of sparse learned features in autoencoder models is referred to as ¡®sparse autoencoders.¡¯ A sparse autoencoder is simply an autoencoder whose training criterion involves a sparsity penalty on the code layer, in addition to the reconstruction error <U+2014> Page 505, Deep Learning, 2016. Sparsity is most commonly sought when a larger-than-required hidden layer (e.g. over-complete) is used to learn features that may encourage over-fitting. The introduction of a sparsity penalty counters this problem and encourages better generalization. A sparse overcomplete learned feature has been shown to be more effective than other types of learned features offering better robustness to noise and even transforms in the input, e.g. learned features of images may have improved invariance to the position of objects in the image. Sparse-overcomplete representations have a number of theoretical and practical advantages, as demonstrated in a number of recent studies. In particular, they have good robustness to noise, and provide a good tiling of the joint space of location and frequency. In addition, they are advantageous for classifiers because classification is more likely to be easier in higher dimensional spaces. <U+2014> Sparse Feature Learning for Deep Belief Networks, 2007. There is a general focus on sparsity of the representations rather than small vector magnitudes. A study of these representations that is more general than the use of neural networks is known as ¡®sparse coding.¡¯ Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it learns basis functions that capture higher-level features in the data. <U+2014> Efficient sparse coding algorithms, 2007. An activation penalty can be applied per-layer, perhaps only at one layer that is the focus of the learned representation, such as the output of the encoder model or the middle (bottleneck) of an autoencoder model. A constraint can be applied that adds a penalty proportional to the magnitude of the vector output of the layer. The activation values may be positive or negative, so we cannot simply sum the values. Two common methods for calculating the magnitude of the activation are: The L1 norm encourages sparsity, e.g. allows some activations to become zero, whereas the l2 norm encourages small activations values in general. Use of the L1 norm may be a more commonly used penalty for activation regularization. A hyperparameter must be specified that indicates the amount or degree that the loss function will weight or pay attention to the penalty. Common values are on a logarithmic scale between 0 and 0.1, such as 0.1, 0.001, 0.0001, etc. Activity regularization can be used in conjunction with other regularization techniques, such as weight regularization. This section provides some examples of activation regularization in order to provide some context for how the technique may be used in practice. Regularized or sparse activations were originally sought as an approach to support the development of much deeper neural networks, early in the history of deep learning. As such, many examples may make use of architectures like restricted Boltzmann machines (RBMs) that have been replaced by more modern methods. Another big application of weight regularization is in autoencoders with semi-labeled or unlabeled data, so-called sparse autoencoders. Xavier Glorot, et al. at the University of Montreal introduced the use of the rectified linear activation function to encourage sparsity of representation. They used an L1 penalty and evaluate deep supervised MLPs on a range of classical computer vision classification tasks such as MNIST and CIFAR10. Additionally, an L1 penalty on the activations with a coefficient of 0.001 was added to the cost function during pre-training and fine-tuning in order to increase the amount of sparsity in the learned representations <U+2014> Deep Sparse Rectifier Neural Networks, 2011. Stephen Merity, et al. from Salesforce Research used L2 activation regularization with LSTMs on outputs and recurrent outputs for natural language process in conjunction with dropout regularization. They tested a suite of different activation regularization coefficient values on a range of language modeling problems. While simple to implement, activity regularization and temporal activity regularization are competitive with other far more complex regularization techniques and offer equivalent or better results. <U+2014> Revisiting Activation Regularization for Language RNNs, 2017. This section provides some tips for using activation regularization with your neural network. Activation regularization is a generic approach. It can be used with most, perhaps all, types of neural network models, not least the most common network types of Multilayer Perceptrons, Convolutional Neural Networks, and Long Short-Term Memory Recurrent Neural Networks. Activity regularization may be best suited to those model types that explicitly seek an efficient learned representation. These include models such as autoencoders (i.e. sparse autoencoders) and encoder-decoder models, such as encoder-decoder LSTMs used for sequence-to-sequence prediction problems. The most common activation regularization is the L1 norm as it encourages sparsity. Experiment with other types of regularization such as the L2 norm or using both the L1 and L2 norms at the same time, e.g. like the Elastic Net linear regression algorithm. The rectified linear activation function, also called relu, is an activation function that is now widely used in the hidden layer of deep neural networks. Unlike classical activation functions such as tanh (hyperbolic tangent function) and sigmoid (logistic function), the relu function allows exact zero values easily. This makes it a good candidate when learning sparse representations, such as with the l1 vector norm activation regularization. It is common to use small values for the regularization hyperparameter that controls the contribution of each activation to the penalty. Perhaps start by testing values on a log scale, such as 0.1, 0.001, and 0.0001. Then use a grid search at the order of magnitude that shows the most promise. It is a generally good practice to rescale input variables to have the same scale. When input variables have different scales, the scale of the weights of the network will, in turn, vary accordingly. Large weights can saturate the nonlinear transfer function and reduce the variance in the output from the layer. This may introduce a problem when using activation regularization. This problem can be addressed by either normalizing or standardizing input variables. Configure the layer chosen to be the learned features, e.g. the output of the encoder or the bottleneck in the autoencoder, to have more nodes that may be required. This is called an overcomplete representation that will encourage the network to overfit the training examples. This can be countered with a strong activation regularization in order to encourage a rich learned representation that is also sparse. This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered activation regularization as a technique to improve the generalization of learned features. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): feature(23), value(12), model(10), activation(9), representation(9), network(8), autoencoder(5), example(5), type(5), weight(4)"
"7","mastery",2018-11-26,"How to Reduce Overfitting in Deep Neural Networks Using Weight Constraints in Keras","https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-neural-networks-with-weight-constraints-in-keras/","Weight constraints provide an approach to reduce the overfitting of a deep learning neural network model on the training data and improve the performance of the model on new data, such as the holdout test set. There are multiple types of weight constraints, such as maximum and unit vector norms, and some require a hyperparameter that must be configured. In this tutorial, you will discover the Keras API for adding weight constraints to deep learning neural network models to reduce overfitting. After completing this tutorial, you will know: Let¡¯s get started. How to Reduce Overfitting in Deep Neural Networks With Weight Constraints in KerasPhoto by Ian Sane, some rights reserved. This tutorial is divided into three parts; they are: The Keras API supports weight constraints. The constraints are specified per-layer, but applied and enforced per-node within the layer. Using a constraint generally involves setting the kernel_constraint argument on the layer for the input weights and the bias_constraint for the bias weights. Generally, weight constraints are not used on the bias weights. A suite of different vector norms can be used as constraints, provided as classes in the keras.constraints module. They are: For example, a constraint can imported and instantiated: The weight norms can be used with most layers in Keras. In this section, we will look at some common examples. The example below sets a maximum norm weight constraint on a Dense fully connected layer. The example below sets a maximum norm weight constraint on a convolutional layer. Unlike other layer types, recurrent neural networks allow you to set a weight constraint on both the input weights and bias, as well as the recurrent input weights. The constraint for the recurrent weights is set via the recurrent_constraint argument to the layer. The example below sets a maximum norm weight constraint on an LSTM layer. Now that we know how to use the weight constraint API, let¡¯s look at a worked example. In this section, we will demonstrate how to use weight constraints to reduce overfitting of an MLP on a simple binary classification problem. This example provides a template for applying weight constraints to your own neural network for classification and regression problems. We will use a standard binary classification problem that defines two semi-circles of observations, one semi-circle for each class. Each observation has two input variables with the same scale and a class output value of either 0 or 1. This dataset is called the ¡°moons¡± dataset because of the shape of the observations in each class when plotted. We can use the make_moons() function to generate observations from this problem. We will add noise to the data and seed the random number generator so that the same samples are generated each time the code is run. We can plot the dataset where the two variables are taken as x and y coordinates on a graph and the class value is taken as the color of the observation. The complete example of generating the dataset and plotting it is listed below. Running the example creates a scatter plot showing the semi-circle or moon shape of the observations in each class. We can see the noise in the dispersal of the points making the moons less obvious. Scatter Plot of Moons Dataset With Color Showing the Class Value of Each Sample This is a good test problem because the classes cannot be separated by a line, e.g. are not linearly separable, requiring a nonlinear method such as a neural network to address. We have only generated 100 samples, which is small for a neural network, providing the opportunity to overfit the training dataset and have higher error on the test dataset: a good case for using regularization. Further, the samples have noise, giving the model an opportunity to learn aspects of the samples that don¡¯t generalize. We can develop an MLP model to address this binary classification problem. The model will have one hidden layer with more nodes than may be required to solve this problem, providing an opportunity to overfit. We will also train the model for longer than is required to ensure the model overfits. Before we define the model, we will split the dataset into train and test sets, using 30 examples to train the model and 70 to evaluate the fit model¡¯s performance. Next, we can define the model. The hidden layer uses 500 nodes in the hidden layer and the rectified linear activation function. A sigmoid activation function is used in the output layer in order to predict class values of 0 or 1. The model is optimized using the binary cross entropy loss function, suitable for binary classification problems and the efficient Adam version of gradient descent. The defined model is then fit on the training data for 4,000 epochs and the default batch size of 32. We will also use the test dataset as a validation dataset. We can evaluate the performance of the model on the test dataset and report the result. Finally, we will plot the performance of the model on both the train and test set each epoch. If the model does indeed overfit the training dataset, we would expect the line plot of accuracy on the training set to continue to increase and the test set to rise and then fall again as the model learns statistical noise in the training dataset. We can tie all of these pieces together; the complete example is listed below. Running the example reports the model performance on the train and test datasets. We can see that the model has better performance on the training dataset than the test dataset, one possible sign of overfitting. Your specific results may vary given the stochastic nature of the neural network and the training algorithm. Because the model is overfit, we generally would not expect much, if any, variance in the accuracy across repeated runs of the model on the same dataset. A figure is created showing line plots of the model accuracy on the train and test sets. We can see that expected shape of an overfit model where test accuracy increases to a point and then begins to decrease again. Line Plots of Accuracy on Train and Test Datasets While Training Showing an Overfit We can update the example to use a weight constraint. There are a few different weight constraints to choose from. A good simple constraint for this model is to simply normalize the weights so that the norm is equal to 1.0. This constraint has the effect of forcing all incoming weights to be small. We can do this by using the unit_norm in Keras. This constraint can be added to the first hidden layer as follows: We can also achieve the same result by using the min_max_norm and setting the min and maximum to 1.0, for example: We cannot achieve the same result with the maximum norm constraint as it will allow norms at or below the specified limit; for example: The complete updated example with the unit norm constraint is listed below: Running the example reports the model performance on the train and test datasets. We can see that indeed the strict constraint on the size of the weights has improved the performance of the model on the holdout set without impacting performance on the training set. Reviewing the line plot of train and test accuracy, we can see that it no longer appears that the model has overfit the training dataset. Model accuracy on both the train and test sets continues to increase to a plateau. Line Plots of Accuracy on Train and Test Datasets While Training With Weight Constraints This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered the Keras API for adding weight constraints to deep learning neural network models. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. Is gradient clipping similar to a weight constraint? Great question!  Not quite. Weight constraints are applied to the weights and is a regularization technique. Gradient clipping is applied to the error gradient used to update the weights and is used to avoid exploding gradients. Awesome article. This helps to impove theprediction  in the kaggle competition, ¡°Don¡¯t call me turkey!¡±. Wishes, I¡¯m happy to hear that, well done! Does containing the weights in each layer say to sum up to one make the model easier to interpret? Maybe on the input layer, but perhaps not on hidden layers. Say if  kernel_constraint=max_norm(A). On what basis should I set up the value of ¡®A¡¯ ? Experiment with a range of small integer values, often in [1,4] On which layer this should be applied ? As most of the networks in CNN are too deeper. Is there a way to figure that out ? Use on all layers. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): constraint(15), weight(12), set(6), dataset(4), kera(4), norm(4), observation(4), sample(4), layer(3), model(3)"
"8","vidhya",2018-11-29,"Tutorial on Text Classification (NLP) using ULMFiT and fastai Library in Python","https://www.analyticsvidhya.com/blog/2018/11/tutorial-text-classification-ulmfit-fastai-library/","Natural Language Processing (NLP) needs no introduction in today¡¯s world. It¡¯s one of the most important fields of study and research, and has seen a phenomenal rise in interest in the last decade. The basics of NLP are widely known and easy to grasp. But things start to get tricky when the text data becomes huge and unstructured. That¡¯s where deep learning becomes so pivotal. Yes, I¡¯m talking about deep learning for NLP tasks <U+2013> a still relatively less trodden path. DL has proven its usefulness in computer vision tasks like image detection, classification and segmentation, but NLP applications like text generation and classification have long been considered fit for traditional ML techniques. Source: Tryolabs And deep learning has certainly made a very positive impact in NLP, as you¡¯ll see in this article. We will focus on the concept of transfer learning and how we can leverage it in NLP to build incredibly accurate models using the popular fastai library. I will introduce you to the ULMFiT framework as well in the process. Note- This article assumes basic familiarity with neural networks, deep learning and transfer learning. If you are new to deep learning, I would strongly recommend reading the following articles first: If you are a beginner in NLP, check out this video course<U+00A0>with 3 real life projects. I praised deep learning in the introduction, and deservedly so. However, everything comes at a price, and deep learning is no different. The biggest challenge in deep learning is the massive data requirements for training the models. It is difficult to find datasets of such huge sizes, and it is way too costly to prepare such datasets. It¡¯s simply not possible for most organizations to come up with them. Another obstacle is the high cost of GPUs needed to run advanced deep learning algorithms. Thankfully, we can use pre-trained state-of-the-art deep learning models and tweak them to work for us. This is known as transfer learning. It is not as resource intensive as training a deep learning model from scratch and produces decent results even on small amounts of training data. This concept will be expanded upon later in the article when we implement our learning on quite a small dataset. Pre-trained models help data scientists start off on a new problem by providing an existing framework they can leverage. You don¡¯t always have to build a model from scratch, especially when someone else has already put in their hard work and effort! And these pre-trained models have proven to be truly effective and useful in the field of computer vision (check out this article to see our pick of the top 10 pre-trained models in CV). Their success is popularly attributed to the Imagenet dataset. It has over 14 million labeled images with over 1 million images also accompanying bounding boxes. This dataset was first published in 2009 and has since become one of the most sought-after image datasets ever. It led to several breakthroughs in deep learning research for computer vision, with transfer learning being one of them. However, in NLP, transfer learning has not been as successful (as compared to computer vision, anyway). Of course we have pre-trained word embeddings like word2vec, GloVe, and fastText, but they are primarily used to initialize only the first layer of a neural network. The rest of the model still needs to be trained from scratch and it requires a huge number of examples to produce a good performance. What do we really need in this case? Like the aforementioned computer vision models, we require a pre-trained model for NLP which can be fine-tuned and used on different text datasets. One of the contenders for pre-trained natural language models is the Universal Language Model Fine-tuning for Text Classification, or ULMFiT (Imagenet dataset [cs.CL]). How does it work? How widespread are it¡¯s applications? How can we make it work in Python? In the rest of this article, we will put ULMFiT to the test by solving a text classification problem and check how well it performs. Proposed by fast.ai¡¯s Jeremy Howard and NUI Galway Insight Center¡¯s Sebastian Ruder, ULMFiT is essentially a method to enable transfer learning for any NLP task and achieve great results. All this, without having to train models from scratch. That got your attention, didn¡¯t it? ULMFiT achieves state-of-the-art result using novel techniques like: This method involves fine-tuning a pre-trained language model (LM), trained on the<U+00A0>Wikitext 103 dataset, to a new dataset in such a manner that it does not forget what it previously learned. Language modeling can be considered a counterpart of Imagenet for NLP. It captures general properties of a language and provides an enormous amount of data which can be fed to other downstream NLP tasks. That is why Language modeling has been chosen as the source task for ULMFiT. I highly encourage you to go through the original ULMFiT<U+00A0>paper<U+00A0>to understand more about how it works, the way Jeremy and Sebastian went about deriving it, and parse through other interesting details. Alright, enough theoretical concepts <U+2013> let¡¯s get our hands dirty by implementing ULMFiT on a dataset and see what the hype is all about. Our objective here is to fine-tune a pre-trained model and use it for text classification on a new dataset. We will implement ULMFiT in this process. The interesting thing here is that this new data is quite small in size (<1000 labeled instances). A neural network model trained from scratch would overfit on such a small dataset. Hence, I would like to see whether ULMFiT does a great job at this task as promised in the paper. Dataset: We will use the 20 Newsgroup dataset available in sklearn.datasets. As the name suggests, it includes text documents from 20 different newsgroups. We will perform the python implementation on Google<U+00A0>Colab instead of our local machines. If you have never worked on colab before, then consider this a bonus! Colab, or Google Colaboratory, is a free cloud service for running Python. One of the best things about it is that it provides GPUs and TPUs for free and hence, it is pretty handy for training deep learning models. So, it doesn¡¯t matter even if you have a system with pretty ordinary hardware specs <U+2013> as long as you have a steady internet connection, you are good to go. The only other requirement is that you must have a Google account. Let¡¯s get started! First, sign in to your Google account. Then select ¡®NEW PYTHON 3 NOTEBOOK¡¯. This notebook is similar to your typical Jupyter Notebook, so you won¡¯t have much trouble working on it if you are familiar with the Jupyter environment. A Colab notebook looks something like the screenshot below: Then go to Runtime, select Change runtime type, then select GPU as the hardware accelerator to utilise GPU for free. Most of the popular libraries like pandas, numpy, matplotlib, nltk, and<U+00A0>keras, come preinstalled with Colab. However, 2 libraries, PyTorch and fastai v1 (which we need in this exercise), will need to be installed manually. So, let¡¯s load them into our Colab environment: Import the dataset which we downloaded earlier. Let¡¯s create a dataframe consisting of the text documents and their corresponding labels (newsgroup names). (11314, 2) We¡¯ll convert this into a binary classification problem by selecting only 2 out of the 20 labels present in the dataset. We will select labels 1 and 10 which correspond to ¡®comp.graphics¡¯ and ¡®rec.sport.hockey¡¯, respectively. Let¡¯s have a quick look at the target distribution. The distribution looks pretty even. Accuracy would be a good evaluation metric to use in this case. It¡¯s always a good practice to feed clean data to your models, especially when the data comes in the form of unstructured text. Let¡¯s clean our text by retaining only alphabets and removing everything else. Now, we will get rid of the stopwords from our text data. If you have never used stopwords before, then you will have to download them from the nltk package as I¡¯ve shown below: Now let¡¯s split our cleaned dataset into training and validation sets in a 60:40 ratio. Perfect! Before proceeding further, we¡¯ll need to prepare our data for the language model and for the classification model separately. The good news? This can be done quite easily using the fastai library: We can use the data_lm object we created earlier to fine-tune a pre-trained language model. We can create a learner object, ¡®learn¡¯, that will directly create a model, download the pre-trained weights, and be ready for fine-tuning: The one cycle and cyclic momentum allows the model to be trained on higher learning rates and converge faster. The one cycle policy provides some form of regularisation. We won¡¯t go into the depth of how this works as this article is about learning the implementation. However, if you wish to know more about one cycle policy, then feel free to refer to this excellent paper by Leslie Smith <U+2013> ¡°A disciplined approach to neural network hyper-parameters: Part 1 <U+2014> learning rate, batch size, momentum, and weight decay¡±. Total time: 00:09 We will save this encoder to use it for classification later. Let¡¯s now use the data_clas object we created earlier to build a classifier with our fine-tuned encoder. We will again try to fit our model. Total time: 00:32 Wow! We got a whopping increase in the accuracy and even the validation loss is far less than the training loss. It is a pretty outstanding performance on a small dataset. You can even get the predictions for the validation set out of the learner object by using the below code: With the emergence of methods like ULMFiT, we are moving towards more generalizable NLP systems. These models would be able to perform multiple tasks at once. Moreover, these models would not be limited just to the English language, but to several other languages spoken across the globe. We also have upcoming techniques like ELMo, a new word embedding technique, and BERT, a new language representation model designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. These techniques have already achieved state-of-the-art results on many NLP tasks. Hence, the golden period for NLP has just arrived and it is here to stay. I hope you found this article helpful. However, there are still a lot more things to explore in ULMFiT using the fastai library which I encourage you guys to go after. If you have any recommendations/suggestions, then feel free to let me know in the comments section below. Also, try to use ULMFiT on different problems and domains of your choice and see how the results pan out. Code: You can find the complete code here. Thanks for reading and happy learning! Nice article. Thanks for sharing. really such a nice article Thanks Ashwin! Nice tutorial. I just walked through it, but I wondered why you removed stop words? I think there is a belief in NLP that it¡¯s always good to remove stop words, but this is often not true. I tried re-running the tutorial but skipped the remove stop words part and I got a 2.4% increase in accuracy. I thought you might want to try that and see if you see the same increase. I¡¯m Glad you liked this tutorial. Yes, you are right that removing stop words does not always help. However, it might work for another dataset and that is why I have included it in this article. Hey, I was able to run succesfully on Google collab. But I am not able to run same code with required library installed on my local machine. It gives following error for line below
` learn = language_model_learner(data_lm, pretrained_model=URLs.WT103, drop_mult=0.7)` `
Traceback (most recent call last):
File ¡°transfer_learning_classification_nlp_rir_classification.py¡±, line 114, in
learn = language_model_learner(data_lm, pretrained_model=URLs.WT103, drop_mult=0.7)
File ¡°/home/abhay/mml/venv-nerapi/lib/python3.6/site-packages/fastai/text/learner.py¡±, line 135, in language_model_learner
model_path = untar_data(pretrained_model, data=False)
File ¡°/home/abhay/mml/venv-nerapi/lib/python3.6/site-packages/fastai/datasets.py¡±, line 108, in untar_data
tarfile.open(fname, ¡®r:gz¡¯).extractall(dest.parent)
File ¡°/home/abhay/mml/venv-nerapi/lib/python3.6/tarfile.py¡±, line 1587, in open
return func(name, filemode, fileobj, **kwargs)
File ¡°/home/abhay/mml/venv-nerapi/lib/python3.6/tarfile.py¡±, line 1641, in gzopen
t = cls.taropen(name, mode, fileobj, **kwargs)
File ¡°/home/abhay/mml/venv-nerapi/lib/python3.6/tarfile.py¡±, line 1617, in taropen
return cls(name, mode, fileobj, **kwargs)
File ¡°/home/abhay/mml/venv-nerapi/lib/python3.6/tarfile.py¡±, line 1480, in __init__
self.firstmember = self.next()
File ¡°/home/abhay/mml/venv-nerapi/lib/python3.6/tarfile.py¡±, line 2310, in next
raise ReadError(¡°empty file¡±)
tarfile.ReadError: empty file","Keyword(freq): model(13), dataset(6), task(5), result(4), technique(4), kwarg(3), label(3), thank(3), application(2), document(2)"
"9","vidhya",2018-11-28,"Highlights from DataHack Summit 2018 <U+2013> a Truly Overwhelming and Resounding Success!","https://www.analyticsvidhya.com/blog/2018/11/highlights-from-datahack-summit-2018-a-truly-overwhelming-and-resounding-success/","What do you do when you have to improve upon the best? When you need to deliver what has never been done before? And you need to deliver it at a scale which differentiates itself because of the scale. These are some questions we had when we started thinking about DataHack Summit 2018. We promised you an experience like never before <U+2013> where we would bring together people, machines, and their collaborative experience. A chance to see artificial intelligence in a way no other conference in India has even shown. With more than 1,200 attendees from various diverse industries and domains (more than 400 organizations), DataHack Summit 2018 was an unqualified success. This year¡¯s conference<U+00A0>was even bigger than last year, from the speaker line up and the enriching content, to the massive sprawling venue. There were more than 50+ power talks and hack sessions from the best industry leaders, thought leaders, practitioners, data scientists, and folks from all sorts of data related roles. The venue, NIMHANS Convention Centre, was bigger and better than last year, with three massive auditoriums, all jam packed with data science professionals eager to learn from the best in the business. We would like to thank all our sponsors for making DataHack Summit an unparalleled success. And this was just about the first 2 days! 9 stimulating workshops (yes, 9!) were conducted on topics ranging from Applied Machine Learning to Computer Vision using PyTorch and we received an overwhelmingly positive response on them. Our aim of curating and delivering only the best data science knowledge to our community all culminated and reflected in our content at DataHack Summit 2018. But enough talk <U+2013> here are a few awesome highlights from the blockbuster conference! Kunal Jain, the man behind Analytics Vidhya, kicked things off on Day 1 as he set the tone for entire conference with a superbly eloquent opening speech. He spoke about the significance of DataHack Summit and what lay in store for all the attendees. Kunal also spoke about the importance of ethics in AI later in the day, a very relevant and timely talk on a sensitive subject. Ronald van Loon, Director at Adversitement and a well-respected thought leader, was the keynote speaker on day 1 of DataHack Summit 2018. He spoke about the future of data in the digital enterprise and kept the audience enraptured throughout his 60-minute talk. Wondering how to get your data science career started? Then this panel discussion was the place to be! Featuring Ronald van Loon, Dr. Sarabjot Singh Anand, Rohit Pandharkar, Charanpreet Singh, and moderated by Kunal Jain, a range of topics were pondered upon, including how to make a career switch from an entirely different domain. Throughout the conference, auditorium 3 saw the most love from the community. EVERY SINGLE talk and hack session was jam-packed as the audience flocked to audi 3 <U+2013> as you can see in the image above! We have such a wonderful community that is so eager to learn new things, and that¡¯s what keeps us working so hard to make DataHack Summit a fulfilling experience for everyone. Recognize this person? Of course you do <U+2013> it¡¯s none other than Tarry Singh! He was one of the most sought-after speakers at the conference and he brought his relentless work ethic and supreme energy to the stage. Tarry was the keynote speaker on day 2, as he spoke about the different nuances of deep learning. He was also part of the panel discussion on GANs and conducted a very successful workshop for CxO¡¯s on how to make the leap from a business executive to an AI leader. Reinforcement Learning was a prevalent topic throughout the summit with eminent personalities like Professor Balaraman Ravindran and Xander Steenbrugge lending their voice to this crucial subject. In fact, Xander even took a live hack session on RL, showing how you can build your own intelligent agent that can play ATARI games! The workshops we hosted just added to the uniqueness of DataHack Summit 2018. 9 interactive and fully sold out workshops were held on the following topics: Business Executive to AI leader <U+2013> A CXO¡¯s Invite-Only Roundtable Hands-On Workshop Giving our community the chance to network with fellow professionals is something DataHack Summit excels at <U+2013> and this year was no different. There were plenty of chances to connect with thought leaders, industry veterans and even intermediate level folks <U+2013> whether it was during lunch, tea, or in-between sessions. We are proud to offer our community the chance to enhance their careers. There were also a number of interactive booths at the venue, set up by Intel, IBM, H2O.ai, Praxis, and Great Learning. Attendees had a chance to interact with them and find out more about their offerings. Check out this interaction between a young data scientist and a robot <U+2013> truly creating a place WHERE HUMANS MEET ARTIFICIAL INTELLIGENCE The Startup Showcase track, introduced for the first time, was a massive success with Rice Inc., Empower Energy, and woroxogo brandishing their awesome machine learning-powered services.","Keyword(freq): attendee(3), leader(3), topic(3), workshop(3), folk(2), professional(2), session(2), analytics(1), auditorium(1), career(1)"
