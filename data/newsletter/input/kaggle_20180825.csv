"site","date","headline","url_address","text"
"kaggle",2018-08-23,"Data Notes: Drought and the War in Syria","http://blog.kaggle.com/2018/08/23/data-notes-drought-and-the-war-in-syria/","ARIMA, Syria, and mapping The Cure: Enjoy these new, intriguing, and overlooked datasets and kernels. 1.<U+00A0><U+0001F4A6> Did Drought Cause the War in Syria? (link) 2.<U+0001F4C8> Time Series for Beginners with ARIMA (link) 3.<U+0001F914> Understand ARIMA and Tune P, D, Q (link) 4.<U+0001F4B5> A Hitchhiker's Guide to Lending Club Loan Data (link) 5.<U+0001F981> Yellowbrick <U+2014> Regression Visualizer Examples (link) 6.<U+0001F489> (Bio)statistics in R: Part #3 (link) 7.<U+0001F918> EDA - The Cure Discography (link) 8.<U+0001F9E0> Dataset: Example Brain Mapping Data (link) 9.<U+0001F924> Dataset: Face Dataset with Age, Emotion, Ethnicity (link) 10.<U+26BD> Dataset: FIFA World Cup 2018 Tweets (link) Interested in exploring machine learning packages? Try H2O or fastAI!  Copyright ¨Ï 2018 Kaggle, All rights reserved. 
  

 These newsletters are really helpful Paul! Nice work!"
"kaggle",2018-08-22,"Getting Started with Competitions - A Peer to Peer Guide","http://blog.kaggle.com/2018/08/22/machine-learning-kaggle-competition-part-one-getting-started/","Originally published: Towards Data Science<U+00A0>by<U+00A0>William Koehrsen. In the field of data science, there are almost too many resources available: from Datacamp to Udacity to KDnuggets, there are thousands of places online to learn about data science. However, if you are someone who likes to jump in and learn by doing, Kaggle might be the single best location for expanding your skills through hands-on data science projects. While it originally was known as a place for<U+00A0>machine learning competitions,<U+00A0>Kaggle<U+200A><U+2014><U+200A>which bills itself as ¡°Your Home for Data Science¡±<U+200A><U+2014><U+200A>now offers an array of<U+00A0>data science resources. Although this series of articles will focus on a competition, it¡¯s worth pointing out the main aspects of Kaggle: Overall, Kaggle is a great place to learn, whether that¡¯s through the more traditional learning tracks or by competing in competitions. When I want to find out about the latest machine learning method, I could go read a book, or, I could go on Kaggle, find a competition, and see how people use it in practice. Personally, I find this much more enjoyable and a more effective teaching method. Moreover, the community is extremely supportive and always willing to answer questions or provide feedback on a project. In this article, we¡¯ll focus on getting started with a Kaggle machine learning competition: the<U+00A0>Home Credit Default Risk problem. This is a fairly straightforward competition with a reasonable sized dataset (which can¡¯t be<U+00A0>said for all of the competitions) which means we can compete entirely using Kaggle¡¯s kernels. This significantly lowers the barrier to entry because you don¡¯t have to worry about any software on your computer and you don¡¯t even have to download the data! As long as you have a Kaggle account and an Internet connection, you can connect to a kernel and run the code. I plan to do the entire competition on Kaggle and the kernel (a Python Jupyter Notebook) for this post is<U+00A0>available here. To get the most from this article, copy the kernel by creating a Kaggle account, then hitting the blue Fork Notebook button. This will open up the notebook for editing and running in the kernel environment. The<U+00A0>Home Credit Default Risk competition<U+00A0>is a standard supervised machine learning task where the goal is to use historical loan application data to predict whether or not an applicant will repay a loan. During training, we provide our model with the features<U+200A><U+2014><U+200A>the variables describing a loan application<U+200A><U+2014><U+200A>and the label<U+200A><U+2014><U+200A>a binary 0 if the loan was repaid and a 1 if the loan was not repaid<U+200A><U+2014><U+200A>and the model learns a mapping from the features to the label. Then, during testing, we feed the model the features for a new batch of applications and ask it to predict the label. All the data for this competition is structured meaning it exists in neat rows and columns<U+200A><U+2014><U+200A>think a spreadsheet. This means we won¡¯t need to use any<U+00A0>convolutional neural networks<U+00A0>(which excel at processing image data ) and it will give us great practice on a real-world dataset. Home Credit, the host of the competition, is a finance provider that focuses on serving the unbanked population. Predicting whether or not an application will repay a loan is a vital business need, and Home Credit has developed this competition in the hopes that the Kaggle community can develop an effective algorithm for this task. This competition follows the general idea of most Kaggle competitions: a company has data and a problem to solve, and rather than (or in addition to) hiring internal data scientists to build a model, they put up a modest prize to entice the entire world to contribute solutions. A community of thousands of skilled data scientists (Kagglers) then work on the problem for basically no charge to come up with the best solution. As far as cost effective business plans go, this seems like a brilliant idea! When you go to the competition homepage, you¡¯ll see this: Here¡¯s a quick run through of the tabs Although they are called competitions, Kaggle machine learning events should really be termed ¡°collaborative projects¡± because the main goal is not necessarily to win but to practice and learn from fellow data scientists. Once you realize that it¡¯s not so much about beating others but about expanding your own skills, you will get a lot more out of the competitions. When you sign up for Kaggle, you not only get all the resources<U+00A0>, you also get to be part of a community of data scientists with thousands of years of collective experience. Take advantage of all that experience by trying to be an active part of the community! That means anything from sharing a kernel to asking a question in a discussion forum. While it can be intimidating to make your work public, we learn best by making mistakes, receiving feedback, and improving so we don¡¯t make the same mistake again. Everyone starts out a beginner, and the community is very supportive of data scientists of all skill levels. In that mindset, I want to emphasize that discussion with others and building on others¡¯ code is not only acceptable, but encouraged! In school, working with others is called cheating and gets you a zero, but in the real world, it¡¯s called collaboration and an extremely important skill. A great method for throwing yourself into a competition is to find a kernel someone has shared with a good leaderboard score, fork the kernel, edit it to try and improve the score, and then run it to see the results. Then, make the kernel public so others can use your work.<U+00A0>Data scientists stand not on the shoulders of giants, but on the backs of thousands of individuals who have made their work public for the benefit of all. (Sorry for getting philosophical, but this is why I love data science!) Once you have a basic understanding of how Kaggle works and the philosophy of how to get the most out of a competition, it¡¯s time to get started. Here, I¡¯ll briefly outline a<U+00A0>Python Jupyter Notebook I put together in a kernelfor the Home Credit Default Risk problem, but to get the full benefit, you¡¯ll want to fork the notebook on Kaggle and run it yourself (you don¡¯t have to download or set-up anything so I¡¯d highly encourage checking it out). When you open the notebook in a kernel, you¡¯ll see this environment: Think of this as a standard Jupyter Notebook with slightly different aesthetics. You can write Python code and text (using Markdown syntax) just like in Jupyter and run the code completely in the cloud on Kaggle¡¯s servers. However, Kaggle kernels have some unique features not available in Jupyter Notebook. Hit the leftward facing arrow in the upper right to expand the kernel control panel which brings up three tabs (if the notebook is not in fullscreen, then these three tabs may already be visible next to the code). In the data tab, we can view the datasets to which our Kernel is connected. In this case, we have the entire competition data, but we can also connect to any other dataset on Kaggle or upload our own data and access it in the kernel. Data files are available in the<U+00A0>../input/<U+00A0>directory from within the code: The Settings tab lets us control different technical aspects of the kernel. Here we can add a GPU to our session, change the visibility, and install any Python package which is not already in the environment. Finally, the Versions tab lets us see any previous committed runs of the code. We can view changes to the code, look at log files of a run, see the notebook generated by a run, and download the files that are output from a run. To run the entire notebook and record a new Version, hit the blue Commit & Run button in the upper right of the kernel. This executes all the code, shows us the completed notebook (or any errors if there are mistakes), and saves any files that are created during the run. When we commit the notebook, we can then access any predictions our models made and submit them for scoring. The<U+00A0>first notebook<U+00A0>is meant to get you familiar with the problem. We start off much the same way as any data science problem: understanding the data and the task. For this problem, there is 1 main training data file (with the labels included), 1 main testing data file, and 6 additional data files. In this first notebook, we use only the main data, which will get us a decent score, but later work will have to incorporate all the data in order to be competitive. To understand the data, it¡¯s best to take a couple minutes away from the keyboard and read through the problem documentation, such as the<U+00A0>column descriptions of each data file. Because there are multiple files, we need to know how they are all linked together, although for this first notebook we only use the main file to keep things simple. Reading through other kernels can also help us get familiar with the data and which variables are important. Once we understand the data and the problem, we can start structuring it for a machine learning task This means dealing with categorical variables (through one-hot encoding), filling in the missing values (imputation), and scaling the variables to a range. We can do exploratory data analysis, such as finding correlations with the label, and graphing these relationships. We can use these relationships later on for modeling decisions, such as including which variables to use. (See the notebook for implementation). Of course, no exploratory data analysis is complete without my favorite plot, the<U+00A0>Pairs Plot. After thoroughly exploring the data and making sure it¡¯s acceptable for machine learning, we move on to creating baseline models. However, before we quite get to the modeling stage, it¡¯s critical we understand the performance metric for the competition. In a Kaggle competition, it all comes down to a single number, the metric on the test data. While it might make intuitive sense to use accuracy for a binary classification task,<U+00A0>that is a poor choice<U+00A0>because we are dealing with an imbalanced class problem. Instead of accuracy, submissions are judged in terms of ROC AUC or<U+00A0>Receiver Operating Characteristic curve Area Under the Curve. I¡¯ll let you do the<U+00A0>research on this one, or read the explanation in the notebook. Just know that higher is better, with a random model scoring 0.5 and a perfect model scoring 1.0. To calculate a ROC AUC, we need to make predictions in terms of probabilities rather than a binary 0 or 1. The ROC<U+00A0>then shows the True Positive Rate versus the False Positive Rate<U+00A0>as a function of the threshold according to which we classify an instance as positive. Usually we like to make a naive baseline prediction, but in this case, we already know that random guessing on the task would get an ROC AUC of 0.5. Therefore, for our baseline model, we will use a slightly more sophisticated method,<U+00A0>Logistic Regression. This is a popular simple algorithm for binary classification problems and it will set a low bar for future models to surpass. After implementing the logistic regression, we can save the results to a csv file for submission. When the notebook is committed, any results we write will show up in the Output sub-tab on the Versions tab: From this tab, we can download the submissions to our computer and then upload them to the competition. In this notebook, we make four different models with scores as follows: These scores don¡¯t get us anywhere close to the top of the leaderboard, but they leave room for plenty of future improvement! We also get an idea of the performance we can expect using only a single source of data. (Not surprisingly, the extraordinary<U+00A0>Gradient Boosting Machine<U+00A0>(using the<U+00A0>LightGBM library) performs the best. This<U+00A0>model wins nearly every structured Kaggle competition<U+00A0>(where the data is in table format) and we will likely need to use some form of this model if we want to seriously compete!) This article and introductory kernel demonstrated a basic start to a Kaggle competition. It¡¯s not meant to win, but rather to show you the basics of how to approach a machine learning competition and also a few models to get you off the ground (although the LightGBM model is like jumping off the deep end). Furthermore, I laid out my philosophy for machine learning competitions, which is to learn as much as possible by taking part in discussions, building on other¡¯s code, and sharing your own work. It¡¯s enjoyable to best your past scores, but I view doing well not as the main focus but as a positive side effect of learning new data science techniques. While these are known as competitions, they are really collaborative projects where everyone is welcome to participate and hone their abilities. There remains a ton of work to be done, but thankfully we don¡¯t have to do it alone. In later articles and notebooks we¡¯ll see how to build on the work of others to make even better models. I hope this article (and the<U+00A0>notebook kernel) has given you the confidence to start competing on Kaggle or taking on any data science project. As always, I welcome constructive criticism and discussion and can be reached on Twitter<U+00A0>@koehrsen_will. What a great article.. Inspired me to try. Thanks a lot.. A lot of labor has gone into writing this highly systemic article. Thank you for your patience. Google will down grade your website accordingly. The burglar instinctively got up and started to
run but couldn't see because of the tearing wartrol caused.
Trying vehicles video online strategy? https://Www.argo-shop.com.ua/gotourl.php?url=http://win88.today/download-play8oy-android-ios/ For newest information you have to go to see internet and on the web I found this web
page as a finest web page for most recent updates."
