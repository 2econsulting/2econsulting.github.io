"site","date","headline","url_address","text"
"vidhya",2018-08-24,"The Ultimate Data Science and Machine Learning Blogathon <U+2013> More than $2500 up for grabs!","https://www.analyticsvidhya.com/blog/2018/08/data-science-machine-learning-blogathon-lucrative-prizes-bonus/","

DATA DAYS DISCOUNT: Get FLAT 33% OFF on All Courses | Use COUPON CODE: DATA33 |
OFFER ENDS: 26th Aug 23:59 hrs IST | 
Valid for first 33 users(daily) | Buy Now 

 ¡°If you want to change the world, pick up a pen and write.¡±<U+200A><U+2014><U+200A>Martin Luther We are delighted to announce the launch of<U+00A0>Analytics Vidhya¡¯s Blogathon, the ultimate competition which combines your writing prowess with your machine learning skills! Analytics Vidhya has always been at the forefront of knowledge sharing, and we want to continue this trend among our community members. We will not only publish the best articles on Analytics Vidhya¡¯s Medium page, but also provide feedback to every writer on their article. Along with recognition in front of a broad audience, there are lucrative prizes to be won! The focus of the articles can be on any data related field<U+200A><U+2014><U+200A>data science, machine learning, deep learning, Artificial Intelligence, Business Analytics, etc. The blogathon starts today, 25th August, and will conclude on 23rd September. Beyond this, you will be given an extra week to work on any feedback we have provided on an already submitted article. We will announce the winners in each category on 30th September. To enter the competition, you need to<U+00A0>fill in your details here. We will then add you as a writer to our Medium publication and you can start sending your drafts to us. It really is that simple! Every article which meets Analytics Vidhya¡¯s standards will get published on our Medium page.<U+00A0>There are $2500 + bonuses to be won!<U+00A0>Prizes will be given in 3 categories: Most popular article:<U+00A0>This will be decided by the number of unique fans the article gets. Most number of articles with more than 10 unique fans:<U+00A0>As the name suggests, the more articles you submit and the more fans you gather, the better your chance of grabbing this prize! Editor¡¯s Prize:<U+00A0>This category will be judged by AV¡¯s in-house editing team and the winner will receive $500! There¡¯s more.. The top 25 bloggers will each receive free access for 6 months to AV¡¯s ¡®Computer Vision using Deep Learning¡¯ course!<U+00A0>It is a one-of-a-kind course that will introduce you to the world of CV and ensure you come out a master of the field. The top 50 participants will get access to DataHack 2017<U+00A0>day 1<U+00A0>and<U+00A0>day 2<U+00A0>talks<U+2014> a collection of all the sessions that happened in India¡¯s premier analytics conference last year. Hear from business leaders, domain experts, senior data scientists and other eminent personalities in the analytics domain. An unmissable opportunity! For those articles which do not meet our standards, we will ensure you are provided with feedback on how to make those articles better in the future."
"vidhya",2018-08-22,"A Hands-On Guide to Automated Feature Engineering using Featuretools in Python","https://www.analyticsvidhya.com/blog/2018/08/guide-automated-feature-engineering-featuretools-python/","

DATA DAYS DISCOUNT: Get FLAT 33% OFF on All Courses | Use COUPON CODE: DATA33 |
OFFER ENDS: 26th Aug 23:59 hrs IST | 
Valid for first 33 users(daily) | Buy Now 

 Anyone who has participated in machine learning hackathons and competitions can attest to how crucial feature engineering can be. It is often the difference between getting into the top 10 of the leaderboard and finishing outside the top 50! I have been a huge advocate of feature engineering ever since I realized it¡¯s immense potential. But it can be a slow and arduous process when done manually. I have to spend time brainstorming over what features to come up, and analyze their usability them from different angles. Now, this entire FE process can be automated and I¡¯m going to show you how in this article. <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> <U+00A0> Source: VentureBeat We will be using the Python feature engineering library called Featuretools to do this. But before we get into that, we will first look at the basic building blocks of FE, understand them with intuitive examples, and then finally dive into the awesome world of automated feature engineering using the BigMart Sales dataset. In the context of machine learning, a feature can be described as a characteristic, or a set of characteristics, that explains the occurrence of a phenomenon. When these characteristics are converted into some measurable form, they are called features. For example, assume you have a list of students. This list contains the name of each student, number of hours they studied, their IQ, and their total marks in the previous examinations. Now you are given information about a new student<U+2014> the number of hours he/she studied and his IQ, but his/her marks are missing. You have to estimate his/her probable marks. Here, you¡¯d use IQ and study_hours to build a predictive model to estimate these missing marks. So, IQ and study_hours are called the features for this model. Feature Engineering can simply be defined as the process of creating new features from the existing features in a dataset. Let¡¯s consider a sample data that has details about a few items, such as their weight and price. Now, to create a new feature we can use Item_Weight and Item_Price. So, let¡¯s create a feature called Price_per_Weight. It is nothing but the price of the item divided by the weight of the item. This process is called feature engineering. This was just a simple example to create a new feature from existing ones, but in practice, when we have quite a lot of features, feature engineering can become quite complex and cumbersome. Let¡¯s take another example. In the popular Titanic dataset, there is a passenger name feature and below are some of the names in the dataset: These names can actually be broken down<U+00A0>into additional meaningful features. For example, we can extract and group similar titles into single categories. Let¡¯s have a look at the unique number of titles in the passenger names. It turns out that titles like<U+00A0>¡®Dona¡¯, ¡®Lady¡¯, ¡®the Countess¡¯, ¡®Capt¡¯, ¡®Col¡¯, ¡®Don¡¯, ¡®Dr¡¯, ¡®Major¡¯, ¡®Rev¡¯, ¡®Sir¡¯, and ¡®Jonkheer¡¯ are quite rare and can be put under a single label. Let¡¯s call it rare_title. Apart from this, the titles ¡®Mlle¡¯ and ¡®Ms¡¯ can be placed under ¡®Miss¡¯, and ¡®Mme¡¯ can be replaced with ¡®Mrs¡¯. Hence, the new title feature would have only 5 unique values as shown below: So, this is how we can extract useful information with the help of feature engineering, even from features like passenger names which initially seemed fairly pointless. The performance of a predictive model is heavily dependent on the quality of the features in the dataset used to train that model. If you are able to create new features which help in providing more information to the model about the target variable, it¡¯s performance will go up. Hence, when we don¡¯t have enough quality features in our dataset, we have to lean on feature engineering. In one of the most popular Kaggle competitions, Bike Sharing Demand Prediction, the participants are asked to forecast the rental demand in Washington, D.C based on historical usage patterns in relation with weather, time and other data. As explained in this article, smart feature engineering was instrumental in securing a place in the top 5 percentile of the leaderboard. Some of the features created are given below: Creating such features is no cakewalk <U+2013> it takes a great deal of brainstorming and extensive data exploration. Not everyone is good at feature engineering because it is not something that you can learn by reading books or watching videos. This is why feature engineering is also called an art. If you are good at it, then you have a major edge over the competition. Quite like Roger Federer, the master of feature engineering when it comes to Tennis shots. Analyze the two images shown above. The left one shows a car being assembled by a group of men during early 20th century, and the right picture shows robots doing the same job in today¡¯s world. Automating any process has the potential to make it much more efficient and cost-effective. For similar reasons, feature engineering can, and has been, automated in machine learning. Building machine learning models can often be a painstaking and tedious process. It involves many steps so if we are able to automate a certain percentage of feature engineering tasks, then the data scientists or the domain experts can focus on other aspects of the model. Sounds too good to be true, right? Now that we have understood that automating feature engineering is the need of the hour, the next question to ask is <U+2013> how is it going to happen? Well, we have a great tool to address this issue and it¡¯s called Featuretools. Featuretools is an open source library for performing automated feature engineering. It is a great tool designed to fast-forward the feature generation process, thereby giving more time to focus on other aspects of machine learning model building. In other words, it makes your data ¡°machine learning ready¡±. Before taking Featuretools for a spin, there are three major components of the package that we should be aware of: a) An Entity<U+00A0>can be considered as a representation of a Pandas DataFrame. A collection of multiple entities is called an Entityset. b) Deep Feature Synthesis<U+00A0>(DFS) has got nothing to do with deep learning. Don¡¯t worry. DFS is actually a Feature Engineering method and is the backbone of Featuretools. It enables the creation of new features from single, as well as multiple dataframes. c) DFS create features by applying Feature primitives to the Entity-relationships in an EntitySet. These primitives are the often-used methods to generate features manually. For example, the primitive ¡°mean¡± would find the mean of a variable at an aggregated level. The best way to understand and become comfortable with Featuretools is by applying it on a dataset. So, we will use the dataset from our BigMart Sales practice problem<U+00A0>in the next section to solidify our concepts. The objective of the BigMart Sales challenge is to build a predictive model to estimate the sales of each product at a particular store. This would help the decision makers at BigMart to find out the properties of any product or store, which play a key role in increasing the overall sales. Note that there are 1559 products across 10 stores in the given dataset. The below table shows the features provided in our data: You can download the data from here. Featuretools is available for Python 2.7, 3.5, and 3.6. You can easily install Featuretools using pip. To start off, we¡¯ll just store the target Item_Outlet_Sales in a variable called sales and id variables in test_Item_Identifier and<U+00A0>test_Outlet_Identifier. Then we will combine the train and test set as it saves us the trouble of performing the same step(s) twice. Let¡¯s check the missing values in the dataset. Quite a lot of missing values in the Item_Weight and Outlet_size variables. Let¡¯s quickly deal with them: I will not do an extensive preprocessing operation since the objective of this article is to get you started with Featuretools. It seems Item_Fat_Content contains only two categories, i.e., ¡°Low Fat¡± and ¡°Regular¡± <U+2013> the rest of them we will consider redundant. So, let¡¯s convert it into a binary variable. Now we can start using Featuretools to perform automated feature engineering! It is necessary to have a unique identifier feature in the dataset (our dataset doesn¡¯t have any right now). So, we will create one unique ID for our combined dataset. If you notice, we have two IDs in our data<U+2014>one for the item and another for the outlet. So, simply concatenating both will give us a unique ID. Please note that I have dropped the feature Item_Identifier as it is no longer required. However, I have retained the feature<U+00A0>Outlet_Identifier because I plan to use it later. Now before proceeding, we will have to create an EntitySet. An EntitySet is a structure that contains multiple dataframes and relationships between them. So, let¡¯s create an EntitySet and add the dataframe combination to it. Our data contains information at two levels<U+2014>item level and outlet level. Featuretools offers a functionality to split a dataset into multiple tables. We have created a new table ¡®outlet¡¯ from the BigMart table based on the outlet ID Outlet_Identifier. Let¡¯s check the summary of our EntitySet. As you can see above, it contains two entities <U+2013> bigmart and outlet. There is also a relationship formed between the two tables, connected by Outlet_Identifier. This relationship will play a key role in the generation of new features. Now we will use Deep Feature Synthesis to create new features automatically. Recall that DFS uses Feature Primitives to create features using multiple tables present in the EntitySet. target_entity is nothing but the entity ID for which we wish to create new features (in this case, it is the entity ¡®bigmart¡¯). The parameter max_depth<U+00A0>controls the complexity of the features being generated by stacking the primitives. The parameter n_jobs helps in parallel feature computation by using multiple cores. That¡¯s all you have to do with Featuretools. It has generated a bunch of new features on its own. Let¡¯s have a look at these newly created features. DFS has created 29 new features in such a quick time. It is phenomenal as it would have taken much longer to do it manually. If you have datasets with multiple interrelated tables, Featuretools would still work. In that case, you wouldn¡¯t have to normalize a table as multiple tables will already be available. Let¡¯s print the first few rows of feature_matrix. There is one issue with this dataframe <U+2013> it<U+00A0>is not sorted properly. We will have to sort it based on the id variable from the combi dataframe. Now the dataframe feature_matrix is in proper order. It is time to check how useful these generated features actually are. We will use them to build a model and predict Item_Outlet_Sales. Since our final data (feature_matrix) has many categorical features, I decided to use the CatBoost algorithm. It can use categorical features directly and is scalable in nature. You can refer to this article to read more about CatBoost. CatBoost requires all the categorical variables to be in the string format. So, we will convert the categorical variables in our data to string first: Let¡¯s split feature_matrix back into train and test sets. Split the train data into training and validation set to check the model¡¯s performance locally. Finally, we can now train our model. The evaluation metric we will use is RMSE (Root Mean Squared Error). 1091.244 The RMSE score on the validation set is ~1092.24. The same model got a score of 1155.12 on the public leaderboard. Without any feature engineering, the scores were ~1103 and ~1183 on the validation set and the public leaderboard, respectively. Hence, the features created by Featuretools<U+00A0>are not just random features, they are valuable and useful. Most importantly, the amount of time it saves in feature engineering is incredible. Making our data science solutions interpretable is a very important aspect of performing machine learning. Features generated by Featuretools can be easily explained even to a non-technical person because they are based on the primitives, which are easy to understand. For example, the features<U+00A0>outlet.SUM(bigmart.Item_Weight)<U+00A0>and outlet.STD(bigmart.Item_MRP)<U+00A0>mean outlet-level sum of weight of the items and standard deviation of the cost of the items, respectively."
"vidhya",2018-08-22,"A Practical Introduction to K-Nearest Neighbors Algorithm for Regression (with Python code)","https://www.analyticsvidhya.com/blog/2018/08/k-nearest-neighbor-introduction-regression-python/","

DATA DAYS DISCOUNT: Get FLAT 33% OFF on All Courses | Use COUPON CODE: DATA33 |
OFFER ENDS: 26th Aug 23:59 hrs IST | 
Valid for first 33 users(daily) | Buy Now 

 Out of all the machine learning algorithms I have come across, KNN has easily been the simplest to pick up. Despite it¡¯s simplicity, it has proven to be incredibly effective at certain tasks (as you will see in this article). And even better? It can be used for both classification and regression problems! It¡¯s far more popularly used for classification problems, however. I have seldom seen KNN being implemented on any regression task. My aim here is to illustrate and emphasize how KNN can be equally effective when the target variable is continuous in nature. In this article, we will first understand the intuition behind KNN algorithms, look at the different ways to calculate distances between points, and then finally implement the algorithm in Python on the Big Mart Sales dataset. Let¡¯s go! Let us start with a simple example. Consider the following table <U+2013> it consists of the height, age and weight (target) value for 10 people. As you can see, the weight value of ID11 is missing. We need to predict the weight of this person based on their height and age. Note: The data in this table does not represent actual values. It is merely used as an example to explain this concept. For a clearer understanding of this, below is the plot of height versus age from the above table: In the above graph, the y-axis represents the height of a person (in feet) and the x-axis represents the age (in years). The points are numbered according to the ID values. The yellow point (ID 11) is our test point. If I ask you to identify the weight of ID11 based on the plot, what would be your answer? You would likely say that since ID11 is closer to<U+00A0>points 5 and 1, so it must<U+00A0> have a weight similar to these IDs, probably between 72-77 kgs (weights of ID1 and ID5 from the table). That actually makes sense, but how do you think the algorithm predicts the values? We will find that out in this article. As we saw above, KNN can be used for both classification and regression problems. The algorithm uses ¡®feature similarity¡¯ to predict values of any new data points. This means that the new point is assigned a value based on how closely it resembles the points in the training set. From our example, we know that ID11 has height and age similar to ID1 and ID5, so the weight would also approximately be the same. Had it been a classification problem, we would have taken the mode as the final prediction. In this case, we have two values of weight <U+2013> 72 and 77. Any guesses how the final value will be calculated? The average of the values is taken to be the final prediction. Below is a stepwise explanation of the algorithm: 2. The closest k data points are selected (based on the distance). In this example, points 1, 5, 6 will be selected if value of k is 3. We will further explore the method to select the right value of k later in this article. 3. The average of these data points is the final prediction for the new point. Here, we have weight of ID11 = (77+72+60)/3 = 69.66 kg. In the next few sections we will discuss each of these three steps in detail. The first step is to calculate the distance between the new point and each training point. There are various methods for calculating this distance, of which the most commonly known methods are <U+2013> Euclidian, Manhattan (for continuous) and Hamming distance (for categorical). Once the distance of a new observation from the points in our training set has been measured, the next step is to pick the closest points. The number of points to be considered is defined by the value of k. The second step is to select the k value. This determines the number of neighbors we look at when we assign a value to any new observation. In our example, for a value k = 3, the closest points are ID1, ID5 and ID6. The prediction of weight for ID11 will be: For the value of k=5, the closest point will be ID1, ID4, ID5, ID6, ID10. The prediction for ID11 will be : We notice that based on the k value, the final result tends to change. Then how can we figure out the optimum value of k? Let us decide it based on the error calculation for our train and validation set (after all, minimizing the error is our final goal!). Have a look at the below graphs for training error and validation error for different values of k. For a very low value of k (suppose k=1), the model overfits on the training data, which leads to a high error rate on the validation set. On the other hand, for a high value of k, the model performs poorly on both train and validation set. If you observe closely, the validation error curve reaches a minima at a value of k = 9. This value of k is the optimum value of the model (it will vary for different datasets). This curve is known as an ¡®elbow curve¡® (because it has a shape like an elbow) and is usually used to determine the k value. You can also use the grid search technique to find the best k value. We will implement this in the next section. By now you must have a clear understanding of the algorithm. If you have any questions regarding the same, please use the comments section below and I will be happy to answer them. We will now go ahead and implement the algorithm on a dataset. I have used the Big Mart sales dataset to show the implementation and you can download it from this link. 1. Read the file 2. Impute missing values 3. Deal with categorical variables and drop the id columns 4. Create train and test set 5. Preprocessing <U+2013> Scaling the features 6. Let us have a look at the error rate for different k values Output : As we discussed, when we take k=1, we get a very high RMSE value. The RMSE value decreases as we increase the k value. At k= 7, the RMSE is approximately 1219.06, and shoots up on further increasing the k value. We can safely say that k=7 will give us the best result in this case. These are the predictions using our training dataset. Let us now predict the values for test dataset and make a submission. 7. Predictions on the test dataset On submitting this file, I get an RMSE of 1279.5159651297. 8. Implementing GridsearchCV<U+00A0> For deciding the value of k, plotting the elbow curve every time is be a cumbersome and tedious process. You can simply use gridsearch to find the best value. Output : In this article, we covered the workings of the KNN algorithm and its implementation in Python. It¡¯s one of the most basic, yet effective machine learning techniques. For KNN implementation in R, you can go through this article : kNN Algorithm using R. In this article, we used the KNN model directly from the sklearn library. You can also implement KNN from scratch (I recommend this!), which is covered in the this article: KNN simplified. If you think you know KNN well and have a solid grasp on the technique, test your skills in this MCQ quiz:<U+00A0>30 questions on kNN Algorithm. Good luck! Hi Aishwarya, your explanation on KNN is really helpful. I have a doubt though. KNN suffers from the dimensionality curse i.e. Euclidean distance is not helpful when subjected to high dimensions as it is equidistant for different vectors.
What was your viewpoint while using the KNN despite this fact ? Curious to know. Thank you. Hi,  KNN works well for dataset with less number of features and fails to perform well has the number of inputs increase. Certainly other algorithms would show a better performance in that case. With this article I have tried to introduce the algorithm and explain how it actually works (instead of simply using it as a black box). Hi. I have been following you now for a while. Love your post. I wish you cold provide a pdf format also, because it is hard to archive and read web posts when you are offline. Hi Osman, Really glad you liked the post. Although certain articles and cheat sheets are converted and shared as pdf, but not all articles are available in the format. Will certainly look into it and see if we can have an alternate. Hi thanks for the explanations. Can you explain the intuition behind categorical variable calculations. For example, assume data set has height,age,gender as independent variable and weight as dependent variable.
Now if an unseen data comes with male as gender, then how it works? how it predicts the weight? Hi, Excellent question!  Suppose we have gender as a feature, we would use hamming distance to find the closest point (We need to find the distance with each training point as discussed in the article). Let us take the first training point, if it has the gender male and my test point also has the gender male, then the distance D will be 0. Now we take the second training point, it it has gender female and the test point has gender male, the value of D will be 1. Simply put, we will consider the test point closer to first training point and predict the target value accordingly. cannot find the data on https://datahack.analyticsvidhya.com/contest/practice-problem-big-mart-sales-iii/ Hi Steffen,"
"vidhya",2018-08-20,"Launching Analytics Vidhya¡¯s Medium Publication and AV Editor¡¯s club!","https://www.analyticsvidhya.com/blog/2018/08/analytics-vidhyas-medium-publication-av-editors-club/","

DATA DAYS DISCOUNT: Get FLAT 33% OFF on All Courses | Use COUPON CODE: DATA33 |
OFFER ENDS: 26th Aug 23:59 hrs IST | 
Valid for first 33 users(daily) | Buy Now 

 Analytics Vidhya started as a blog in 2013 and has grown into a thriving community of data science professionals. We have evolved the platform based on our community needs and feedback. Today Analytics Vidhya offers Meetups, Webinars,<U+00A0>DataHack platform,<U+00A0>job portal,<U+00A0>training portal<U+00A0>and our flagship event DataHack Summit. Across these engagement points offered by Analytics Vidhya <U+2013> the most popular continues to be our blog. Our blog continues to play an anchor in the way we have built this community. Thousands of people have benefitted from the resource provided on our blog. We are planning to step up our efforts in this direction today. We are extremely excited to announce the launch of Now you might ask, why start a Medium publication? Simply because we believe that we can grow our community and its impact multi-fold by creating a channel where people can contribute in the same way they learnt from the platform. Medium offers seamless manner for a community run publication and we intend to run it that way. We want community members to come and share their knowledge so that everyone in the community benefits further. Here are a few things you can expect from our Medium publication: Excited? Here is what you need to do next As we launch this publication, we are thrilled to open the floor for budding and experienced writers who want to showcase their work to our broad and thriving community. Our aim has always been to spread data science knowledge and make it accessible to as many people as possible, around the globe. We have played an active role in this regard in the last 5 years, and continue to do so proudly. You bring the creativity and your passion for data science, and we now provide you with a platform to display it to the world. Some of the key benefits you will receive as a writer: Interested? Go ahead and fill this form for us.<U+00A0>You can read more about what benefits we offer, and what we expect from you, in this detailed article. Wait, there¡¯s more.. Yes, that¡¯s right <U+2013> there is an opportunity to work with us in the capacity of an Editor! We are looking for Editors and Content creators who can drive our blog and content platform to the next level. Are you passionate about data science journalism? Do you have an itch to try and experiment with the latest tools, techniques and frameworks as soon as they come out? Can you take complex data ideas and explain them in simple easy to understand content? Do you love to share your knowledge and skill with the rest of the universe?<U+00A0>Do you think your writing skills can enable and inspire the next generation of data scientists across the globe? If the answer to the above questions was yes, then you are<U+00A0>welcome to become the Founding member of AV¡¯s Editor Club."
"vidhya",2018-08-19,"DataHack Radio Episode #8: How Self-Driving Cars Work with Drive.ai¡¯s Brody Huval","https://www.analyticsvidhya.com/blog/2018/08/datahack-radio-episode-8-how-self-driving-cars-work-with-drive-ais-brody-huval/","

DATA DAYS DISCOUNT: Get FLAT 33% OFF on All Courses | Use COUPON CODE: DATA33 |
OFFER ENDS: 26th Aug 23:59 hrs IST | 
Valid for first 33 users(daily) | Buy Now 

 Self-driving cars are expected to rule the streets in the next few years. In fact, countries like the USA, China and Japan have already started using them in real-world situations! One of the leaders in this space is Andrew Ng backed Drive.ai, a self-driving car start-up based in California. So how do these autonomous cars work? How difficult is it making one from scratch? What kind of machine learning techniques are used? In this podcast, Drive.ai¡¯s co-founder Brody Huval sheds light on these questions put forward by Kunal, along with other really intriguing facets of autonomous vehicles. It¡¯s a podcast you better not miss! Check out all the key takeaways from this really cool podcast below. You can check our the research paper Brody has co-authored on highway driving and deep learning (explained in the podcast) here. Happy listening! You can subscribe to DataHack Radio and listen to this, and all previous episodes, on any of the below platforms: Brody completed his graduation in mechanical engineering from the University of Florida in 2010. Back then, AI had just started to become popular and enter the mainstream industry space. He became interested in the field and started applying to computer science programs in the United States. His application was accepted by Stanford in 2011 for their mechanical engineering program but Brody almost immediately switched to the computer science stream to work with Andrew Ng. His first couple of projects there were in deep learning and natural language processing (NLP). In total, he spent 4 years at Stanford doing various projects and research in machine learning and deep learning. Post that, in 2015, he co-founded Drive.ai along with fellow students from Andrew Ng¡¯s Stanford AI lab. Brody¡¯s interest in self-driving cars dates back almost 6 years ago to 2012. He and his fellow researchers worked on an autonomous driving project where they tried to replicate Google¡¯s work in this space. Google had used 16,000 CPU cores to create neural networks that watched YouTube videos all day in order to learn everything about autonomous cars. Brody¡¯s group decided to use 12 GPUs instead to replicate the power of those 16,000 CPUs. At the end of this project, the team was left with a cluster of approximately 64 machines (each with 4 GPUs). Brody and his team knew at this point that they wanted to work with tons of data, and eventually they landed on the idea of self-driving cars. They would use a camera-only approach, along with other hardware equipment to automatically annotate the data. In short, they were looking to solve a meaningful problem with tons of data, and that¡¯s how Drive.ai was born. Unsurprisingly, it took a significant amount of time to label the data! For lanes, they had a special GPS unit using which they could map a path of where the car was driving. Based on this route, the team could understand how far away the lanes were from the car. For dynamic obstacles like pedestrians, Brody set up a mechanical pipeline and that¡¯s when he faced the difficulties of annotating data manually. This process involved a lot of logistics and other overhead, something only folks who have set up a ML project from scratch will truly understand. The initial testing produced mixed results. Because they had collected a ton of highway driving data, the testing on highways went pretty well. But when it came to urban areas, the system did not perform as well because of a lack of proper data. One of the biggest weak points during their testing process was the huge number of false positives regarding gauging the side of the road. After trying out various camera based approaches, the Drive.ai team started exploring LIDAR and RADAR based solutions. They tested out different sensors with the aim of getting a much better and improved precision and recall for their system. Brody mentioned that he believes camera based approaches will definitely get better in urban areas with time. You need a lot of data to understand all the nuances of the various images the cameras collect, and thus it comes down to how much computational power you have. This section was a really insightful explanation of LIDAR, RADAR and camera sensors. If you¡¯re interested in self-driving cars, make sure you listen to this part especially carefully. Right now, Drive.ai has 7 fully functional self-driving cars on the streets in Texas which can service up to 10,000 people. They have been set up to try and solve the ¡®micro-transit problem¡¯, which means distances that are too far for walking but too close for driving. This is especially useful in Texas where it can get blisteringly hot during the summer months. Some of the challenges Brody and his team currently face are with perception and performance. When he mentions perception, it refers to understanding where certain objects are placed, and how the system goes around these dynamic objects given the uncertainty in predictions. One of the ways of dealing with constantly changing scenarios (like re-painting roads, or avoiding construction areas) is to be in touch with the local government and understanding well in advance what changes are expected to happen. Another option Drive.ai have explored is tele-operators, who operate through networks to guide the car using different routes. Rain is also a major problem for LIDAR units, while cameras are not at their best during the night. These are acknowledged weaknesses that AI has not been able to fully solve so far. Currently Drive.ai¡¯s cars operate during daylight and if there¡¯s inclement weather, the company pauses the service until it clears up. Simulators are also becoming a major part of any self-driving car setup. They help the team understand where certain things are going awry, or how long a certain scenario takes to run, etc. These are all machine learning problems. There are certain aspects where classic machine learning or deep learning techniques fit. For example, deep learning algorithms work really well in the perception facet (like geospatial data) of these cars. The motion planning system, on the other hand, is a combination of classic ML and learned ML. Brody made a great point about how reinforcement learning, as powerful as it is, hasn¡¯t made as many advancements as deep learning. It¡¯s getting better and the Drive.ai team do experiment with it, but to use it in a real-world use case is not a feasible option right now."
