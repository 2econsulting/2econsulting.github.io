"site","date","headline","url_address","text"
"vidhya",2018-07-27,"Top 10 Pretrained Models to get you Started with Deep Learning (Part 1 <U+2013> Computer Vision)","https://www.analyticsvidhya.com/blog/2018/07/top-10-pretrained-models-get-started-deep-learning-part-1-computer-vision/","Pretrained models are a wonderful source of help for people looking to learn an algorithm or try out an existing framework. Due to time restrictions or computational restraints, it¡¯s not always possible to build a model from scratch which is why pretrained models exist! You can use a pretrained model as a benchmark to either improve the existing model, or test your own model against it. The potential and<U+00A0>possibilities are vast. Source: Facebook AI In this article, we will look at various pretrained models in Keras that have applications in computer vision. Why Keras? First, because I believe it¡¯s a great library for people starting out with neural networks. Second, I wanted to stick with one framework throughout the article. This will help you in moving from one model to the next without having to worry about frameworks. I encourage you to try each model out on your own machine, understand how it works, and how you can improve or tweak the internal parameters. We have segmented this topic into a series of articles. Part II will focus on Natural Language Processing (NLP) and Part III will look at Audio and Speech models. The aim is to get you up and running in these fields with existing solutions which will fast track your learning process. Object detection is one of the most common applications in the field of computer vision. It has applications in all walks of life, from self-driving cars to counting the number of people in a crowd. This section deals with pretrained models that can be used for detecting objects. You can also check out the below articles to get familiar with this topic: Mask R-CNN is a flexible framework developed for the purpose of object instance segmentation. This pretrained model is an implementation of this Mask R-CNN technique on Python and Keras. It generates bounding boxes and segmentation masks for each instance of an object in a given image (like the one shown above). This GitHub repository features a plethora of resources to get you started. It includes the source code of Mask R-CNN, the training code and pretrained weights for MS COCO, Jupyter notebooks to visualize each step of the detection pipeline, among other things. YOLO is an ultra popular object detection framework for deep learning applications. This repository contains implementations of YOLOv2 in Keras. While the developers have tested the framework on all sorts of object images <U+2013> like kangaroo detection, self-driving car, red blood cell detection, etc., they have released the pretrained model for raccoon detection. You can download the raccoon dataset here and get started with this pretrained model now! The dataset<U+00A0>consists of 200 images (160-training, 40-validation). You can download the pretrained weights for the entire model here. According to the developers, these weights can be used for an object detector for one class. As the name suggests, MobileNet is an architecture designed for mobile devices. It has been built by none other than Google. This particular model, which we have linked above, comes with pretrained weights on the popular ImageNet database (it¡¯s a database containing millions of images belonging to more than 20,000 classes). As you can see above, the applications of MobileNet are not just limited to object detection but span a variety of computer vision tasks <U+2013> like facial attributes, landmark recognition, finegrain classification, etc. If you were given a few hundred images of tomatoes, how would you classify them <U+2013> say defective/non-defective, or ripe/unripe? When it comes to deep learning, the go-to technique for this problem is image processing. In this classification problem, we have to<U+00A0>identify whether the tomato in the given image is grown or unripe using a pretrained Keras VGG16 model. The model was trained on 390 images of grown and unripe tomatoes from the ImageNet dataset and was tested on 18 different validation images of tomatoes.<U+00A0>The overall result on these validation images is given below: There are numerous ways of classifying a vehicle <U+2013> by it¡¯s body style, number of doors, open or closed roof, number of seats, etc. In this particular problem, we have to classify the images of cars into various classes. These classes include make, model, year, e.g. 2012 Tesla Model S. To develop this model, the car dataset from Stanford was used which contains<U+00A0>16,185 images of 196 classes of cars. The model was trained using pretrained VGG16, VGG19 and InceptionV3 models. The VGG network is characterized by its simplicity, using only 3¡¿3 convolutional layers stacked on top of each other in increasing depth. The 16 and 19 stand for the number of weight layers in the network. As the dataset is small, the simplest model, i.e. VGG16, was the most accurate. Training the VGG16 network gave an accuracy of 66.11% on the cross validation dataset.<U+00A0>More complex models like InceptionV3 were less accurate due to bias/variance issues. Facial recognition is all the rage in the deep learning community. More and more techniques and models are being developed at a remarkable pace to design facial recognition technology. Its applications span a wide range of tasks <U+2013> phone unlocking, crowd detection, sentiment analysis by analyzing the face, among other things. Face regeneration on the other hand, is the generation of a 3D modelled face from a closeup image of a face. The creation of 3D structured objects from mere two dimensional information is another thought out problem in the industry. The applications of face regeneration are vast in the film and gaming industry. Various CGI models can be automated thus saving tons of time and money in the process. This section of our article deals with pretrained models for these two domains. Creating a facial recognition model from scratch is a daunting task. You need to find, collect and then annotate a ton of images to have any hope of building a decent model. Hence using a pretrained model in this domain makes a lot of sense. VGG-Face is a<U+00A0>dataset that contains 2,622 unique identities with more than two million faces. This pretrained model<U+00A0>has been designed through the following method: This is a really cool implementation of deep learning. You can infer from the above image how this model works in order to reconstruct the facial features into a 3 dimensional space. This pretrained model was originally developed using Torch and then transferred to Keras. Semantic image segmentation is the task of assigning a semantic label to every single pixel in an image. These labels can be ¡°sky¡±, ¡°car¡±, ¡°road¡±, ¡°giraffe¡±, etc. What this technique does is it finds the outlines of objects and thus places restrictions on the accuracy requirements (this is what separates it from image level classification which has a much looser accuracy requirement). Deeplabv3 is Google¡¯s latest semantic image segmentation model. It was originally created using TensorFlow and has now been implemented using Keras. This GitHub repository also has code for how to get labels, how to use this pretrained model with custom number of classes, and of course how to trail your own model. This model attempts to address the problem of image segmentation of surgical instruments in a robot-assisted surgery scenario. The problem is further divided into two parts, which are as follows: This pretrained model is based on the U-Net network architecture and is further improved by using state-of-the-art semantic segmentation neural networks known as LinkNet and TernausNet. The model was trained on 8 ¡¿ 225-frame sequences of high resolution stereo camera images. Remember those games where you were given images and had to come up with captions? That¡¯s basically what image captioning is. It uses a combination of NLP and Computer Vision to produce the captions.<U+00A0>This task has been a challenging one for a long time as it requires huge datasets with unbiased images and scenarios. Given all these constraints, the algorithm must be generalized for any given image. A lot of businesses are leveraging this technique nowadays but how can you go about using it?<U+00A0>The solution lies in converting a given input image into a short and meaningful description. The encoder-decoder framework is widely used for this task. The image encoder is a convolutional neural network (CNN). This is a<U+00A0>VGG 16 pretrained model on the MS COCO dataset where the decoder is a long short-term memory (LSTM) network predicting the captions for the given image. For detailed explanation and walk through it¡¯s recommended that you follow up with our article on Automated Image Captioning."
"vidhya",2018-07-27,"Infographic <U+2013> 13 Common Mistakes Amateur Data Scientists Make and How to Avoid Them","https://www.analyticsvidhya.com/blog/2018/07/infographic-common-mistakes-amateur-data-scientists-make-how-avoid-them/","The path to becoming a data scientist is one riddled with traps and challenges. If you¡¯re not careful, you might just fall into one of them! There are so many resources out there which aim to help aspiring data scientists become experts, but most of them are half-baked efforts that leave a gaping hole in your data scientist journey. This infographic lists down 13 common mistakes we have seen fresher data scientists make, and we even porvide tips on how to avoid them. To get a more comprehensive overview of this infographic, and to get free resources on how to avoid each mistake, head over to our popular article <U+2013> 13 Mistakes Amateur Data Scientists Make and How to Avoid Them."
"vidhya",2018-07-24,"Top Highlights from AV¡¯s Record-Breaking Weekend Online Hackathon","https://www.analyticsvidhya.com/blog/2018/07/top-highlights-from-avs-record-breaking-weekend-online-hackathon/","What an eventful and intense hackathon weekend! Our recently concluded McKinsey Analytics Online Hackathon saw an overwhelming number of people participate and compete to win a NIPS conference ticket + $5,000, as well as interview opportunities with McKinsey Analytics. The entries kept on rolling as the weekend progressed, leading to a thrilling deadline finish. The hackathon saw over 5,000 data scientists and aspiring data science enthusiasts vying for a spot at the top of the leaderboard. As parameters were tweaked, and models were finalized, the number of submissions soared to over 11,000! A huge shout out to all who participated and made this a wonderfully competitive hackathon. Check out a few of the comments from the participants: You can check out the leaderboard standings<U+00A0>here. Those who were not able to participate missed out on one of the most exciting hackathons Analytics Vidhya has ever hosted. But don¡¯t worry, we have plenty in store for you ahead. Head over to our DataHack platform and check out upcoming events and practice problems. The problem statement posed by McKinsey Analytics was a fascinating one. The participants were tasked with building a model for an insurance company<U+00A0>for predicting the propensity to pay renewal premium and build an incentive plan for its agents to maximize the net revenue (i.e. renewals <U+2013> incentives given to collect the renewals) collected from the policies post their issuance. The participants were provided information about<U+00A0>past transactions from the policy holders along with their demographics.<U+00A0>Given this information, the the challenge was to predict the propensity of renewal collection and create an incentive plan for agents (at policy level) to maximise the net revenues from these policies. The final solutions will be evaluated on 2 criteria: There were some amazing approaches that went into this hackathon. We have shared a few of them in this section to help you understand how to structure and compete in these hackathons."
"vidhya",2018-07-22,"DataHack Radio Episode #5: Building High Performance Data Science teams with Kiran R","https://www.analyticsvidhya.com/blog/2018/07/datahack-radio-building-data-science-teams-kiran-r/","Very rarely do you come across someone who brings all of the following skills to the table: What do you do when you get some one with these skills? Well <U+2013> you ask them to be your guest on DataHack Radio and be a speaker on DataHack Summit and you let them do the talking! Ladies and Gentleman <U+2013> today we have a guest like this on the show <U+2013> please give a round of applause to Kiran R,<U+00A0>Director of the Data Sciences Center of Excellence at VMware. He has also agreed to do a talk at DataHack Summit 2018 this year <U+2013> so stay tuned. Kiran is a computer science engineer from MSRIT and a post-graduate from the Indian Institute of Management. Kozhikode (IIM-K). He brings more than<U+00A0>14 years of experience building high performance teams for several leading organizations. He is currently leading the Data Sciences Center of Excellence at VMware. Kiran is one of the few Grandmasters on Kaggle who has made his mark in the overall top 10 data scientists there. He is also a winner of the KDD 2014 Data Mining Cup. He is very passionate about data science, as you will discover in his conversation with Kunal. This is a MUST-LISTEN podcast that spans the length and breadth of the various components in the world of data science. This article is essentially a highlight reel of this incredibly knowledge-rich podcast. Happy listening!  You can subscribe to DataHack Radio or listen to previous episodes on any of the below platforms:  Kiran¡¯s first job was at Motorola as a software engineer writing C/C++ code. After spending a year there, he decided to pursue higher studies and completed his MBA from IIM Kozhikode in 2006. He took up an offer from Dell as an Analytics Manager and worked on various tools like SQL, Access and SAS (to build propensity models). His brilliant work ethic and keen innovative mind led him to win Dell¡¯s India Innovator of the Year award in 2012, handed to him directly by Michael Dell! He set up his own proprietorship under the name ¡®Chaotic Experiments¡¯ which he used while participating in several Kaggle competitions. He regularly finished in the top 10 in various competitions and achieved his highest rank of 7 till date during this period. He also did some freelance work in data mining at the time. From there, Kiran had stints at Amazon and Flipkart. He has been in his current role at VMware since the last four years. His role involves managing a team of data scientists that work across various domains including marketing, pricing and strategy. One of the more fascinating things about Kiran¡¯s career arc has been his choice to stay in a technical role even after getting his management degree. He was determined not to lose touch with his technical skills and carved out a path for himself accordingly. Kiran started off his Kaggle journey back when there weren¡¯t too many data science ¡°enthusiasts¡± around. The way he used to approach these problems was to build a diverse set of models <U+2013> a linear model was always a good start, random forest, gradient boosting and then an ensemble model. He made a lot of acquaintances through the platform and these competitions always pushed him to get out of his comfort zone and learn new techniques and build frameworks. This is something he stressed on, because most data scientist jobs restrict you to only those tools and techniques which the business can support. Platforms like Kaggle and Analytics Vidhya allow you to spread your wings. ¡°If you have a hammer and only one tool, everything looks like a nail to you.¡± Participating and learning through competitions helped Kiran a lot when it came to his professional job. There is obviously a difference between the two <U+2013> like in the industry, you won¡¯t get a ready-made dataset and you will need to convert the business problem into a data mining problem. But competitions and platforms like Analytics Vidhya help you become an expert in techniques and tools, and finding out why and how things work (or don¡¯t work). Concepts like overfitting can really help you in the industry and competitions also expose you to problems from various domains, which is an added advantage. Kiran also used his experience to tell us that most problems in the industry are classification based. Regression and segmentation are used as well, but pale in comparison to classification problems. Some of the most widely used techniques are logistic regression and tree-based models. Kiran¡¯s experience has mostly been in the B2C space (with the last 4 years in the B2B sector). He uses the following parameters to classify the data science work companies do: Kiran used these pointers to tell us his take on how Amazon, Flipkart and Dell leverage data science. His insights are fascinating and will enrich your knowledge of how data science works in the real-world. He also elaborated on how his current organization, VMware, uses data science. He went on tell us the difference between B2B and B2C, and how it¡¯s easier to test your models and results in a B2B space than a B2C space (primarily because of the longer purchase cycle the former has). Kiran¡¯s role is divided into 2 parts <U+2013> 90% of it is a functional role, which is to lead the Data Sciences Center of Excellence. The other 10% is to play a sight leader role for enterprise information management (BI, Analytics, and Data Science). His typical day involves a lot of operational activities, tons of people activities, project deliveries, data science technique brainstorming, interacting with stakeholders, among other things. He has a very hands-on leadership style which helps him juggle and manage these various aspects on a daily basis. Kiran believes hiring is the most important part of team building, because if you have the right people, the job will get done. When he hires people for his team, there are a few critical areas he looks at: I highly recommend listening to the podcast to understand the details of these pointers which Kiran has elaborated on in detail. This will give you a brilliant insight into what an industry leader looks for when hiring a data scientist. One thing that has stood out for Kiran is the pace at which data science has grown in recent years. The interest in this field is extremely high right now and he sees this continuing in the future. Advancement in the type of models we make will increase and the time it takes to train models will decrease, according to Kiran. This will of course be helped by far better computational power (GPUs could well get cheaper) and we should see deep learning being used more and more on structured data. Another change Kiran predicts in the near future is the increasing pressure on business leaders to adopt AI and data mining. As more and more techniques are developed and become interpretable, businesses will be forced into situations where they have the leverage AI to gain any competitive edge they can get. Most of the proprietor software, like SAS, will go away according to Kiran. Open source languages like R and Python have already eaten away into the lead SAS had and will continue to gain popularity in the coming years. Open source is the driving force behind machine learning and more and more researchers and organizations are realizing this. Kiran firmly believes one needs to be good in programming to make a career in machine learning. If you can¡¯t do programming, you will have an almighty struggle to get into this field. If you¡¯re new to this field, Kiran¡¯s advice is to pick up Python and get very familiar with it. You should also eventually learn more than one language which will help expand your skillset."
