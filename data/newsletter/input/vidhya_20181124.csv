"site","date","headline","url_address","text"
"vidhya",2018-11-19,"4 Secrets for a Future Ready Career in Data Science","https://www.analyticsvidhya.com/blog/2018/11/4-secrets-for-a-future-ready-career-in-data-science/","Automation has impacted, and will continue impacting, jobs in many domains. Every single job on this planet is subject to a risk of job replacement by bots <U+2013> just the intensity might differ. Automation makes running a business more efficient on one hand, and on the other, it keeps on changing the skill set required to stay relevant in the industry. This inevitably leads to unemployment due to mismatches in the skill set. Let me take you through a few scenarios to illustrate my thoughts. You are an HR professional in the year 2000, when most of the company employee documents were on paper. You are very efficient in sorting documents and retrieving them when needed and have been a star performer for more than 5 years because of these skills. Given that the HR processes did not change much over time, you did not pick up computer skills over the next 18 years. However, the way industries work have changed a lot from 2000 to 2018, and now all the employee documentations are on the cloud or a private server. So, your most sell-able skills are now suddenly not that important. You might face difficulties finding a job unless you upgrade yourself for today¡¯s evolved industry. Note that your skill set mismatch was not because of the evolution of HR specific processes, but the dynamically changing business processes that you support. You work as a news reader on the radio in the era when there was no television. You are very well informed about current affairs and hence you were a strong performer. But after television became mainstream, radios almost went out of business. Your radio employer had to let you go because they were sustaining heavy losses. Now, given your skill set, you can still try to get a job as a TV news reader but you need to work on your body language and the crippling fear of facing the camera. The good news? You have been hanging out with people who work in the TV news industry and hence you know your opportunity areas and have been actively working on them. Note that this time neither your profession evolved, nor your industry. It¡¯s just that the customer started preferring an alternate product/service to the business you support, making your skill a mismatch (or obsolete) in the industry. In the scenarios above, we witnessed that the changes around us are making businesses easy to run but at the same time are<U+00A0> creating job skill mismatches, leading to unemployment in specific domains. Below are the three main reasons of job skill shifts in the industry: It is no surprise that automation and changing business domains have disrupted many jobs. An important questions now is: Will some jobs be impacted more than others? Even though no one really knows what jobs will be more/less impacted by automation, here is a framework that helps understand the broad idea.<U+00A0>Machines are not good at learning from too few examples and machines are not good at being creative. So if your job has these two attributes, you should be just fine. For instance, driving a car is a very repetitive process and does not involve a lot of creativity. Hence, cab drivers are at a high risk of their job facing automation. In this ever-evolving AI-led world, we (data scientists) are definitely on the better side of the deal. Where does the role of a data scientist fall in the graph shown above? As a data scientist, we do a varied set of jobs to help businesses grow. Each of these jobs fall at a different place in the graph above. The below image shows my thoughts on the different sub-jobs we do as data scientists (proportion might vary with individual roles): As you can see, Not all the parts of a data scientist¡¯s job come with a 10 year warranty. Depending on your specific role and proportion of work that is difficult to automate, you can estimate your risk of automation. Consider a data scientist in 2010. Key skill sets were knowing logistic and linear regression, and conversant with base SAS and MS Excel. Now, if we bring this data scientist of 2018 without any significant upgrades on tools and technique, he/she can face hard time finding data scientist job.<U+00A0> With good certainty it can be said that even though the data science stream will stay up and running for long term, the roles and responsibilities of these jobs are up for big changes. People who have challenges upgrading to these new roles and responsibilities will face strong setbacks in progressing in career. Given the young workforce in data science field, skill set match is not a concern over short term as most of the people working in this field have recently picked up knowledge in latest tools and technique. However, as the field gets old so does the workforce and skill set mismatch within data science domain is definitely possible if this workforce is not able to upgrade their skill set while managing their daily jobs. Four things I would recommend for data scientists in any kind of role to build a future proof profile: With a high focus on data-driven strategies across domains, data scientists are kept busy with their job at hand. Not staying updated on each of the 4 pointers mentioned above can<U+00A0>be dangerous in the long run. To fill this gap in the industry, Analytics Vidhya has handcrafted a four day conference <U+2013><U+00A0>DataHack Summit 2018. After the success of the Summit last year, we have further optimized the schedule to pack it with everything you need to know to come up to speed in terms of tools, technologies, and business domains. Sounds too good an opportunity to pass up? Good! Tickets are almost sold on, so grab yours<U+00A0>here<U+00A0>TODAY!"
"vidhya",2018-11-19,"Reinforcement Learning: Introduction to Monte Carlo Learning using the OpenAI Gym Toolkit","https://www.analyticsvidhya.com/blog/2018/11/reinforcement-learning-introduction-monte-carlo-learning-openai-gym/","What¡¯s the first thing that comes to your mind when you hear the words ¡°reinforcement learning¡±? The most common thought is <U+2013> too complex with way too much math. But I¡¯m here to assure you that this is quite a fascinating field of study <U+2013> and I aim to break down these techniques in my articles into easy-to-understand concepts. I¡¯m sure you must have heard of OpenAI and DeepMind. These are two leading AI organizations who have made significant progress in this field. A team of OpenAI bots was able to defeat a team of amateur gamers in Dota 2, a phenomenally popular and complex battle arena game. Do you think it¡¯s feasible to build a bot using dynamic programming for something as complex as Dota 2? It¡¯s unfortunately a no-go. There are just too many states (millions and millions), and collecting all the specifics of DOTA 2 is an impossible task. This is where we enter the realm of reinforcement learning or more specifically model-free learning. In this article, we will try to understand the basics of Monte Carlo learning. It¡¯s used when there is no prior information of the environment and all the information is essentially collected by experience. We¡¯ll use the OpenAI Gym toolkit in Python to implement this method as well. Let¡¯s get the ball rolling! If you¡¯re a beginner in this field or need a quick refresher of some basic reinforcement learning terminologies, I highly recommend going through the below articles to truly maximize your learning from this post: We know that dynamic programming is used to solve problems where the underlying model of the environment is known beforehand (or more precisely, model-based learning). Reinforcement Learning is all about learning from experience in playing games. And yet, in none of the dynamic programming algorithms, did we actually play the game/experience the environment. We had a full model of the environment, which included all the state transition probabilities. However, in most real life situations as we saw in the introduction, the transition probabilities from one state to another (or the so called model of the environment) are not known beforehand. It is not even necessary that the task follows a Markov property. Let¡¯s say we want to train a bot to learn how to play chess. Consider converting the chess environment into an MDP. Now, depending on the positioning of pieces, this environment will have many states (more than 1050), as well as a large number of possible actions. The model of this environment is almost impossible to design! One potential solution could be to repeatedly play a complete game of chess and receive a positive reward for winning, and a negative reward for losing, at the end of each game. This is called learning from experience. Any method which solves a problem by generating suitable random numbers, and observing that fraction of numbers obeying some property or properties, can be classified as a Monte Carlo method. Let¡¯s do a fun exercise where we will try to find out the value of pi using pen and paper. Let¡¯s draw a square of unit length and draw a quarter circle with unit length radius. Now, we have a helper bot C3PO with us. It is tasked with putting as many dots as possible on the square randomly 3,000 times, resulting in the following figure: C3PO needs to count each time it puts a dot inside a circle. So, the value of pi will be given by: where N is the number of times a dot was put inside the circle. As you can see, we did not do anything except count the random dots that fall inside the circle and then took a ratio to approximate the value of pi. The Monte Carlo method for reinforcement learning learns directly from episodes of experience without any prior knowledge of MDP transitions. Here, the random component is the return or reward. One caveat is that it can only be applied to episodic MDPs. Its fair to ask why, at this point. The reason is that the episode has to terminate before we can calculate any returns. Here, we don¡¯t do an update after every action, but rather after every episode. It uses the simplest idea <U+2013> the value is the mean return of all sample trajectories for each state. Recalling the idea from multi-armed bandits discussed in this article, every state is a separate multi-armed bandit problem and the idea is to behave optimally for all multi-armed bandits at once. Similar to dynamic programming, there is a policy evaluation (finding the value function for a given random policy) and policy improvement step (finding the optimum policy). We will cover both these steps in the next two sections. The goal here, again, is to learn the value function vpi(s) from episodes of experience under a policy pi. Recall that the return is the total discounted reward: S1, A1, R2, ¡¦.Sk ~ pi Also recall that the value function is the expected return: We know that we can estimate any expected value simply by adding up samples and dividing by the total number of samples: The question is how do we get these sample returns? For that, we need to play a bunch of episodes and generate them. For every episode we play, we¡¯ll have a sequence of states and rewards. And from these rewards, we can calculate the return by definition, which is just the sum of all future rewards. First Visit Monte Carlo:<U+00A0>Average returns only for first time s is visited in an episode. Here¡¯s a step-by-step view of how the algorithm works: Every visit Monte Carlo:<U+00A0>Average returns for every time s is visited in an episode. For this algorithm, we just change step #3.1 to ¡®Add to a list<U+00A0>the return received after every occurrence of this state¡¯. Let¡¯s consider a simple example to further understand this concept. Suppose there¡¯s an environment where we have 2 states <U+2013> A and B. Let¡¯s say we observed 2 sample episodes: A+3 => A indicates a transition from state A to state A, with a reward +3. Let¡¯s find out the value function using both methods: It is convenient to convert the mean return into an incremental update so that the mean can be updated with each episode and we can understand the progress made with each episode. We already<U+00A0>learnt this when solving the multi-armed bandit problem. We update v(s) incrementally after episodes. For each state St, with return Gt: In non-stationary problems, it can be useful to track a running mean, i.e., forget old episodes: V(St) ¡ç V(St) + ¥á (Gt <U+2212> V(St)) Similar to dynamic programming, once we have the value function for a random policy, the important task that still remains is that of finding the optimal policy using Monte Carlo. Recall that the formula for policy improvement in DP required the model of the environment as shown in the following equation: This equation finds out the optimal policy by finding actions that maximize the sum of rewards. However, a major caveat here is that it uses transition probabilities, which is not known in the case of model-free learning. Since we do not know the state transition probabilities p(s¡¯,r/s,a), we can¡¯t do a look-ahead search like DP.<U+00A0>Hence, all the information is obtained via experience of playing the game or exploring the environment. Policy improvement is done by making the policy greedy with respect to the current value function.<U+00A0>In this case, we have an action-value function, and therefore no model is needed to construct the greedy<U+00A0>policy. A greedy policy (like the above mentioned one) will always favor a certain action if most actions are not explored properly. There are two solutions for this: Monte Carlo with exploring starts All the state action pairs have non-zero probability of being the starting pair, in this algorithm. This will ensure each episode which is played will take the agent to new states and hence, there is more exploration of the environment. Monte Carlo with epsilon-Soft What if there is a single start point for an environment (for example, a game of chess)? Exploring starts is not the right option in such cases. Recall here that in a multi-armed bandit problem, we discussed the<U+00A0>epsilon-greedy approach. Simplest idea for ensuring continual exploration all actions are tried with non-zero probability 1 <U+2013> epsilon choose the action which maximises the action value function and with probability epsilon choose an action at random. Now that we understand the basics of Monte Carlo Control and Prediction, let¡¯s implement the algorithm in Python. We will import the frozen lake environment from the popular OpenAI Gym toolkit. The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile. The surface is described using a grid like the following: (S: starting point, safe),<U+00A0><U+00A0>(F: frozen surface, safe),<U+00A0>(H: hole, fall to your doom),<U+00A0>(G: goal) The idea is to reach the goal from the starting point by walking only on a frozen surface and avoiding all the holes. Installation details and documentation for the OpenAI Gym are available at this<U+00A0>link. Let¡¯s begin! ¡©¡© First, we will define a few helper functions to set up the Monte Carlo algorithm. Create Environment Function for Random Policy Dictionary for storing the state action value Function to play episode Function to test policy and print win percentage First Visit Monte Carlo Prediction and Control Now, it is time to run this algorithm to solve an 8¡¿8 frozen lake environment and check the reward: On running this for 50,000 episodes, we get a score of 0.9. And with more episodes, it eventually reaches the optimal policy. The story of Monte Carlo learning does not end here. There is another set of algorithms under this which are called off policy Monte Carlo methods. Off policy methods try to learn an optimal policy using returns generated from another policy."
