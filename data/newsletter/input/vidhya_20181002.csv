"site","date","headline","url_address","text"
"vidhya",2018-10-01,"Text Mining 101: A Stepwise Introduction to Topic Modeling using Latent Semantic Analysis (using Python)","https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 Have you ever been inside a well-maintained library? I¡¯m always incredibly impressed with the way the librarians keep everything organized, by name, content, and other topics. But if you gave these librarians thousands of books and asked them to arrange each book on the basis of their genre, they will struggle to accomplish this task in a day, let alone an hour! However, this won¡¯t happen to you if these books came in a digital format, right? All the arrangement seems to happen in a matter of seconds, without requiring any manual effort. All hail Natural Language Processing (NLP). Source: confessionsofabookgeek.com Have a look at the below text snippet: As you might gather from the highlighted text, there are three topics (or concepts) <U+2013> Topic 1, Topic 2, and Topic 3. A good topic model will identify similar words and put them under one group or topic. The most dominant topic in the above example is Topic 2, which indicates that this piece of text is primarily about fake videos. Intrigued, yet? Good! In this article, we will learn about a text mining approach called Topic Modeling. It is an extremely useful technique for extracting topics, and one you will work with a lot when faced with NLP challenges. Note: I highly recommend going through this article<U+00A0>to understand terms like SVD and UMAP. They are leveraged in this article so having a basic understanding of them will help solidify these concepts. A Topic Model can be defined as an unsupervised technique to discover topics across various text documents. These topics are abstract in nature, i.e., words which are related to each other form a topic. Similarly, there can be multiple topics in an individual document. For the time being, let¡¯s understand a topic model as a black box, as illustrated in the below figure: This black box (topic model) forms clusters of similar and related words which are called topics. These topics have a certain distribution in a document, and every topic is defined by the proportion of different words it contains. Recall the example we saw earlier of arranging similar books together. Now suppose you have to perform a similar task with a few digital text documents. You would be able to manually accomplish this, as long as the number of documents is manageable (aka not too many of them). But what happens when there¡¯s an impossible number of these digital text documents? That¡¯s where NLP techniques come to the fore. And for this particular task, topic modeling is the technique we will turn to. Source: topix.io/tutorial/tutorial.html Topic modeling helps in exploring large amounts of text data, finding clusters of words, similarity between documents, and discovering abstract topics. As if these reasons weren¡¯t compelling enough, topic modeling is also used in search engines wherein the search string is matched with the results. Getting interesting, isn¡¯t it? Well, read on then! All languages have their own intricacies and nuances which are quite difficult for a machine to capture (sometimes they¡¯re even misunderstood by us humans!). This can include different words that mean the same thing, and also the words which have the same spelling but different meanings. For example, consider the following two sentences: In the first sentence, the word ¡®novel¡¯ refers to a book, and in the second sentence it means new or fresh. We can easily distinguish between these words because we are able to understand the context behind these words. However, a machine would not be able to capture this concept as it cannot understand the context in which the words have been used. This is where Latent Semantic Analysis (LSA) comes into play as it attempts to leverage the context around the words to capture the hidden concepts, also known as topics.  So, simply mapping words to documents won¡¯t really help. What we really need is to figure out the hidden concepts or topics behind the words. LSA is one such technique that can find these hidden topics. Let¡¯s now deep dive into the inner workings of LSA. Let¡¯s say we have m number of text documents with n number of total unique terms (words). We wish to extract k topics from all the text data in the documents. The number of topics, k, has to be specified by the user. It¡¯s time to power up Python and understand how to implement LSA in a topic modeling problem. Once your Python environment is open, follow the steps I have mentioned below. Let¡¯s load the required libraries before proceeding with anything else. In this article, we will use the ¡¯20 Newsgroup¡¯ dataset from sklearn. You can download the dataset here, and follow along with the code. Output: 11,314 The dataset has 11,314 text documents distributed across 20 different newsgroups. To start with, we will try to clean our text data as much as possible. The idea is to remove the punctuations, numbers, and special characters all in one step using the regex replace(¡°[^a-zA-Z#]¡±, ¡± ¡°), which will replace everything, except alphabets with space. Then we will remove shorter words because they usually don¡¯t contain useful information. Finally, we will make all the text lowercase to nullify case sensitivity. It¡¯s good practice to remove the stop-words from the text data as they are mostly clutter and hardly carry any information. Stop-words are terms like ¡®it¡¯, ¡®they¡¯, ¡®am¡¯, ¡®been¡¯, ¡®about¡¯, ¡®because¡¯, ¡®while¡¯, etc. To remove stop-words from the documents, we will have to tokenize the text, i.e., split the string of text into individual tokens or words. We will stitch the tokens back together once we have removed the stop-words. This is the first step towards topic modeling. We will use sklearn¡¯s TfidfVectorizer to create a document-term matrix with 1,000 terms. We could have used all the terms to create this matrix but that would need quite a lot of computation time and resources. Hence, we have restricted the number of features to 1,000. If you have the computational power, I suggest trying out all the terms. The next step is to represent each and every term and document as a vector. We will use the document-term matrix and decompose it into multiple matrices. We will use sklearn¡¯s TruncatedSVD to perform the task of matrix decomposition. Since the data comes from 20 different newsgroups, let¡¯s try to have 20 topics for our text data. The number of topics can be specified by using the<U+00A0>n_components<U+00A0>parameter. The components of svd_model are our topics, and we can access them using svd_model.components_. Finally, let¡¯s print a few most important words in each of the 20 topics and see how our model has done. To find out how distinct our topics are, we should visualize them. Of course, we cannot visualize more than 3 dimensions, but there are techniques like PCA and t-SNE which can help us visualize high dimensional data into lower dimensions. Here we will use a relatively new technique called UMAP (Uniform Manifold Approximation and Projection). As you can see above, the result is quite beautiful. Each dot represents a document and the colours represent the 20 newsgroups. Our LSA model seems to have done a good job. Feel free to play around with the parameters of UMAP to see how the plot changes its shape. The entire code for this article can be found in this GitHub repository. Latent Semantic Analysis can be very useful as we saw above, but it does have its limitations. It¡¯s important to understand both the sides of LSA so you have an idea of when to leverage it and when to try something else. Pros: Cons: Apart from LSA, there are other advanced and efficient topic modeling techniques such as<U+00A0>Latent Dirichlet Allocation (LDA) and<U+00A0>lda2Vec. We have a wonderful article on LDA which you can check out here. lda2vec is a much more advanced topic modeling which is based on word2vec word embeddings. If you want to find out more about it, let me know in the comments section below and I¡¯ll be happy to answer your questions/. This article is an attempt to share my learnings with all of you. Topic modeling is quite an interesting topic and equips you with the skills and techniques to work with many text datasets. So, I urge you all to use the code given in this article and apply it to a different dataset. Let me know if you have any questions or feedback related to this article. Happy text mining!"
"vidhya",2018-09-27,"Building DataHack Summit 2018 <U+2013> India¡¯s Most Advanced AI Conference. Are you Ready?","https://www.analyticsvidhya.com/blog/2018/09/building-datahack-summit-2018-indias-most-advanced-ai-conference-are-you-ready/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 Usain Bolt created a World record by running 200m sprint in<U+00A0>19.30 seconds in 2008. What do you think he thought while he was preparing for 2009? He had his mind set to beat his own personal best and he did! Why am I talking about Bolt here? Well, I find myself in a similar situation. DataHack Summit 2017 was an unprecedented success. We created India¡¯s largest conference with unilateral focus on data science practitioners. The community loved the focus, the content and the knowledge sharing at the event. If you haven¡¯t seen already <U+2013> check out the highlights below. What are we thinking now as we are building India¡¯s most advanced data science conference? If you cut through my mind and get a peek inside <U+2013> this is what you will find <U+0001F642> The venue is bigger than last year but tickets are selling like hot cakes so make sure you grab yours before they¡¯re sold out. Prices go up on September 30th so avail the discount today! Head over here to book your seat<U+00A0>for India¡¯s most advanced conference on AI, Machine Learning, Deep Learning, and IoT! Let¡¯s take a quick tour around DHS 2018 to see how it¡¯s shaping up and what we have in store for you. If there is one place we bet our reputation on <U+2013> it is the content we create and we curate. DataHack Summit 2018 will be an epitome of this. To be honest, we are having a tough time saying no to very exciting talk proposals. Here are a few<U+00A0>eminent speakers in AI and ML who will be speaking at DataHack Sumit 2018: The most exciting thing which people would see are the<U+00A0>Hack sessions.<U+00A0>They saw a tremendous response from the audience last year, and have been expanded to reflect the latest breakthrough developments. Below are a few topics to whet your appetite (click on each session to read more about what will be covered): And here are a few awesome hackers, who will be performing live hack sessions: Check out the full speaker line-up<U+00A0>here. We will top up the sessions and Hack Sessions with an exclusive Startup Showcase and Research Track. We will showcase some of the most exciting AI and ML startups across the globe to showcase their offerings. Prepare to have your mind blown by some of the most amazing uses of AI and ML in a variety of domains. In addition to this, there is an entire track dedicated to cutting-edge research! We are giving individuals the opportunity to come and present their work in front of our community. This year¡¯s venue is none other than the NIMHANS Convention Center in Bengaluru. There are three auditoriums (yes, three!) <U+2013> so you are going to see 3 parallel tracks. So you can look forward to more sessions, more industry leaders, and more engagement! And all this space means an opportunity for even more events. There will be more hack sessions this year, and each session will have an even bigger audience than before. DataHack Summit 2018 will have bigger and swankier LED screens as well! So regardless of where you¡¯re sitting, the presentation and code will be visible from all corners of the room. Reserve your seat TODAY! There is an incredible deal on offer and prices will go up on September 30th. So act now and become a part of India¡¯s most advanced AI and ML conference."
"vidhya",2018-09-27,"A Multivariate Time Series Guide to Forecasting and Modeling (with Python codes)","https://www.analyticsvidhya.com/blog/2018/09/multivariate-time-series-guide-forecasting-modeling-python-codes/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 Time is the most critical factor that decides whether a business will rise or fall. That¡¯s why we see sales in stores and e-commerce platforms aligning with festivals. These businesses analyze years of spending data to understand the best time to throw open the gates and see an increase in consumer spending. But how can you, as a data scientist, perform this analysis? Don¡¯t worry, you don¡¯t need to build a time machine! Time Series modeling is a powerful technique that acts as a gateway to understanding and forecasting trends and patterns. But even a time series model has different facets. Most of the examples we see on the web deal with univariate time series. Unfortunately, real-world use cases don¡¯t work like that. There are multiple variables at play, and handling all of them at the same time is where a data scientist will earn his worth. In this article, we will understand what a multivariate time series is, and how to deal with it. We will also take a case study and implement it in Python to give you a practical understanding of the subject. This article assumes some familiarity with univariate time series, its properties and various techniques used for forecasting. Since this article will be focused on multivariate time series, I would suggest you go through the following articles which serve as a good introduction to univariate time series: But I¡¯ll give you a quick refresher of what a univariate time series is, before going into the details of a multivariate time series. Let¡¯s look at them one by one to understand the difference. A univariate time series, as the name suggests, is a series with a single time-dependent variable. For example, have a look at the sample dataset below that consists of the temperature values (each hour), for the past 2 years. Here, temperature is the dependent variable (dependent on Time). If we are asked to predict the temperature for the next few days, we will look at the past values and try to gauge and extract a pattern. We would notice that the temperature is lower in the morning and at night, while peaking in the afternoon. Also if you have data for the<U+00A0>past few years, you would observe that it is colder during the months of November to January, while being comparatively hotter in April to June. Such observations will help us in predicting future values. Did you notice that we used only one variable (the temperature of the past 2 years,)? Therefore, this is called Univariate Time Series Analysis/Forecasting. A Multivariate time series has more than one time-dependent variable. Each variable depends not only on its past values but also has some dependency on other variables. This dependency is used for forecasting future values. Sounds complicated? Let me explain. Consider the above example. Now suppose our dataset includes perspiration percent, dew point, wind speed, cloud cover percentage, etc. along with the temperature value for the past two years. In this case, there are multiple variables to be considered to optimally predict temperature. A series like this would fall under the category of multivariate time series. Below is an illustration of this: Now that we understand what a multivariate time series looks like, let us understand how can we use it to build a forecast. In this section, I will introduce you to one of the most commonly used methods for multivariate time series forecasting <U+2013> Vector Auto Regression (VAR). In a VAR model, each variable is a linear function of the past values of itself and the past values of all the other variables. To explain this in a better manner, I¡¯m going to use a simple visual example: We have two variables, y1 and y2. We need to forecast the value of these two variables at time t, from the given data for past n values. For simplicity, I have considered the lag value to be 1. For calculating y1(t), we will use the past value of y1 and y2. Similarly, to calculate y2(t), past values of both y1 and y2 will be used. Below is a simple mathematical way of representing this relation: Here, These equations are similar to the equation of an<U+00A0>AR process. Since the AR process is used for univariate time series data, the future values are linear combinations of their own past values only. Consider the AR(1) process: y(t) = a + w*y(t-1) +e In this case, we have only one variable <U+2013> y, a constant term <U+2013> a, an error term <U+2013> e, and a coefficient <U+2013> w. In order to accommodate the multiple variable terms in each equation for VAR, we will use vectors.<U+00A0> We can write the equations (1) and (2) in the following form : The two variables are y1 and y2, followed by a constant, a coefficient metric, lag value, and an error metric. This is the vector equation for a VAR(1) process. For a VAR(2) process, another vector term for time (t-2) will be added to the equation to generalize for p lags: The above equation represents a VAR(p) process with variables y1, y2 ¡¦yk. The same can be written as: The term ¥åt in the equation represents multivariate vector white noise. For a multivariate time series,<U+00A0>¥åt should be a continuous random vector that satisfies the following conditions: Recall the temperate forecasting example we saw earlier. An argument can be made for it to be treated as a multiple univariate series. We can solve it using simple univariate forecasting methods like AR. Since the aim is to predict the temperature, we can simply remove the other variables (except temperature) and fit a model on the remaining univariate series. Another simple idea is to forecast values for each series individually using the techniques we already know. This would make the work extremely straightforward! Then why should you learn another forecasting technique? Isn¡¯t this topic complicated enough already? From the above equations (1) and (2), it is clear that each variable is using the past values of every variable to make the predictions. Unlike AR, VAR is able to understand and use the relationship between several variables. This is useful for describing the dynamic behavior of the data and also provides better forecasting results. Additionally, implementing VAR is as simple as using any other univariate technique (which you will see in the last section). We know from studying the univariate concept that a stationary time series will more often than not give us a better set of predictions. If you are not familiar with the concept of stationarity, please go through this article first: A Gentle Introduction to handling non-stationary Time Series. To summarize, for a given univariate time series: y(t) = c*y(t-1) + ¥å t The series is said to be stationary if the value of |c| < 1. Now, recall the equation of our VAR process: Note: I is the identity matrix. Representing the equation in terms of Lag operators, we have: Taking all the y(t) terms on the left-hand side: The coefficient of y(t) is called the lag polynomial. Let us represent this as ¥Õ(L): For a series to be stationary, the eigenvalues of |¥Õ(L)-1| should be less than 1 in modulus. This might seem complicated given the number of variables in the derivation. This idea has been explained using a simple numerical example in the following video. I highly encourage watching it to solidify your understanding: Similar to the Augmented Dickey-Fuller test for univariate series, we have Johansen¡¯s test for checking the stationarity of any multivariate time series data. We will see how to perform the test in the last section of this article. If you have worked with univariate time series data before, you¡¯ll be aware of the train-validation sets. The idea of creating a validation set is to analyze the performance of the model before using it for making predictions. Creating a validation set for time series problems is tricky because we have to take into account the time component. One cannot directly use the train_test_split or k-fold validation since this will disrupt the pattern in the series. The validation set should be created considering the date and time values. Suppose we have to forecast the temperate, dew point, cloud percent, etc. for the next two months using data from the last two years. One possible method is to keep the data for the last two months aside and train the model on the remaining 22 months. Once the model has been trained, we can use it to make predictions on the validation set. Based on these predictions and the actual values, we can check how well the model performed, and the variables for which the model did not do so well. And for making the final prediction, use the complete dataset (combine the train and validation sets). In this section, we will implement the Vector AR model on a toy dataset. I have used the Air Quality dataset for this and you can download it from here. The data type of the<U+00A0>Date_Time column is object<U+00A0>and we need to change it to datetime. Also, for preparing the data, we need the index to have datetime. Follow the below commands: The next step is to deal with the missing values. Since the missing values in the data are replaced with a value -200, we will have to impute the missing value with a better number. Consider this <U+2013> if the present dew point value is missing, we can safely assume that it will be close to the value of the previous hour. Makes sense, right? Here, I will impute -200 with the previous value. You can choose to substitute the value using the average of a few previous values, or the value at the same time on the previous day (you can share your idea(s) of imputing missing values in the comments section below). Below is the result of the test: We can now go ahead and create the validation set to fit the model, and test the performance of the model: The predictions are in the form of an array, where each list represents the predictions of the row. We will transform this into a more presentable format. Output of the above code: After the testing on validation set, lets fit the model on the complete dataset Before I started this article, the idea of working with a multivariate time series seemed daunting in its scope. It is a complex topic, so take your time in understanding the details. The best way to learn is to practice, and so I hope the above Python implemenattion will be useful for you. I enocurage you to use this approach on a dataset of your choice. This will further cement your understanding of this complex yet highly useful topic. If you have any suggestions or queries, share them in the comments section. HI. Thanks for sharing the knowledge and the great article! Could you pls add some details regarding the stationarity test process described in the article : the test is done and the results are presented but it is not clear if it could be concluded that the data is stationary; after the test is done no further actions to make the data stationary are performed¡¦why so. Thanks Why not just use Random Forest for this?
Thank you Hi John, random forest can be used for supervised machine learning algorithms. In this case, we don¡¯t have a test set. Considering the example for weather prediction used in section 1 -if you consider temperature as target variable and the rest as independent variables, the test set must have the independent variables, which is not the case here. Using VAR , we predict all the variables ."
"vidhya",2018-09-27,"The Winning Approaches from codeFest 2018 <U+2013> NLP, Computer Vision and Machine Learning!","https://www.analyticsvidhya.com/blog/2018/09/the-winning-approaches-from-codefest-2018-nlp-computer-vision-and-machine-learning/","

Course on Computer Vision Using Deep Learning | Limited Period Offer at only Rs 11999 | Use COUPON CODE: CVLAUNCH60 |
Buy Now 

 Analytics Vidhya¡¯s hackathons are one of the best ways to evaluate how far you¡¯ve traveled in your data science journey. And what better way than to put your skills to the test against the top data scientists from around the globe? Participating in these hackathons also helps you understand where you need to improve and what else you can learn to get a better score in the next competition. And a very popular demand after each hackathon is to see how the winning solution was designed and the thought process behind it. There¡¯s a lot to learn from this, including how you can develop your own unique framework for future hackathons. We are all about listening to our community, so we decided to curate the winning approaches from our recently concluded hackathon series, codeFest! This was a series of three hackathons in partnership with IIT-BHU, conducted between 31st August and 2nd September. The competition was intense, with more than 1,900 aspiring data scientists going head-to-head to grab the ultimate prize! Each hackathon had a unique element to it. Interested in finding out more? You can view the details of each competition below: It¡¯s time to check out the winners¡¯ approaches! Abhinav Gupta and Abhishek Sharma. The participants were given a list of tweets from customers about various tech firms who manufacture and sell mobiles, computers, laptops, etc. The challenge was to find the tweets which showed a negative sentiment towards such companies or products. The metric used for evaluating the performance of the classification model was weighted F1-Score. Abhinav and Abhishek have summarized their approach in a very intuitive manner, explaining everything from preprocessing and feature engineering to model building. Pre-processing: Feature Extraction: Classifiers used: They<U+00A0>hypertuned each of the above classifiers and found that LSTM (with attention mechanism) produced the best result. Ensemble Deepak Rawat. The Vista hackathon had a pretty intriguing problem statement. The participants had to build a model that counted the number of people in a given group selfie/photo. The dataset provided had already been split, wherein the training set consisted of images with coordinates of the bounding boxes and headcount for each image. The evaluation metric for this competition was RMSE (root mean squared error) over the headcounts predicted for test images. Check out Deepak¡¯s approach in his own words below: Mask R-CNN and<U+00A0>ResNet101 Both stages are connected to the backbone structure. Pre-processing  Model Building Raj Shukla. As a part of enigma competition, the target was to predict the number of upvotes on a question based on other information provided. For every question <U+2013> its tag, number of views received, number of answers, username and reputation of the question author, was provided. Using this information, the participant had to predict the upvote count that the question will receive. The evaluation metric for this competition was RMSE (root mean squared error). Below is the data dictionary for your reference: Here is Raj¡¯s approach to cracking the Enigma hackathon: Feature Engineering: My focus was on feature engineering, i.e., using the existing features to create new features. Below are some key features I cooked up: Model Building: A big thank you to everyone for participating in codeFest 2018! This competition was all about quick and structured thinking, coding, experimentation, and finding the one approach that got you up the leaderboard. In short, what machine learning is all about! Missed out this time? Don¡¯t worry, you can check out all upcoming hackathons on our DataHack platform and register yourself today!"
