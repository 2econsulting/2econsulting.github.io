"site","date","headline","url_address","text"
"mastery",2018-09-21,"How to Develop 1D Convolutional Neural Network Models for Human Activity Recognition","https://machinelearningmastery.com/cnn-models-for-human-activity-recognition-time-series-classification/","Human activity recognition is the problem of classifying sequences of accelerometer data recorded by specialized harnesses or smart phones into known well-defined movements. Classical approaches to the problem involve hand crafting features from the time series data based on fixed-sized windows and training machine learning models, such as ensembles of decision trees. The difficulty is that this feature engineering requires deep expertise in the field. Recently, deep learning methods such as recurrent neural networks and one-dimensional convolutional neural networks, or CNNs, have been shown to provide state-of-the-art results on challenging activity recognition tasks with little or no data feature engineering, instead using feature learning on raw data. In this tutorial, you will discover how to develop one-dimensional convolutional neural networks for time series classification on the problem of human activity recognition. After completing this tutorial, you will know: Let¡¯s get started. How to Develop 1D Convolutional Neural Network Models for Human Activity RecognitionPhoto by Wolfgang Staudt, some rights reserved. This tutorial is divided into four parts; they are: Human Activity Recognition, or HAR for short, is the problem of predicting what a person is doing based on a trace of their movement using sensors. A standard human activity recognition dataset is the ¡®Activity Recognition Using Smart Phones Dataset¡¯ made available in 2012. It was prepared and made available by Davide Anguita, et al. from the University of Genova, Italy and is described in full in their 2013 paper ¡°A Public Domain Dataset for Human Activity Recognition Using Smartphones.¡± The dataset was modeled with machine learning algorithms in their 2012 paper titled ¡°Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine.¡± The dataset was made available and can be downloaded for free from the UCI Machine Learning Repository: The data was collected from 30 subjects aged between 19 and 48 years old performing one of six standard activities while wearing a waist-mounted smartphone that recorded the movement data. Video was recorded of each subject performing the activities and the movement data was labeled manually from these videos. Below is an example video of a subject performing the activities while their movement data is being recorded. The six activities performed were as follows: The movement data recorded was the x, y, and z accelerometer data (linear acceleration) and gyroscopic data (angular velocity) from the smart phone, specifically a Samsung Galaxy S II. Observations were recorded at 50 Hz (i.e. 50 data points per second). Each subject performed the sequence of activities twice, once with the device on their left-hand-side and once with the device on their right-hand side. The raw data is not available. Instead, a pre-processed version of the dataset was made available. The pre-processing steps included: Feature engineering was applied to the window data, and a copy of the data with these engineered features was made available. A number of time and frequency features commonly used in the field of human activity recognition were extracted from each window. The result was a 561 element vector of features. The dataset was split into train (70%) and test (30%) sets based on data for subjects, e.g. 21 subjects for train and nine for test. Experiment results with a support vector machine intended for use on a smartphone (e.g. fixed-point arithmetic) resulted in a predictive accuracy of 89% on the test dataset, achieving similar results as an unmodified SVM implementation. The dataset is freely available and can be downloaded from the UCI Machine Learning repository. The data is provided as a single zip file that is about 58 megabytes in size. The direct link for this download is below: Download the dataset and unzip all files into a new directory in your current working directory named ¡°HARDataset¡±. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this section, we will develop a one-dimensional convolutional neural network model (1D CNN) for the human activity recognition dataset. Convolutional neural network models were developed for image classification problems, where the model learns an internal representation of a two-dimensional input, in a process referred to as feature learning. This same process can be harnessed on one-dimensional sequences of data, such as in the case of acceleration and gyroscopic data for human activity recognition. The model learns to extract features from sequences of observations and how to map the internal features to different activity types. The benefit of using CNNs for sequence classification is that they can learn from the raw time series data directly, and in turn do not require domain expertise to manually engineer input features. The model can learn an internal representation of the time series data and ideally achieve comparable performance to models fit on a version of the dataset with engineered features. This section is divided into 4 parts; they are: The first step is to load the raw dataset into memory. There are three main signal types in the raw data: total acceleration, body acceleration, and body gyroscope. Each has three axes of data. This means that there are a total of nine variables for each time step. Further, each series of data has been partitioned into overlapping windows of 2.65 seconds of data, or 128 time steps. These windows of data correspond to the windows of engineered features (rows) in the previous section. This means that one row of data has (128 * 9), or 1,152, elements. This is a little less than double the size of the 561 element vectors in the previous section and it is likely that there is some redundant data. The signals are stored in the /Inertial Signals/ directory under the train and test subdirectories. Each axis of each signal is stored in a separate file, meaning that each of the train and test datasets have nine input files to load and one output file to load. We can batch the loading of these files into groups given the consistent directory structures and file naming conventions. The input data is in CSV format where columns are separated by whitespace. Each of these files can be loaded as a NumPy array. The load_file() function below loads a dataset given the file path to the file and returns the loaded data as a NumPy array. We can then load all data for a given group (train or test) into a single three-dimensional NumPy array, where the dimensions of the array are [samples, time steps, features]. To make this clearer, there are 128 time steps and nine features, where the number of samples is the number of rows in any given raw signal data file. The load_group() function below implements this behavior. The dstack() NumPy function allows us to stack each of the loaded 3D arrays into a single 3D array where the variables are separated on the third dimension (features). We can use this function to load all input signal data for a given group, such as train or test. The load_dataset_group() function below loads all input signal data and the output data for a single group using the consistent naming conventions between the train and test directories. Finally, we can load each of the train and test datasets. The output data is defined as an integer for the class number. We must one hot encode these class integers so that the data is suitable for fitting a neural network multi-class classification model. We can do this by calling the to_categorical() Keras function. The load_dataset() function below implements this behavior and returns the train and test X and y elements ready for fitting and evaluating the defined models. Now that we have the data loaded into memory ready for modeling, we can define, fit, and evaluate a 1D CNN model. We can define a function named evaluate_model() that takes the train and test dataset, fits a model on the training dataset, evaluates it on the test dataset, and returns an estimate of the models performance. First, we must define the CNN model using the Keras deep learning library. The model requires a three-dimensional input with [samples, time steps, features]. This is exactly how we have loaded the data, where one sample is one window of the time series data, each window has 128 time steps, and a time step has nine variables or features. The output for the model will be a six-element vector containing the probability of a given window belonging to each of the six activity types. These input and output dimensions are required when fitting the model, and we can extract them from the provided training dataset. The model is defined as a Sequential Keras model, for simplicity. We will define the model as having two 1D CNN layers, followed by a dropout layer for regularization, then a pooling layer. It is common to define CNN layers in groups of two in order to give the model a good chance of learning features from the input data. CNNs learn very quickly, so the dropout layer is intended to help slow down the learning process and hopefully result in a better final model. The pooling layer reduces the learned features to 1/4 their size, consolidating them to only the most essential elements. After the CNN and pooling, the learned features are flattened to one long vector and pass through a fully connected layer before the output layer used to make a prediction. The fully connected layer ideally provides a buffer between the learned features and the output with the intent of interpreting the learned features before making a prediction. For this model, we will use a standard configuration of 64 parallel feature maps and a kernel size of 3. The feature maps are the number of times the input is processed or interpreted, whereas the kernel size is the number of input time steps considered as the input sequence is read or processed onto the feature maps. The efficient Adam version of stochastic gradient descent will be used to optimize the network, and the categorical cross entropy loss function will be used given that we are learning a multi-class classification problem. The definition of the model is listed below. The model is fit for a fixed number of epochs, in this case 10, and a batch size of 32 samples will be used, where 32 windows of data will be exposed to the model before the weights of the model are updated. Once the model is fit, it is evaluated on the test dataset and the accuracy of the fit model on the test dataset is returned. The complete evaluate_model() function is listed below. There is nothing special about the network structure or chosen hyperparameters; they are just a starting point for this problem. We cannot judge the skill of the model from a single evaluation. The reason for this is that neural networks are stochastic, meaning that a different specific model will result when training the same model configuration on the same data. This is a feature of the network in that it gives the model its adaptive ability, but requires a slightly more complicated evaluation of the model. We will repeat the evaluation of the model multiple times, then summarize the performance of the model across each of those runs. For example, we can call evaluate_model() a total of 10 times. This will result in a population of model evaluation scores that must be summarized. We can summarize the sample of scores by calculating and reporting the mean and standard deviation of the performance. The mean gives the average accuracy of the model on the dataset, whereas the standard deviation gives the average variance of the accuracy from the mean. The function summarize_results() below summarizes the results of a run. We can bundle up the repeated evaluation, gathering of results, and summarization of results into a main function for the experiment, called run_experiment(), listed below. By default, the model is evaluated 10 times before the performance of the model is reported. Now that we have all of the pieces, we can tie them together into a worked example. The complete code listing is provided below. Running the example first prints the shape of the loaded dataset, then the shape of the train and test sets and the input and output elements. This confirms the number of samples, time steps, and variables, as well as the number of classes. Next, models are created and evaluated and a debug message is printed for each. Finally, the sample of scores is printed followed by the mean and standard deviation. We can see that the model performed well achieving a classification accuracy of about 90.9% trained on the raw dataset, with a standard deviation of about 1.3. This is a good result, considering that the original paper published a result of 89%, trained on the dataset with heavy domain-specific feature engineering, not the raw dataset. Note: Given the stochastic nature of the algorithm, your specific results may vary. Now that we have seen how to load the data and fit a 1D CNN model, we can investigate whether we can further lift the skill of the model with some hyperparameter tuning. In this section, we will tune the model in an effort to further improve performance on the problem. We will look at three main areas: In the previous section, we did not perform any data preparation. We used the data as-is. Each of the main sets of data (body acceleration, body gyroscopic, and total acceleration) have been scaled to the range -1, 1. It is not clear if the data was scaled per-subject or across all subjects. One possible transform that may result in an improvement is to standardize the observations prior to fitting a model. Standardization refers to shifting the distribution of each variable such that it has a mean of zero and a standard deviation of 1. It really only makes sense if the distribution of each variable is Gaussian. We can quickly check the distribution of each variable by plotting a histogram of each variable in the training dataset. A minor difficulty in this is that the data has been split into windows of 128 time steps, with a 50% overlap. Therefore, in order to get a fair idea of the data distribution, we must first remove the duplicated observations (the overlap), then remove the windowing of the data. We can do this using NumPy, first slicing the array and only keeping the second half of each window, then flattening the windows into a long vector for each variable. This is quick and dirty and does mean that we lose the data in the first half of the first window. The complete example of loading the data, flattening it, and plotting a histogram for each of the nine variables is listed below. Running the example creates a figure with nine histogram plots, one for each variable in the training dataset. The order of the plots matches the order in which the data was loaded, specifically: We can see that each variable has a Gaussian-like distribution, except perhaps the first variable (Total Acceleration x). The distributions of total acceleration data is flatter than the body data, which is more pointed. We could explore using a power transform on the data to make the distributions more Gaussian, although this is left as an exercise. Histograms of each variable in the training data set The data is sufficiently Gaussian-like to explore whether a standardization transform will help the model extract salient signal from the raw observations. The function below named scale_data() can be used to standardize the data prior to fitting and evaluating the model. The StandardScaler scikit-learn class will be used to perform the transform. It is first fit on the training data (e.g. to find the mean and standard deviation for each variable), then applied to the train and test sets. The standardization is optional, so we can apply the process and compare the results to the same code path without the standardization in a controlled experiment. We can update the evaluate_model() function to take a parameter, then use this parameter to decide whether or not to perform the standardization. We can also update the run_experiment() to repeat the experiment 10 times for each parameter; in this case, only two parameters will be evaluated [False, True] for no standardization and standardization respectively. This will result in two samples of results that can be compared. We will update the summarize_results() function to summarize the sample of results for each configuration parameter and to create a boxplot to compare each sample of results. These updates will allow us to directly compare the results of a model fit as before and a model fit on the dataset after it has been standardized. It is also a generic change that will allow us to evaluate and compare the results of other sets of parameters in the following sections. The complete code listing is provided below. Running the example may take a few minutes, depending on your hardware. The performance is printed for each evaluated model. At the end of the run, the performance of each of the tested configurations is summarized showing the mean and the standard deviation. We can see that it does look like standardizing the dataset prior to modeling does result in a small lift in performance from about 90.4% accuracy (close to what we saw in the previous section) to about 91.5% accuracy. Note: Given the stochastic nature of the algorithm, your specific results may vary. A box and whisker plot of the results is also created. This allows the two samples of results to be compared in a nonparametric way, showing the median and the middle 50% of each sample. We can see that the distribution of results with standardization is quite different from the distribution of results without standardization. This is likely a real effect. Box and whisker plot of 1D CNN with and without standardization Now that we have an experimental framework, we can explore varying other hyperparameters of the model. An important hyperparameter for the CNN is the number of filter maps. We can experiment with a range of different values, from less to many more than the 64 used in the first model that we developed. Specifically, we will try the following numbers of feature maps: We can use the same code from the previous section and update the evaluate_model() function to use the provided parameter as the number of filters in the Conv1D layers. We can also update the summarize_results() function to save the boxplot as exp_cnn_filters.png. The complete code example is listed below. Running the example repeats the experiment for each of the specified number of filters. At the end of the run, a summary of the results with each number of filters is presented. We can see perhaps a trend of increasing average performance with the increase in the number of filter maps. The variance stays pretty constant, and perhaps 128 feature maps might be a good configuration for the network. A box and whisker plot of the results is also created, allowing the distribution of results with each number of filters to be compared. From the plot, we can see the trend upward in terms of median classification accuracy (orange line on the box) with the increase in the number of feature maps. We do see a dip at 64 feature maps (the default or baseline in our experiments), which is surprising, and perhaps a plateau in accuracy across 32, 128, and 256 filter maps. Perhaps 32 would be a more stable configuration. Box and whisker plot of 1D CNN with different numbers of filter maps The size of the kernel is another important hyperparameter of the 1D CNN to tune. The kernel size controls the number of time steps consider in each ¡°read¡± of the input sequence, that is then projected onto the feature map (via the convolutional process). A large kernel size means a less rigorous reading of the data, but may result in a more generalized snapshot of the input. We can use the same experimental set-up and test a suite of different kernel sizes in addition to the default of three time steps. The full list of values is as follows: The complete code listing is provided below: Running the example tests each kernel size in turn. The results are summarized at the end of the run. We can see a general increase in model performance with the increase in kernel size. The results suggest a kernel size of 5 might be good with a mean skill of about 91.8%, but perhaps a size of 7 or 11 may also be just as good with a smaller standard deviation. A box and whisker plot of the results is also created. The results suggest that a larger kernel size does appear to result in better accuracy and that perhaps a kernel size of 7 provides a good balance between good performance and low variance. Box and whisker plot of 1D CNN with different numbers of kernel sizes This is just the beginning of tuning the model, although we have focused on perhaps the more important elements. It might be interesting to explore combinations of some of the above findings to see if performance can be lifted even further. It may also be interesting to increase the number of repeats from 10 to 30 or more to see if it results in more stable findings. Another popular approach with CNNs is to have a multi-headed model, where each head of the model reads the input time steps using a different sized kernel. For example, a three-headed model may have three different kernel sizes of 3, 5, 11, allowing the model to read and interpret the sequence data at three different resolutions. The interpretations from all three heads are then concatenated within the model and interpreted by a fully-connected layer before a prediction is made. We can implement a multi-headed 1D CNN using the Keras functional API. For a gentle introduction to this API, see the post: The updated version of the evaluate_model() function is listed below that creates a three-headed CNN model. We can see that each head of the model is the same structure, although the kernel size is varied. The three heads then feed into a single merge layer before being interpreted prior to making a prediction. When the model is created, a plot of the network architecture is created; provided below, it gives a clear idea of how the constructed model fits together. Plot of the Multi-Headed 1D Convolutional Neural Network Other aspects of the model could be varied across the heads, such as the number of filters or even the preparation of the data itself. The complete code example with the multi-headed 1D CNN is listed below. Running the example prints the performance of the model each repeat of the experiment and then summarizes the estimated score as the mean and standard deviation, as we did in the first case with the simple 1D CNN. We can see that the average performance of the model is about 91.6% classification accuracy with a standard deviation of about 0.8. This example may be used as the basis for exploring a variety of other models that vary different model hyperparameters and even different data preparation schemes across the input heads. It would not be an apples-to-apples comparison to compare this result with a single-headed CNN given the relative tripling of the resources in this model. Perhaps an apples-to-apples comparison would be a model with the same architecture and the same number of filters across each input head of the model. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop one-dimensional convolutional neural networks for time series classification on the problem of human activity recognition. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning."
"mastery",2018-09-19,"How to Evaluate Machine Learning Algorithms for Human Activity Recognition","https://machinelearningmastery.com/evaluate-machine-learning-algorithms-for-human-activity-recognition/","Human activity recognition is the problem of classifying sequences of accelerometer data recorded by specialized harnesses or smart phones into known well-defined movements. Classical approaches to the problem involve hand crafting features from the time series data based on fixed-sized windows and training machine learning models, such as ensembles of decision trees. The difficulty is that this feature engineering requires deep expertise in the field. Recently, deep learning methods such as recurrent neural networks and one-dimensional convolutional neural networks, or CNNs, have been shown to provide state-of-the-art results on challenging activity recognition tasks with little or no data feature engineering, instead using feature learning on raw data. In this tutorial, you will discover how to evaluate a diverse suite of machine learning algorithms on the ¡®Activity Recognition Using Smartphones¡® dataset. After completing this tutorial, you will know: Let¡¯s get started. How to Evaluate Machine Learning Algorithms for Human Activity RecognitionPhoto by Murray Foubister, some rights reserved. This tutorial is divided into three parts; they are: Human Activity Recognition, or HAR for short, is the problem of predicting what a person is doing based on a trace of their movement using sensors. A standard human activity recognition dataset is the ¡®Activity Recognition Using Smart Phones¡¯ dataset made available in 2012. It was prepared and made available by Davide Anguita, et al. from the University of Genova, Italy and is described in full in their 2013 paper ¡°A Public Domain Dataset for Human Activity Recognition Using Smartphones.¡± The dataset was modeled with machine learning algorithms in their 2012 paper titled ¡°Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine.¡± The dataset was made available and can be downloaded for free from the UCI Machine Learning Repository: The data was collected from 30 subjects aged between 19 and 48 years old performing one of six standard activities while wearing a waist-mounted smartphone that recorded the movement data. Video was recorded of each subject performing the activities and the movement data was labeled manually from these videos. Below is an example video of a subject performing the activities while their movement data is being recorded. The six activities performed were as follows: The movement data recorded was the x, y, and z accelerometer data (linear acceleration) and gyroscopic data (angular velocity) from the smart phone, specifically a Samsung Galaxy S II. Observations were recorded at 50 Hz (i.e. 50 data points per second). Each subject performed the sequence of activities twice, once with the device on their left-hand-side and once with the device on their right-hand side. The raw data is not available. Instead, a pre-processed version of the dataset was made available. The pre-processing steps included: Feature engineering was applied to the window data, and a copy of the data with these engineered features was made available. A number of time and frequency features commonly used in the field of human activity recognition were extracted from each window. The result was a 561 element vector of features. The dataset was split into train (70%) and test (30%) sets based on data for subjects, e.g. 21 subjects for train and nine for test. Experiment results with a support vector machine intended for use on a smartphone (e.g. fixed-point arithmetic) resulted in a predictive accuracy of 89% on the test dataset, achieving similar results as an unmodified SVM implementation. The dataset is freely available and can be downloaded from the UCI Machine Learning repository. The data is provided as a single zip file that is about 58 megabytes in size. The direct link for this download is below: Download the dataset and unzip all files into a new directory in your current working directory named ¡°HARDataset¡±. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this section, we will develop code to load the feature-engineered version of the dataset and evaluate a suite of nonlinear machine learning algorithms, including SVM used in the original paper. The goal is to achieve at least 89% accuracy on the test dataset. The results of methods using the feature-engineered version of the dataset provide a baseline for any methods developed for the raw data version. This section is divided into five parts; they are: The first step is to load the train and test input (X) and output (y) data. Specifically, the following files: The input data is in CSV format where columns are separated via whitespace. Each of these files can be loaded as a NumPy array. The load_file() function below loads a dataset given the file path to the file and returns the loaded data as a NumPy array. We can call this function to load the X and y files for a given train or test set group, given the similarity in directory layout and filenames.<U+00A0> The load_dataset_group() function below will load both of these files for a group and return the X and y elements as NumPy arrays. This function can then be used to load the X and y elements for both the train and test groups. Finally, we can load both the train and test dataset and return them as NumPy arrays ready for fitting and evaluating machine learning models. We can call this function to load all of the required data; for example: Next, we can define a list of machine learning models to evaluate on this problem. We will evaluate the models using default configurations. We are not looking for optimal configurations of these models at this point, just a general idea of how well sophisticated models with default configurations perform on this problem. We will evaluate a diverse set of nonlinear and ensemble machine learning algorithms, specifically: Nonlinear Algorithms: Ensemble Algorithms: We will define the models and store them in a dictionary that maps the model object to a short name that will help in analyzing the results. The define_models() function below defines the eight models that we will evaluate. This function is quite extensible and you can easily update to define any machine learning models or model configurations you wish. The next step is to evaluate the defined models in the loaded dataset. This step is divided into the evaluation of a single model and the evaluation of all of the models. We will evaluate a single model by first fitting it on the training dataset, making a prediction on the test dataset, and then evaluating the prediction using a metric. In this case we will use classification accuracy that will capture the performance (or error) of a model given the balance observations across the six activities (or classes). The evaluate_model() function below implements this behavior, evaluating a given model and returning the classification accuracy as a percentage. We can now call the evaluate_model() function repeatedly for each of the defined model. The evaluate_models() function below implements this behavior, taking the dictionary of defined models, and returns a dictionary of model names mapped to their classification accuracy. Because the evaluation of the models may take a few minutes, the function prints the performance of each model after it is evaluated as some verbose feedback. The final step is to summarize the findings. We can sort all of the results by the classification accuracy in descending order because we are interested in maximizing accuracy. The results of the evaluated models can then be printed, clearly showing the relative rank of each of the evaluated models. The summarize_results() function below implements this behavior. We know that we have all of the pieces in place. The complete example of evaluating a suite of eight machine learning models on the feature-engineered version of the dataset is listed below. Running the example first loads the train and test datasets, showing the shape of each of the input and output components. The eight models are then evaluated in turn, printing the performance for each. Finally, a rank of the models by their performance on the test set is displayed. We can see that both the ExtraTrees ensemble method and the Support Vector Machines nonlinear methods achieve a performance of about 94% accuracy on the test set. This is a great result, exceeding the reported 89% by SVM in the original paper. The specific results may vary each time the code is run, given the stochastic nature of the algorithms. Nevertheless, given the size of the dataset, the relative relationships between the algorithm¡¯s performance should be reasonably stable. These results show what is possible given domain expertise in the preparation of the data and the engineering of domain-specific features. As such, these results can be taken as a performance upper-bound of what could be pursued through more advanced methods that may be able to automatically learn features as part of fitting the model, such as deep learning methods. Any such advanced methods would be fit and evaluated on the raw data from which the engineered features were derived. And as such, the performance of machine learning algorithms evaluated on that data directly may provide an expected lower bound on the performance of any more advanced methods. We will explore this in the next section. We can use the same framework for evaluating machine learning models on the raw data. The raw data does require some more work to load. There are three main signal types in the raw data: total acceleration, body acceleration, and body gyroscope. Each has three axes of data. This means that there are a total of nine variables for each time step. Further, each series of data has been partitioned into overlapping windows of 2.65 seconds of data, or 128 time steps. These windows of data correspond to the windows of engineered features (rows) in the previous section. This means that one row of data has 128 * 9 or 1,152 elements. This is a little less than double the size of the 561 element vectors in the previous section and it is likely that there is some redundant data. The signals are stored in the /Inertial Signals/ directory under the train and test subdirectories. Each axis of each signal is stored in a separate file, meaning that each of the train and test datasets have nine input files to load and one output file to load. We can batch the loading of these files into groups given the consistent directory structures and file naming conventions. First, we can load all data for a given group into a single three-dimensional NumPy array, where the dimensions of the array are [samples, time steps, features]. To make this clearer, there are 128 time steps and nine features, where the number of samples is the number of rows in any given raw signal data file. The load_group() function below implements this behavior. The dstack() NumPy function allows us to stack each of the loaded 3D arrays into a single 3D array where the variables are separated on the third dimension (features). We can use this function to load all input signal data for a given group, such as train or test. The load_dataset_group() function below loads all input signal data and the output data for a single group using the consistent naming conventions between the directories. Finally, we can load each of the train and test datasets. As part of preparing the loaded data, we must flatten the windows and features into one long vector. We can do this with the NumPy reshape function and convert the three dimensions of [samples, timesteps, features] into the two dimensions of [samples, timesteps * features]. The load_dataset() function below implements this behavior and returns the train and test X and y elements ready for fitting and evaluating the defined models. Putting this all together, the complete example is listed below. Running the example first loads the dataset. We can see that the raw train and test sets have the same number of samples as the engineered features (7352 and 2947 respectively) and that the three-dimensional data was loaded correctly. We can also see the flattened data and the 1152 input vectors that will be provided to the models. Next the eight defined models are evaluated in turn. The final results suggest that ensembles of decision trees perform the best on the raw data. Gradient Boosting and Extra Trees perform the best with about 87% and 86% accuracy, about seven points below the best performing models on the feature-engineered version of the dataset. It is encouraging that the Extra Trees ensemble method performed well on both datasets; it suggests it and similar tree ensemble methods may be suited to the problem, at least in this simplified framing. We can also see the drop of SVM to about 72% accuracy. The good performance of ensembles of decision trees may suggest the need for feature selection and the ensemble methods ability to select those features that are most relevant to predicting the associated activity. As noted in the previous section, these results provide a lower-bound on accuracy for any more sophisticated methods that may attempt to learn higher order features automatically (e.g. via feature learning in deep learning methods) from the raw data. In summary, the bounds for such methods extend on this dataset from about 87% accuracy with GBM on the raw data to about 94% with Extra Trees and SVM on the highly processed dataset, [87% to 94%]. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to evaluate a diverse suite of machine learning algorithms on the ¡®Activity Recognition Using Smartphones¡® dataset. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning."
"mastery",2018-09-17,"How to Model Human Activity From Smartphone Data","https://machinelearningmastery.com/how-to-model-human-activity-from-smartphone-data/","Human activity recognition is the problem of classifying sequences of accelerometer data recorded by specialized harnesses or smart phones into known well-defined movements. It is a challenging problem given the large number of observations produced each second, the temporal nature of the observations, and the lack of a clear way to relate accelerometer data to known movements. Classical approaches to the problem involve hand crafting features from the time series data based on fixed-sized windows and training machine learning models, such as ensembles of decision trees. The difficulty is that this feature engineering requires deep expertise in the field. Recently, deep learning methods such as recurrent neural networks and one-dimensional convolutional neural networks, or CNNs, have been shown to provide state-of-the-art results on challenging activity recognition tasks with little or no data feature engineering. In this tutorial, you will discover the ¡®Activity Recognition Using Smartphones¡® dataset for time series classification and how to load and explore the dataset in order to make it ready for predictive modeling. After completing this tutorial, you will know: Let¡¯s get started. How to Model Human Activity From Smartphone DataPhoto by photographer, some rights reserved. This tutorial is divided into 10 parts; they are: Human Activity Recognition, or HAR for short, is the problem of predicting what a person is doing based on a trace of their movement using sensors. Movements are often normal indoor activities such as standing, sitting, jumping, and going up stairs. Sensors are often located on the subject, such as a smartphone or vest, and often record accelerometer data in three dimensions (x, y, z). Human Activity Recognition (HAR) aims to identify the actions carried out by a person given a set of observations of him/herself and the surrounding environment. Recognition can be accomplished by exploiting the information retrieved from various sources such as environmental or body-worn sensors. <U+2014> A Public Domain Dataset for Human Activity Recognition Using Smartphones, 2013. The idea is that once the subject¡¯s activity is recognized and known, an intelligent computer system can then offer assistance. It is a challenging problem because there is no clear analytical way to relate the sensor data to specific actions in a general way. It is technically challenging because of the large volume of sensor data collected (e.g. tens or hundreds of observations per second) and the classical use of hand crafted features and heuristics from this data in developing predictive models. More recently, deep learning methods have been demonstrated successfully on HAR problems given their ability to automatically learn higher-order features. Sensor-based activity recognition seeks the profound high-level knowledge about human activities from multitudes of low-level sensor readings. Conventional pattern recognition approaches have made tremendous progress in the past years. However, those methods often heavily rely on heuristic hand-crafted feature extraction, which could hinder their generalization performance. [¡¦] Recently, the recent advancement of deep learning makes it possible to perform automatic high-level feature extraction thus achieves promising performance in many areas. <U+2014> Deep Learning for Sensor-based Activity Recognition: A Survey Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course A standard human activity recognition dataset is the ¡®Activity Recognition Using Smartphones¡® dataset made available in 2012. It was prepared and made available by Davide Anguita, et al. from the University of Genova, Italy and is described in full in their 2013 paper ¡°A Public Domain Dataset for Human Activity Recognition Using Smartphones.¡± The dataset was modeled with machine learning algorithms in their 2012 paper titled ¡°Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine.¡± The dataset was made available and can be downloaded for free from the UCI Machine Learning Repository: The data was collected from 30 subjects aged between 19 and 48 years old performing one of 6 standard activities while wearing a waist-mounted smartphone that recorded the movement data. Video was recorded of each subject performing the activities and the movement data was labeled manually from these videos. Below is an example video of a subject performing the activities while their movement data is being recorded. The six activities performed were as follows: The movement data recorded was the x, y, and z accelerometer data (linear acceleration) and gyroscopic data (angular velocity) from the smart phone, specifically a Samsung Galaxy S II. Observations were recorded at 50 Hz (i.e. 50 data points per second). Each subject performed the sequence of activities twice, once with the device on their left-hand-side and once with the device on their right-hand side. A group of 30 volunteers with ages ranging from 19 to 48 years were selected for this task. Each person was instructed to follow a protocol of activities while wearing a waist-mounted Samsung Galaxy S II smartphone. The six selected ADL were standing, sitting, laying down, walking, walking downstairs and upstairs. Each subject performed the protocol twice: on the first trial the smartphone was fixed on the left side of the belt and on the second it was placed by the user himself as preferred <U+2014> A Public Domain Dataset for Human Activity Recognition Using Smartphones, 2013. The raw data is not available. Instead, a pre-processed version of the dataset was made available. The pre-processing steps included: These signals were preprocessed for noise reduction with a median filter and a 3rd order low-pass Butterworth filter with a 20 Hz cutoff frequency. [¡¦] The acceleration signal, which has gravitational and body motion components, was separated using another Butterworth low-pass filter into body acceleration and gravity. <U+2014> A Public Domain Dataset for Human Activity Recognition Using Smartphones, 2013. Feature engineering was applied to the window data, and a copy of the data with these engineered features was made available. A number of time and frequency features commonly used in the field of human activity recognition were extracted from each window. The result was a 561 element vector of features. The dataset was split into train (70%) and test (30%) sets based on data for subjects, e.g. 21 subjects for train and nine for test. This suggests a framing of the problem where a sequence of movement activity is used as input to predict the portion (2.56 seconds) of the current activity being performed, where a model trained on known subjects is used to predict the activity from movement on new subjects. Early experiment results with a support vector machine intended for use on a smartphone (e.g. fixed-point arithmetic) resulted in a predictive accuracy of 89% on the test dataset, achieving similar results as an unmodified SVM implementation. This method adapts the standard Support Vector Machine (SVM) and exploits fixed-point arithmetic for computational cost reduction. A comparison with the traditional SVM shows a significant improvement in terms of computational costs while maintaining similar accuracy [¡¦] <U+2014> Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine, 2012. Now that we are familiar with the prediction problem, we will look at loading and exploring this dataset. The dataset is freely available and can be downloaded from the UCI Machine Learning repository. The data is provided as a single zip file that is about 58 megabytes in size. The direct link for this download is below: Download the dataset and unzip all files into a new directory in your current working directory named ¡°HARDataset¡°. Inspecting the decompressed contents, you will notice a few things: The contents of the ¡°train¡± and ¡°test¡± folders are similar (e.g. folders and file names), although with differences in the specific data they contain. Inspecting the ¡°train¡± folder shows a few important elements: The number of lines in each file match, indicating that one row is one record in each data file. The ¡°Inertial Signals¡± directory contains 9 files. The structure is mirrored in the ¡°test¡± directory. We will focus our attention on the data in the ¡°Inertial Signals¡± as this is most interesting in developing machine learning models that can learn a suitable representation, instead of using the domain-specific feature engineering. Inspecting a datafile shows that columns are separated by whitespace and values appear to be scaled to the range -1, 1. This scaling can be confirmed by a note in the README.txt file provided with the dataset. Now that we know what data we have, we can figure out how to load it into memory. In this section, we will develop some code to load the dataset into memory. First, we need to load a single file. We can use the read_csv() Pandas function to load a single data file and specify that the file has no header and to separate columns using white space. We can wrap this in a function named load_file(). The complete example of this function is listed below. Running the example loads the file ¡®total_acc_y_train.txt¡®, returns a NumPy array, and prints the shape of the array. We can see that the training data is comprised of 7,352 rows or windows of data, where each window has 128 observations. Next, it would be useful to load a group of files, such as all of the body acceleration data files as a single group. Ideally, when working with multivariate time series data, it is useful to have the data structured in the format: This is helpful for analysis and is the expectation of deep learning models such as convolutional neural networks and recurrent neural networks. We can achieve this by calling the above load_file() function multiple times, once for each file in a group. Once we have loaded each file as a NumPy array, we can combine or stack all three arrays together. We can use the dstack() NumPy function to ensure that each array is stacked in such a way that the features are separated in the third dimension, as we would prefer. The function load_group() implements this behavior for a list of file names and is listed below. We can demonstrate this function by loading all of the total acceleration files. The complete example is listed below. Running the example prints the shape of the returned NumPy array, showing the expected number of samples and time steps with the three features, x, y, and z for the dataset. Finally, we can use the two functions developed so far to load all data for the train and the test dataset. Given the parallel structure in the train and test folders, we can develop a new function that loads all input and output data for a given folder. The function can build a list of all 9 data files to load, load them as one NumPy array with 9 features, then load the data file containing the output class. The load_dataset() function below implements this behaviour. It can be called for either the ¡°train¡± group or the ¡°test¡± group, passed as a string argument. The complete example is listed below. Running the example loads the train and test datasets. We can see that the test dataset has 2,947 rows of window data. As expected, we can see that the size of windows in the train and test sets matches and the size of the output (y) in each the train and test case matches the number of samples. Now that we know how to load the data, we can start to explore it. A good first check of the data is to investigate the balance of each activity. We believe that each of the 30 subjects performed each of the six activities. Confirming this expectation will both check that the data is indeed balanced, making it easier to model, and confirm that we are correctly loading and interpreting the dataset. We can develop a function that summarizes the breakdown of the output variables, e.g. the y variable. The function class_breakdown() below implements this behavior, first wrapping the provided NumPy array in a DataFrame, grouping the rows by the class value, and calculating the size of each group (number of rows). The results are then summarized, including the count and the percentage. It may be useful to summarize the breakdown of the classes in the train and test datasets to ensure they have a similar breakdown, then compare the result to the breakdown on the combined dataset. The complete example is listed below. Running the example first summarizes the breakdown for the training set. We can see a pretty similar distribution of each class hovering between 13% and 19% of the dataset. The result on the test set and on both datasets together look very similar. It is likely safe to work with the dataset assuming the distribution of classes is balanced per train and test set and perhaps per subject. We are working with time series data, therefore an import check is to create a line plot of the raw data. The raw data is comprised of windows of time series data per variable, and the windows do have a 50% overlap. This suggests we may see some repetition in the observations as a line plot unless the overlap is removed. We can start off by loading the training dataset using the functions developed above. Next, we can load the ¡®subject_train.txt¡® in the ¡®train¡® directory that provides a mapping of rows to the subject to which it belongs. We can load this file using the load_file() function. Once loaded, we can also use the unique() NumPy function to retrieve a list of the unique subjects in the training dataset. Next, we need a way to retrieve all of the rows for a single subject, e.g. subject number 1. We can do this by finding all of the row numbers that belong to a given subject and use those row numbers to select the samples from the loaded X and y data from the training dataset. The data_for_subject() function below implements this behavior. It will take the loaded training data, the loaded mapping of row number to subjects, and the subject identification number for the subject that we are interested in, and will return the X and y data for only that subject. Now that we have data for one subject, we can plot it. The data is comprised of windows with overlap. We can write a function to remove this overlap and squash the windows down for a given variable into one long sequence that can be plotted directly as a line plot. The to_series() function below implements this behavior for a given variable, e.g. array of windows. Finally, we have enough to plot the data. We can plot each of the nine variables for the subject in turn and a final plot for the activity level. Each series will have the same number of time steps (length of x-axis), therefore, it may be useful to create a subplot for each variable and align all plots vertically so we can compare the movement on each variable. The plot_subject() function below implements this behavior for the X and y data for a single subject. The function assumes the same order of the variables (3rd axis) as was loaded in the load_dataset()<U+00A0>function. A crude title is also added to each plot so we don¡¯t get easily confused about what we are looking at. The complete example is listed below. Running the example prints the unique subjects in the training dataset, the sample of the data for the first subject, and creates one figure with 10 plots, one for each of the nine input variables and the output class. In the plot, we can see periods of large movement corresponding with activities 1, 2, and 3: the walking activities. We can also see much less activity (i.e. a relatively straight line) for higher numbered activities, 4, 5, and 6 (sitting, standing, and laying). This is good confirmation that we have correctly loaded interpreted the raw dataset. We can see that this subject has performed the same general sequence of activities twice, and some activities are performed more than two times. This suggests that for a given subject, we should not make assumptions about what activities may have been performed or their order. We can also see some relatively large movement for some stationary activities, such as laying. It is possible that these are outliers or related to activity transitions. It may be possible to smooth or remove these observations as outliers. Finally, we see a lot of commonality across the nine variables. It is very likely that only a subset of these traces are required to develop a predictive model. Line plot for all variables for a single subject We can re-run the example for another subject by making one small change, e.g. choose the identifier of the second subject in the training dataset. The plot for the second subject shows similar behavior with no surprises. The double sequence of activities does appear more regular than the first subject. Line plot for all variables for a second single subject As the problem is framed, we are interested in using the movement data from some subjects to predict activities from the movement of other subjects. This suggests that there must be regularity in the movement data across subjects. We know that the data has been scaled between -1 and 1, presumably per subject, suggesting that the amplitude of the detected movements will be similar. We would also expect that the distribution of movement data would be similar across subjects, given that they performed the same actions. We can check for this by plotting and comparing the histograms of the movement data across subjects. A useful approach would be to create one plot per subject and plot all three axis of a given data (e.g. total acceleration), then repeat this for multiple subjects. The plots can be modified to use the same axis and aligned horizontally so that the distributions for each variable across subjects can be compared. The plot_subject_histograms() function below implements this behavior. The function takes the loaded dataset and mapping of rows to subjects as well as a maximum number of subjects to plot, fixed at 10 by default. A plot is created for each subject and the three variables for one data type are plotted as histograms with 100 bins, to help to make the distribution obvious. Each plot shares the same axis, which is fixed at the bounds of -1 and 1. The complete example is listed below. Running the example creates a single figure with 10 plots with histograms for the three axis of the total acceleration data. Each of the three axes on a given plot have a different color, specifically x, y, and z are blue, orange, and green respectively. We can see that the distribution for a given axis does appear Gaussian with large separate groups of data. We can see some of the distributions align (e.g. main groups in the middle around 0.0), suggesting there may be some continuity of the movement data across subjects, at least for this data. Histograms of the total acceleration data for 10 subjects We can update the plot_subject_histograms() function to next plot the distributions of the body acceleration. The updated function is listed below. Running the updated example creates the same plot with very different results. Here we can see all data clustered around 0.0 across axis within a subject and across subjects. This suggests that perhaps the data was centered (zero mean). This strong consistency across subjects may aid in modeling, and may suggest that the differences across subjects in the total acceleration data may not be as helpful. Histograms of the body acceleration data for 10 subjects Finally, we can generate one final plot for the gyroscopic data. The updated function is listed below. Running the example shows very similar results to the body acceleration data. We see a high likelihood of a Gaussian distribution for each axis across each subject centered on 0.0. The distributions are a little wider and show fatter tails, but this is an encouraging finding for modeling movement data across subjects. Histograms of the body gyroscope data for 10 subjects We are interested in discriminating between activities based on activity data. The simplest case for this would be to discriminate between activities for a single subject. One way to investigate this would be to review the distribution of movement data for a subject by activity. We would expect to see some difference in the distribution between the movement data for different activities by a single subject. We can review this by creating a histogram plot per activity, with the three axis of a given data type on each plot. Again, the plots can be arranged horizontally to compare the distribution of each data axis by activity. We would expect to see differences in the distributions across activities down the plots. First, we must group the traces for a subject by activity. The data_by_activity() function below implements this behaviour. We can now create plots per activity for a given subject. The plot_activity_histograms() function below implements this function for the traces data for a given subject. First, the data is grouped by activity, then one subplot is created for each activity and each axis of the data type is added as a histogram. The function only enumerates the first three features of the data, which are the total acceleration variables. The complete example is listed below. Running the example creates the plot with six subplots, one for each activity for the first subject in the train dataset. Each of the x, y, and z axes for the total acceleration data have a blue, orange, and green histogram respectively. We can see that each activity has a different data distribution, with a marked difference between the large movement (first three activities) with the stationary activities (last three activities). Data distributions for the first three activities look Gaussian with perhaps differing means and standard deviations. Distributions for the latter activities look multi-modal (i.e. multiple peaks). Histograms of the total acceleration data by activity We can re-run the same example with an updated version of the plot_activity_histograms() that plots the body acceleration data instead. The updated function is listed below. Running the updated example creates a new plot. Here, we can see more similar distributions across the activities amongst the in-motion vs. stationary activities. The data looks bimodal in the case of the in-motion activities and perhaps Gaussian or exponential in the case of the stationary activities. The pattern we see with the total vs. body acceleration distributions by activity mirrors what we see with the same data types across subjects in the previous section. Perhaps the total acceleration data is the key to discriminating the activities. Histograms of the body acceleration data by activity Finally, we can update the example one more time to plot the histograms per activity for the gyroscopic data. The updated function is listed below. Running the example creates plots with the similar pattern as the body acceleration data, although showing perhaps fat-tailed Gaussian-like distributions instead of bimodal distributions for the in-motion activities. Histograms of the body gyroscope data by activity All of these plots were created for the first subject, and we would expect to see similar distributions and relationships for the movement data across activities for other subjects. A final area to consider is how long a subject spends on each activity. This is closely related to the balance of classes. If the activities (classes) are generally balanced within a dataset, then we expect the balance of activities for a given subject over the course of their trace would also be reasonably well balanced. We can confirm this by calculating how long (in samples or rows) each subject spends on each activity and look at the distribution of durations for each activity. A handy way to review this data is to summarize the distributions as boxplots showing the median (line), the middle 50% (box), the general extent of the data as the interquartile range (the whiskers), and outliers (as dots). The function plot_activity_durations_by_subject() below implements this behavior by first splitting the dataset by subject, then the subjects data by activity and counting the rows spent on each activity, before finally creating a boxplot per activity of the duration measurements. The complete example is listed below. Running the example creates six box plots, one for each activity. Each boxplot summarizes how long (in rows or the number of windows) subjects in the training dataset spent on each activity. We can see that the subjects spent more time on stationary activities (4, 5 and 6) and less time on the in motion activities (1, 2 and 3), with the distribution for 3 being the smallest, or where time was spent least. The spread across the activities is not large, suggesting little need to trim the longer duration activities or oversampling of the in-motion activities. Although, these approaches remain available if skill of a predictive model on the in-motion activities is generally worse. Boxplot of activity durations per subject on train set We can create a similar boxplot for the training data with the following additional lines. Running the updated example shows a similar relationship between activities. This is encouraging, suggesting that indeed the test and training dataset are reasonably representative of the whole dataset. Boxplot of activity durations per subject on test set Now that we have explored the dataset, we can suggest some ideas for how it may be modeled. In this section, we summarize some approaches to modeling the activity recognition dataset. These ideas are divided into the main themes of a project. The first important consideration is the framing of the prediction problem. The framing of the problem as described in the original work is the prediction of activity for a new subject given their movement data, based on the movement data and activities of known subjects. We can summarize this as: This is a reasonable and useful framing of the problem. Some other possible ways to frame the provided data as a prediction problem include the following: Some of these framings may be too challenging or too easy. Nevertheless, these framings provide additional ways to explore and understand the dataset. Some data preparation may be required prior to using the raw data to train a model. The data already appears to have been scaled to the range [-1,1]. Some additional data transforms that could be performed prior to modeling include: Generally, the problem is a time series multi-class classification problem. As we have seen, it may also be framed as a binary classification problem and a multi-step time series classification problem. The original paper explored the use of a classical machine learning algorithm on a version of the dataset where features were engineered from each window of data. Specifically, a modified support vector machine. The results of an SVM on the feature-engineered version of the dataset may provide a baseline in performance on the problem. Expanding from this point, the evaluation of multiple linear, non-linear, and ensemble machine learning algorithms on this version of the dataset may provide an improved benchmark. The focus of the problem may be on the un-engineered or raw version of the dataset. Here, a progression in model complexity may be explored in order to determine the most suitable model for the problem; some candidate models to explore include: The evaluation of the model in the original paper involved using a train/test split of the data by subject with a 70% and 30% ratio. Exploration of this pre-defined split of the data suggests that both sets are reasonably representative of the whole dataset. Another alternative methodology may be to use leave-one-out cross-validation, or LOOCV, per subject. In addition to giving the data for each subject the opportunity for being used as the withheld test set, the approach would provide a population of 30 scores that can be averaged and summarized, which may offer a more robust result. Model performance was presented using classification accuracy and a confusion matrix, both of which are suitable for the multi-class nature of the prediction problem. Specifically, the confusion matrix will aid in determining whether some classes are easier or more challenging to predict than others, such as those for stationary activities versus those activities that involve motion. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered the Activity Recognition Using Smartphones Dataset for time series classification and how to load and explore the dataset in order to make it ready for predictive modeling. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Jason, nice peace of work like always. I only wish that you have had the code for communicating to the phone (android for instance) instead of loading sampled data. Thanks. Good suggestion! There are several open sources  demoing how to collect data from the phone (cf github).
In my app (under development) I am collecting accel data (using a sliding window of 10 sec), and upon certain trigger, save to temporary file and upload to google cloud. Not trivial task due to all the pesky security.
If this is on interest to anyone, please let me know and I will try to derive a sample app. Sounds like a challenging problem! Thank you very much , please if you have any article about activity recognition or anomaly detection from activity let me know thank you again. I have a number of posts coming on activity recognition. I also cover the topic in detail in this book:https://machinelearningmastery.com/deep-learning-for-time-series-forecasting/ Thanks for sharing it. You¡¯re welcome. Thank you Jason. You¡¯re welcome. Is there a similar dataset for falls of sorts? I found a lot of references regarding fall detection of elderly people but not of younger (20-60 year old). Specifically I want to identify fall during riding (which is NOT easier as one expects since I don¡¯t have velocity data). Thanks for the blog posts! I expect there is, I don¡¯t know off hand, perhaps try a search? I have implemented a model using Keras to model a LSTM for this same problem, you can take a look at https://github.com/servomac/Human-Activity-Recognition. Nice work! Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning."
