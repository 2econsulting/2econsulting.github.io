"site","date","headline","url_address","text"
"vidhya",2018-12-21,"DataHack Radio #14: Quantum Computing and Quantum Machine Learning with Dr. Mandaar Pande","https://www.analyticsvidhya.com/blog/2018/12/datahack-radio-quantum-machine-learning/","

New Year's Grand Sale - 40% Discount On All Courses (Use Coupon: HNY2019) Click To Enroll Today !

 Quantum computing and quantum machine learning <U+2013> most of us have come across these concepts at some point without getting the opportunity to delve deeper. But what if I told you that these could potentially disrupt the way we see and use technology? We are joined by Dr. Mandaar Pande in episode #14 of the DataHack Radio podcast, where he navigates us through the wonderfully complex world of quantum computing. Here¡¯s a mind-blowing fact to give you a taste of what to expect: ¡°The number of bits in a 300 qubit quantum computer will be more than the known atoms in the universe.¡±<U+00A0> <U+2013> Dr. Mandaar Pande Before I personally met Dr. Mandaar at DataHack Summit 2018 (where he also spoke on this subject), I only had a vague sense of what quantum computers are and the gigantic amount of power they can process. But as you¡¯ll soon find out, there¡¯s a lot more that goes on behind the scenes that one might never have thought of. I have briefly covered the main topics discussed in this episode but the true joy and knowledge lies in listening to Dr. Mandaar himself. Subscribe to DataHack Radio today and listen to this, as well as all previous episodes, on any of the below platforms: Dr. Mandaar holds a Ph.D degree in theoretical physics from the University of Hyderabad, with a specialization in non-linear optics. After completing his Ph.D in 1994, he took up a post as a lecturer at BITS Pilani for the next four years in the EEE department (electrical engineering). Experienced folks will recall that it was in the late ¡¯90s when IT started picking up steam in India, and Dr. Mandaar decided to take the plunge and explore other avenues outside of academia. He joined Tech Mahindra in 1998 as part of the modeling and simulation centre. There, he worked<U+00A0>in the capacity of Group Head and Principal Consultant in the area of Performance<U+00A0>Engineering and Management. Following a 12 year stint at Tech Mahindra, he spent 7 years at Wipro <U+2013> first as a Lead Architect, then as the Global Practice Head for Performance Engineering <U+2013> Quality Engineering and Testing. Dr. Mandaar¡¯s experience in this field, as you can tell, is incredibly rich and not many folks come close to rivalling his experience and know-how. But for Dr. Mandaar, it felt inevitable that he would return to academia at some point and he joined Symbiosis as the professor of IT last year. So where does his interest and passion for quantum computing and machine learning fit into the picture? Well, towards his final years with Wipro, he got some exposure to the digital way of working (and data science was a big part of that). He built up an interest while working there, and kept that up during his transition back to academia with Symbiosis. His Ph.D in quantum optics obviously helped while he pursued quantum computing. But what is this field exactly? And how does it tie into machine learning? Let¡¯s hear that from Dr. Mandaar himself: ¡°Quantum Computing is a field that is at the intersection of quantum physics, information science, as well as function theory. And one of the largest applications of quantum computing in the near future is going to be quantum machine learning.<U+00A0>¡° This is a tough one, and a question I have been wondering about ever since I heard about this subject. Below are the two key points Dr. Mandaar mentioned: There¡¯s a lot more to it, but if you don¡¯t possess a solid base in these two aspects, it¡¯s going to be next to impossible to make headway. The current version of digital devices we use, like computers and smartphones, use metal chips in them (which are based on integrated circuits that are in turn based on transistors). Whatever computing we do today, including the data we capture and the analysis we perform, is done using the 2 bits we see in these transistors <U+2013> 0 and 1. Any algorithm that we write eventually gets broken down into 0 and 1 at the machine level. At the most fundamental level, the physical principles that govern the nature in quantum computing follow the laws of quantum mechanics. So then what is quantum mechanics? It¡¯s a theory in physics that describes nature at the smallest level (atoms and molecules). At this really basic level, these particles behave very differently. Now, there are a couple of things that are very important to understand: This is just a taste of what Dr. Mandaar described in the podcast. He broke down this complex topic into easy-to-digest bits of information using examples. If you¡¯ve ever wondered how quantum computers work from the ground up, this section will feel like you¡¯ve hit the jackpot."
"vidhya",2018-12-19,"A Technical Overview of AI & ML (NLP, Computer Vision, Reinforcement Learning) in 2018 & Trends for 2019","https://www.analyticsvidhya.com/blog/2018/12/key-breakthroughs-ai-ml-2018-trends-2019/","

New Year's Grand Sale - 40% Discount On All Courses (Use Coupon: HNY2019) Click To Enroll Today !

 The last few years have been a dream run for Artificial Intelligence enthusiasts and machine learning professionals. These technologies have evolved from being a niche to becoming mainstream, and are impacting millions of lives today. Countries now have dedicated AI ministers and budgets to make sure they stay relevant in this race. The same has been true for a data science professional. A few years back <U+2013> you would have been comfortable knowing a few tools and techniques. Not anymore! There is so much happening in this domain and so much to keep pace with <U+2013> it feels mind boggling at times. This is why I thought of taking a step back and looking at the developments in some of the key areas in Artificial Intelligence from a data science practitioners¡¯ perspective. What were these breakthroughs? What happened in 2018 and what can be expected in 2019? Read this article to find out! P.S. As with any forecasts, these are my takes. These are based on me trying to connect the dots. If you have a different perspective <U+2013> I would love to hear it. Do let me know what you think might change in 2019. Making machines parse words and sentences has always seemed like a dream. There are way too many nuances and aspects of a language that even humans struggle to grasp at times. But 2018 has truly been a watershed moment for NLP. We saw one remarkable breakthrough after another <U+2013> ULMFiT, ELMO, OpenAI¡¯s Transformer and Google¡¯s BERT to name a few. The successful application of transfer learning (the art of being able to apply pretrained models to data) to NLP tasks has blown open the door to potentially unlimited applications. Our podcast with Sebastian<U+00A0>Ruder further cemented our belief in how far his field has traversed in recent times. As a side note, that¡¯s a<U+00A0>must-listen podcast for all NLP enthusiasts. Let¡¯s look at some of these key developments in a bit more detail. And if you¡¯re looking to learn the ropes in NLP and are looking for a place to get started, make sure you head over to this ¡®NLP using Python¡® course. It¡¯s as good a place as any to start your text-fuelled journey! Designed by Sebastian Ruder and fast.ai¡¯s Jeremy Howard, ULMFiT was the first framework that got the NLP transfer learning party started this year. For the uninitiated, it stands for Universal Language Model Fine-Tuning. Jeremy and Sebastian have truly put the word Universal in ULMFiT <U+2013> the framework can be applied to almost any NLP task! The best part about ULMFiT and the subsequent frameworks we¡¯ll see soon? You don¡¯t need to train models from scratch! These researchers have done the hard bit for you <U+2013> take their learning and apply it in your own projects. ULMFiT outperformed state-of-the-art methods in six text classification tasks. You can read this excellent tutorial by Prateek Joshi on how to get started with ULMFiT for any text classification problem. Want to take a guess at what ELMo stands for? It¡¯s short for Embeddings from Language Models. Pretty creative, eh? Apart from it¡¯s name resembling the famous Sesame Street character, ELMo grabbed the attention of the ML community as soon as it was released. ELMo uses language models to obtain embeddings for each word while also considering the context in which the word fits into the sentence or paragraph. Context is such a crucial aspect of NLP that most people failed to grasp before. ELMo uses bi-directional LSTMs to create the embeddings. Don¡¯t worry if that sounds like a mouthful <U+2013> check out this article to get a really simple overview of what LSTMs are and how they work. Like ULMFiT, ELMo significantly improves the performance of a wide variety of NLP tasks, like sentiment analysis and question answering. Read more about it here. Quite a few experts have claimed that the release of BERT marks a new era in NLP. Following ULMFiT and ELMo, BERT really blew away the competition with it¡¯s performance. As the original paper states, ¡°BERT is conceptually simple and empirically powerful¡±. BERT obtained state-of-the-art results on 11 (yes, 11!) NLP tasks. Check out their results on the SQuAD benchmark: Interested in getting started? You can use either the PyTorch implementation or Google¡¯s own TensorFlow code to try and replicate the results on your own machine. I¡¯m fairly certain you are wondering what BERT stands for at this point. <U+0001F642> It¡¯s<U+00A0>Bidirectional Encoder Representations from Transformers. Full marks if you got it right the first time. How could Facebook stay out of the race? They have open-sourced their own deep learning NLP framework called PyText. It was released earlier this week so I¡¯m still to experiment with it, but the early reviews are extremely promising. According to research published by FB, PyText has led to a 10% increase in accuracy of conversational models and reduced the training time as well. PyText is actually behind a few of Facebook¡¯s own products like the FB Messenger. So working on this adds some real-world value to your own portfolio (apart from the invaluable knowledge you¡¯ll gain obviously). You can try it out yourself by downloading the code from this GitHub repo. If you haven¡¯t heard of Google Duplex yet, where have you been?! Sundar Pichai knocked it out of the park with this demo and it has been in the headlines ever since: Since this is a Google product, there¡¯s a slim chance of them open sourcing the code behind it. But wow! That¡¯s a pretty awesome audio processing application to showcase. Of course it raises a lot of ethical and privacy questions, but that¡¯s a discussion for later in this article. For now, just revel in how far we have come with ML in recent years. Who better than Sebastian Ruder himself to provide a handle on where NLP is headed in 2019? Here are his thoughts: This is easily the most popular field right now in the deep learning space. I feel like we have plucked the low-hanging fruits of computer vision to quite an extent and are already in the refining stage. Whether it¡¯s image or video, we have seen a plethora of frameworks and libraries that have made computer vision tasks a breeze. We at Analytics Vidhya spent a lot of time this year working on democratizing these concepts. Check out our computer vision specific articles here, covering topics from object detection in videos and images to lists of pretrained models to get your deep learning journey started. Here¡¯s my pick of the best developments we saw in CV this year. And if you¡¯re curious about this wonderful field (actually going to become one of the hottest jobs in the industry soon), then go ahead and start your journey with our ¡®Computer Vision using Deep Learning¡¯ course. Ian Goodfellow designed GANs in 2014, and the concept has spawned multiple and diverse applications since. Year after year we see the original concept being tweaked to fit a practical use case. But one thing has remained fairly consistent till this year <U+2013> images generated by machines were fairly easy to spot. There would always be some inconsistency in the frame which made the distinction fairly obvious. But that boundary has started to seep away in recent months. And with the creation of BigGANs, that boundary could be removed permanently. Check out the below images generated using this method: Unless you take a microscope to it, you won¡¯t be able to tell if there¡¯s anything wrong with that collection. Concerning or exciting? I¡¯ll leave that up to you, but there¡¯s no doubt GANs are changing the way we perceive digital images (and videos). For the data scientists out there, these models were trained on the ImageNet dataset first and then the JFT-300M data to showcase that these models transfer well from one set to the other. I would also to direct you to the GAN Dissection page <U+2013> a really cool way to visualize and understand GANs. This was a really cool development. There is a very common belief that you need a ton of data along with heavy computational resources to perform proper deep learning tasks. That includes training a model from scratch on the ImageNet dataset. I understand that perception <U+2013> most of us thought the same before a few folks at fast.ai found a way to prove all of us wrong. Their model gave an accuracy of 93% in an impressive 18 minutes timeframe. The hardware they used, detailed in their<U+00A0>blog post, contained 16 public AWS cloud instances, each with 8 NVIDIA V100 GPUs. They built the algorithm using the fastai and PyTorch libraries. The total cost of putting the whole thing together came out to be just<U+00A0>$40!<U+00A0>Jeremy has described their approach, including techniques, in much more detail<U+00A0>here. A win for everyone! Image processing has come leaps and bounds in the last 4-5 years, but what about video? Translating methods from a static frame to a dynamic one has proved to be a little tougher than most imagined. Can you take a video sequence and predict what will happen in the next frame? It had been explored before but the published research had been vague, at best. NVIDIA decided to open source their approach earlier this year, and it was met with widespread praise. The goal of their vid2vid approach is to learn a mapping function from a given input video in order to produce an output video which depicts the contents of the input video with incredible precision. You can try out their PyTorch implementation available on their GitHub here. Like I mentioned earlier, we might see modifications rather than inventions in 2019. It might feel like more of the same <U+2013> self-driving cars, facial recognition algorithms, virtual reality, etc. Feel free to disagree with me here and add your point of view <U+2013> I would love to know what else we can expect next year that we haven¡¯t already seen. Drones, pending political and government approvals, might finally get the green light in the United States (India is far behind there). Personally, I would like to see a lot of the research being implemented in real-world scenarios. Conferences like CVPR and ICML portray the latest in this field but how close are those projects to being used in reality? Visual question answering and visual dialog systems could finally make their long-awaited debut soon. These systems lack the ability to generalize but the expectation is that we¡¯ll see an integrated multi-modal approach soon. Self-supervised learning came to the forefront this year. I can bet on that being used in far more studies next year. It¡¯s a really cool line of learning <U+2013> the labels are directly determined from the data we input, rather than wasting time labelling images manually. Fingers crossed! This section will appeal to all data science professionals. Tools and libraries are the bread and butter of data scientists. I have been part a part of plenty of debates about which tool is the best, which framework supersedes the other, which library is the epitome of economical computations, etc. I¡¯m sure quite a lot of you will be able to relate to this as well. But one thing we can all agree on <U+2013> we need to be on top of the latest tools in the field, or risk being left behind. The pace with which Python has overtaken everything else and planted itself as the industry leader is example enough of this. Of course a lot of this comes down to subjective choices (what tool is your organization using, how feasible is it to switch from the current framework to a new one, etc.), but if you aren¡¯t even considering the state-of-the-art out there, then I implore you to start NOW. So what made the headlines this year? Let¡¯s find out! What¡¯s all the hype about PyTorch? I¡¯ve mentioned it multiple times already in this article (and you¡¯ll see more instances later). I¡¯ll leave it to my colleague Faizan Shaikh to acquaint you with the framework. That¡¯s one of my favorite deep learning articles on AV <U+2013> a must-read! Given how slow TensorFlow can be at times, it opened the door for PyTorch to capture the deep learning market in double-quick time. Most of the code that I see open soruced on GitHub is a PyTorch implemnantation of the concept. It¡¯s not a coincidence <U+2013> PyTorch is super flexible and the latest version (v1.0) already powers many Facebook products and services at scale, including performing 6 billion text translations a day. PyTorch¡¯s adoption rate is only going to go up in 2019 so now is as good a time as any to get on board. Automated machine learning (or AutoML) has been gradually making inroads in the last couple of years. Companies like RapidMiner, KNIME, DataRobot and H2O.ai have released excellent products showcasing the immense potential of this service. Can you imagine working on a ML project where you only need to work with a drag-and-drop interface without coding? It¡¯s a scenario that¡¯s not too far off in the future. But apart from these companies, there was a significant release in the ML/DL space <U+2013> Auto Keras! It¡¯s an open source library for performing AutoML tasks. The idea behind it is to make deep learning accessible to domain experts who perhaps don¡¯t have a ML background. Make sure you check it out here. It is primed to make a huge run in the coming years. We¡¯ve been building and designing machine learning and deep learning models in our favorite IDEs and notebooks since we got into this line of work. How about taking a step out and trying something different? Yes, I¡¯m talking about performing deep learning in your web browser itself! This is now a reality thanks to the release of TensorFlow.js. That link has a few demos as well which demonstrate how cool this open source concept is. There are primarily three advantages/features of TensorFlow.js: I wanted to focus particularly on AutoML in this thread. Why? Because I feel it¡¯s going to be a real-game changer in the data science space in the next few years. But dont just take my word for it! Here¡¯s H2O.ai¡¯s Marios Michailidis, Kaggle Grandmaster, with his view of what to expect from AutoML in 2019: Machine learning continues its march into being one of the most important trends of the future <U+2013> of where the world is going towards to. This expansion has increased the demand for skilled applications in this space. Given its growth , it is imperative that automation is the key into utilising the data science resources as best as possible. The applications are limitless: Credit, insurance, fraud, computer vision, acoustics,sensors, recommenders, forecasting, NLP <U+2013> you name it. It is a privilege to be working in this space . The trends that will continue being important can be defined as: If I had to pick one field where I want to see more penetration, it would be reinforcement learning. Apart from the occasional headlines we see at irregular intervals, there hasn¡¯t yet been a game-changing breakthrough. The general perception I have seen in the community is that it¡¯s too math-heavy and there are no real industry applications to work on. While this is true to a certain extent, I would love to see more practical use cases coming out of RL next year. In my monthly GitHub and Reddit series, I tend to keep at least one repository or discussion on RL to at least foster a discussion around the topic. This might well be the next big thing to come out of all that research. OpenAI have released a really helpful toolkit to get beginners started with the field, which I have mentioned below. You can also check out this beginner-friendly introduction on the topic (it has been super helpful for me). If there¡¯s anything I have missed, would love to hear your thoughts on it. If research in RL has been slow, the educational material around it has been minimal (at best). But true to their word, OpenAI have open sourced some awesome material on the subject. They are calling this project ¡®Spinning Up in Deep RL¡¯ and you can read all about it here. It¡¯s actually quite a comprehensive list of resources on RL and they have attempted to keep the code and explanations as simple as possible. There is quite a lot of material which includes things like RL terminologies, how to grow into an RL research role, a list of important papers, a supremely well-documented code repository, and even a few exercised to get you started. No more procrastinating now <U+2013> if you were planning to get started with RL, your time has come! To accelerate research and get the community more involved in reinforcement learning, the Google AI team has open sourced Dopamine, a TensorFlow framework that aims to create research by making it more flexible and reproducible. You can find the entire training data along with the TensorFlow code (just 15 Python notebooks!) on this GitHub repository. Here¡¯s the perfect platform for performing easy experiments in a controlled and flexible environment. Sounds like a dream for any data scientist. Xander Steenbrugge, speaker at DataHack Summit 2018 and founder of the ArxivInsights channel, is quite the expert in reinforcement learning. Here are his thoughts on the current state of RL and what to expect in 2019: BONUS: Check out Xander¡¯s video about overcoming sparse rewards in Deep RL (the first challenge highlighted above). Imagine a world ruled by algorithms that dictate every action humans take. Not exactly a rosy scenario, is it? Ethics in AI is a topic we at Analytics Vidhya have always been keen to talk about. It becomes bogged down amid all the technical discussions when it should be considered along with those topics. Quite a few organizations were left with egg on their face this year with Facebook¡¯s Cambridge Analytica scandal and Google¡¯s internal rife about designing weapons headlining the list of scandals. But all of this led to the big tech companies penning down charters and guidelines they intend to follow. There isn¡¯t one out-of-the-box solution or one size fits all solution to handling the ethical aspect of AI. It requires a nuanced approach combined with a structured path put forward by the leadership. Let¡¯s see a couple of major moves that shook the landscape earlier this year. It was heartening to see the big corporations putting emphasis on this side of AI (even though the road that led to this point wasn¡¯t pretty). I want to direct your attention to the guidelines and principles released by a couple of these companies: These all essentially talk about fairness in AI and when and where to draw the line. Always a good idea to reference them when you¡¯re starting a new AI based project. GDPR, or the General Data Protection Regulation, has definitely had an impact on the way data is collected for building AI applications. GDPR came into play to ensure users have more control over their data (what information is collected and shared about them). So how does that affect AI? Well, if the data scientist does not have data (or enough of it), building any model becomes a non-starter. This has certainly put a spanner in the works of how social platforms and other sites used to work. GDPR will make for a fascinating case study down the line but for now, it has limited the usefulness of AI for a lot of platforms. This is a bit of a grey field. Like I mentioned, there¡¯s no one solution to it. We have to come together as a community to integrate ethics within AI projects. How can we make that happen? As Analytics Vidhya¡¯s Founder and CEO Kunal Jain highlighted in his talk at DataHack Summit 2018, we will need to pen down a framework which others can follow. I expect to see new roles being added in organizations that primarily deal with ethical AI. Corporate best practices will need to be re-structured and governance approaches re-drawn as AI becomes central to the company¡¯s vision. I also expect the Government to play a more active role in this regard with new or modified policies coming into play. 2019 will be a very interesting year, indeed. Impactful <U+2013> the only word that succinctly describes the amazing developments in 2018. I¡¯ve become an avid user of ULMFiT this year and I¡¯m looking forward to exploring BERT soon. Exciting times, indeed. I would love to hear from you as well! What developments did you find the most useful? Are you working on any project using the frameworks/tools/concepts we saw in this article? And what are your predictions for the coming year? I look forward to hearing your thoughts and ideas in the comments section below."
"vidhya",2018-12-19,"Top Highlights from the Amazing Machine Learning Tutorials Presented at NeurIPS (NIPS) 2018","https://www.analyticsvidhya.com/blog/2018/12/top-highlights-tutorials-neurips-2018/","

New Year's Grand Sale - 40% Discount On All Courses (Use Coupon: HNY2019) Click To Enroll Today !

 NeurIPS (formerly called NIPS <U+2013> Neural Information Processing Systems) is one of the premier machine learning conferences in the world. Researchers from across the globe present their latest projects in this field, but getting past the review screening? Not so easy. Thousands of papers are submitted every year out of which only a handful make the final conference. The audience tickets for NeurIPS 2018 sold out within 12 minutes of the portal being opened! That might give you an inkling of how popular this annual conference is. For those who couldn¡¯t be there <U+2013> we are thrilled to present a quick summary of the best tutorials from NeurIPS 2018! This year¡¯s edition was held in Montreal, Canada between 2nd to 8th December. There were a variety of topics being showcased <U+2013> from fairness and transparency in AI to visualizing deep learning models. You can check out the full schedule here. There are hours and hours of videos, so our team went through all of them to bring you the best in the form of this article. Note: We have embedded the videos for most sessions as well. A couple of videos are not being embedded due to some technical issue with FB¡¯s video platform, and we have provided their direct links. While the summary is a good starting point, we encourage everyone to watch the videos as well <U+2013> this is a great chance to learn from the top minds in this field. Speakers: Frank Hutter and Joaquin Vanschoren (function(d, s, id) {  var js, fjs = d.getElementsByTagName(s)[0];  if (d.getElementById(id)) return;  js = d.createElement(s); js.id = id;  js.src = 'https://connect.facebook.net/mr_IN/sdk.js#xfbml=1&version=v3.2';  fjs.parentNode.insertBefore(js, fjs);}(document, 'script', 'facebook-jssdk')); Tutorial Session: Automatic Machine Learning Welcome back to the NeurIPS 2018 Tutorial SessionThis tutorial Automatic Machine Learning will cover the methods underlying the current state of the art in this fast-paced field.Persented by Frank Hutter (University of Freiburg) and Joaquin Vanschoren (Eindhoven University of Technology, OpenML) Neural Information Processing Systems <U+092F><U+093E><U+0902><U+0928><U+0940> <U+0935><U+0930> <U+092A><U+094B><U+0938><U+094D><U+091F> <U+0915><U+0947><U+0932><U+0947> <U+0938><U+094B><U+092E><U+0935><U+093E><U+0930>, <U+0969> <U+0921><U+093F><U+0938><U+0947><U+0902><U+092C><U+0930>, <U+0968><U+0966><U+0967><U+096E> Building an end-to-end machine learning model involves a number of steps, such as preprocessing data, creating features, selecting model, and tuning the hyperparameters. Automatic Machine Learning, or AutoML, aims to automate these processes <U+2013> this tutorial covers methods underlying the state-of-the-art in AutoML. Quite a relevant topic in today¡¯s environment. Frank Hutter kicked off the tutorial by discussing the various applications of deep learning and an expert¡¯s role in building a successful model. This can potentially be replaced by an AutoML service that tries to learn the features, architecture and parameters to use based on the raw data that we provide. Followed by this basic introduction to AutoML, Frank spoke about the types of hyperparameters and modern approaches to Hyperparameter Optimisation. This is broadly divided into three sub-topics: The next topic Frank covered was about Neural Architecture, which is again divided into three parts <U+2013> Search Space Design, Blackbox optimization and Beyond Blackbox optimization. After a short Q&A session with Frank, Joaquin<U+00A0>Vanschoren took over for the second half of the tutorial. His focus was mainly on Meta-Learning. He spoke about various approaches, configuration space design, surrogate model transfer and warm-started multi-task learning. Joaquin further discussed the Learning Pipeline followed by transfer learning and transfer features. He also spent some time discussing topics like gradient descent and LSTM meta-learner. Speakers: Deirdre Mulligan, Nitin Kohli, Joshua A. Kroll (function(d, s, id) {  var js, fjs = d.getElementsByTagName(s)[0];  if (d.getElementById(id)) return;  js = d.createElement(s); js.id = id;  js.src = 'https://connect.facebook.net/mr_IN/sdk.js#xfbml=1&version=v3.2';  fjs.parentNode.insertBefore(js, fjs);}(document, 'script', 'facebook-jssdk')); Tutorial Session: Common Pitfalls for Studying the Human Side of Machine Learning Welcome Back to the NeurIPS 2018 Tutorial Sessions.This tutorial Common Pitfalls for Studying the Human Side of Machine Learning will present common misconceptions machine learning researchers and practitioners hold.Presented by Deirdre Mulligan (UC Berkley), Nitin Kohli (UC Berkley) and Joshua A. Kroll (Princeton University) Neural Information Processing Systems <U+092F><U+093E><U+0902><U+0928><U+0940> <U+0935><U+0930> <U+092A><U+094B><U+0938><U+094D><U+091F> <U+0915><U+0947><U+0932><U+0947> <U+0938><U+094B><U+092E><U+0935><U+093E><U+0930>, <U+0969> <U+0921><U+093F><U+0938><U+0947><U+0902><U+092C><U+0930>, <U+0968><U+0966><U+0967><U+096E> Tutorial summary: Machine learning is being used in almost every domain in the industry and researchers all over the world are examining how it can affect people and society. Ethics, essentially, should be at the heart of every ML project. The main idea behind this tutorial was to put forward some common misconceptions machine learning researchers and practitioners hold when thinking about certain topics. This is a video we implore everyone to watch! Some of the terms, like fairness, accountability, transparency and interpretability, are often reused to represent different meanings which may cause an unnecessary misunderstanding. This tutorial examined how the same words can be used to refer to different ideas. The presenters also showcased a few case studies where these learnings are being applied to ML problems. The session started with focusing on the necessity of having certain definitions for the terms we use and how these terms carry different meanings for different people. We were introduced to the term ¡®sociotechnical¡¯. To explain the concept better, Nitin Kohli took the term ¡®Fairness¡¯ and showcased how it can come across differently for statisticians, computer scientists, or lawyers. The next example, quite naturally, was of the word ¡®Transparency¡¯. The presenters picked up some very common examples to differentiate the meaning of the word for someone working in the government sector, to a machine learning engineer. Post this, Nitin described the word ¡®Explanation¡¯ and its various types with suitable instances. They also spoke about the terms ¡®accountability¡¯ and ¡®interpretability¡¯ during the session and the Q&A that followed was pretty informative as well. Speakers: John Shawe-Taylor, Omar Rivasplata Tutorial summary: John Shawe-Taylor initiated the tutorial by giving an introduction to statistical learning theory (SLT) followed by some basic definitions and notations for terms used quite frequently, such as generalization gap, upper bound, etc. Both speakers provided a broad outline of the session where they listed down some important topics: After familiarizing the audience with important terminologies, Omar<U+00A0>Rivasplata took over the baton by discussing about First Generation SLT. He started with talking about the building blocks of a single function and then explains the finite function class and infinite function class. In the next few slides, Omar discussed the VM bounds along with the limitations to the VM framework. Once the audience had been familiarized with the first generation of SLT in the first half of the talk, John gave an overview of what comes after that <U+2013> the second generation of SLT. He elaborated on the different ways to make the bound function dependent and the techniques that can be used for detecting a benign distribution. We also saw the<U+00A0>Three Proof Techniques <U+2013> Covering numbers, Rademacher Complexity, and PAC Bayes Analysis. He compared the PAC-Bayes bounds with Bayesian Learning. This part of the talk is really interesting <U+2013> do watch the video to gain a deeper insight into it. The last section of the tutorial is a discussion over the <U+00A0>Next Generation SLT, where the speakers talks about Performance of neural networks and stability. Throughout the tutorial, the speakers explain all the concepts using plots and mathematical equations which makes the topics crystal clear. Speakers: Alex Graves and Marc¡¯Aurelio Ranzato (function(d, s, id) {  var js, fjs = d.getElementsByTagName(s)[0];  if (d.getElementById(id)) return;  js = d.createElement(s); js.id = id;  js.src = 'https://connect.facebook.net/mr_IN/sdk.js#xfbml=1&version=v3.2';  fjs.parentNode.insertBefore(js, fjs);}(document, 'script', 'facebook-jssdk')); Tutorial Sessions: Unsupervised Deep Learning Welcome Back to the NeurIPS 2018 Tutorial Sessions.This tutorial Unsupervised Deep Learning will cover in detail, the approach to simply 'predict everything' in the data, typically with a probabilistic model, which can be seen through the lens of the Minimum Description Length principle as an effort to compress the data as compactly as possible.Presented by Alex Graves (Google DeepMind) and Marc Aurelio Ranzato (Facebook) Neural Information Processing Systems <U+092F><U+093E><U+0902><U+0928><U+0940> <U+0935><U+0930> <U+092A><U+094B><U+0938><U+094D><U+091F> <U+0915><U+0947><U+0932><U+0947> <U+0938><U+094B><U+092E><U+0935><U+093E><U+0930>, <U+0969> <U+0921><U+093F><U+0938><U+0947><U+0902><U+092C><U+0930>, <U+0968><U+0966><U+0967><U+096E> A topic most of you will be curious to explore more! This tutorial is divided into two parts: In Part 1, Alex explained why we need unsupervised learning in the first place. Why can¡¯t we just provide the true labels for training the model? There are mainly three reasons for that: Targets in supervised learning contain very less information as compared to the input data. Using supervised learning, we are bounding the model to learn only a few bits of information. Unsupervised learning on the other hand, gives us an essentially unlimited supply of information to learn. So instead of learning the data points, the model learns the dataset. Unsupervised learning gives us more of a signal to learn from, but the learning objective is not entirely clear. Autoregressive neural networks can be used for density modelling which help to learn information from the data. Methods such as auto-encoding and predictive coding can yield useful latent representations. In Part 2, Marc discussed various applications of unsupervised learning which are based on other frameworks and principles. He explained how to learn representations and samples and how to map between two domains. Some of the tips for learning representations are: He also mentioned how to extract features in NLP using unsupervised learning. Some of the applications of learning how to map between two domains are: Unsupervised learning has tons of sub-areas like feature learning, learning to align domains, learning to generate samples, etc. The biggest challenges with unsupervised learning are: Speakers: Zico Kolter and Aleksander M<U+0105>dry (function(d, s, id) {  var js, fjs = d.getElementsByTagName(s)[0];  if (d.getElementById(id)) return;  js = d.createElement(s); js.id = id;  js.src = 'https://connect.facebook.net/mr_IN/sdk.js#xfbml=1&version=v3.2';  fjs.parentNode.insertBefore(js, fjs);}(document, 'script', 'facebook-jssdk')); Tutorial Session: Adversarial Robustness, Theory and Practice Welcome to NeurIPS 2018 Tutorial Sessions. This tutorial will survey some of the key challenges in this context and then focus on the topic of adversarial robustness: the widespread vulnerability of state-of-the-art deep learning models to adversarial misclassification (aka adversarial examples)Presented by J Zico Kolter (CMU/Bosch Center for AI) and Aleksander Madry (MIT) Neural Information Processing Systems <U+092F><U+093E><U+0902><U+0928><U+0940> <U+0935><U+0930> <U+092A><U+094B><U+0938><U+094D><U+091F> <U+0915><U+0947><U+0932><U+0947> <U+0938><U+094B><U+092E><U+0935><U+093E><U+0930>, <U+0969> <U+0921><U+093F><U+0938><U+0947><U+0902><U+092C><U+0930>, <U+0968><U+0966><U+0967><U+096E> In the tutorial, both researchers spoke about how machine learning predictions are mostly accurate, but at the same time, brittle as well. Intrigued? Adding just a little noise to the data can change the predictions drastically, resulting in a drop in performance. Trying data augmentation also does not help much in improving the performance. Some of the problems that the brittleness of machine learning can cause are: Zico and Aleksander proposed three commandments in order to make our machine learning model more secure: They talked about adversarial examples and verification, and how to train adversarially robust models. Zico further propounded on whether robust deep networks overfit or not. Even for training adversarial robust models, more data is required <U+2013> this is a known fact. Data Augmentation can be used to make the model robust. Adversarial training is also an ultimate version of data augmentation as we train on the most confusing version of the given training set. Some of the keypoints to make a model robust are: Finally, to summarize adversarial robustness: Apart from this, the advantages of using adversarial robust models are massive. The model becomes more semantically meaningful. We will be able to rely on it far more. And it leads to machine learning that is not only safe and secure, but also better. Sounds like a good bet to us! Speakers: Fernanda Viegas and Martin Wattenberg Visualization is a topic all of us can relate to at some level. Who among us hasn¡¯t done a thorough EDA before? Fernanda Viegas and Martin Wattenburg covered one of the most interesting and fundamental topics of machine learning <U+2013> visualization. They first spoke about what data visualization is, how it works and what are some of the best practices for it. The talk then focused on how visualization has been applied to machine learning till date. A special case of high dimensional data has also been covered in this tutorial. Data visualization is good for almost every field and some of its applications include: Using colors for visualization makes it more interpretable and even faster. Visualization makes calculations easier and less tedious (and who doesn¡¯t appreciate that?!). Some of the examples where it helps in calculation are: The tutorial also dove into the interpretability and model inspection facets of a ML project. Visualizing different layers of convolutional neural networks (CNNs) helps us to understand how it classifies images (this also helps in case the model is not performing well). We can interpret the model layer by layer and finally conclude where it is going wrong. They recommend using Jupyter notebooks for visualization which have libraries like matplotlib and plotly which have pre-built codes for most visualizations. Speaker: Susan Athey (function(d, s, id) {  var js, fjs = d.getElementsByTagName(s)[0];  if (d.getElementById(id)) return;  js = d.createElement(s); js.id = id;  js.src = 'https://connect.facebook.net/mr_IN/sdk.js#xfbml=1&version=v3.2';  fjs.parentNode.insertBefore(js, fjs);}(document, 'script', 'facebook-jssdk')); Tutorial Session: Counterfactual Inference Welcome back to the NeurIPS 2018 Tutorial SessionsThis tutorial Counterfactual Inference will review the literature that brings together recent developments in machine learning with methods for counterfactual inference.Presented by Susan Athey (Stanford University) Neural Information Processing Systems <U+092F><U+093E><U+0902><U+0928><U+0940> <U+0935><U+0930> <U+092A><U+094B><U+0938><U+094D><U+091F> <U+0915><U+0947><U+0932><U+0947> <U+0938><U+094B><U+092E><U+0935><U+093E><U+0930>, <U+0969> <U+0921><U+093F><U+0938><U+0947><U+0902><U+092C><U+0930>, <U+0968><U+0966><U+0967><U+096E> In this absorbing tutorial, Susan primarily spoke about estimating the magnitude of causal effects and how researchers¡¯ uncertainty about these magnitudes can be quantified. She brought up the developments in machine learning with methods for counterfactual inference. Next, Susan presented a cold fact <U+2013> there are gaps between what researchers are doing and what firms are applying. She mentioned three Counterfactual Inference approaches: In addition, Susan briefly mentioned the key challenges faced by causal inference, For example, lack of variation in the data, data insufficiency, and the practitioner¡¯s lack of knowledge about the model. Tech firms who are proactively conducting lots of experiments and interacting with experts on a large scale should focus more on learning about causal effects. Speaker: David Dunson (function(d, s, id) {  var js, fjs = d.getElementsByTagName(s)[0];  if (d.getElementById(id)) return;  js = d.createElement(s); js.id = id;  js.src = 'https://connect.facebook.net/mr_IN/sdk.js#xfbml=1&version=v3.2';  fjs.parentNode.insertBefore(js, fjs);}(document, 'script', 'facebook-jssdk')); Tutorial Session: Scalable Bayesian Inference Welcome to NeurlIPS 2018 Tutorial Sessions. This tutorial on Scalable Bayesian Inference will provide a practical overview of state-of-the-art approaches for analyzing massive data sets using Bayesian statistical methods.Presented by David Dunson (Duke) Neural Information Processing Systems <U+092F><U+093E><U+0902><U+0928><U+0940> <U+0935><U+0930> <U+092A><U+094B><U+0938><U+094D><U+091F> <U+0915><U+0947><U+0932><U+0947> <U+0938><U+094B><U+092E><U+0935><U+093E><U+0930>, <U+0969> <U+0921><U+093F><U+0938><U+0947><U+0902><U+092C><U+0930>, <U+0968><U+0966><U+0967><U+096E> Bayesian learning as a topic has fascinated us for a long time. The objective of this session was to motivate people to work more on Bayesian methods as these methods offer an attractive general approach for modeling complex data.<U+00A0>David Dunson gave an overview of the state-of-the-art approaches for analyzing huge datasets using Bayesian statistical methods. David explained how the Markov Chain Monte Carlo (MCMC) algorithm is becoming more and more scalable and faster thanks to the emerging rich and practical literature on the subject. Apart from that, he put quite an emphasis on tweaking the Bayesian paradigm to be more robust with respect to Big Data and scaling of Bayes to high-dimensional data (no. of features > no. of samples), which in itself is quite a hot topic. If you are interested in Bayesian statistics, then this is a must-watch video, and has the following key takeaways: Speakers: Survit Sra and Stefanie Jegalka (function(d, s, id) {  var js, fjs = d.getElementsByTagName(s)[0];  if (d.getElementById(id)) return;  js = d.createElement(s); js.id = id;  js.src = 'https://connect.facebook.net/mr_IN/sdk.js#xfbml=1&version=v3.2';  fjs.parentNode.insertBefore(js, fjs);}(document, 'script', 'facebook-jssdk')); Tutorial Session: Negative Dependence, Stable Polynomials and All That Welcome back to NeurIPS 2018 Tutorial Sessions.This tutorial Negative Dependence, Stable Polynomials and All That provides an introduction to a rapidly evolving topic: the theory of negative dependence and its numerous ramifications in machine learning.Presented by Survit Sra (MIT) and Stefanie Jegalka (MIT) Neural Information Processing Systems <U+092F><U+093E><U+0902><U+0928><U+0940> <U+0935><U+0930> <U+092A><U+094B><U+0938><U+094D><U+091F> <U+0915><U+0947><U+0932><U+0947> <U+0938><U+094B><U+092E><U+0935><U+093E><U+0930>, <U+0969> <U+0921><U+093F><U+0938><U+0947><U+0902><U+092C><U+0930>, <U+0968><U+0966><U+0967><U+096E> This tutorial gives an introduction to the topic: the theory of negative dependence. This can impact all aspects of machine learning, including both supervised and unsupervised learning. It is a rich mathematical toolbox which aids in tasks like anomaly detection, information maximization, experimental design, validation of black-box systems, architecture learning, fast MCMC sampling, and much more."
"vidhya",2018-12-17,"A Comprehensive Guide to Digital Marketing and Analytics Every Data Science Professional Must Read","https://www.analyticsvidhya.com/blog/2018/12/guide-digital-marketing-analytics/","

New Year's Grand Sale - 40% Discount On All Courses (Use Coupon: HNY2019) Click To Enroll Today !

 One of the biggest challenges of breaking into the field of digital analytics is that the landscape of digital marketing is extremely complex. It¡¯s a hard task finding professionals who know the best of both worlds <U+2013> digital marketing and data science. There is a serious shortage in the supply of adequate talent, while spending on digital marketing continues to rise unabated. This spending is quite prevalent in the developed economies. But here¡¯s the good news <U+2013> developing countries are starting to catch up, and are not far behind the curve. Check out the below chart and see for yourself the growth in the digital marketing spend share over time in India: Figure source : eMarketer I have seen positions open for years in the digital analytics space because of the shortage of such niche talent coupled with the immense growth of this field. Over the last few months, I have spent some time trying to understand the digital marketing landscape and how it integrates with our field of data science. In this article, I have shared my compiled notes that should simplify this complex world of digital marketing and help you understand how you can use your data science tools in this terrain. Excited yet? Good. Let¡¯s start with the most basic question and take it from there! Marketing is all about getting 4 things right <U+2013> reaching out to the right customer with the right product at the right time through the right channel. Marketing also needs a lot of testing to see which combination works for these 4 factors, so we need a channel that takes minimum time to hit the market. Traditional media channels like cable TV, flyers, radio, physical banners, etc. present a lot of challenges getting all of these 4 things right. To name a few: 1. Once printed or aired, an advertisement cannot be changed. 2. High time to market. This is actually a big challenge for any dynamic industry. A lot of offers require a quick response time (like Amazon Prime sales) and advertisers don¡¯t wish to announce the sale beforehand to make sure customers regularly come to their website to check out offers. 3. No way to accurately measure viewed ads. All we know is the response from ad campaigns. 4. Limited ways to reach out to a specific audience. 5. Even if we manage to reach the right customers, we might not really reach out at the right time. For instance, if we advertise during specific TV shows that are viewed by our target audience, we don¡¯t really know whether the target customer is looking at the TV at the time when our ad is being played. Similarly, if we send a mail to a specific prospect who is traveling at the time, it¡¯s a lost opportunity. 6. Given a low response from these channels, most of these traditional methods have a high cost per acquisition. Digital marketing is able to overcome all the challenges mentioned above.¡±How¡± will become clearer as we cover other related topics in this article. With internet penetration reaching 60%+ around the globe and rapidly increasing (see chart below), digital media is now an ideal place to catch your audience¡¯s attention. Figure Source : Research Gate Today, we can reach out to 4 billion+ internet users through digital media. Another survey (below) indicates 2 hr. 28 mins spent per day by adult internet users in India. Noting that average time spent by the total adult population is 1 hour, total internet user average should be somewhere between 1 hr to 2.5 hrs (say, 2 hours). An estimate of the ad opportunity available per day on the internet is 4 Bn*2 hours = 8 Bn hours/day. Even if we assume only 1% of this internet time can be monetized with ads, we are still talking about 80 Million hours/day! Figure source : eMarketer This exercise would have given you a sense of the magnitude of opportunity in digital marketing. And as an analyst, you would have definitely noticed the incremental curve for both internet users and the average time spent on the internet. Given the scale and precision of this channel, digital media presents multiple methods for advertising. Let¡¯s broadly classify these sub-channels of digital media. A data scientist can contribute literally in any domain and any industry. For instance, there are data science roles in creating new medicine, intelligent voice assistants like Google Home, targeting strategy, etc. For each of these roles, a data scientist might need minimal knowledge of the domain, but can still make strong contributions with the tools we have. For instance, to contribute towards building a new medicine, you need not know all the biology behind it. All you need to know is an objective function and the parameters you need to optimize. In these scenarios, you can easily break down the work into what the business person needs to tell and what you need to know already. However, digital analytics is slightly different. Given the fast changing dynamics in the industry, we are getting better data everyday. As an analyst, you need to decide: The barrier between a business person and an analyst becomes very blurry in this space. Here is how companies describe this new skill set they need for digital analytics: The T-shaped talent suffices for most data scientist roles in different domains. However, digital analytics professionals need the Pi-shaped talent to be a rockstar. If you already understand the world of data science, then this article will help you grasp the second aspect of the Pi-shaped talent requirement, the digital ecosystem, and open up a land of opportunities for you. In no way does this article capture everything under the digital marketing umbrella, but this is as good a place as any to get your journey started. We generally talk about 3 types of digital media <U+2013> owned media, paid media, and earned media. Here¡¯s a description of each: Data science has a strong application in each of these 3 types of media. Before we get into the actual applications of data science, let¡¯s first broadly break down the journey of a prospect into stages. The Different Stages of Customer Acquisition There are three journeys involved in customer acquisition. Our end objective of a successful campaign is to show ads to only those prospect that have a strong propensity of being a profitable customer. Looks simple, right? But in the real world, it really isn¡¯t! Here¡¯s why: As you might have guessed, it becomes very difficult to collaborate between all the three stages to reach ensure a successful campaign. Let¡¯s look at a a real-life scenario that will make this challenge a lot more clearer. Company X sells life insurance and acquires 50% of customers through paid digital media. A typical life insurance starts low on profit because of all the overhead costs involved, and gradually increases to maximum profit within 2 to 4 years. Year 5 onward, good customers start leaving the insurance to pick up better deals in the market, and hence the profit starts to decline. See this illustrative table that shows year wise profit: Just relying on the average can so often be deceiving. It seems like every acquisition is worth (lifetime value) $1,130 to your company, but all customers are different and as analysts, we try to find the most profitable segment. Now suppose you found that there is one attribute, total number of insurance policies held by the customer till date, that can strongly separate highly profitable customers from others. Here is how simple this rule can be: Now, here is the challenge. To stay compliant with the regulatory authority, you cannot deny a policy just because of profitability reasons. Hence, the only way you can craft your acquisition portfolio is if you can control who is looking at your ads. For a simplistic view, let¡¯s say 50% of your acquisition comes from your website directly and the remaining comes through Google¡¯s Paid Search. Acquisition coming directly to your site is hard to control as these prospects are already down the conversion funnel. So all you can do is control who looks at your paid search ads. Controlling paid search targeting looks straight forward <U+2013> just tell Google to show ads to those people with #insurance policy <= 2. But there are a few challenges in doing this: So you see the challenge <U+2013> the variable that is available for real-time targeting by ad server companies is not available with you to link profitability. Similarly, companies cannot make their profitability linked information available to Google to do specific targeting. Even though such direct targeting is difficult with paid media, we still have a lot of ways to get around this challenge. We will talk more about these targeting audience methods later in this article. Before we move on to targeting audience methods on digital media, let¡¯s get some jargon out of the way. The best way to understand these jargon terms is to put them in contrast against each other. When a common person thinks about Google, we think about the Search Engine (well, at least I do!). When we search for a keyword, we see a list of links. Advertisers have to pay for sponsored ads that appear at the top of the rankings. The list of links that come in organic search are free of cost and the rank is determined by Google¡¯s proprietary Page Rank algorithm. In the picture below, Adobe has paid for the sponsored ad, whereas the Google Analytics link below it is an organic search. The Google Search network is far bigger than just the search engine. It currently includes: What about Google Display Network (GDN)? In addition to the search network, Google also partners with 2 million sites that can publish ad banners on their site. GDN has a huge coverage of about 90% internet users. The biggest challenge with GDN is the low response rate because these visitors are not really looking out for your product. These visitors are simply looking at news, or the weather or are watching a video, etc. So, we classify such a visitor as a top-of-the-funnel prospect as he/she has a long way to go before making a purchase. Another challenge with this display network is attribution. Because a customer might view your ad today and make a purchase significantly later, attribution of these purchases becomes very subjective. GDN is very successful for brand awareness and multi-touch acquisitions (which requires a customer to see your product¡¯s ad multiple times before making the final purchase). You might see GDN ads on your favorite sites if they partner with Google. Here¡¯s an example: You should now have an understanding of Google¡¯s core business of advertising. This is the majority revenue generator for Google. Every company wants to partner with Google to advertise their product/service. But how does Google choose which ads have to be displayed and when? Let¡¯s review the Google Ad auction briefly before we jump to the digital marketing jargon. You¡¯ll be amazed to know how Google does this. Let¡¯s try to understand the logic through an illustrative example. I searched for the term ¡°insurance liberty mutual¡± on Google Search. At this point, Google hosts a live auction for this one instance ad inventory. This auction will be conducted in mili/microseconds and the winners will get the top positions on the sponsored ad slots. Is a high bid all you need to get the top spots? Of course not. Because my search was specifically for liberty mutual insurance, no other competitor should ideally get the top spot. Otherwise Google¡¯s customer will have a hard time getting to the relevant links (the core skill Google offers). Following is the result for an actual Google search of the term: As you can see, ¡°insurance-quote-instantly¡± also participated in this auction but did not win against liberty mutual. So how does Google bring in this dimension of relevancy to remove monetary bias? Google has a concept of quality score for each rank which is kind of a page rank score on organic search. This quality score is then multiplied by the bid to calculate the ad score. This ad score is finally used to rank order the ads for a search instance. Here is a simplistic view of what goes on behind the scenes and how advertisers are charged for their ads: Couple of things you should notice in this Google Auction model: CPC vs CPM bidding <U+2013> Quick note of the types of bidding we do on both search and display networks. CPC bidding is when the advertiser wants to bid on how much they are willing to pay for a click. CPM bidding is based on per 1000 impressions. It really depends on the type of objective an advertiser is looking to achieve. If the objective of the advertiser is branding, all that matters is impressions. However, if the advertiser wants customers to progress in the conversion funnel, they will generally bid on CPC. One exception here are multi-view purchases, where a customer generally takes a decision after looking at an ad many times. In such cases, the advertiser might bid on CPM even when the objective is conversion. Simply put <U+2013> Adwords is used by advertisers to post ads on Google¡¯s display and search networks. Adsense is used by publishers to monetize their content. Adwords manages the demand side of ad inventory whereas Adsense manages the supply side. The below picture illustrates this concept well: Even though Google does the heavy lifting for both the publisher and advertiser in terms of ad serving, it still provides many tools to match the right ad to the right placement. This is where analytics plays such a crucial role. Google delivers Ads to Ad Spaces with publishers through 3 methods: What levers do Publishers have to subset in the pool bidding for their ad space? What levers do Advertisers have to subset in the pool bidding for their ad space? We will cover this topic in our section on Google Analytics later. For now, just note that you have levers on Demographic and Behavioral attributes you can use to target ad inventory. Further, you can choose keywords to make a precise selection of the auction you wish to participate in. DoubleClick is an ad-serving company that was bought by Google for $3.1 billion in 2008. In simple words, DoubleClick for Publishers makes it easier for publishers to monitize their content. It also provides effective tracking of how your content is performing in context of advertisements. DoubleClick for Advertisers, on the other hand, helps advertisers optimize their Search and Display campaigns. Google recently rebranded DFP as Google Ad Manager. DFP comes out much stronger than AdSense when you wish to manage your ads beyond Google Display Network <U+2013> including Affiliated Brands, Real-time Ad exchanges, etc. Let¡¯s see a few examples of how we analysts use the ad tracking done by DFP. Suppose you own a travel blog and are using DFP to manage your ad inventory. There are 4 primary sections in your blog <U+2013> Food, Destinations, People and Latest Deals. On each blog page, you have 3 banners <U+2013> Leaderboard, Skyscraper and Square. The below picture will help you visualize each of these positions on your blog page: The below table covers the key metrics that DoubleClick publishes by default: If you use DFA with Google Analytics 360, you will get endless dimensions. However, Google Analytics 360 requires you to part with a significant amount, not something everyone can afford. So let¡¯s stick to some of the basic dimensions and what we can do with them. Here are a few dimensions which you can leverage to analyze your site¡¯s performance with respect to ad revenue: These dimensions are just examples of dimensions available to you for slicing and dicing information. Let¡¯s take a look at a dimension metric view to learn more. Starting with a basic view to see how each category is performing: Clearly, the food section brings in the majority of the revenue (50%+) even though the destination category gets most of the impressions. The destination category has both click through rate (CTR) and revenue per 1000 impressions at the lowest value, indicating our ads are not being optimized well for this category. Seems like quite a huge opportunity, right? Breaking down the destination category further by the traffic sources gives the following results: The above table shows that FB is the major source of traffic for our destination category. However, this source performs sub-optimal on both CTR and Revenue. This narrows down our search further. Are we presenting different information than what customers coming from Facebook are expecting? We can look out for this information by checking the bounce rate for this audience. We will hold that discussion for later. Let us also review how each banner type is performing across pages: Leaderboard (LB) has the maximum number of impressions, which makes sense as it comes on the top and should appear during most visits. Skyscraper has a low CTR and revenue per 1000 impressions, indicating these banners might not be completely visible (definitely scope of improvement there). We can further deep dive into this analysis with DoubleClick for effective targeting, but we¡¯ll keep that for a future article. DoubleClick is a suite of product provided by Google. The below diagram will help explain the types of services DoubleClick provides in the world of digital media: DoubleClick Search is primarily used by advertisers to manage their ads on multiple Search Engines, including Google Search Network. DoubleClick Bid Manager is used to manage Display Ads across the Google Display Network and Real-Time Bidding platforms. DoubleClick Ad Exchange is like the New York Stock Exchange where ad inventory is bought and sold. DoubleClick also provides campaign management services like DoubleClick Studio. Cookies are small text files that are placed on a visitor¡¯s machine through websites they browse. A cookie contains some key information that can be used when the visitor returns. For instance, a lot of sites use cookies to save ID and passwords. Others use them to refill the checkout cart when the visitor returns. These are primarily first-party cookies. Services like DoubleClick, LiveRamp, etc. place a 3rd party cookie that they use to track user activity across the web space. An important thing to note here is that you can only read those cookies that you have placed, because all cookies have unique properties. For example, Amazon cannot read a cookie that Wells Fargo has placed in a visitor¡¯s browser. Pixels (tags) are typically an invisible single pixel. These pixels fire, or a JAVA code executes (both mean the same thing), when the webpage is loaded and capture important information about the visitor. Pixels can also place a new cookie in a visitor¡¯s browser or check if there is an existing cookie already there. Websites need to embed this Java script in the site source code, which looks something like this: Tag containers can contain multiple pixels or tags. One tag can trigger another set of tags and so on. Hence, containers are used to make conditional decisions that can determine if a set of pixels should fire or not. Google Tag Manager (GTM):<U+00A0>By now, you would have realized that the digital world is all about maintaining these tags. Why? So you can measure campaigns and create new audiences for prospecting. Here is a simple example <U+2013> Tom runs a food blog. He actively markets on paid search using Adwords. He also re-markets his customer to return to his blog if the customer has not come back for some time. He additionally tracks his online traffic using Google Analytics. He even leverages DoubleClick to target display ads beyond Google Network. Imagine the number of tags Tom might have to put on his site <U+2013> one for Adwords, one for re-marketing, one for DoubleClick floodlight and some custom DMP/DSP tags for specific re-marketing campaigns. Handling so many tags within the source code of the site is extremely risky for Tom¡¯s blog. Google Tag Manager is a solution to this problem. Google Tag Manager provides a single Java script which Tom needs to put in his source code which will allow him to manage all the tags to his site straight from the GTM interface. Additionally, Tom does not need to inform his IT team when making small changes to the tags because tags and the site source code are two separate entities superficially linked by Google Tag Manager. Below is a list of tags that Google Tag Manager can manage for you: Data layer is another key concept you should know. This component is what makes Google Tag Manager such a powerful tool. Simply put, you can think of it as a bridge for data between your site and GTM. GTM can do a two-way communication with the data layer. GTM will then make this data available for all the tags sitting in it. Think of the additional capability this adds to the dynamics! Now your marketing tags and analytics tag can directly pull segments of visitors you have created with on-site and DMP data. This can help you achieve the same level of targetability on off-site as that of on-site. The below schematic will make the process clear: Quick note on Floodlight tags:<U+00A0>DoubleClick uses floodlight tags to note visitor activities and sales. They provide two types of tags <U+2013> FL Counter and FL Sales. FL Counter is primarily used to link some conversion to ad exposure. Hence, it is used to track subscriptions after an ad click from a visitor. FL Sales is primarily to store transactions with a dollar value. It can help you optimize your campaign on total ad spends instead of maximizing conversions. I must confess this is the most confusing and difficult aspect of digital marketing. Feel free to skip this section if it becomes too complex. I will try my best to make these concepts as simple as possible. All these terms are related to technologies used primarily in display media. So before getting into these technologies, let¡¯s first try to understand the display media ecosystem. Search media is always bought through programmatic buying, however, display media can be bought directly or programmatically. Programmatic buying is basically technology assisted buying of media. Consider the following scenario: Victor runs a very popular travel blog. He gets about a million visits a month. He now wants to monetize his blog by putting an ad banner at the bottom of the article in the square placement. Out of the million visits, 500k visits will be on blogs that are related to hotel stays and the remaining 500k visits on blogs related to other topics. Victor wants the hotel blogs to specifically have Hotel related ads. He leverages various ad selling options to sell all his ad spaces. The above list is in the priority order of how ad inventory is distributed. The below diagram provides more clarity on the hierarchy: To make this entire process possible, we use technologies like DSP, SSP and Ads Exchange. Here is a schematic of how it generally works: As you can clearly see from the diagram, Direct Buy is done without any trading involved. Programmatic Guaranteed Selling is through DSP to the Publisher. If the inventory is unsold in these two options, it will be made available as Preferred Ads buyer, before it goes into the pool of Ads Exchange. Finally, we go through the private auction and RTB stages respectively. Google DoubleClick Bid Manager is a Demand Side platform. However, Adwords is not truly a Demand Side platform because of multiple reasons <U+2013> it restricts the ad inventory to only the Google Display Network. There is no option for programmatic Direct Buy or private auctions. Data Management Platform is at the heart of analytics for Display Marketing. Let¡¯s try to understand it with an example. First, a quick concept <U+2013> Remarketing is a way to win back a prospect who has visited your website but has not made an immediate purchase or enquiry. You might have witnessed Amazon ads following you on the web <U+2013> that¡¯s remarketing.  XYZ co. is an e-commerce company that wants to create a niche audience for display marketing. They only want to use the display channel for customers: The remarketing ad will be an offer of 5% off on this Laptop A.<U+00A0> XYZ wants to publish these ads by placing bids on Ad Exchanges. How can XYZ execute such complex targeting? Short Answer <U+2013> through a Data Management Platform (DMP). If you want to know the technical details of how this can be done, continue reading this section. Otherwise, all you need to know is that DMP can make specific targeting possible. DMP also provides third-party data, like preferences, interests, etc. You can skip the rest of this section if you want to ignore the technical details. Here is what actually goes on behind the scenes: The above 11 steps happen in less than a mini-second! DMP is used for many different use cases, such as: This is the concept that fascinated and scared me the most. We already know that the entire web world is tracking visitors through cookies. Cookies have been around for a long time. Using cookie DoubleClick can really stitch a persona <U+2013> for instance, a visitor that reads AV blogs clicks on an online analytics course, but does not convert. The same visitor comes back to another university analytics course and as he/she was tagged as an analytics enthusiast, this new university gave him/her a discount of 20% on the course through on-site optimization. The visitor finally converts and complets the course in 60 days. And so on¡¦. So, essentially we know everything about the visitor. What¡¯s left? A very key information, which is, ¡°This visitor is John Bell¡±. What is so important about this new information? This is a Personal Identification Information (PII) and will never change over time. Cookies get deleted all the time. But if we can link these cookies to PII, we not only have a persona that can describe a small subset of the population, but we directly have an individual that only describes one person on the planet. This is exactly what an Identity Resolution does. Let¡¯s try to get a broad idea of how Identity Resolution works in the industry currently: Given all the background information, let¡¯s now talk about the various tools you have at your disposal that can process huge amounts of data coming from a number of channels. These help you analyze the data and implement your strategy in real-time. Even though there are many solutions in the market, most of the companies (big or small) are using one of the two <U+2013> Google Analytics or Adobe Analytics. Let¡¯s review them briefly by comparing a few key attributes: Both the tools have their pros and cons. If you are a small scale company, choosing Google Analytics is a no-brainer. Even if you are a large corporate, the choice is tricky because Adobe on one hand gives active support, but Google integrates seamlessly with well-managed ad inventory. Note that Adobe can also integrate third-party tools for ad targeting, but not all of them are well-managed and might be prone to bot attacks, thus wasting your ad spends. You might have realized that each concept we covered in this guide is closely linked with each other. A comprehensive view is very important to appreciate the entire digital ecosystem. Trust me, it was impossible to find all this information in one place. As a data scientist, we do the hard part of learning all these concepts on the job. So I decided to put everything in one place with the aim of helping any future analyst get up to speed really quickly and keep up with the pace of this dynamically evolving industry. With the knowledge provided in this article, you will not only understand how to build a successful digital analytics driven strategy, but will also start appreciating how your strategy fits into the broader world of digital marketing. This combination of knowledge is extremely rare in the industry because most professionals focus on only one of these aspects. What it takes to create a successful marketing campaign on digital media lies at the intersection of the digital ecosystem, marketing and analytics. Superb article. As a person who knows a bit of analytics and bit of Digital marketing, I would say you have summarized most of the digital marketing concepts from Google per se, with its relation to analytics. Your illustrations too are good, especially the one explaining how ad biding works. Looking forward to your article wherein you would mention about Social media analytics covering FB, Twitter, Snapchat etc"
