"","site","date","headline","url_address","text","keyword"
"1","datacamp",2018-10-22,"Cassie Kozyrkov discusses decision making and decision intelligence!","https://www.datacamp.com/community/blog/decision-intelligence-data-science","Hugo Bowne-Anderson, the host of DataFramed, the DataCamp podcast, recently interviewed Cassie Kozyrkov, Chief Decision Scientist at Google. Here is the podcast link. Hugo:               Hi there, Cassie, and welcome to DataFramed. Cassie:              Hi. Thanks, Hugo. Honored to be here. Hugo:               It's a great pleasure to have you on the show. And I'm really excited to have you here to talk about data science in general, decision making, decision science, and decision intelligence. But before that, I'd like to find out a bit about you. First I want to know what your colleagues would say that you do. Cassie:              Oh, goodness. Well, it depends on the colleague, I think. But I think the consensus would be they'd say that I have some expertise in applied data science, especially. And I help Google teams and our cloud customers apply machine learning effectively. Hugo:               Great. Could you tell me a bit about what applied data science means to you? Cassie:              Yeah, so when it comes to data science, well, let's start with what data science means, and then we'll take it deeper. So data science, to me, is that umbrella discipline that has underneath it statistical inference, machine learning, and analytics or data mining. And the difference between these three, for me, boils down not to the algorithms that are used in them, because if you're smart, you can use any algorithm for any of them. Also not down to the tools, but really boils down to the number of decisions under uncertainty that you want to make with them. With data mining, you really wanna get inspired. There are no specific decisions that you want yet, but you wanna see what your data inspires you to start thinking and dreaming about. Statistical inference is one where a few really important decisions under uncertainty, and then machine learning and AI, they boil down to a recipe for repeated decision making. So many, many decisions under uncertainty. Cassie:              I actually see data science as a discipline of decision making, turning information into action. Now, where is this applied versus research side of things? Researchers focus more on enabling fundamental tools that other people will use to solve business problems. Whereas applied folks will go and find among the available tools, what they need to solve those problems. So I'm not focused on how do I develop a new neural network architecture. I'm focused more on, there seems to be this germ of an idea in a business leader. How do we bring that out, make it real, build the team that's going to end up working on that, and then ensuring that the entire process from start to finish is well thought out and executed, and then at the end, there is a safe, reliable result. Hugo:               And I think framing this form of data science and applied data science as a subdiscipline as decision making is something we're gonna unpack more and more in this conversation. So that really sets the scene very nicely. So you said that your colleagues would view you as an expert on applied data science and thinking about how to use machine learning effectively. Now, what do you actually do? Are they pretty on track with what you do? Cassie:              They're close. But I think at the heart of what I care about, is this notion of type three error in statistics. For those that don't remember your errors, let's have a quick reminder. Type one is incorrectly rejecting a null hypothesis. Type two is incorrectly failing to reject a null hypothesis, and type three is correctly rejecting the wrong null hypothesis. Or, if you prefer a Bayesian statement on the same thing, it's all the right math to solve entirely the wrong problem. Hugo:               Great. So could you give me an example of a type three error? Cassie:              Yeah. So it's any rabbit hole that a data scientist goes down meticulously, carefully answering a question that just didn't need to be answered. So maybe this'll be a familiar thing to some of the data scientists listening, and I hope you don't suffer from those too much, 'cause that's a bit of a newbie gotcha, but it kind of goes like this: There you are, finished with most of your work for the week. It's maybe 4:00 p.m. on a Friday, and you're excited for a nice, free weekend, because that's the whole point of being in industry and not being in academia anymore. Right? Just kidding. Anyways- Hugo:               You're not kidding at all. Cassie:              I'm ... Okay. I'm kidding some. Hugo:               Sure. A bit. Cassie:              Yeah. All right. Hugo:               Great. Go on, go on. Cassie:              Fine. I'm not kidding at all. So there you are, and you're just ready to go home, and say a product manager comes up to you. And with this sense of urgency in their voice, wants to get a specific measurement from you. Or a specific question answered. And you think to yourself, ""My goodness. But that is difficult. That's gonna take me at least all weekend. And well into the nights as well. I'm gonna first have to figure out getting the data, then I have to sync up with data engineers. I'm gonna have to look up all these methods in the textbook. This is gonna be a difficult thing. But look, I am a great data scientist, and I can do this. And I can do this correctly and I can make sure that all the statistical assumptions are met. And come Monday morning, I'm going to deliver this thing perfectly."" And so on Monday morning, you come on bloody knees, you lift this result up to that product manager. And they kind of poke their head and look at you, and go, ""Oh. I didn't even realize that that was what I was asking for."" Cassie:              So there you were, meticulously, very correctly solving this problem, but rather uselessly as well. And it goes nowhere, the product manager doesn't use ut for anything, it just gets abandoned behind the sofa of lost results. So that's a type three error. Hugo:               And so how do you stop that happening? Presumably, on the Friday afternoon. Presumably involves communication. Right? Cassie:              Communication is one of those things but process, as well. So the data science team should know what the other stakeholders that they depend on are responsible for and what it looks like for those pieces of work to be completed correctly. So I like to talk about this wide versus deep approach to data science. So a rigorous approach versus a more shallow, inspiration gathering approach. And the second one is always good as long as you don't end up wasting your data on it, and hopefully you'll prod me about that shortly. But as long as you have data allocated to inspiration, having a light, gentle look at it is always a good idea. Putting your eyes on that data and seeing what it inspires you to think about. It helps you frame your ideas. Hugo:               And so in that case, we're thinking about some sort of rapid prototyping in order to- Cassie:              We're thinking about something even more basic. We're thinking about just plotting the thing. And that is separate from a very careful pursuit, rigorously, of a specific and important goal. So first, separating these two and saying the former, that broad, wide, shallow approach, that is always ... that's indicated for every patient. Let's just put it like that. Doctor prescribes that always. As long as you have the data to spare for it, do it. But don't take your results too seriously, and don't do anything too meticulous. Cassie:              On the other hand, this more rigorous approach, this takes a lot of effort. And it takes a lot of effort not just from the data science team. And the context for that, how the question is being asked, what assumptions are palatable and so forth, that is actually the responsibility of the decision maker, the business leader. And they have to have done their part properly in order for that meticulous work to make sense. So if you're going to go and do rigorous things, you need to make sure that that work was properly framed for you. Hugo:               Right. So in this case of the data scientist going and spending their weekend working on this problem doing essentially work that the product manager didn't think they'd be doing, a way to solve that would've been doing some sort of rapid datavis, exploratory data analysis, and then having a conversation with the product manager about what they really wanted. Cassie:              I would say, actually, the other way around. Have a conversation with the product manager about what they really wanted first. And if what they want is something emotional, to get a feel for something, that needs to be unpacked a little more, and perhaps what they're looking for is possible, perhaps it isn't. Perhaps taking a look at the data generates the inspiration that is required for what they want. Perhaps they're hoping that the data scientist is a genie in a magic lamp that grants wishes that can't be granted. So talking to them and figuring out what they want should actually be the first step. But better even than that, would be an organization where there isn't this adversarial relationship that assumes that the product manager doesn't know their part. Better to staff the project with trained, skilled decision makers who know how to do their part, and the data scientist will simply check the requests coming in. And if the request has a certain characteristic, they will tend to go for no work or light work, and if the request has a different kind of characteristic, they will go and do things carefully, rigorously and meticulously at the level of rigor and complexity requested by that skilled business leader. Hugo:               I love it. So then we're actually talking about specifically, with clarity, defining roles and defining a process around the work--- Cassie:              Absolutely. So this can get pretty big and interesting, of how you arrange these teams and how you arrange these processes. In its lightest form, it can be a matter of who talks to whom in what order, but it can be much bigger than that. Hugo:               Great. And this is something we're gonna delve into later in the conversation, is common organizational models for this type of work. But before that, I want to prod you a bit, and something you mentioned earlier was the idea of wasting your data. And maybe you could tell me what you mean by that. Cassie:              Sure. Well, we all learn something rather straightforward and obvious-sounding in statistics and data science class. And then unfortunately, we end up forgetting it a little bit. And it's something that we really shouldn't forget. And that is that a data point can be used for inspiration, or rigor, but not both if you're dealing with uncertainty, if you wanna go beyond your data. Because when you're assessing, whether or not your opinion actually holds up in reality and in general, then you need to make sure that you check that opinion on something that you didn't use to form the opinion. Because we, humans, are the sorts of creatures that find Elvis's face in a piece of toast. And if we use the same piece of toast to get inspired to wonder whether toasts look like Elvis, and then to also answer whether toast does, in general, look like Elvis, we have a problem. You're going to need to go to a different piece of toast. Cassie:              And so you can use data for inspiration or rigor, but not both. And so if you use all the data that you have for getting inspired, for figuring out what questions you even wanna ask, then you have no data left over to rigorously answer them. Hugo:               And I think similar comparisons can be made to null hypothesis significance testing. Right? So for example, you'll do exploratory data analysis, start to notice something, and then do a test there, because you were inspired in your null hypothesis and alternative hypothesis by the original data, you may actually be over fitting your model of the world to that dataset. Cassie:              Yeah, and I think that that kind of thing actually happens in practice in the real world, because of the way in which students are taught in class. So it makes sense in class for you to get a look at the conditions of a toy dataset, see what sorts of assumptions might or might not hold in that dataset, and then see what it looks like when you apply a particular method to that dataset. And that poor toy dataset would've been torn to shreds with respect to what you could actually reasonably learn from it, by all the thousands of times that the student and professor are torturing this poor little dataset. But that's okay, because all you're supposed to do in class is see how the math interacts with the data. But you get used to this idea that you're first allowed to look and examine this dataset, and then you're allowed to apply the algorithm or statistical test to it. Cassie:              In real life, though, you end up running into exactly this problem, where you invalidate your own conclusions by going through that process. You really should not use the same dataset for both purposes. You shouldn't pick your statistical hypothesis and test it right then and there. I mean, think about it like this: Here you are, with x variable and y variable, nice scatter plot. And you take this little dataset, and you plot it and you see sort of the ghost of a upward, up and to the right lift in this little point cloud that you've just plotted. Well, you've just seen this, and so you ask yourself, ""Well maybe I can put a straight line through it and see whether I statistically significantly have a positive correlation there."" Congratulations, you are going to get the result that, yes you do statistically significantly have a positive correlation, on account of having been inspired to ask the question in the first place by how these particular points fell onto your scatterplot. The conclusion you make might be entirely unrelated to reality. If you're inspired to do that by this data set, go get another data set from the same process in physical reality, and make sure that your inspiration holds up, there. Cassie:              We humans, we do see patterns that are convenient, interesting to, whatever we're interested in, and might touch reality at no point at all. Hugo:               There are several ways that we think about battling this endemic issue. One that you mentioned, of course, is after noticing things in exploratory analysis of your dataset and coming up with hypotheses, then going and gathering more data, generated by the same processes. Another, of course, is preregistering techniques before looking at any data initially. I was wondering if there are any other ways that you've thought about, or you think may be worth discussing to help with this challenge. Cassie:              The problem, really, is the psychological element of data analysis. How you're looking for things. How your mind tricks you as you look for things. And the mathematical techniques that are supposed to help you do things like validate under extreme circumstances with cross validation, those are really easy to break. They don't actually protect you from the going about things the wrong way, psychologically. Cassie:              So what I suggest people do, when they start thinking about this is, if you were pitted against a data scientist who absolutely wants to lead you astray and trick you into all kinds of things, when you give them certain constraints on their process, can they still give you a bad result? Can they still mess with you? Can they still trick you? And most of those methods out there ... In fact, I can't think of one off the top of my head that wouldn't be, most of them are susceptible to that kind of mucking about. And unfortunately, as a good data scientist, you are likely to trick yourself in that same manner. Hugo:               That's very interesting, because I think it hints at the fact that we actually due to a lot of our cognitive and psychological biases, we don't necessarily have good techniques. We need to develop processes, but we don't necessarily have good techniques to deal with this, yet. Cassie:              When you talk about preregistration of a study, that is less of a technique and more of a statement that in these data, you're not going to go and adjust your perspective and question. So you're saying wherever your hypothesis comes from in advance of gathering and processing these data, it is now fixed. That's kind of how it should be, anyway. So even when you ask the question and separate the two, you're actually talking about two aspects of the same thing. If you want to form a hypothesis, go and explore data, but nuke that dataset from orbit if you're going to go and do some rigorous process where you have the intention of taking yourself seriously. You should have your entire question, all the assumptions, all the code, even, submitted ideally before the data's collected, but certainly before the data hits that code. Hugo:               So I'd like to move now, and talk about decision intelligence, which you yourself, a chief decision scientist at Google Cloud and you work in decision intelligence. And I was wondering if you could frame for us, what decision intelligence actually is, and how it differs from data science, as a whole? Cassie:              So I like to think of decision intelligence as data science plus plus, augmented with the social and managerial sciences. And with a focus on solving real business problems and turning information into action. So it's oriented on decision making. If we had to start again with designing a discipline like that, we would wanna ask every science out there what does that science have to say about how do we turn information into action? How do we actually do it, because of the sort of animal that we are? And if we want to build a reliable system or reliable result for a specific goal, how do we do that in a way that actually reaches that goal rather than takes a nasty detour along the way? Cassie:              So it's very process-oriented. It's very decision-oriented. But of course, a large chunk of that is applied data science. Hugo:               So can you tell me why data science plus plus? Why do we have two pluses there? Cassie:              Ah, that plus plus, as in upgraded with the next thing, I suppose. In the same way as in Syntax, you would have I plus plus. That's just some cuteness, there, I suppose. But think of the upgrade like this: a data scientist is taught how to analyze survey data and how to think through a lot of the careful, mathematical stuff, how to deal with what happens if their data are continuous, if they're categorical. What if the person was using some sliding scale, et cetera? How many questions? How do we correct for this many questions? That sort of thing. But what they're not taught, not directly in their training, is how do you construct that survey? How do you make sure that that survey minimizes say, response bias, which is where the user or participant simply lies to you and gives you the wrong answer to your question? And how do you think about what the original purpose of that survey is? Why are we doing this in the first place? And was a survey the right way to go about it? And how did we decide what was worth measuring? Those things are not typically taught to data scientists. Cassie:              So if data scientists want their work to be useful, then someone, whether it's themselves or a teammate, who has the skills to think thorough that stuff, has to be participating. Hugo:               Right. And is it important, or is best case scenario the data scientist being involved in every step of the process from data collection, experimental design, question design through to actual decision making? Cassie:              That depends on what your budget is. Right? If you have infinite money, perhaps you might be able to hire one of the very, very rare unicorns who've actually thought about all of it, and who are skilled in all of it. There are not that many of those people. And if you intend to hire them, you are going to have to pay them. So intending to staff your projects in that manner, well, no wonder you're going to be complaining about a talent shortage. So the reality of it is that you're going to have to work with interdisciplinary teams. And also, even if you have someone who gets all of it, in a large scale project, there's still more work than someone can do with the hours that there are in the day. And so why do you really need all these identical copies of the perfectly knowledgeable worker, if they're going to have to work on different parts of the process, in any case? So the data scientist upskilling to perfection and then owning all of it, it's a nice dream, but it doesn't sound very practical. Cassie:              Instead, I imagine that they would be best suited to the part that they have spent the most time learning. And what they should really worry about more, is how the baton is passed from colleagues who are responsible for other parts of the process, and having the skills to check that that part was done well enough for their own work to be worthwhile. Because unfortunately, data science is right in the middle of the process. And it relies on the bookends. And if the bookends, like that decision making side and that product leadership side and that social science side, if that wasn't done correctly, or if downstream, you have no way to put this into production reliably, and even if the prototype has beautiful math, it will be too much of a mess to actually use in practice. Then there is no point to the data scientist's work. It all becomes a type three error. Cassie:              So they're gonna be working with a very interdisciplinary team, probably. And they should focus on the parts where they can have the best impact. Hugo:               Great. So in terms of decision making, I wanna know about these teams. I love that your response to my previous question was, ""The reality of the situation is ... "" I wanna know more about reality, and I wanna know more about the practical nature of how data scientists and their work are included or embedded in decision making processes. So could you tell me a bit about the most common organizational models for how data scientists are included in this? Cassie:              Yeah, sure. An obvious way to do it is to collect a whole lot of data scientists and put them together into a centralized data science team, and that tends to be guided jealously by their data science director, who buffers them from the most egregious type three error requests, and makes sure that the rest of the organization uses them to a good purpose, or at least to the most impactful business purpose. And the junior data scientists in that structure, they don't need to navigate the politics. Cassie:              There's another model, which is simply embedding a data scientist in a large engineering team, and telling them to be useful. Cassie:              And there's the decision support model. This is where you append the data scientist to a leader, and the data scientist helps that leader make decisions. Cassie:              And then, of course, there is the data scientist owning most of the process, especially the decision making. So here, data science is responsible for framing decision context, figuring out which questions are even worth asking, and then also taking ownership of answering them. Hugo:               So we have the pure data science team, the embedded in engineering, decision support, and data scientists as decision maker. And I think- Cassie:              The fifth will be the decision intelligence option, which is none of these. Hugo:               I look forward to discussing that. And it seems like, generally, this order goes from less decision making to more decision making on the part of the data scientist. Is that fair to say? Cassie:              Ah, fair enough. Hugo:               And so what are the pros and cons of being at different points along this spectrum? Cassie:              With a super centralized one, an obvious con is that if you are a small, scrappy organization, forget it. You are not going to be able to have this large data science org. Another con is that they tend to be put towards what the business already knows is worth doing properly and carefully. So in some sense, this is a pro. They're going to be associated with the most delicate or high-value questions in the business. The con is that there's flexibility to help out the broader organization to seize unusual opportunities, because there is this sort of single point through which all the requests come. And that tends to homogenize the requests a little bit. And that also means that individual data scientists will have very low contact with the decision function. That might be a pro for them. Maybe that's a stressful thing for a junior data scientist. But it's very hard for their work and their contribution to have visibility in this way. Cassie:              And all of this is really at the mercy of data science leadership. So if their data science director does not know what they're doing, we'll have a problem. And the industry is really suffering from a shortage of data science leaders. There are people who call themselves data science leaders or analytics managers, but these folks might not really know how to play the organizational politics. They might not have a good business sense. Or maybe they are primarily leaders, they have all those ... that nose for impact, but they don't understand how to make a data science team effective. So there can be some problems with that. Cassie:              The embedded in engineering: pro is that you get to influence engineering. However, you end up doing a variety of tasks, which may or may not have anything to do with data science. Quite often, the engineering team don't really know what sort of animal you are, and doesn't really know what you're for, doesn't know if you're useful. They think of you as a sort of not-so-good programmer. ""What's wrong with you? And what is this stuff that you're constantly fussing about on whiteboards?"" You might not be seen as very useful, and you might find yourself taking on product management tasks that you might not want to do, that you didn't think you were going to have to do, and that you didn't train for. So you end up with non-specialist tasks and there's no buffer against politics for you, there. Hugo:               And is this something that also happens as we move more towards data scientists who work in decision support and as decision makers, themselves? Cassie:              There's some element of this, as well. With decision support, the leader, a good leader quickly figures out how to make you useful. So you don't spend an awful lot of time sort of wafting about, figuring out how to even contribute in the first place. Now, it might be that your best contribution is nothing to do with complex methodology that you spent so many years in grad school studying, and your data science tasks might end up getting diluted with a whole host of other things you might be working on. But your value does tend to get better protected under this setup. Hugo:               And how about for a data scientist as an actual decision maker? Cassie:              So of course, the pro there is that you don't have this loss in moving between the data science, engineering, and decision functions, because the data scientist owns all of those things. The con is that in order to do it, you need to really get several black belts. And if you don't have them, you might think you're being useful, but you might be doing more damage than good. So maybe you think you're good at understanding business impact, but really, what you're much better at is doing the math. And you end up pushing the organization down rabbit holes, bad rabbit holes at a much worse rate than they would've had without you. So you really do need these multiple black belts, and you need to understand that you have to train for these things separately. Because a standard training program just does not prepare you to be a two-in-one or three-in-one worker. Cassie:              So in practice, this is a rare animal. Hugo:               And then, of course the fifth model that you mentioned, in passing, that I'd like to focus on now, is data scientist as decision intelligence operative. What happens here? Cassie:              So there will be some allocation of time and human resources toward the analytics or data mining side of data science. And so there will be an ongoing pause check for the company. So there will just be this broad, light touch analytics going on all the time, and whoever is best at that model of working under data science will be doing that and will be partially driven by what leadership wants, but will also be driven by the explore over exploit attitude. Cassie:              Then, if something else is going to be requested, there will be certain stages in the project lifecycle that have to be completed in order to get that work. So it's sort of like a combination of those two models, where you get embedded in engineering, or you get embedded with decision making, but that match happens out of a centralized pool of labor, and it happens based on the project being framed in the required way. So for example, you might have the decision support framing where you need statistical assistance on a project. In order for that to happen, there has to be certain steps like a selection, if you're going to go frequentist way, selection of the default action, what the decision maker actually wants to do by default, an understanding of what it takes to convince them, what their metrics are, that sort of passes a social science function, there. What population they're thinking about. What assumptions they're willing to deal with. That will be someone from social science or from data science working with the decision maker to help them frame their decision context. Cassie:              And then once that is all ready, then you staff folks who can actually do the heavy lifting, the calculations, the data stuff to the project. And of course, you need to staff data engineering to that project as well. So when everyone comes together, they know what they are there for. Hugo:               And this actually speaks to kind of a broader challenge. I mean, we've discussed this previously, but this idea that a lot of people want to hire data scientists or do machine learning or state of the art deep learning or AI before they even know what questions they want to answer. Right? Cassie:              Yeah. So what you should do ... Here's my advice for everybody. If you don't know what you want, think of your data as a big old pile of photographs in the attic. And think of analytics or data mining as the person or function that is going to go to that attic and their opportunity to actually go to the attic and look at the data is going to be supported by data engineering. They're gonna go to that attic, they're going to upend those big box of photographs on the floor. They're going to look at them, then they are going to summarize what they see there to the people waiting patiently in the house and ask those folks whether they are considering doing anything more with it. That kind of approach always makes sense. You'll never know what's in this pile of photographs. And you'll never know whether it's worth doing anything serious with it. But also because it's a pile of photographs and you don't know who took it, and for what purpose, you should never learn anything beyond what is there. Cassie:              So we, as citizens, we already know how to think about a pile of photographs, or a photo you find on the side of the road. The only thing that you can reasonably say about it is, ""Hey, this is what is here."" Does that inspire me? Does it make me dream? Does it make me want to ask other questions about the world? Sure. Perhaps. But do I take any of that seriously? No, of course not. It's some photograph, and data science is essentially Photoshop, as we all know, and we don't know much about how that photo was taken or why. And we can't really make serious decisions based on it. But taking a look always makes sense. As long as you continue to think about it reasonably, the same way as you would think about those photos. So that's always good for every project. And if any team, any organization says, ""I'd like to know a little more about my data. I'd like to get into mining my data, looking at my data, finding out what's in there,"" that is always a good thing. Cassie:              But now, if you don't actually control the quality of that data, you might end up doing very careful, rigorous things with it. And the photographs were all, I don't know, blank. Right? There's no point to it. Or maybe they were all taken in a way that's entirely unreliable for the question that you want to answer, as you didn't actually plan the data collection, so if you look at the photographs that I take in my travels, you notice there's all these super touristy landmarks. And yet somehow, I'm the only person pictured in the photograph at that landmark. You can't conclude anything about how many people go to these landmarks based on my pile of photographs. But you can still take a look, as long as you don't take them too seriously, and then you might start thinking about the sorts of things you might wanna do with them. And when you start figuring out what you might like to do, then you start planning the entire process as directed towards that goal. And then it makes sense to start thinking about hiring people who can do that extra stuff. Hugo:               So Cassie, given the variety of different models for embedding data scientists in the decision making process, I'm wondering why so many organizations fail at using data science to inform decision making properly and robustly. Cassie:              Well, this comes down to a problem of turning information into action and how decision makers are organized and trained to do that. So it may be a case of the decision maker actually doesn't know what their own role in the process is, and they don't know how to properly frame a decision context for a data science project that isn't simply data mining and analytics, this wide and shallow approach. Without the decision maker taking control of the process, what always makes sense is a nice, shallow, wide data mining approach. Mine everything for inspiration, don't take yourself too seriously. Don't spend a whole lot of effort. And if you just stick to this, and you really, truly don't take yourself more seriously than you're supposed to, the biggest danger is overspending on personnel. Maybe you've ended up hiring a bunch of professors, and now you've used them for tasks that they consider to be far too easy given their intense training. Cassie:              But, what tends to happen is that decision makers don't end up owning the dive into the careful, rigorous stuff properly. So maybe they just hire a bunch of data scientists and they leave them in a room, all on their own. They don't give them any instructions, and then they are surprised when the only thing that comes out of that room is researched white papers. Maybe there's a case of all those folks pursuing research and rigor for its own sake, because that's the most comfortable thing, their comfort mode from their research training, and those folks aren't really qualified to diagnose what is useful for the business, and the decision function just leaves them alone. Cassie:              It might be a case of the entire organization not understanding that there's a difference between inspiration and rigor, and how to use data for these things, and how much effort each one takes. So another failure is where you get the opposite. You end up using data for inspiration, and then you think that you've done something rigorous there, where you really haven't. And you start taking those results much more seriously than you ought to. And you become over confident and you run headlong into a wall. Cassie:              Another problem that organizations have is that it's very convenient to use the outputs of data science work as a way to bludgeon your fellow decision makers in a meeting. So everyone wants to argue and put forth their personal opinion about a thing that cannot be solved with data, really. It has to do maybe with the strategy of the organization, and instead of sticking with humility, owning what you don't know, and using argument to discuss with your fellow decision makers what should be done next, you bring some inscrutable report that's covered in equations and you say, ""Because my magical data scientists have said, this is the truth."" But, you know about statistical inference, you know that the question matters more than the answer does, almost. And if all you do is you bring an answer, well, it may or may not be an answer to the question that is being asked or assumed by everybody else. It's like that Douglas Adams thing, where you just bring 42 to the meeting, and you say look at all these equations that have gotten us to 42. And because it says 42, I'm correct. Actually, it doesn't make much sense. Takes a lot of effort. And it wastes a lot of time. Cassie:              And then there's also an element of misguided and misdirected delegation of decision responsibilities. That's where you have someone who wants to have decision responsibility and they want decision making to be done rigorously, but they want that for more decisions than they actually have time to deal with. And so they sort of fool themselves into thinking that they can be in that decision maker role without spending the time to actually frame decision context, think through assumptions, work with the data science teams and so forth. And so what ends up happening is that people junior to them end up usurping those roles and make the decision however they make it. Maybe they make it rigorously, maybe they don't, and then spend all of the data science team's effort in persuading or convincing this pretend decision maker that it's actually their idea. Now, there's an element of fuss there, which could just be avoided if decision responsibility were delegated appropriately. There's no need for this usurping thing. If you don't have the time to put in the effort that it takes, then hand over that decision to someone who does have that time, if they're going to pursue it carefully, rigorously, and in this intense statistical way. Or, say, ""We're going to base it on inspiration. It's going to be a light case of analytics and plotting, but we're not going to allow ourselves to become more confident than our methods deserve."" Cassie:              So really, most of that disconnect has to do either with the people hired, or with the decision makers themselves not knowing what their own role is, since they are the ones who kick off the whole process. It really does matter that they have the skills to do it. Hugo:               And is there another disconnect with respect to how many people in an org can speak data, in the sense that data literacy and data fluency aren't necessarily spread or distributed across organizations. I suppose my question is: In organizations you have seen, how is data literacy spread through the org, and how would you like to see this change? Cassie:              So I'm not gonna speak specifically to Google, here. I'm gonna speak much more generally, about all of us at once. Cassie:              In this world, data literacy is in a sorry state. At least from my perspective, I really wish that we were better at this stuff. And we are surprisingly good at thinking through photograph data. And we're fairly reasonable, fairly reasonable ... We still might do some silly things. But we're fairly reasonable about that. And we're fairly reasonable about laughing and saying, ""Oh, ha-ha, just because it's in a book doesn't mean it's true."" But somehow, when it involves math and data, we start to pronounce data with a capital D, like it is some source of objective truth that's entirely disconnected from the humans that decided to collect it in the first place, and made decisions over how they were gonna collect it and why. So data literacy is in a sorry state. And what I keep seeing in the world at large is that we lack the humility to say, ""Well, if we had no one on the team who could have played this role, who had the skills to take on the decision maker's part, then we shouldn't take ourselves too seriously."" Cassie:              Instead, what one sees out there in the wild, is that there are these teams that are staffed with very meticulous mathematical minds and unskilled decision makers and that whole team, that whole ... So what I see missing in the world at large is teams with the humility to say, ""Taking ourselves seriously actually takes work, and it takes skills. And if we lack those skills, we're not going to be able to do it. The best that we can get from this is the same kind of thing that we get from looking at a pile of photos."" And that's actually still something. It's amazing that we have the ability to take an SD card, which means nothing to you when it's laying in the palm of your hand, and you plug it into your computer, and you use some visualization software, I don't know, Microsoft Paint, or something, and now you can get inspired and see what's there. That's an incredibly powerful thing. That's good for everybody. Everyone should be doing more of that on more data types. Cassie:              But not to assume that just any old data plus very complex mathematics can create something out of nothing. Certainty out of uncertainty, for example. A good decision process where the fundamental initial skills were lacking. I like to say inspiration is cheap, but rigor is expensive. And if you're not willing to pay up, don't expect that there is some magical formula that's going to give it to you. Without that data literacy, please don't be trying to do very complicated things. Hugo:               Right so this is the present state of data science, decision making, decision intelligence and data literacy. What do the futures and their intersection of data science and decision science look like to you? Cassie:              As we start to do more with data, I hope to see the world taking the quality of the decision skills and kicking off and also guiding those projects, growing. We can't really afford to be automating things with data at scale, and have that all based on bad decision making skills. That's going to be a disaster for the company who's doing it. So we'll have to move towards taking those skills more seriously and not treating it just as something that you have a flair for or a talent. But, even though I imagine that whether we learn it now or we learn it the hard way later, that those skills are going to get better, they don't have to be carried entirely by the people with decision responsibility currently delegated to them. There is another option. Cassie:              And that other option is to hire a helper who can do that rigorous thinking for you. The part of decision making that is a science can be done by a scientist, helping the decision maker who is owning the part that has to do with intuition and politics and so forth. So you can hire a helper to upgrade your skills if you don't want to go and learn the stuff yourself. But I do think that on the whole, the future involves us taking that first bit much, much more seriously. Hugo:               So towards that future, my final question is: Do you have a final call to action for our listeners out there? Cassie:              Yeah, two. One is, it's time to start shifting our focus away from only research, and more to a choice of whether you want to be doing research, or you want to be doing applied stuff. These are both equally valuable, important approaches. One of them is tremendously understaffed right now. I could argue both are and it's a really exciting time if you want to get into that sphere, because that's going to become more and more important as those general purpose techniques that the researchers make become more readily available for application. An analogy that I have for this is, researchers might be folks who build microwaves, new and better microwaves. Whereas applied folks think about innovating in the kitchen and recipes at scale. And I wanna point out that if you want to say, create McDonald's, just because you don't have to wire your own microwave doesn't mean it's easy. So this is an exciting time for a new area of investigation and new discipline. Cassie:              And the other thing I want to leave you with is, the world is generating more and more data. We really owe it to ourselves to make that data useful. Wasting all of our time and resources on type three error after type three error is a very sad state of affairs. So it's really time for us to take this seriously, because we just have so much of it. Let's do good, beneficial things with it. Hugo:               I love that, because it really brings this conversation full circle in terms of, you stated at the start of our conversation that a big part of what you do is to help teams avoid or lower the rate of type three errors in data science. We've come full circle, essentially. And that's one of the calls to action, here. Right? That we all work together and use the data and our modeling techniques and question us in capabilities to lower type three errors more and more. Cassie:              Yeah, and as I think back on our conversation, I think that a disservice that I did to decision intelligence as a whole, is that I really spoke to you a lot about data scientists. I spoke a lot about decision makers. I vaguely mentioned social scientists. But it's a much, much more diverse game. And I really left out all the other people that should be involved. The engineers, the reliability folks, the ethicists, the designers. There is a lot of important work to be done in this space by a great variety of people. And I want to ask everyone who's thinking about sneaking off right now because this doesn't apply to them, to reconsider. Decision making is important for all of us. And if we are going to do this seriously and at scale, then there is a role for everyone to play if you have anything to say about turning information into action. Hugo:               I couldn't agree more. And Cassie, it's been such a pleasure having you on the show. Cassie:              Thank you so much.","Keyword(freq): scientist(16), skill(12), folk(9), maker(9), decision(8), photograph(8), team(8), analytics(7), question(7), assumption(6)"
"2","mastery",2018-10-26,"How to Grid Search Naive Methods for Univariate Time Series Forecasting","https://machinelearningmastery.com/how-to-grid-search-naive-methods-for-univariate-time-series-forecasting/","Simple forecasting methods include naively using the last observation as the prediction or an average of prior observations. It is important to evaluate the performance of simple forecasting methods on univariate time series forecasting problems before using more sophisticated methods as their performance provides a lower-bound and point of comparison that can be used to determine of a model has skill or not for a given problem. Although simple, methods such as the naive and average forecast strategies can be tuned to a specific problem in terms of the choice of which prior observation to persist or how many prior observations to average. Often, tuning the hyperparameters of these simple strategies can provide a more robust and defensible lower bound on model performance, as well as surprising results that may inform the choice and configuration of more sophisticated methods. In this tutorial, you will discover how to develop a framework from scratch for grid searching simple naive and averaging strategies for time series forecasting with univariate data. After completing this tutorial, you will know: Let¡¯s get started. How to Grid Search Naive Methods for Univariate Time Series ForecastingPhoto by Rob and Stephanie Levy, some rights reserved. This tutorial is divided into six parts; they are: It is important and useful to test simple forecast strategies prior to testing more complex models. Simple forecast strategies are those that assume little or nothing about the nature of the forecast problem and are fast to implement and calculate. The results can be used as a baseline in performance and used as a point of a comparison. If a model can perform better than the performance of a simple forecast strategy, then it can be said to be skillful. There are two main themes to simple forecast strategies; they are: Let¡¯s take a closer look at both of these strategies. A naive forecast involves using the previous observation directly as the forecast without any change. It is often called the persistence forecast as the prior observation is persisted. This simple approach can be adjusted slightly for seasonal data. In this case, the observation at the same time in the previous cycle may be persisted instead. This can be further generalized to testing each possible offset into the historical data that could be used to persist a value for a forecast. For example, given the series: We could persist the last observation (relative index -1) as the value 9 or persist the second last prior observation (relative index -2) as 8, and so on. One step above the naive forecast is the strategy of averaging prior values. All prior observations are collected and averaged, either using the mean or the median, with no other treatment to the data. In some cases, we may want to shorten the history used in the average calculation to the last few observations. We can generalize this to the case of testing each possible set of n-prior observations to be included into the average calculation. For example, given the series: We could average the last one observation (9), the last two observations (8, 9), and so on. In the case of seasonal data, we may want to average the last n-prior observations at the same time in the cycle as the time that is being forecasted. For example, given the series with a 3-step cycle: We could use a window size of 3 and average the last one observation (-3 or 1), the last two observations (-3 or 1, and -(3 * 2) or 1), and so on. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this section, we will develop a framework for grid searching the two simple forecast strategies described in the previous section, namely the naive and average strategies. We can start off by implementing a naive forecast strategy. For a given dataset of historical observations, we can persist any value in that history, that is from the previous observation at index -1 to the first observation in the history at -(len(data)). The naive_forecast() function below implements the naive forecast strategy for a given offset from 1 to the length of the dataset. We can test this function out on a small contrived dataset. Running the example first prints the contrived dataset, then the naive forecast for each offset in the historical dataset. We can now look at developing a function for the average forecast strategy. Averaging the last n observations is straight-forward; for example: We may also want to test out the median in those cases where the distribution of observations is non-Gaussian. The average_forecast() function below implements this taking the historical data and a config array or tuple that specifies the number of prior values to average as an integer, and a string that describe the way to calculate the average (¡®mean¡® or ¡®median¡®). The complete example on a small contrived dataset is listed below. Running the example forecasts the next value in the series as the mean value from contiguous subsets of prior observations from -1 to -10, inclusively. We can update the function to support averaging over seasonal data, respecting the seasonal offset. An offset argument can be added to the function that when not set to 1 will determine the number of prior observations backwards to count before collecting values from which to include in the average. For example, if n=1 and offset=3, then the average is calculated from the single value at n*offset or 1*3 = -3. If n=2 and offset=3, then the average is calculated from the values at 1*3 or -3 and 2*3 or -6. We can also add some protection to raise an exception when a seasonal configuration (n * offset) extends beyond the end of the historical observations. The updated function is listed below. We can test out this function on a small contrived dataset with a seasonal cycle. The complete example is listed below. Running the example calculates the mean values of [10], [10, 10] and [10, 10, 10]. It is possible to combine both the naive and the average forecast strategies together into the same function. There is a little overlap between the methods, specifically the n-offset into the history that is used to either persist values or determine the number of values to average. It is helpful to have both strategies supported by one function so that we can test a suite of configurations for both strategies at once as part of a broader grid search of simple models. The simple_forecast() function below combines both strategies into a single function. Next, we need to build up some functions for fitting and evaluating a model repeatedly via walk-forward validation, including splitting a dataset into train and test sets and evaluating one-step forecasts. We can split a list or NumPy array of data using a slice given a specified size of the split, e.g. the number of time steps to use from the data in the test set. The train_test_split() function below implements this for a provided dataset and a specified number of time steps to use in the test set. After forecasts have been made for each step in the test dataset, they need to be compared to the test set in order to calculate an error score. There are many popular error scores for time series forecasting. In this case, we will use root mean squared error (RMSE), but you can change this to your preferred measure, e.g. MAPE, MAE, etc. The measure_rmse() function below will calculate the RMSE given a list of actual (the test set) and predicted values. We can now implement the walk-forward validation scheme. This is a standard approach to evaluating a time series forecasting model that respects the temporal ordering of observations. First, a provided univariate time series dataset is split into train and test sets using the train_test_split() function. Then the number of observations in the test set are enumerated. For each we fit a model on all of the history and make a one step forecast. The true observation for the time step is then added to the history, and the process is repeated. The simple_forecast() function is called in order to fit a model and make a prediction. Finally, an error score is calculated by comparing all one-step forecasts to the actual test set by calling the measure_rmse() function. The walk_forward_validation() function below implements this, taking a univariate time series, a number of time steps to use in the test set, and an array of model configuration. If you are interested in making multi-step predictions, you can change the call to predict() in the simple_forecast() function and also change the calculation of error in the measure_rmse() function. We can call walk_forward_validation() repeatedly with different lists of model configurations. One possible issue is that some combinations of model configurations may not be called for the model and will throw an exception. We can trap exceptions and ignore warnings during the grid search by wrapping all calls to walk_forward_validation() with a try-except and a block to ignore warnings. We can also add debugging support to disable these protections in the case we want to see what is really going on. Finally, if an error does occur, we can return a None result; otherwise, we can print some information about the skill of each model evaluated. This is helpful when a large number of models are evaluated. The score_model() function below implements this and returns a tuple of (key and result), where the key is a string version of the tested model configuration. Next, we need a loop to test a list of different model configurations. This is the main function that drives the grid search process and will call the score_model() function for each model configuration. We can dramatically speed up the grid search process by evaluating model configurations in parallel. One way to do that is to use the Joblib library. We can define a Parallel object with the number of cores to use and set it to the number of scores detected in your hardware. We can then create a list of tasks to execute in parallel, which will be one call to the score_model() function for each model configuration we have. Finally, we can use the Parallel object to execute the list of tasks in parallel. That¡¯s it. We can also provide a non-parallel version of evaluating all model configurations in case we want to debug something. The result of evaluating a list of configurations will be a list of tuples, each with a name that summarizes a specific model configuration and the error of the model evaluated with that configuration as either the RMSE or None if there was an error. We can filter out all scores set to<U+00A0>None. We can then sort all tuples in the list by the score in ascending order (best are first), then return this list of scores for review. The grid_search() function below implements this behavior given a univariate time series dataset, a list of model configurations (list of lists), and the number of time steps to use in the test set. An optional parallel argument allows the evaluation of models across all cores to be tuned on or off, and is on by default. We¡¯re nearly done. The only thing left to do is to define a list of model configurations to try for a dataset. We can define this generically. The only parameter we may want to specify is the periodicity of the seasonal component in the series (offset), if one exists. By default, we will assume no seasonal component. The simple_configs() function below will create a list of model configurations to evaluate. The function only requires the maximum length of the historical data as an argument and optionally the periodicity of any seasonal component, which is defaulted to 1 (no seasonal component). We now have a framework for grid searching simple model hyperparameters via one-step walk-forward validation. It is generic and will work for any in-memory univariate time series provided as a list or NumPy array. We can make sure all the pieces work together by testing it on a contrived 10-step dataset. The complete example is listed below. Running the example first prints the contrived time series dataset. Next, the model configurations and their errors are reported as they are evaluated. Finally, the configurations and the error for the top three configurations are reported. We can see that the persistence model with a configuration of 1 (e.g. persist the last observation) achieves the best performance of the simple models tested, as would be expected. Now that we have a robust framework for grid searching simple model hyperparameters, let¡¯s test it out on a suite of standard univariate time series datasets. The results demonstrated on each dataset provide a baseline of performance that can be used to compare more sophisticated methods, such as SARIMA, ETS, and even machine learning methods. The ¡®daily female births¡¯ dataset summarizes the daily total female births in California, USA in 1959. The dataset has no obvious trend or seasonal component. Line Plot of the Daily Female Births Dataset You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®daily-total-female-births.csv¡® in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). The dataset has one year, or 365 observations. We will use the first 200 for training and the remaining 165 as the test set. The complete example grid searching the daily female univariate time series forecasting problem is listed below. Running the example prints the model configurations and the RMSE are printed as the models are evaluated. The top three model configurations and their error are reported at the end of the run. We can see that the best result was an RMSE of about 6.93 births with the following configuration: This is surprising given the lack of trend or seasonality, I would have expected either a persistence of -1 or an average of the entire historical dataset to result in the best performance. The ¡®shampoo¡¯ dataset summarizes the monthly sales of shampoo over a three-year period. The dataset contains an obvious trend but no obvious seasonal component. Line Plot of the Monthly Shampoo Sales Dataset You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®shampoo.csv¡® in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). The dataset has three years, or 36 observations. We will use the first 24 for training and the remaining 12 as the test set. The complete example grid searching the shampoo sales univariate time series forecasting problem is listed below. Running the example prints the configurations and the RMSE are printed as the models are evaluated. The top three model configurations and their error are reported at the end of the run. We can see that the best result was an RMSE of about 95.69 sales with the following configuration: This is surprising as the trend structure of the data would suggest that persisting the previous value (-1) would be the best approach, not persisting the second last value. The ¡®monthly mean temperatures¡¯ dataset summarizes the monthly average air temperatures in Nottingham Castle, England from 1920 to 1939 in degrees Fahrenheit. The dataset has an obvious seasonal component and no obvious trend. Line Plot of the Monthly Mean Temperatures Dataset You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®monthly-mean-temp.csv¡® in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). The dataset has 20 years, or 240 observations. We will trim the dataset to the last five years of data (60 observations) in order to speed up the model evaluation process and use the last year or 12 observations for the test set. The period of the seasonal component is about one year, or 12 observations. We will use this as the seasonal period in the call to the simple_configs() function when preparing the model configurations. The complete example grid searching the monthly mean temperature time series forecasting problem is listed below. Running the example prints the model configurations and the RMSE are printed as the models are evaluated. The top three model configurations and their error are reported at the end of the run. We can see that the best result was an RMSE of about 1.501 degrees with the following configuration: This finding is not too surprising. Given the seasonal structure of the data, we would expect a function of the last few observations at prior points in the yearly cycle to be effective. The ¡®monthly car sales¡¯ dataset summarizes the monthly car sales in Quebec, Canada between 1960 and 1968. The dataset has an obvious trend and seasonal component. Line Plot of the Monthly Car Sales Dataset You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®monthly-car-sales.csv¡® in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). The dataset has 9 years, or 108 observations. We will use the last year or 12 observations as the test set. The period of the seasonal component could be six months or 12 months. We will try both as the seasonal period in the call to the simple_configs() function when preparing the model configurations. The complete example grid searching the monthly car sales time series forecasting problem is listed below. Running the example prints the model configurations and the RMSE are printed as the models are evaluated. The top three model configurations and their error are reported at the end of the run. We can see that the best result was an RMSE of about 1841.155 sales with the following configuration: It is not surprising that the chosen model is a function of the last few observations at the same point in prior cycles, although the use of the median instead of the mean may not have been immediately obvious and the results were much better than the mean. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop a framework from scratch for grid searching simple naive and averaging strategies for time series forecasting with univariate data. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. sir It is one of the nice blog about learning.I am also writing blogs related to machine learning and block chain, if you could help me out I don¡¯t know about block chain, sorry. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): observation(26), configuration(23), strategy(14), method(9), model(9), sale(9), value(8), implement(6), result(5), birth(4)"
"3","mastery",2018-10-24,"How to Grid Search SARIMA Model Hyperparameters for Time Series Forecasting in Python","https://machinelearningmastery.com/how-to-grid-search-sarima-model-hyperparameters-for-time-series-forecasting-in-python/","The Seasonal Autoregressive Integrated Moving Average, or SARIMA, model is an approach for modeling univariate time series data that may contain trend and seasonal components. It is an effective approach for time series forecasting, although it requires careful analysis and domain expertise in order to configure the seven or more model hyperparameters. An alternative approach to configuring the model that makes use of fast and parallel modern hardware is to grid search a suite of hyperparameter configurations in order to discover what works best. Often, this process can reveal non-intuitive model configurations that result in lower forecast error than those configurations specified through careful analysis. In this tutorial, you will discover how to develop a framework for grid searching all of the SARIMA model hyperparameters for univariate time series forecasting. After completing this tutorial, you will know: Let¡¯s get started. How to Grid Search SARIMA Model Hyperparameters for Time Series Forecasting in PythonPhoto by Thomas, some rights reserved. This tutorial is divided into six parts; they are: Seasonal Autoregressive Integrated Moving Average, SARIMA or Seasonal ARIMA, is an extension of ARIMA that explicitly supports univariate time series data with a seasonal component. It adds three new hyperparameters to specify the autoregression (AR), differencing (I), and moving average (MA) for the seasonal component of the series, as well as an additional parameter for the period of the seasonality. A seasonal ARIMA model is formed by including additional seasonal terms in the ARIMA [¡¦] The seasonal part of the model consists of terms that are very similar to the non-seasonal components of the model, but they involve backshifts of the seasonal period. <U+2014> Page 242, Forecasting: principles and practice, 2013. Configuring a SARIMA requires selecting hyperparameters for both the trend and seasonal elements of the series. There are three trend elements that require configuration. They are the same as the ARIMA model; specifically: There are four seasonal elements that are not part of ARIMA that must be configured; they are: Together, the notation for a SARIMA model is specified as: The SARIMA model can subsume the ARIMA, ARMA, AR, and MA models via model configuration parameters. The trend and seasonal hyperparameters of the model can be configured by analyzing autocorrelation and partial autocorrelation plots, and this can take some expertise. An alternative approach is to grid search a suite of model configurations and discover which configurations work best for a specific univariate time series. Seasonal ARIMA models can potentially have a large number of parameters and combinations of terms. Therefore, it is appropriate to try out a wide range of models when fitting to data and choose a best fitting model using an appropriate criterion ¡¦ <U+2014> Pages 143-144, Introductory Time Series with R, 2009. This approach can be faster on modern computers than an analysis process and can reveal surprising findings that might not be obvious and result in lower forecast error. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this section, we will develop a framework for grid searching SARIMA model hyperparameters for a given univariate time series forecasting problem. We will use the implementation of SARIMA provided by the statsmodels library. This model has hyperparameters that control the nature of the model performed for the series, trend and seasonality, specifically: If you know enough about your problem to specify one or more of these parameters, then you should specify them. If not, you can try grid searching these parameters. We can start-off by defining a function that will fit a model with a given configuration and make a one-step forecast. The sarima_forecast() below implements this behavior. The function takes an array or list of contiguous prior observations and a list of configuration parameters used to configure the model, specifically two tuples and a string for the trend order, seasonal order trend, and parameter. We also try to make the model robust by relaxing constraints, such as that the data must be stationary and that the MA transform be invertible. Next, we need to build up some functions for fitting and evaluating a model repeatedly via walk-forward validation, including splitting a dataset into train and test sets and evaluating one-step forecasts. We can split a list or NumPy array of data using a slice given a specified size of the split, e.g. the number of time steps to use from the data in the test set. The train_test_split() function below implements this for a provided dataset and a specified number of time steps to use in the test set. After forecasts have been made for each step in the test dataset, they need to be compared to the test set in order to calculate an error score. There are many popular error scores for time series forecasting. In this case we will use root mean squared error (RMSE), but you can change this to your preferred measure, e.g. MAPE, MAE, etc. The measure_rmse() function below will calculate the RMSE given a list of actual (the test set) and predicted values. We can now implement the walk-forward validation scheme. This is a standard approach to evaluating a time series forecasting model that respects the temporal ordering of observations. First, a provided univariate time series dataset is split into train and test sets using the train_test_split() function. Then the number of observations in the test set are enumerated. For each we fit a model on all of the history and make a one step forecast. The true observation for the time step is then added to the history and the process is repeated. The sarima_forecast() function is called in order to fit a model and make a prediction. Finally, an error score is calculated by comparing all one-step forecasts to the actual test set by calling the measure_rmse() function. The walk_forward_validation() function below implements this, taking a univariate time series, a number of time steps to use in the test set, and an array of model configuration. If you are interested in making multi-step predictions, you can change the call to predict() in the sarima_forecast() function and also change the calculation of error in the measure_rmse() function. We can call walk_forward_validation() repeatedly with different lists of model configurations. One possible issue is that some combinations of model configurations may not be called for the model and will throw an exception, e.g. specifying some but not all aspects of the seasonal structure in the data. Further, some models may also raise warnings on some data, e.g. from the linear algebra libraries called by the statsmodels library. We can trap exceptions and ignore warnings during the grid search by wrapping all calls to walk_forward_validation() with a try-except and a block to ignore warnings. We can also add debugging support to disable these protections in the case we want to see what is really going on. Finally, if an error does occur, we can return a None result, otherwise we can print some information about the skill of each model evaluated. This is helpful when a large number of models are evaluated. The score_model() function below implements this and returns a tuple of (key and result), where the key is a string version of the tested model configuration. Next, we need a loop to test a list of different model configurations. This is the main function that drives the grid search process and will call the score_model() function for each model configuration. We can dramatically speed up the grid search process by evaluating model configurations in parallel. One way to do that is to use the Joblib library. We can define a Parallel object with the number of cores to use and set it to the number of scores detected in your hardware. We can then can then create a list of tasks to execute in parallel, which will be one call to the score_model() function for each model configuration we have. Finally, we can use the Parallel object to execute the list of tasks in parallel. That¡¯s it. We can also provide a non-parallel version of evaluating all model configurations in case we want to debug something. The result of evaluating a list of configurations will be a list of tuples, each with a name that summarizes a specific model configuration and the error of the model evaluated with that configuration as either the RMSE or None if there was an error. We can filter out all scores with a None. We can then sort all tuples in the list by the score in ascending order (best are first), then return this list of scores for review. The grid_search() function below implements this behavior given a univariate time series dataset, a list of model configurations (list of lists), and the number of time steps to use in the test set. An optional parallel argument allows the evaluation of models across all cores to be tuned on or off, and is on by default. We¡¯re nearly done. The only thing left to do is to define a list of model configurations to try for a dataset. We can define this generically. The only parameter we may want to specify is the periodicity of the seasonal component in the series, if one exists. By default, we will assume no seasonal component. The sarima_configs() function below will create a list of model configurations to evaluate. The configurations assume each of the AR, MA, and I components for trend and seasonality are low order, e.g. off (0) or in [1,2]. You may want to extend these ranges if you believe the order may be higher. An optional list of seasonal periods can be specified, and you could even change the function to specify other elements that you may know about your time series. In theory, there are 1,296 possible model configurations to evaluate, but in practice, many will not be valid and will result in an error that we will trap and ignore. We now have a framework for grid searching SARIMA model hyperparameters via one-step walk-forward validation. It is generic and will work for any in-memory univariate time series provided as a list or NumPy array. We can make sure all the pieces work together by testing it on a contrived 10-step dataset. The complete example is listed below. Running the example first prints the contrived time series dataset. Next, the model configurations and their errors are reported as they are evaluated, truncated below for brevity. Finally, the configurations and the error for the top three configurations are reported. We can see that many models achieve perfect performance on this simple linearly increasing contrived time series problem. Now that we have a robust framework for grid searching SARIMA model hyperparameters, let¡¯s test it out on a suite of standard univariate time series datasets. The datasets were chosen for demonstration purposes; I am not suggesting that a SARIMA model is the best approach for each dataset; perhaps an ETS or something else would be more appropriate in some cases. The ¡®daily female births¡¯ dataset summarizes the daily total female births in California, USA in 1959. The dataset has no obvious trend or seasonal component. Line Plot of the Daily Female Births Dataset You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®daily-total-female-births.csv¡® in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). The dataset has one year, or 365 observations. We will use the first 200 for training and the remaining 165 as the test set. The complete example grid searching the daily female univariate time series forecasting problem is listed below. Running the example may take a few minutes on modern hardware. Model configurations and the RMSE are printed as the models are evaluated The top three model configurations and their error are reported at the end of the run. We can see that the best result was an RMSE of about 6.77 births with the following configuration: It is surprising that a configuration with some seasonal elements resulted in the lowest error. I would not have guessed at this configuration and would have likely stuck with an ARIMA model. The ¡®shampoo¡¯ dataset summarizes the monthly sales of shampoo over a three-year period. The dataset contains an obvious trend but no obvious seasonal component. Line Plot of the Monthly Shampoo Sales Dataset You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®shampoo.csv¡¯ in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). The dataset has three years, or 36 observations. We will use the first 24 for training and the remaining 12 as the test set. The complete example grid searching the shampoo sales univariate time series forecasting problem is listed below. Running the example may take a few minutes on modern hardware. Model configurations and the RMSE are printed as the models are evaluated The top three model configurations and their error are reported at the end of the run. We can see that the best result was an RMSE of about 54.76 sales with the following configuration: The ¡®monthly mean temperatures¡¯ dataset summarizes the monthly average air temperatures in Nottingham Castle, England from 1920 to 1939 in degrees Fahrenheit. The dataset has an obvious seasonal component and no obvious trend. Line Plot of the Monthly Mean Temperatures Dataset You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®monthly-mean-temp.csv¡® in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). The dataset has 20 years, or 240 observations. We will trim the dataset to the last five years of data (60 observations) in order to speed up the model evaluation process and use the last year, or 12 observations, for the test set. The period of the seasonal component is about one year, or 12 observations. We will use this as the seasonal period in the call to the sarima_configs() function when preparing the model configurations. The complete example grid searching the monthly mean temperature time series forecasting problem is listed below. Running the example may take a few minutes on modern hardware. Model configurations and the RMSE are printed as the models are evaluated The top three model configurations and their error are reported at the end of the run. We can see that the best result was an RMSE of about 1.5 degrees with the following configuration: As we would expect, the model has no trend component and a 12-month seasonal ARMA component. The ¡®monthly car sales¡¯ dataset summarizes the monthly car sales in Quebec, Canada between 1960 and 1968. The dataset has an obvious trend and seasonal component. Line Plot of the Monthly Car Sales Dataset You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®monthly-car-sales.csv¡® in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). The dataset has 9 years, or 108 observations. We will use the last year, or 12 observations, as the test set. The period of the seasonal component could be six months or 12 months. We will try both as the seasonal period in the call to the sarima_configs() function when preparing the model configurations. The complete example grid searching the monthly car sales time series forecasting problem is listed below. Running the example may take a few minutes on modern hardware. Model configurations and the RMSE are printed as the models are evaluated The top three model configurations and their error are reported at the end of the run. We can see that the best result was an RMSE of about 1,551 sales with the following configuration: This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop a framework for grid searching all of the SARIMA model hyperparameters for univariate time series forecasting. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Thanks for sharing.can you support a pay way of Zhi fubao or Wechat?Because i am in china,and payment of visa or paypal is kind of complex. Thanks for the suggestion, I answer this question here:https://machinelearningmastery.com/faq/single-faq/can-i-pay-via-wechat-pay-or-alipay Many thanks for the article. It¡¯s really impressive, but eight nested for loops are not pythonic. Probably, there is a smarter way to do it. For instance, use ¡®itertools.product¡¯ I could not agree more! Yes, cartesian product or similar. Thanks for this, I¡¯ve been learning about ARIMA for my applications. I learnt a lot from Forecasting Principles and Practice which has an opentexts online version: https://otexts.org/fpp2/arima.html One of the main issues I have with SARIMA (or maybe it¡¯s just statsmodels) is that it doesn¡¯t handle very large datasets very well. I have timeseries data with records every 5 minutes for ~6 years with 1 day or 1 week season length, ie. monday 9am is likely to correlate with monday 9am next week, which shows up in the differencing and ACF plots. Is there a better way or a better library to do this? You could try exposing/engineering the features and fit a linear regression model directly to the data. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): configuration(29), hyperparameter(11), model(11), observation(11), sale(9), element(5), implement(5), minute(5), parameter(5), birth(4)"
"4","mastery",2018-10-22,"How to Grid Search Triple Exponential Smoothing for Time Series Forecasting in Python","https://machinelearningmastery.com/how-to-grid-search-triple-exponential-smoothing-for-time-series-forecasting-in-python/","Exponential smoothing is a time series forecasting method for univariate data that can be extended to support data with a systematic trend or seasonal component. It is common practice to use an optimization process to find the model hyperparameters that result in the exponential smoothing model with the best performance for a given time series dataset. This practice applies only to the coefficients used by the model to describe the exponential structure of the level, trend, and seasonality. It is also possible to automatically optimize other hyperparameters of an exponential smoothing model, such as whether or not to model the trend and seasonal component and if so, whether to model them using an additive or multiplicative method. In this tutorial, you will discover how to develop a framework for grid searching all of the exponential smoothing model hyperparameters for univariate time series forecasting. After completing this tutorial, you will know: Let¡¯s get started. How to Grid Search Triple Exponential Smoothing for Time Series Forecasting in PythonPhoto by john mcsporran, some rights reserved. This tutorial is divided into six parts; they are: Exponential smoothing is a time series forecasting method for univariate data. Time series methods like the Box-Jenkins ARIMA family of methods develop a model where the prediction is a weighted linear sum of recent past observations or lags. Exponential smoothing forecasting methods are similar in that a prediction is a weighted sum of past observations, but the model explicitly uses an exponentially decreasing weight for past observations. Specifically, past observations are weighted with a geometrically decreasing ratio. Forecasts produced using exponential smoothing methods are weighted averages of past observations, with the weights decaying exponentially as the observations get older. In other words, the more recent the observation, the higher the associated weight. <U+2014> Page 171, Forecasting: principles and practice, 2013. Exponential smoothing methods may be considered as peers and an alternative to the popular Box-Jenkins ARIMA class of methods for time series forecasting. Collectively, the methods are sometimes referred to as ETS models, referring to the explicit modeling of Error, Trend, and Seasonality. There are three types of exponential smoothing; they are: A triple exponential smoothing model subsumes single and double exponential smoothing by the configuration of the nature of the trend (additive, multiplicative, or none) and the nature of the seasonality (additive, multiplicative, or none), as well as any dampening of the trend. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this section, we will develop a framework for grid searching exponential smoothing model hyperparameters for a given univariate time series forecasting problem. We will use the implementation of Holt-Winters Exponential Smoothing provided by the statsmodels library. This model has hyperparameters that control the nature of the exponential performed for the series, trend, and seasonality, specifically: All four of these hyperparameters can be specified when defining the model. If they are not specified, the library will automatically tune the model and find the optimal values for these hyperparameters (e.g. optimized=True). There are other hyperparameters that the model will not automatically tune that you may want to specify; they are: If you know enough about your problem to specify one or more of these parameters, then you should specify them. If not, you can try grid searching these parameters. We can start-off by defining a function that will fit a model with a given configuration and make a one-step forecast. The exp_smoothing_forecast() below implements this behavior. The function takes an array or list of contiguous prior observations and a list of configuration parameters used to configure the model. The configuration parameters in order are: the trend type, the dampening type, the seasonality type, the seasonal period, whether or not to use a Box-Cox transform, and whether or not to remove the bias when fitting the model. Next, we need to build up some functions for fitting and evaluating a model repeatedly via walk-forward validation, including splitting a dataset into train and test sets and evaluating one-step forecasts. We can split a list or NumPy array of data using a slice given a specified size of the split, e.g. the number of time steps to use from the data in the test set. The train_test_split() function below implements this for a provided dataset and a specified number of time steps to use in the test set. After forecasts have been made for each step in the test dataset, they need to be compared to the test set in order to calculate an error score. There are many popular errors scores for time series forecasting. In this case, we will use root mean squared error (RMSE), but you can change this to your preferred measure, e.g. MAPE, MAE, etc. The measure_rmse() function below will calculate the RMSE given a list of actual (the test set) and predicted values. We can now implement the walk-forward validation scheme. This is a standard approach to evaluating a time series forecasting model that respects the temporal ordering of observations. First, a provided univariate time series dataset is split into train and test sets using the train_test_split() function. Then the number of observations in the test set are enumerated. For each, we fit a model on all of the history and make a one step forecast. The true observation for the time step is then added to the history, and the process is repeated. The exp_smoothing_forecast() function is called in order to fit a model and make a prediction. Finally, an error score is calculated by comparing all one-step forecasts to the actual test set by calling the measure_rmse() function. The walk_forward_validation() function below implements this, taking a univariate time series, a number of time steps to use in the test set, and an array of model configurations. If you are interested in making multi-step predictions, you can change the call to predict() in the exp_smoothing_forecast() function and also change the calculation of error in the measure_rmse() function. We can call walk_forward_validation() repeatedly with different lists of model configurations. One possible issue is that some combinations of model configurations may not be called for the model and will throw an exception, e.g. specifying some but not all aspects of the seasonal structure in the data. Further, some models may also raise warnings on some data, e.g. from the linear algebra libraries called by the statsmodels library. We can trap exceptions and ignore warnings during the grid search by wrapping all calls to walk_forward_validation() with a try-except and a block to ignore warnings. We can also add debugging support to disable these protections in case we want to see what is really going on. Finally, if an error does occur, we can return a None result; otherwise, we can print some information about the skill of each model evaluated. This is helpful when a large number of models are evaluated. The score_model() function below implements this and returns a tuple of (key and result), where the key is a string version of the tested model configuration. Next, we need a loop to test a list of different model configurations. This is the main function that drives the grid search process and will call the score_model() function for each model configuration. We can dramatically speed up the grid search process by evaluating model configurations in parallel. One way to do that is to use the Joblib library. We can define a Parallel object with the number of cores to use and set it to the number of CPU cores detected in your hardware. We can then create a list of tasks to execute in parallel, which will be one call to the score_model() function for each model configuration we have. Finally, we can use the Parallel object to execute the list of tasks in parallel. That¡¯s it. We can also provide a non-parallel version of evaluating all model configurations in case we want to debug something. The result of evaluating a list of configurations will be a list of tuples, each with a name that summarizes a specific model configuration and the error of the model evaluated with that configuration as either the RMSE or None if there was an error. We can filter out all scores with a None. We can then sort all tuples in the list by the score in ascending order (best are first), then return this list of scores for review. The grid_search() function below implements this behavior given a univariate time series dataset, a list of model configurations (list of lists), and the number of time steps to use in the test set. An optional parallel argument allows the evaluation of models across all cores to be tuned on or off, and is on by default. We¡¯re nearly done. The only thing left to do is to define a list of model configurations to try for a dataset. We can define this generically. The only parameter we may want to specify is the periodicity of the seasonal component in the series, if one exists. By default, we will assume no seasonal component. The exp_smoothing_configs() function below will create a list of model configurations to evaluate. An optional list of seasonal periods can be specified, and you could even change the function to specify other elements that you may know about your time series. In theory, there are 72 possible model configurations to evaluate, but in practice, many will not be valid and will result in an error that we will trap and ignore. We now have a framework for grid searching triple exponential smoothing model hyperparameters via one-step walk-forward validation. It is generic and will work for any in-memory univariate time series provided as a list or NumPy array. We can make sure all the pieces work together by testing it on a contrived 10-step dataset. The complete example is listed below. Running the example first prints the contrived time series dataset. Next, the model configurations and their errors are reported as they are evaluated. Finally, the configurations and the error for the top three configurations are reported. We do not report the model parameters optimized by the model itself. It is assumed that you can achieve the same result again by specifying the broader hyperparameters and allow the library to find the same internal parameters. You can access these internal parameters by refitting a standalone model with the same configuration and printing the contents of the ¡®params¡® attribute on the model fit; for example: Now that we have a robust framework for grid searching ETS model hyperparameters, let¡¯s test it out on a suite of standard univariate time series datasets. The datasets were chosen for demonstration purposes; I am not suggesting that an ETS model is the best approach for each dataset, and perhaps an SARIMA or something else would be more appropriate in some cases. The ¡®daily female births¡¯ dataset summarizes the daily total female births in California, USA in 1959. The dataset has no obvious trend or seasonal component. Line Plot of the Daily Female Births Dataset You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®daily-total-female-births.csv¡® in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). The dataset has one year, or 365 observations. We will use the first 200 for training and the remaining 165 as the test set. The complete example grid searching the daily female univariate time series forecasting problem is listed below. Running the example may take a few minutes as fitting each ETS model can take about a minute on modern hardware. Model configurations and the RMSE are printed as the models are evaluated The top three model configurations and their error are reported at the end of the run. We can see that the best result was an RMSE of about 6.96 births with the following configuration: What is surprising is that a model that assumed an multiplicative trend performed better than one that didn¡¯t. We would not know that this is the case unless we threw out assumptions and grid searched models. The ¡®shampoo¡¯ dataset summarizes the monthly sales of shampoo over a three-year period. The dataset contains an obvious trend but no obvious seasonal component. Line Plot of the Monthly Shampoo Sales Dataset You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®shampoo.csv¡¯ in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). The dataset has three years, or 36 observations. We will use the first 24 for training and the remaining 12 as the test set. The complete example grid searching the shampoo sales univariate time series forecasting problem is listed below. Running the example is fast given there are a small number of observations. Model configurations and the RMSE are printed as the models are evaluated. The top three model configurations and their error are reported at the end of the run. We can see that the best result was an RMSE of about 83.74 sales with the following configuration: The ¡®monthly mean temperatures¡¯ dataset summarizes the monthly average air temperatures in Nottingham Castle, England from 1920 to 1939 in degrees Fahrenheit. The dataset has an obvious seasonal component and no obvious trend. Line Plot of the Monthly Mean Temperatures Dataset You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®monthly-mean-temp.csv¡¯ in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). The dataset has 20 years, or 240 observations. We will trim the dataset to the last five years of data (60 observations) in order to speed up the model evaluation process and use the last year, or 12 observations, for the test set. The period of the seasonal component is about one year, or 12 observations. We will use this as the seasonal period in the call to the exp_smoothing_configs() function when preparing the model configurations. The complete example grid searching the monthly mean temperature time series forecasting problem is listed below. Running the example is relatively slow given the large amount of data. Model configurations and the RMSE are printed as the models are evaluated. The top three model configurations and their error are reported at the end of the run. We can see that the best result was an RMSE of about 1.50 degrees with the following configuration: The ¡®monthly car sales¡¯ dataset summarizes the monthly car sales in Quebec, Canada between 1960 and 1968. The dataset has an obvious trend and seasonal component. Line Plot of the Monthly Car Sales Dataset You can learn more about the dataset from DataMarket. Download the dataset directly from here: Save the file with the filename ¡®monthly-car-sales.csv¡¯ in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). The dataset has nine years, or 108 observations. We will use the last year, or 12 observations, as the test set. The period of the seasonal component could be six months or 12 months. We will try both as the seasonal period in the call to the exp_smoothing_configs() function when preparing the model configurations. The complete example grid searching the monthly car sales time series forecasting problem is listed below. Running the example is slow given the large amount of data. Model configurations and the RMSE are printed as the models are evaluated. The top three model configurations and their error are reported at the end of the run. We can see that the best result was an RMSE of about 1,672 sales with the following configuration: This is a little surprising as I would have guessed that a six-month seasonal model would be the preferred approach. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop a framework for grid searching all of the exponential smoothing model hyperparameters for univariate time series forecasting. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. =>sorry,dear.my nam¡¦ Jon. Living now ¡°Bangladesh¡± proffsanally me 15yeasr my life supot my brothers business said, mobilephone service and seal said,so technology said allows real us so good,bat i am not Happy because i no job, only miss us moneys, so one qus???
=>god son for you,any good said us me,and my barind,?give me you my life.
=>one help full job, (i no interst job service moneys/only free service? Your couasc god son any poor man)us me?pleasc dear,
@-joni Sorry, I don¡¯t follow. Thanks a lot , very interesting article !!! Thanks. Hey Jason,
So I tried this above code, but one interesting issue that¡¯s occurring is that it is not performing the ETS on whenever there is multiplicative trend or seasonality. If you see your results as well, you¡¯ll notice this thing. Since you are using try and except, this isn¡¯t giving an error, but in the actual case, it gives error by saying that endog<=0.0 Now with a 0 value in the TS data, this analogy makes sense, but even your dataset cases, there is no 0 value as such. So why is it happening and can you just check if that's what happening in your case as well. Moreover, I converted my time series data into just an array of values and this worked. But the same thing you have done but it still doesn't give any multiplicative results in your code that you've created. Can you take a look. Thanks, I¡¯ll investigate. UPDATE You¡¯re right. I have updated the usage of the ExponentialSmoothing to take a numpy array instead of a list. This results in correct usage of ¡®mul¡¯ for trend and seasonality. I have updated the code in all examples. Thanks! Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): configuration(24), observation(18), hyperparameter(11), model(9), sale(9), method(7), parameter(7), implement(5), birth(4), forecast(4)"
"5","vidhya",2018-10-25,"Stock Prices Prediction Using Machine Learning and Deep Learning Techniques (with Python codes)","https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/","Predicting how the stock market will perform is one of the most difficult things to do. There are so many factors involved in the prediction <U+2013> physical factors vs. physhological, rational and irrational behaviour, etc. All these aspects combine to make share prices volatile and very difficult to predict with a high degree of accuracy. Can we use machine learning as a game changer in this domain? Using features like the latest announcements about an organization, their quarterly revenue results, etc., machine learning techniques have the potential to unearth patterns and insights we didn¡¯t see before, and these can be used to make unerringly accurate predictions. In this article, we will work with historical data about the stock prices of a publicly listed company. We will implement a mix of machine learning algorithms to predict the future stock price of this company,<U+00A0>starting with simple algorithms like averaging and linear regression, and then moving on to advanced techniques like Auto ARIMA and LSTM. The core idea behind this article is to showcase how these algorithms are implemented, so I will briefly describe the technique and provide relevant links to brush up on the concepts as and when necessary. In case you¡¯re a newcomer to the world of time series,<U+00A0>I suggest going through the following articles first: We¡¯ll dive into the implementation part of this article soon, but first it¡¯s important to establish what we¡¯re aiming to solve. Broadly, stock market analysis is divided into two parts <U+2013> Fundamental Analysis and Technical Analysis. As you might have guessed, our focus will be on the technical analysis part. We¡¯ll be using a dataset from<U+00A0>Quandl<U+00A0>(you can find historical data for various stocks here) and for this particular project, I have used the data for ¡®Tata Global Beverages¡¯. Time to dive in! We will first load the dataset and define the target variable for the problem: There are multiple variables in the dataset <U+2013> date, open, high, low, last, close, total_trade_quantity, and turnover. Another important thing to note is that the market is closed on weekends and public holidays.Notice the above table again, some date values are missing <U+2013> 2/10/2018, 6/10/2018, 7/10/2018. Of these dates, 2nd is a national holiday while 6th and 7th fall on a weekend. The profit or loss calculation is usually determined by the closing price of a stock for the day, hence we will consider the closing price as the target variable. Let¡¯s plot the target variable to understand how it¡¯s shaping up in our data: In the upcoming sections, we will explore these variables and use different techniques to predict the daily closing price of the stock. ¡®Average¡¯ is easily one of the most common things we use in our day-to-day lives. For instance, calculating the average marks to determine overall performance, or finding the average temperature of the past few days to get an idea about today¡¯s temperature <U+2013> these all are routine tasks we do on a regular basis. So this is a good starting point to use on our dataset for making predictions. The predicted closing price for each day will be the average of a set of previously observed values. Instead of using the simple average, we will be using the moving average technique which uses the latest set of values for each prediction. In other words, for each subsequent step, the predicted values are taken into consideration while removing the oldest observed value from the set. Here is a simple figure that will help you understand this with more clarity. We will implement this technique on our dataset. The first step is to create a dataframe that contains only the Date and Close price columns, then split it into train and validation sets to verify our predictions. While splitting the data into train and validation, we cannot use random splitting since that will destroy the time component. So here I have set the last year¡¯s data into validation and the 4 years¡¯ data before that into train. The next step is to create predictions for the validation set and check the RMSE using the actual values. Just checking the RMSE does not help us in understanding how the model performed. Let¡¯s visualize this to get a more intuitive understanding. So here is a plot of the predicted values along with the actual values. The RMSE value is close to 105 but the results are not very promising (as you can gather from the plot). The predicted values are of the same range as the observed values in the train set (there is an increasing trend initially and then a slow decrease). In the next section, we will look at two commonly used machine learning techniques <U+2013> Linear Regression and kNN, and see how they perform on our stock market data. The most basic machine learning algorithm that can be implemented on this data is linear regression. The linear regression model returns an equation that determines the relationship between the independent variables and the dependent variable. The equation for linear regression can be written as: Here, x1, x2,¡¦.xn represent the independent variables while the coefficients ¥è1, ¥è2, ¡¦. ¥èn<U+00A0> represent the weights. You can refer to the following article to study linear regression in more detail: For our problem statement, we do not have a set of independent variables. We have only the dates instead. Let us use the date column to extract features like <U+2013> day, month, year, <U+00A0>mon/fri etc. and then fit a linear regression model. We will first sort the dataset in ascending order and then create a separate dataset so that any new feature created does not affect the original data. This creates features such as: ¡®Year¡¯, ¡®Month¡¯, ¡®Week¡¯, ¡®Day¡¯, ¡®Dayofweek¡¯, ¡®Dayofyear¡¯, ¡®Is_month_end¡¯, ¡®Is_month_start¡¯, ¡®Is_quarter_end¡¯, ¡®Is_quarter_start¡¯, <U+00A0>¡®Is_year_end¡¯, and<U+00A0> ¡®Is_year_start¡¯. Note: I have used add_datepart from fastai library. If you do not have it installed, you can simply use the command pip install fastai. Otherwise, you can create these feature using simple for loops in python. I have shown an example below. Apart from this, we can add our own set of features that we believe would be relevant for the predictions. For instance, my hypothesis is that the first and last days of the week could potentially affect the closing price of the stock far more than the other days. So I have created a feature that identifies whether a given day is Monday/Friday or Tuesday/Wednesday/Thursday. This can be done using the following lines of code: If the day of week is equal to 0 or 4, the column value will be 1, otherwise 0. Similarly, you can create multiple features. If you have some ideas for features that can be helpful in predicting stock price, please share in the comment section. We will now split the data into train and validation sets to check the performance of the model. The RMSE value is higher than the previous technique, which clearly shows that linear regression has performed poorly. Let¡¯s look at the plot and understand why linear regression has not done well: Linear regression is a simple technique and quite easy to interpret, but there are a few obvious disadvantages. One problem in using regression algorithms is that the model overfits to the date and month column. Instead of taking into account the previous values from the point of prediction, the model will consider the value from the same date a month ago, or the same date/month a year ago. As seen from the plot above, for January 2016 and January 2017, there was a drop in the stock price. The model has predicted the same for January 2018. A linear regression technique can perform well for problems such as Big Mart sales where the independent features are useful for determining the target value. Another interesting ML algorithm that one can use here is kNN (k nearest neighbours). Based on the independent variables, kNN finds the similarity between new data points and old data points. Let me explain this with a simple example. Consider the height and age for 11 people. On the basis of given features (¡®Age¡¯ and ¡®Height¡¯), the table can be represented in a graphical format as shown below: To determine the weight for ID #11, kNN considers the weight of the nearest neighbors of this ID. The weight of ID #11 is predicted to be the average of it¡¯s neighbors. If we consider three neighbours (k=3) for now, the weight for ID#11 would be = (77+72+60)/3 = 69.66 kg. For a detailed understanding of kNN, you can refer to the following articles: Introduction to k-Nearest Neighbors: Simplified<U+00A0> A Practical Introduction to K-Nearest Neighbors Algorithm for Regression Using the same train and validation set from the last section: There is not a huge difference in the RMSE value, but a plot for the predicted and actual values should provide a more clear understanding. The RMSE value is almost similar to the linear regression model and the plot shows the same pattern. Like linear regression, kNN also identified a drop in January 2018 since that has been the pattern for the past years. We can safely say that regression algorithms have not performed well on this dataset. Let¡¯s go ahead and look at some time series forecasting techniques to find out how they perform when faced with this stock prices prediction challenge. ARIMA is a very popular statistical method for time series forecasting. ARIMA models take into account the past values to predict the future values. There are three important parameters in ARIMA: Parameter tuning for ARIMA consumes a lot of time. So we will use auto ARIMA which automatically selects the best combination of (p,q,d) that provides the least error. To read more about how auto ARIMA works, refer to this article: As we saw earlier, an auto ARIMA model uses past data to understand the pattern in the time series. Using these values, the model captured an increasing trend in the series. Although the predictions using this technique are far better than that of the previously implemented machine learning models, these predictions are still not close to the real values. As its evident from the plot, the model has captured a trend in the series, but does not focus on the seasonal part. In the next section, we will implement a time series model that takes both trend and seasonality of a series into account. There are a number of time series techniques that can be implemented on the stock prediction dataset, but most of these techniques require a lot of data preprocessing before fitting the model. Prophet, designed and pioneered by Facebook, is a time series forecasting library that requires no data preprocessing and is extremely simple to implement. The input for Prophet is a dataframe with two columns: date and target (ds and y). Prophet tries to capture the seasonality in the past data and works well when the dataset is large. Here is an interesting article that explains Prophet in a simple and intuitive manner: Prophet (like most time series forecasting techniques) tries to capture the trend and seasonality from past data. This model usually performs well on time series datasets, but fails to live up to it¡¯s reputation in this case. As it turns out, stock prices do not have a particular trend or seasonality. It highly depends on what is currently going on in the market and thus the prices rise and fall. Hence forecasting techniques like ARIMA, SARIMA and Prophet would not show good results for this particular problem. Let us go ahead and try another advanced technique <U+2013> Long Short Term Memory (LSTM). LSTMs are widely used for sequence prediction problems and have proven to be extremely effective. The reason they work so well is because LSTM is able to store past information that is important, and forget the information that is not. LSTM has three gates: For a more detailed understanding of LSTM and its architecture, you can go through the below article: For now, let us implement LSTM as a black box and check it¡¯s performance on our particular data. Wow! LSTM has easily outshone any algorithm we saw so far. The LSTM model can be tuned for various parameters such as changing the number of LSTM layers, adding dropout value or increasing the number of epochs. But are the predictions from LSTM enough to identify whether the stock price will increase or decrease? Certainly not! As I mentioned at the start of the article, stock price is affected by the news about the company and other factors like demonetization or merger/demerger of the companies. There are certain intangible factors as well which can often be impossible to predict beforehand. Time series forecasting is a very intriguing field to work with, as I have realized during my time writing these articles. There is a perception in the community that it¡¯s a complex field, and while there is a grain of truth in there, it¡¯s not so difficult once you get the hang of the basic techniques. I am interested in finding out how LSTM works on a different kind of time series problem and encourage you to try it out on your own as well. If you have any questions, feel free to connect with me in the comments section below. Isn¡¯t the LSTM model using your ¡°validation¡± data as part of its modeling to generate its predictions since it only goes back 60 days. Your other techniques are only using the ¡°training¡± data and don¡¯t have the benefit of looking back 60 days from the target prediction day. Is this a fair comparison? Hi James, The idea isn¡¯t to compare the techniques but to see what works best for stock market predictions. Certainly for this problem LSTM works well, while for other problems, other techniques might perform better. We can add a lookback component with LSTM is an added advantage I think that you cannot say LSTM works well because what it actually does is to predict one day ahead based on the recent 60 days. In other words, the model just go over all the validation data daily basis to predict ¡°tomorrow¡±. This is a totally different prediction scheme from the other prediction methods, which have to predict the entire validation data points without seeing any of information in the validation data. If you use the ¡°daily basis prediction¡± scheme for other mothods, any of methods would produce a good result, I guess. Hi Teru, When James first pointed out, I started looking at how can I use validation in other models (its simpler with LSTM). For ARIMA and PROPHET, the input can only be a univariate series so we can make prediction for one day, change the training set (add that day¡¯s value) after each prediction and retrain before predicting for the next day. For moving average and regression it should be comparatively easier. If you both have any suggestions as to how can I update train data with each day¡¯s values, do share your ideas, I am really interested in finding out how the results turn out to be! Getting index error <U+2013> <U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014>
IndexError Traceback (most recent call last)
in
1 #Results
<U+2014>-> 2 rms=np.sqrt(np.mean(np.power((np.array(valid[¡®Close¡¯]) <U+2013> np.array(valid[¡®Predictions¡¯])),2)))
3 rms IndexError: only integers, slices (`:`), ellipsis (`¡¦`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices Hi Jay, Please use the following command before calculating rmse valid['Predictions'] = 0valid['Predictions'] = closing_price. I have updated the same in the article After running the following codes-
train[¡®Date¡¯].min(), train[¡®Date¡¯].max(), valid[¡®Date¡¯].min(), valid[¡®Date¡¯].max() (Timestamp(¡®2013-10-08 00:00:00¡¯),
Timestamp(¡®2017-10-06 00:00:00¡¯),
Timestamp(¡®2017-10-09 00:00:00¡¯),
Timestamp(¡®2018-10-08 00:00:00¡¯)) I am getting the following error :
name ¡®Timestamp¡¯ is not defined Please help. Hi Pankaj,  The command is only train[¡®Date¡¯].min(), train[¡®Date¡¯].max(), valid[¡®Date¡¯].min(), valid[¡®Date¡¯].max() , the timestamp is the result I got by running the above command. Hi Aishwarya, Just curious. LSTM works just TOO well !! Is splitting dataset to train & valid step carry out after the normalizing step ?! i.e.
#converting dataset into x_train and y_train
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(dataset) then dataset = new_data train = dataset[:987]
valid = dataset[987:] Then this
x_train, y_train = [], []
for i in range(60,len(train )): # <- replace dataset with train ?!
x_train.append(scaled_data[i-60:i,0])
y_train.append(scaled_data[i,0])
x_train, y_train = np.array(x_train), np.array(y_train) Guide me on this. Thanks Hi Zarief, Yes, the train and test set are created after scaling the data using the for loop :  x_train, y_train = [], []
for i in range(60,len(train )):
x_train.append(scaled_data[i-60:i,0]) #we have used the scaled data here
y_train.append(scaled_data[i,0])
x_train, y_train = np.array(x_train), np.array(y_train) Secondly, the command dataset = new_data.values will be before scaling the data, as shown in the article, since dataset is used for scaling and hence must be defined before.  hi I¡¯m getting below error¡¦
¡±¡±>>> #import packages
>>> import pandas as pd Traceback (most recent call last):
File ¡°¡±, line 1, in
import pandas as pd
ImportError: No module named pandas¡±¡±¡±¡± Hi rohit, Is the issue resolved? Have you worked with pandas previously? Hi.. Thanks for nicely elaborating LSTM implementation in the article. However, in LSTM rms part if you can guide, as I am getting the following error :  valid[¡®Predictions¡¯] = 0.0
valid[¡®Predictions¡¯] = closing_price
rms=np.sqrt(np.mean(np.power((np.array(valid[¡®Close¡¯])-np.array(valid[¡®Predictions¡¯])),2)))
rms ##############################################################################
IndexError Traceback (most recent call last)
in ()
<U+2014>-> 1 valid[¡®Predictions¡¯] = 0.0
2 valid[¡®Predictions¡¯] = closing_price
3 rms=np.sqrt(np.mean(np.power((np.array(valid[¡®Close¡¯])-np.array(valid[¡®Predictions¡¯])),2)))
4 rms IndexError: only integers, slices (`:`), ellipsis (`¡¦`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices Still same error <U+2013>  <U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014><U+2014>
IndexError Traceback (most recent call last)
in
<U+2014>-> 1 valid[¡®Predictions¡¯] = closing_price IndexError: only integers, slices (`:`), ellipsis (`¡¦`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices And also why are we doing valid[¡®Predictions¡¯] = 0 and valid[¡®Predictions¡¯] = closing_price instead of valid[¡®Predictions¡¯] = closing_price Yes you can skip the line but it will still show an error because the index hasn¡¯t been defined. Have you followed the code from the start? Please add the following code lines and check if it works  new_data.index = data[¡®Date¡¯] #considering the date column has been set to datetime format
valid.index = new_data[987:].index
train.index = new_data[:987].index Let me know if this works. Other wise share the notebook you are working on and I will look into it. Hi, Nice article. I have installed fastai but I am getting the following error:
ModuleNotFoundError: No module named ¡®fastai.structured¡¯ Any idea? Hi Roberto, Directly clone it from here : https://github.com/fastai/fastai . Let me know if you still face an issue.","Keyword(freq): prediction(21), value(17), technique(13), feature(8), variable(6), algorithm(5), price(5), result(5), factor(4), neighbor(4)"
