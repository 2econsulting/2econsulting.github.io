"site","date","headline","url_address","text"
"mastery",2018-08-31,"How and When to Use ROC Curves and Precision-Recall Curves for Classification in Python","https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/","It can be more flexible to predict probabilities of an observation belonging to each class in a classification problem rather than predicting classes directly. This flexibility comes from the way that probabilities may be interpreted using different thresholds that allow the operator of the model to trade-off concerns in the errors made by the model, such as the number of false positives compared to the number of false negatives. This is required when using models where the cost of one error outweighs the cost of other types of errors. Two diagnostic tools that help in the interpretation of probabilistic forecast for binary (two-class) classification predictive modeling problems are ROC Curves and Precision-Recall curves. In this tutorial, you will discover ROC Curves, Precision-Recall Curves, and when to use each to interpret the prediction of probabilities for binary classification problems. After completing this tutorial, you will know: Let¡¯s get started. How and When to Use ROC Curves and Precision-Recall Curves for Classification in PythonPhoto by Giuseppe Milo, some rights reserved. This tutorial is divided into 6 parts; they are: In a classification problem, we may decide to predict the class values directly. Alternately, it can be more flexible to predict the probabilities for each class instead. The reason for this is to provide the capability to choose and even calibrate the threshold for how to interpret the predicted probabilities. For example, a default might be to use a threshold of 0.5, meaning that a probability in [0.0, 0.49] is a negative outcome (0) and a probability in [0.5, 1.0] is a positive outcome (1). This threshold can be adjusted to tune the behavior of the model for a specific problem. An example would be to reduce more of one or another type of error. When making a prediction for a binary or two-class classification problem, there are two types of errors that we could make. By predicting probabilities and calibrating a threshold, a balance of these two concerns can be chosen by the operator of the model. For example, in a smog prediction system, we may be far more concerned with having low false negatives than low false positives. A false negative would mean not warning about a smog day when in fact it is a high smog day, leading to health issues in the public that are unable to take precautions. A false positive means the public would take precautionary measures when they didn¡¯t need to. A common way to compare models that predict probabilities for two-class problems us to use a ROC curve. A useful tool when predicting the probability of a binary outcome is the Relative Operating Characteristic curve, or ROC curve. It is a plot of the false positive rate (x-axis) versus the true positive rate (y-axis) for a number of different candidate threshold values between 0.0 and 1.0. Put another way, it plots the false alarm rate versus the hit rate. The true positive rate is calculated as the number of true positives divided by the sum of the number of true positives and the number of false negatives. It describes how good the model is at predicting the positive class when the actual outcome is positive. The true positive rate is also referred to as sensitivity. The false positive rate is calculated as the number of false positives divided by the sum of the number of false positives and the number of true negatives. It is also called the false alarm rate as it summarizes how often a positive class is predicted when the actual outcome is negative. The false positive rate is also referred to as the inverted specificity where specificity is the total number of true negatives divided by the sum of the number of true negatives and false positives. Where: The ROC curve is a useful tool for a few reasons: The shape of the curve contains a lot of information, including what we might care about most for a problem, the expected false positive rate, and the false negative rate. To make this clear: If you are confused, remember, when we predict a binary outcome, it is either a correct prediction (true positive) or not (false positive). There is a tension between these options, the same with true negative and false positives. A skilful model will assign a higher probability to a randomly chosen real positive occurrence than a negative occurrence on average. This is what we mean when we say that the model has skill. Generally, skilful models are represented by curves that bow up to the top left of the plot. A model with no skill is represented at the point [0.5, 0.5]. A model with no skill at each threshold is represented by a diagonal line from the bottom left of the plot to the top right and has an AUC of 0.0. A model with perfect skill is represented at a point [0.0 ,1.0]. A model with perfect skill is represented by a line that travels from the bottom left of the plot to the top left and then across the top to the top right. An operator may plot the ROC curve for the final model and choose a threshold that gives a desirable balance between the false positives and false negatives. We can plot a ROC curve for a model in Python using the roc_curve() scikit-learn function. The function takes both the true outcomes (0,1) from the test set and the predicted probabilities for the 1 class. The function returns the false positive rates for each threshold, true positive rates for each threshold and thresholds. The AUC for the ROC can be calculated using the roc_auc_score() function. Like the roc_curve() function, the AUC function takes both the true outcomes (0,1) from the test set and the predicted probabilities for the 1 class. It returns the AUC score between 0.0 and 1.0 for no skill and perfect skill respectively. A complete example of calculating the ROC curve and AUC for a logistic regression model on a small test problem is listed below. Running the example prints the area under the ROC curve. A plot of the ROC curve for the model is also created showing that the model has skill. Line Plot of ROC Curve There are many ways to evaluate the skill of a prediction model. An approach in the related field of information retrieval (finding documents based on queries) measures precision and recall. These measures are also useful in applied machine learning for evaluating binary classification models. Precision is a ratio of the number of true positives divided by the sum of the true positives and false positives. It describes how good a model is at predicting the positive class. Precision is referred to as the positive predictive value. or Recall is calculated as the ratio of the number of true positives divided by the sum of the true positives and the false negatives. Recall is the same as sensitivity. or Reviewing both precision and recall is useful in cases where there is an imbalance in the observations between the two classes. Specifically, there are many examples of no event (class 0) and only a few examples of an event (class 1). The reason for this is that typically the large number of class 0 examples means we are less interested in the skill of the model at predicting class 0 correctly, e.g. high true negatives. Key to the calculation of precision and recall is that the calculations do not make use of the true negatives. It is only concerned with the correct prediction of the minority class, class 1. A precision-recall curve is a plot of the precision (y-axis) and the recall (x-axis) for different thresholds, much like the ROC curve. The no-skill line is defined by the total number of positive cases divide by the total number of positive and negative cases. For a dataset with an equal number of positive and negative cases, this is a straight line at 0.5. Points above this line show skill. A model with perfect skill is depicted as a point at [1.0,1.0]. A skilful model is represented by a curve that bows towards [1.0,1.0] above the flat line of no skill. There are also composite scores that attempt to summarize the precision and recall; three examples include: In terms of model selection, F1 summarizes model skill for a specific probability threshold, whereas average precision and area under curve summarize the skill of a model across thresholds, like ROC AUC. This makes precision-recall and a plot of precision vs. recall and summary measures useful tools for binary classification problems that have an imbalance in the observations for each class. Precision and recall can be calculated in scikit-learn via the precision_score() and recall_score() functions. The precision and recall can be calculated for thresholds using the precision_recall_curve() function that takes the true output values and the probabilities for the positive class as output and returns the precision, recall and threshold values. The F1 score can be calculated by calling the f1_score() function that takes the true class values and the predicted class values as arguments. The area under the precision-recall curve can be approximated by calling the auc() function and passing it the recall and precision values calculated for each threshold. Finally, the average precision can be calculated by calling the average_precision_score() function and passing it the true class values and the predicted class values. The complete example is listed below. Running the example first prints the F1, area under curve (AUC) and average precision (AP) scores. The precision-recall curve plot is then created showing the precision/recall for each threshold compared to a no skill model. Line Plot of Precision-Recall Curve Generally, the use of ROC curves and precision-recall curves are as follows: The reason for this recommendation is that ROC curves present an optimistic picture of the model on datasets with a class imbalance. However, ROC curves can present an overly optimistic view of an algorithm¡¯s performance if there is a large skew in the class distribution. [¡¦] Precision-Recall (PR) curves, often used in Information Retrieval , have been cited as an alternative to ROC curves for tasks with a large skew in the class distribution. <U+2014> The Relationship Between Precision-Recall and ROC Curves, 2006. Some go further and suggest that using a ROC curve with an imbalanced dataset might be deceptive and lead to incorrect interpretations of the model skill. [¡¦] the visual interpretability of ROC plots in the context of imbalanced datasets can be deceptive with respect to conclusions about the reliability of classification performance, owing to an intuitive but wrong interpretation of specificity. [Precision-recall curve] plots, on the other hand, can provide the viewer with an accurate prediction of future classification performance due to the fact that they evaluate the fraction of true positives among positive predictions <U+2014> The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets, 2015. The main reason for this optimistic picture is because of the use of true negatives in the False Positive Rate in the ROC Curve and the careful avoidance of this rate in the Precision-Recall curve. If the proportion of positive to negative instances changes in a test set, the ROC curves will not change. Metrics such as accuracy, precision, lift and F scores use values from both columns of the confusion matrix. As a class distribution changes these measures will change as well, even if the fundamental classifier performance does not. ROC graphs are based upon TP rate and FP rate, in which each dimension is a strict columnar ratio, so do not depend on class distributions. <U+2014> ROC Graphs: Notes and Practical Considerations for Data Mining Researchers, 2003. We can make this concrete with a short example. Below is the same ROC Curve example with a modified problem where there is a 10:1 ratio of class=0 to class=1 observations. Running the example suggests that the model has skill. Indeed, it has skill, but much of that skill is measured as making correct false negative predictions and there are a lot of false negative predictions to make. A plot of the ROC Curve confirms the AUC interpretation of a skilful model for most probability thresholds. Line Plot of ROC Curve Imbalanced Dataset We can also repeat the test of the same model on the same dataset and calculate a precision-recall curve and statistics instead. The complete example is listed below. Running the example first prints the F1, AUC and AP scores. The scores do not look encouraging, given skilful models are generally above 0.5. From the plot, we can see that after precision and recall crash fast. Line Plot of Precision-Recall Curve Imbalanced Dataset This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered ROC Curves, Precision-Recall Curves, and when to use each to interpret the prediction of probabilities for binary classification problems. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of scikit-learn code Discover how in my new Ebook:Machine Learning Mastery With Python Covers self-study tutorials and end-to-end projects like:Loading data, visualization, modeling, tuning, and much more¡¦ Skip the Academics. Just Results. Click to learn more. I don¡¯t think a diagonal straight line is the right baseline for P/R curve. The baseline ¡°dumb¡± classifier should be a straight line with precision=positive% You¡¯re right, thanks!  Fixed. Great tutorial. Thanks! How about the Mathews Correlation Coefficient ? I¡¯ve not used it, got some refs? Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning."
"mastery",2018-08-29,"How to Predict Room Occupancy Based on Environmental Factors","https://machinelearningmastery.com/how-to-predict-room-occupancy-based-on-environmental-factors/","Small computers, such as Arduino devices, can be used within buildings to record environmental variables from which simple and useful properties can be predicted. One example is predicting whether a room or rooms are occupied based on environmental measures such as temperature, humidity, and related measures. This is a type of common time series classification problem called room occupancy classification. In this tutorial, you will discover a standard multivariate time series classification problem for predicting room occupancy using the measurements of environmental variables. After completing this tutorial, you will know: Let¡¯s get started. This tutorial is divided into four parts; they are: A standard time series classification data set is the ¡°Occupancy Detection¡± problem available on the UCI Machine Learning repository. It is a binary classification problem which requires that an observation of environmental factors such as temperature and humidity be used to classify whether a room is occupied or unoccupied. It appears that the data was originally recorded by Zheng Yang, et al. at University of Southern California and described in their 2012 paper ¡°A Multi-Sensor Based Occupancy Estimation Model for Supporting Demand Driven HVAC Operations¡°. In the paper, they describe the use of two Arduino units to collect sensor data across multiple research labs over 20 days. The sensor data was collected for 20 consecutive days, starting from 00:00 AM, Sep. 12th to 00:00 AM, Oct. 1st. At a one-minute sampling rate, after excluding all corrupted data points due to wireless connection breaks, a total of 25,898 data points were collected in both labs. Their objective of the original project appeared to estimate the total occupancy of the rooms based on the sensor data. Arduino Black Widow Sensor NodeTaken from ¡°A Multi-Sensor Based Occupancy Estimation Model for Supporting Demand Driven HVAC Operations¡± The data was somehow retrieved, restructured, and made available on the UCI website. The number of observations and dates don¡¯t appear to match the original paper. It is quite possible that the source paper is unrelated or only partially related to the dataset. Data is provided with date-time information and six environmental measures taken each minute over multiple days, specifically: This dataset has been used in many simple modeling machine learning papers. For example, see the paper ¡°Visible Light Based Occupancy Inference Using Ensemble Learning,¡± 2018 for further references. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course The data is available in CSV format in three files, claimed to be a split of data for training, validation and testing. The three files are as follows: What is obvious at first is that the split in the data is not contiguous in time and that there are gaps. The test dataset is before the train and validation datasets in time. Perhaps this was an error in the naming convention of the files. We can also see that the data extends from Feb 2 to Feb 18, which spans 17 calendar days, not 20. Download the files from here and place them in your current working directory: Each file contains a header line, but includes a column for the row number that does not include an entry in the header line. In order to load the data files correctly, update the header line of each file to read as follows: From: To: Below is a sample of the first five lines of datatraining.txt file with the modification. We can then load the data files using the Pandas read_csv() function, as follows: Once loaded, we can create a plot for each of the six series, clearly showing the separation of the three datasets in time. The complete example is listed below. Running the example creates a plot with a different color for each dataset: We can see the small gap between the test and train sets and the larger gap between the train and validation sets. We can also see corresponding structures (peaks) in the series for each variable with the room occupancy. Line Plot Showing Time Series Plots for all variables and each dataset We can simplify the dataset by preserving the temporal consistency of the data and concatenating all three sets into a single dataset, dropping the ¡°no¡± column. This will allow ad hoc testing of simple direct framings of the problem (in the next section) that can be tested on a temporally consistent way with ad hoc train/test set sizes. Note: This simplification does not account for the temporal gaps in the data and algorithms that rely on a sequence of observations at prior time steps may require a different organization of the data. The example below loads the data, concatenates it into a temporally consistent dataset, and saves the results to a new file named ¡°combined.csv¡°. Running the example saves the concatenated dataset to the new file ¡®combined.csv¡®. The simplest formulation of the problem is to predict occupancy based on the environmental conditions at the current time. I refer to this as a direct model as it does not make use of the observations of the environmental measures at prior time steps. Technically, this is not sequence classification, it is just a straight classification problem where the observations are temporally ordered. This seems to be the standard formulation of the problem from my skim of the literature, and disappointingly, the papers seem to use the train/validation/test data as labeled on the UCI website. We will use the combined dataset described in the previous section and evaluate model skill by holding back the last 30% of the data as a test set. For example: Next, we can evaluate some models of the dataset, starting with a naive prediction model. A simple model for this formulation of the problem is to predict the most prominent class outcome. This is called the Zero Rule, or the naive prediction algorithm. We will evaluate predicting all 0 (unoccupied) and all 1 (occupied) for each example in the test set and evaluate the approach using the accuracy metric. Below is a function that will perform this naive prediction given a test set and a chosen outcome variable The complete example is listed below. Running the example prints the naive prediction and the related score. We can see that the baseline score is about 82% accuracy by predicting all 0, e.g. all no occupancy. For any model to be considered skilful on the problem, it must achieve a skill of 82% or better. A skim of the literature shows a range of sophisticated neural network models applied on this problem. To start with, let¡¯s try a simple logistic regression classification algorithm. The complete example is listed below. Running the example fits a logistic regression model on the training dataset and predicts the test dataset. The skill of the model is about 99% accurate, showing skill over the naive method. Normally, I would recommend centering and normalizing the data prior to modeling, but some trial and error demonstrated that a model on the unscaled data was more skilful. This is an impressive result at first glance. Although the test-setup is different to that presented in the research literature, the reported skill of a very simple model outperforms more sophisticated neural network models. A closer look at the time series plot shows a clear relationship between the times when the rooms are occupied and peaks in the environmental measures. This makes sense and explains why this problem is in fact so easy to model. We can further simplify the model by testing a simple logistic regression model on each environment measure in isolation. The idea is that we don¡¯t need all of the data to predict occupancy; that perhaps just one of the measures is sufficient. This is the simplest type of feature selection where a model is created and evaluated with each feature in isolation. More advanced methods may consider each subgroup of features. The complete example testing a logistic model with each of the five input features in isolation is listed below. Running the example prints the feature index, name, and the skill of a logistic model trained on that feature and evaluated on the test set. We can see that only the ¡°Light¡± variable is required in order to achieve 99% accuracy on this dataset. It is very likely that the lab rooms in which the environmental variables were recorded had a light sensor that turned internal lights on when the room was occupied. Alternately, perhaps the light is recorded during the daylight hours (e.g. sunshine through windows), and the rooms are occupied on each day, or perhaps each week day. At the very least, the results of this tutorial ask some hard questions about any research papers that use this dataset, as clearly it is not a challenging prediction problem. This data may still be interesting for further investigation. Some ideas include: I tried each of these models briefly without exciting results. If you explore any of these extensions or find some examples online, let me know in the comments below. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered a standard multivariate time series classification problem for predicting room occupancy using the measurements of environmental variables. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hi! Awesome blog!. Thanks! One thing, why don¡¯t you use plt.tight_layout() at the end of the script in order to have non-overlapping graphs? Great suggestion, thanks! Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning."
"mastery",2018-08-27,"How to Predict Whether a Persons Eyes are Open or Closed Using Brain Waves","https://machinelearningmastery.com/how-to-predict-whether-eyes-are-open-or-closed-using-brain-waves/","Evaluating machine learning models on time series forecasting problems is challenging. It is easy to make a small error in the framing of a problem or in the evaluation of models that give impressive results but result in an invalid finding. An interesting time series classification problem is predicting whether a subject¡¯s eyes are open or closed based only on their brain wave data (EEG). In this tutorial, you will discover the problem of predicting whether eyes are open or closed based on brain waves and a common methodological trap when evaluating time series forecasting models. Working through this tutorial, you will have an idea of how to avoid common traps when evaluating machine learning algorithms on time series forecast problems. These are traps that catch both beginners, expert practitioners, and academics alike. After completing this tutorial, you will know: Let¡¯s get started. This tutorial is divided into seven parts; they are: In this post, we are going to take a closer look at a problem that involves predicting whether the subjects eyes are open or closed based on brain wave data. The problem was described and data collected by Oliver Rosler and David Suendermann for their 2013 paper titled ¡°A First Step towards Eye State Prediction Using EEG¡°. I saw this dataset and I had to know more. Specifically, an electroencephalography (EEG) recording was made of a single person for 117 seconds (just under two minutes) while the subject opened and closed their eyes, which was recorded via a video camera. The open/closed state was then recorded against each time step in the EEG trace manually. The EEG was recorded using a Emotiv EEG Neuroheadset, resulting in 14 traces. Cartoon of where EEG sensors were located on the subjectTaken from ¡°A First Step towards Eye State Prediction Using EEG¡±, 2013. The output variable is binary, meaning that this is a two-class classification problem. A total of 14,980 observations (rows) were made over the 117 seconds, meaning that there were about 128 observations per second. The corpus consists of 14,977 instances with 15 attributes each (14 attributes representing the values of the electrodes and the eye state). The instances are stored in the corpus in chronological order to be able to analyze temporal dependencies. 8,255 (55.12%) instances of the corpus correspond to the eye open and 6,722 (44.88%) instances to the eye closed state. There were also some EEG observations that have a much larger than expected amplitude. These are likely outliers and can be identified and removed using a simple statistical method such as removing rows that have an observation 3-to-4 standard deviations from the mean. The simplest framing of the problem is to predict the eye-state (open/closed) given the EEG trace at the current time step. More advanced framings of the problem may seek to model the multivariate time series of each EEG trace in order to predict the current eye state. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course The dataset can be downloaded for free from the UCI Machine Learning repository: The raw data is in ARFF format (used in Weka), but can be converted to CSV by deleting the ARFF header. Below is a sample of the first five lines of the data with the ARFF header removed. We can load the data as a DataFrame and plot the time series for each EEG trace and the output variable (open/closed state). The complete code example is listed below. The example assumes that you have a copy of the dataset in CSV format with the filename ¡®EEG_Eye_State.csv¡® in the same directory as the code. Running the example creates a line plot for each EEG trace and the output variable. We can see the outliers washing out the data in each trace. We can also see the open/closed state of the eyes over time with 0/1 respectively. Line Plot for each EEG trace and the output variable It is useful to remove the outliers to better understand the relationship between the EEG traces and the open/closed state of the eyes. The example below removes all rows that have an EEG observation that is four standard deviations or more from the mean. The dataset is saved to a new file called ¡®EEG_Eye_State_no_outliers.csv¡®. It is a quick and dirty implementation of outlier detection and removal, but gets the job done. I¡¯m sure you could engineer a more efficient implementation. Running the example summarizes the rows deleted as each column in the EEG data is processed for outliers above and below the mean. We can now visualize the data without outliers by loading the new ¡®EEG_Eye_State_no_outliers.csv¡® file. Running the example creates a better plot, clearly showing little positive peaks when eyes are closed (1) and negative peaks when eyes are open (0). Line Plot for each EEG trace and the output variable without outliers The simplest predictive model is to predict the eye open/closed state based on the current EEG observation, ignoring the trace information. Intuitively, one would not expect this to be effective, nevertheless, it was the approach used in Rosler and Suendermann¡¯s 2013 paper. Specifically, they evaluated a large suite of classification algorithms in the Weka software using 10-fold cross-validation of this framing of the problem. They achieved better than 90% accuracy with multiple methods, including instance based methods such as k-nearest neighbors and KStar. However, instance-based learners such as IB1 and KStar outperformed decision trees yet again substantially. The latter achieved the clearly best performance with a classification error rate of merely 3.2%. <U+2014> A First Step towards Eye State Prediction Using EEG, 2013. A similar methodology and finding was used with the same and similar datasets in a number of other papers. I was surprised when I read this and so reproduced the result. The complete example is listed below with a k=3 KNN. Running the example prints the score for each fold of the cross validation and the mean score of 97% averaged across all 10 folds. Very impressive! But something felt wrong. I was interested to see how models that took into account the clear peaks in the data at each transition from open-to-closed and closed-to-open performed. Every model I tried using my own test harness that respected the temporal ordering of the data performed much worse. Why? Hint: think about the chosen model evaluation strategy and the type of algorithm that performed the best. Disclaimer: I am not calling out the authors of the paper or related papers. I don¡¯t care. In my experience, most published papers cannot be reproduced or have major methodological flaws (including a lot of the stuff that I have written). I¡¯m only interested in learning and teaching. There is a methodological flaw in the way that time series models are evaluated. I coach against this flaw, but after reading the paper and reproducing the result, it still tripped me up. I hope by working through this example that it will help not trip you up on your own forecast problems. The methodological flaw in the evaluation of the models is the use of k-fold cross-validation. Specifically, the evaluation of the models in a way that does not respect the temporal ordering of the observations. Key to this problem is the finding of instance-based methods, such as k-nearest neighbors, as being skillful on the problem. KNN will seek out the k most similar rows in the dataset and calculate the mode of the output state as the prediction. By not respecting the temporal order of instances when evaluating models, it allows the models to use information from the future in making the prediction. This is pronounced specifically in the KNN algorithm. Because of the high frequency of observations (128 per second), the most similar rows will be those adjacent in time to the instance being predicted, both in the past and in the future. We can make this clearer with some small experiments. The first test we can do is to evaluate the skill of a KNN model with a train/test split both when the dataset is shuffled, and when it is not. In the case when the data is shuffled prior to the split, we expect the result to be similar to the cross-validation result in the previous section, specifically if the test set is 10% of the dataset. If the theory about the importance of temporal ordering and instance-based methods using adjacent examples in the future is true, we would expect the test where the dataset is not shuffled prior to the split to be worse. First, the example below splits the dataset into train/test split with 90%/10% of the data respectively. The dataset is shuffled prior to the split. Running the example, we can see that indeed, the skill matches what we see in the cross-validation example, or close to it, at 96% accuracy. Next, we repeat the experiment without shuffling the dataset prior to the split. This means that the training data are the first 90% of the data respecting the temporal ordering of the observations, and the test dataset is the last 10%, or about 1,400 observations of the data. Running the example shows model skill that is much worse at 52%. This is a good start, but not definitive. It is possible that the last 10% of the dataset is hard to predict, given the very short open/close intervals we can see on the plot of the outcome variable. We can repeat the experiment and use the first 10% of the data in time for test and the last 90% for train. We can do this by reversing the order of the rows prior to splitting the data using the flip() function. Running the experiment produces similar results at about 52% accuracy. This gives more evidence that it is not the specific contiguous block of observations that results in the poor model skill. It looks like immediately adjacent observations are required to make good predictions. It is possible that the model requires the adjacent observations in the past (but not the future) in order to make skillful predictions. This sounds reasonable at first, but also has a problem. Nevertheless we can achieve this using walk-forward validation over the test set. This is where the model is permitted to use all observations prior to the time step being predicted as we validate a new model at each new time step in the test dataset. For more on walk-forward validation, see the post: The example below evaluates the skill of KNN using walk-forward validation over the last 10% of the dataset (about 10 seconds), respecting temporal ordering. Running the example gives an impressive model skill at about 95% accuracy. We can push this test further and only make the previous 10 observations available to the model when making a prediction. The complete example is listed below. Running the example results in a further improved model skill of nearly 99% accuracy. I would expect that the only errors being made are those at the inflection points in the EEG series when the trace transitions from open-to-closed or closed-to-open, the actual hard part of the problem. This aspect requires further investigation. Indeed, we have confirmed that the model requires adjacent observations and their outcome in order to make a prediction, and that it can do very well with only adjacent observations in the past, not the future. This is interesting. But this finding is not useful in practice. If this model was deployed, it would require the model to know the eye open/closed state in the very recent past, such as the previous 128th of a second. This will not be available. The whole idea of a model for predicting eye state based on brain waves is to have it operate without such confirmation. Let¡¯s review what we have learned so far: 1. The model evaluation methodology must take the temporal ordering of observations into account. This means that it is methodologically invalid to use k-fold cross-validation that does not stratify by time (e.g. shuffles or uses a random selection of rows). This also means that it is methodologically invalid to use a train/test split that shuffles the data prior to splitting. We saw this in the evaluation of the high skill of the model with k-fold cross-validation and shuffled train/test split compared to the low skill of the model when directly adjacent observations in time were not available at prediction time. 2. The model evaluation methodology must make sense for the use of the final model. This means that even if you use a methodology that respects the temporal ordering of the observations, the model should only have information available that it would have if the model were being used in practice. We saw this in the high skill of the model under a walk-forward validation methodology that respected the order of observations, but made information available, such as eye-state, that would not be available if the model were being used in practice. The key is to start with a framing of the problem based in the use of the final model and work backwards to the data that would be available and a methodology for evaluating the model in its framing that only operates under information that would be available in that framing. This applies doubly when you are trying to understand other people¡¯s work. Going Forward Hopefully, this helps, both when you are evaluating your own forecast models and when you are evaluating the models of others. So, how would you work this problem if presented with the raw data? I think the keys to this problem are the positive/negative peaks that are obvious in the EEG data at the times when there is a transition from eyes open-to-closed or closed-to-open. I would expect an effective model would exploit this feature, perhaps using half a second or similar of prior EEG observations. This might even be possible with a single trace, rather than 15, and a simple peak detection method from signal processing, rather than a machine learning method. Let me know if you have a go at this; I¡¯d love to see what you discover. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered the problem of predicting whether eyes are open or closed based on brain waves and a common methodological trap when evaluating time series forecasting models. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hi Jason, Thank you for this nice case study! I hope you¡¯ll be tackling more nuanced cases in the future. Best,
Elie Thanks. I may be looking at this wrong, but in the example where you have above 99% accuracy and looking at past eye state, isn¡¯t your model just learning that the current state is most likely the past state as the frequency of open and closing the eye is very slow. Yes, I believe so. It was more about the dangers of not thinking through methodology. Thanks!  Great article! Thanks Tony. Fantastic article about machine learning and evaluation pitfalls!!!
Please,give us more analysis like this,where we can learn from mistakes. Thanks. Happy it helped. Did you try using time series forecasting on stock market?hope to learn more on the financial topic! I try to avoid finance applications, there¡¯s too much emotion around the topic. In your data cleaning, you should probably use standard deviations from the median, since deviations from mean will inherently be biased because outliers will shift the mean. It probably won¡¯t make much of a difference, but I used this code: 
filtered = data[data.apply(lambda x: np.abs(x - x.median()) / x.std() < 4).all(axis=1)]
I used a 2 layer stateful LSTM and got 99%, probably because of overfitting (it predicts a constant output). Someone better than me at LSTMs could probably troubleshoot my approach. It would be good if there was more data. My code is here: https://gist.github.com/JonnoFTW/f94f8d97e57f6796da83b834ce66aa45 Very nice Jonathan! Amazing work. Thanks. Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning."
