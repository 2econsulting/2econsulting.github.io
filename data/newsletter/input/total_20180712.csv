"site","date","headline","url_address","text"
"kaggle",2018-06-28,"Data Notes: Your smartphone knows *what*?","http://blog.kaggle.com/2018/06/28/data-notes-your-smartphone-knows-what/","Mushrooms, rats, and smartphones: Enjoy these new, intriguing, and overlooked datasets and kernels. 1.<U+00A0><U+0001F4DE> What Does Your Smartphone Know About You? (link) 2.<U+00A0><U+26BD> Another World Cup 2018 Predictions Kernel (link) 3.<U+00A0><U+0001F344> Mushroom Classification with K-Means Clustering (link) 4.<U+00A0><U+0001F4DA> Predicting English Pronunciations (link) 5.<U+00A0><U+0001F44F> Medium Article Clap Predictor (link) 6.<U+00A0><U+0001F393> World University Rankings Advanced Analysis (link) 7.<U+00A0><U+0001F307> Maps EDA and Models with NYC Census Data (link) 8.<U+00A0><U+0001F4F6> Kaggle Dataset #1: NYC Wi-Fi Hotspot Locations (link) 9.<U+00A0><U+0001F400> Kaggle Dataset #2: NYC Rat Sightings (link) 10.<U+00A0><U+0001F46E> Kaggle Dataset #3: NY Daily Inmates In Custody (link)  Which parts of your image were most important for your image classifier? <U+00A0>Try using heat maps and attention maps! Copyright ¨Ï 2018 Kaggle, All rights reserved. Thank you for featuring my Kernel on World University Rankings Advanced Analysis (https://www.kaggle.com/gpreda/world-university-rankings-advanced-analysis)"
"datacamp",2018-07-11,"New Course: Machine Learning with Tree-Based Models in Python","https://www.datacamp.com/community/blog/course-machine-learning-with-tree-based-models-in-python","Decision trees are supervised learning models used for problems involving classification and regression. Tree models present a high flexibility that comes at a price: on one hand, trees are able to capture complex non-linear relationships; on the other hand, they are prone to memorizing the noise present in a dataset. By aggregating the predictions of trees that are trained differently, ensemble methods take advantage of the flexibility of trees while reducing their tendency to memorize noise. Ensemble methods are used across a variety of fields and have a proven track record of winning many machine learning competitions. In this course, you'll learn how to use Python to train decision trees and tree-based models with the user-friendly scikit-learn machine learning library. You'll understand the advantages and shortcomings of trees and demonstrate how ensembling can alleviate these shortcomings, all while practicing on real-world datasets. Finally, you'll also understand how to tune the most influential hyperparameters in order to get the most out of your models. Classification and Regression Trees (CART) are a set of supervised learning models used for problems involving classification and regression. In this chapter, you'll be introduced to the CART algorithm. The bias-variance tradeoff is one of the fundamental concepts in supervised machine learning. In this chapter, you'll understand how to diagnose the problems of overfitting and underfitting. You'll also be introduced to the concept of ensembling where the predictions of several models are aggregated to produce predictions that are more robust. Bagging is an ensemble method involving training the same algorithm many times using different subsets sampled from the training data. In this chapter, you'll understand how bagging can be used to create a tree ensemble. You'll also learn how the random forests algorithm can lead to further ensemble diversity through randomization at the level of each split in the trees forming the ensemble. Boosting refers to an ensemble method in which several models are trained sequentially with each model learning from the errors of its predecessors. In this chapter, you'll be introduced to the two boosting methods of AdaBoost and Gradient Boosting. The hyperparameters of a machine learning model are parameters that are not learned from data. They should be set prior to fitting the model to the training set. In this chapter, you'll learn how to tune the hyperparameters of a tree-based model using grid search cross-validation. Supervised Learning with scikit-learn"
"datacamp",2018-07-11,"New Course: Python for R Users","https://www.datacamp.com/community/blog/course-python-r-users","Python and R have seen immense growth in popularity in the ""Machine Learning Age"". They both are high-level languages that are easy to learn and write. The language you use will depend on your background and field of study and work. R is a language made by and for statisticians, whereas Python is a more general purpose programming language. Regardless of the background, there will be times when a particular algorithm is implemented in one language and not the other, a feature is better documented, or simply, the tutorial you found online uses Python instead of R. In either case, this would require the R user to work in Python to get his/her work done, or try to understand how something is implemented in Python for it to be translated into R. This course helps you cross the R-Python language barrier. Learn about some of the most important data types (integers, floats, strings, and booleans) and data structures (lists, dictionaries, numpy arrays, and pandas DataFrames) in Python and how they compare to the ones in R. This chapter covers control flow statements (if-else if-else), for loops and shows you how to write your own functions in Python! In this chapter you will learn more about one of the most important Python libraries, Pandas. In addition to DataFrames, pandas provides several data manipulation functions and methods. You will learn about the rich ecosystem of visualization libraries in Python. This chapter covers matplotlib, the core visualization library in Python along with the pandas and seaborn libraries. As a final capstone, you will apply your Python skills on the NYC Flights 2013 dataset. Writing Functions in R"
"datacamp",2018-07-05,"New Course: Support Vector Machines in R","https://www.datacamp.com/community/blog/course-support-vector-machines-r"," Support Vector Machines in R will help students develop an understanding of the SVM model as a classifier and gain practical experience using R¡¯s libsvm implementation from the e1071 package. Along the way, students will gain an intuitive understanding of important concepts, such as hard and soft margins, the kernel trick, different types of kernels, and how to tune SVM parameters. Get ready to classify data with this impressive model. This chapter introduces some key concepts of support vector machines through a simple 1-dimensional example. Students are also walked through the creation of a linearly separable dataset that is used in the subsequent chapter. Chapter 2 introduces students to the basic concepts of support vector machines by applying the svm algorithm to a dataset that is linearly separable. Key concepts are illustrated through ggplot visualizations that are built from the outputs of the algorithm and the role of the cost parameter is highlighted via a simple example. The chapter closes with a section on how the algorithm deals with multi-class problems. This chapter provides an introduction to polynomial kernels via a dataset that is radially separable (i.e. has a circular decision boundary). After demonstrating the inadequacy of linear kernels for this dataset, students will see how a simple transformation renders the problem linearly separable thus motivating an intuitive discussion of the kernel trick. Students will then apply the polynomial kernel to the dataset and tune the resulting classifier. Chapter 4 builds on the previous three chapters by introducing the highly flexible Radial Basis Function (RBF) kernel. Students will create a ""complex"" dataset that shows up the limitations of polynomial kernels. Then, following an intuitive motivation for the RBF kernel, students see how it addresses the shortcomings of the other kernels discussed in this course. The course requires the following prerequisites: Introduction to R Introduction to Machine Learning"
"datacamp",2018-07-05,"New Course: Interactive Maps with leaflet in R","https://www.datacamp.com/community/blog/course-maps-leaflet-r","Get ready to have some fun with maps! Interactive Maps with leaflet in R will give you the tools to make attractive and interactive web maps using spatial data and the tidyverse. In this course, you will create maps using the IPEDS dataset, which contains data on U.S. colleges and universities. Along the way, you will customize our maps using labels, popups, and custom markers, and add layers to enhance interactivity. Following the course, you will be able to create and customize your own interactive web maps to reveal patterns in your data. This chapter will introduce students to the htmlwidgets package and the leaflet package. Following this introduction, students will build their first interactive web map using leaflet. Through the process of creating this first map students will be introduced to many of the core features of the leaflet package, including adding different map tiles, setting the center point and zoom level, plotting single points based on latitude and longitude coordinates, and storing leaflet maps as objects. In this chapter students will build on the leaflet map they created in chapter 1 to create an interactive web map of every four year college in California. After plotting hundreds of points on an interactive leaflet map, students will learn to customize the markers on their leaflet map. This chapter will also how to color code markers based on a factor variable. In chapter 3, students will expand on their map of all four year colleges in California to create a map of all American colleges. First, in section 3.1 students will review and build on the material from Chapter 2 to create a map of all American colleges. Then students will re-plot the colleges on their leaflet map by sector (public, private, or for-profit) using groups to enable users to toggle the colleges that are displayed on the map. In section 3.3 students will learn to add multiple base maps so that users can toggle between multiple map tiles. In Chapter 4, students will learn to map polygons, which can be used to define geographic regions (e.g., zip codes, states, countries, etc.). Chapter 4 will start by plotting the zip codes in North Carolina that fall in the top quartile of mean family incomes. Students will learn to customize the polygons with color palettes and labels. Chapter 4 will conclude with adding a new layer to the map of every college in America that displays every zip code with a mean income of $200,000 or more during the 2015 tax year. Through the process of mapping zip codes students will learn about spatial data generally, geoJSON data, the @ symbol, and the addPolygons() function. This course requires the following prerequisites: Introduction to R Introduction to the Tidyverse"
"datacamp",2018-07-02,"Machine Learning & Data Science at Github (Transcript)","https://www.datacamp.com/community/blog/machine-learning-github","Hugo:    Hi, there, Omoju, and welcome to DataFramed. Omoju:    Thank you, Hugo. I'm excited. Hugo:    I'm super excited. It's great to have you here to talk about data science at GitHub. But before we get there, I want to find out a bit about you, and I want to talk about how you got into data science, what you do at GitHub, but I'd like to take a slightly tangential approach to finding about you first by just asking you what you're thinking about at the moment with respect to data science, or what keeps you up at night, or what really is exciting you? Omoju:    The thing I've been thinking about a lot is the term artificial intelligence and the fact that it is such a misnomer because the work that we do is not necessarily artificial intelligence. Most of us in industry don't work on A.I. We work on massive mathematical problems that are basically variants of some kind of linear algebra. And that's what we do. We are applied mathematicians.
    So I've been thinking a lot about that, and then using the right kind of terms, like maybe we're doing things like augmenting human intelligence, or been building like data intensive platforms and things like that, like figuring out a word that more represents the work that we actually do so people don't come off with the idea that we're building some kind of completely autonomous being with some conscience and sentience because that's not what we're even doing at all. And we're not even anywhere close to that. Hugo:    Agreed. And I think part of the big challenge is that artificial intelligence is a term that has existed for decades, if not longer, in the cultural consciousness from science fiction, for example. So you watch Blade Runner, and that isn't what we're thinking about or what we're doing. Omoju:    Yeah, it's not, it's not at all. There's an essay I read recently by Michael Jordan from Berkeley, the statistician. And it was talking about how we're not even anywhere close to that. I forgot the name of the essay, but the essay was in Medium, and it's a very fascinating, interesting read. And I like it a lot because it really focuses on the major problems that we have ahead of us and the problems that we have to solve, problems around infrastructure. Like, any kind of machine learning approach often needs robust, solid infrastructure that can scale with it as the data scales.
    So focusing, targeting on those kinds of problems and where the low-hanging fruit in those areas are, those are the kinds of things that I've been thinking about lately, and that fascinates me. Hugo:    Yeah. Fantastic. So it isn't necessarily about self-driving cars and- Omoju:    The self-driving cars thing, I get recruited very often, recruiter emails about self-driving cars and autonomous vehicles. Personally I am not interested in working on those kinds of data sets. It's not going to get me out of bed in the morning. I don't care about that kind of data. I care about data sets that I can reason from a human perspective about. I like that kind of, that I can use my intuition. I can basically leverage the ability of being a human to solve the problem even faster, so data sets that I know that this data set represents some kind of a snapshot of human activity. Those are the kind of things I care about.
    And with regards to self-driving cars, unless they're fully autonomous, I think they're actually quite dangerous because they lull you into a false sense of security. It's going to be very, very difficult if you've gotten, if you have a car that's semi-autonomous and you've been driving it for three or four years, and you never have to engage and do anything, when that instant does happen, it's going to be very difficult for you to like, oh, remember to pay attention because you've just learned to trust this vehicle. And those split seconds, those are the things that actually create lots of danger and I think makes it harder for the public to actually embrace new technology. Hugo:    And I think a certain amount of education, and dialogue around these issues, and data scientists getting in the public eye and having this conversation with the public is incredibly important here, right? Omoju:    Absolutely. Yes. And then having them actually understand what we do for a living, what our work entails, and the kinds of problems that we're solving, and the kinds of problems that we can solve, questions around data privacy, basically a data education so the public has a very good understanding that if they give you data, what does that mean? What is that data being used for, things like that. Hugo:    Where would that data education take place? I mean, essentially in the end maybe we'd want it in primary school, right? But- Omoju:    Yeah, I think in primary school. So younger children are much more savvy. I have a middle schooler, and he's very, very savvy around data, and data sharing, and issues of Internet privacy, and all that kind of stuff. So they are on it. They are very, very savvy. They know exactly what's going on. It's the other people who have not grown up as digital natives who have the bigger chasm to cross in understanding what does this actually mean.
    And then beyond all of that, there's so many things now that we don't even have rules and regulations for, or even have mental models of how to think about it because it's just so far out there that we've never really thought about it. Have you ever thought about who has access to your genetic material? Because it's never been like a real thing. So it's like but now it's a real thing. Hugo:    Very much so. Omoju:    Who owns that data? Do I own that data? These are the complex questions now. Hugo:    Yep. And I think you raise an interesting point about children and younger children being a lot more savvy. I think I remember a while ago you were on hanselminutes, and you were talking about your child using the term ""physics engine"" and knowing what that meant at a very early age because they've been introduced to that, right? Omoju:    Yeah. I was like, ""Whoa, physics engine?"" I didn't even know what that was for a while. I was like, ""Ah."" I mean, I don't play video games, but, yeah, like, they understand intrinsically what a physics engine is and what the physics engine is going to help you do. And the way they learn is so interesting because they've just grown up in the age of YouTube, so knowledge for them is not something that you have to go and acquire. The most important thing for them is figuring out the right sets of questions to ask because they just have the assumption that the knowledge is there: ""I just need to figure out what's the right query to pull that knowledge up, and then I can apply it."" Hugo:    And of course now that we have widespread education, in a lot of respects, it can get a lot better. But we have a lot of education available online. As you say, it isn't about necessarily learning everything at school or having to move to a different city, state, or country to go to the university and sit in the lecture hall. And we've got totally different models of learning that are evolving right now. Omoju:    Absolutely. And the same exact thing happens in it for us too. Machine learning is so ... There's so much happening in ML and in ML research that it's so hard to just keep abreast of everything that's going on in the space. So you find yourself watching videos, reading papers, and trying to build your own versions of those models. And it's basically the same exact kind of thing. You just have to put yourself in the same mindset of the knowledge exists. What's the right question to ask? How can I replicate this experiment? What do I need? Those kinds of things. It's the same exact thing. You have to be in the mind of a child to do the kind of work that we do and do it very well. Hugo:    Yep. Something we've been circling around and something you mentioned explicitly was data science ethics. Data scientists are not necessarily trained to be ethical in their jobs. They're not necessarily incentivized to be ethical at work. Do you think this is a problem? And do you think we need to, all data scientists need to be ethically-minded at work in some sense? Omoju:    Yes, I absolutely think that all data scientists need to be trained around ethics because the kinds of things we're doing today involve real people. And the repercussions, if it goes well, are amazing. We change people's lives. The repercussions, when it goes poorly, are devastating. So we need to have that ethical understanding, we need to have that training, and we need to have something akin to a Hippocratic oath. And we need to actually have like proper licenses so that if you actually do something unethical, perhaps you have some kind of penalty, or disbarment, or some kind of recourse, something to say this is not what we want to do as an industry, and then figure out ways to remediate people who go off the rails and do things because people just aren't trained and they don't know. And it's so far removed, when you're working with large data sets, it's easy to just forget that those data sets eventually are going to be used by people. And you can be so distant from it that you completely forget. Hugo:    Absolutely, and we need to be very rigorous in a lot of respects. For example, if you have a machine learning model, I think one of the famous ones is to predict recidivism rate, and you take out the feature ethnicity. You're like, ""Okay, well, now we're not including ethnicity."" But if you include post code, then you're actually capturing a lot of that information already. Omoju:    Exactly. It's kind of correlated, even asking yourself those kinds of questions and understanding the application of the mathematics. It's not just, yes, you have correlations. Like just things like that, knowing that zip code is a good proxy for ethnicity, just simple things like that, and then just asking yourself those questions.
    I think as data science becomes more mainstream at the university level, I hope that there'll be a track that goes along with it, which is the ethical ramifications of data. So in the undergraduate program at Berkeley, they actually have classes on the social implications of computing. In one of the classes, that's a thing. And I would hope that that continues along the track so students can get exposure to it, not just once but every time they're in a class. What is the social implication, positive and negative, of the technology I'm working on right now?
    What is the best thing that could happen if my technology is successful? How can my technology be weaponized? What are the safeguards I can put in place to prevent that when such a thing occurs? And how do I educate people on what the importance is of that weakness or loophole in my technology is? Hugo:    I think education system's a great place for this. I think having a general reading list of people who necessarily aren't working data scientists who are theorists or social scientist who think a lot about this stuff. I mean, Cathy O'Neil's Weapons of Math Destruction is a great book. A lot of the work that comes out of NYU's Data & Society group is really fantastic. But of course we see rules and legislation emerging in different places. In Europe we have GDPR now. I'm just wondering in your mind where do these movements come from, or is it a confluence? I mean, do we want the movements to be stemmed from data scientists from the ground up thinking about these or directed downwards from legislators? Omoju:    Oh, I would actually hope that we are the ones ahead of the curve, that it's not just the regulators, I mean regulation from governments. And the regulation has its role. And it's a good thing. The biggest challenge is oftentimes policymakers might not have the knowledge necessary to help craft the regulation. So perhaps they do it in tandem with the people with the expertise. And so maybe you know how it is in academia where somebody might go work with the NSF for like a semester or two, we actually should have something akin to that where you take a leave of absence of six months or something like that, and you go work within policy of the government of the country you're working in.
    And it's just part of like the normal core work is. Maybe every four years you work in policy for six months, and it's part of like the requirements to have your license as a data scientist or something like that. Hugo:    For sure. This is really interesting though. This really promotes a dialogue between legislators, working data scientists and stakeholders in the community. What do you think the role of black box models and their counterpart, interpretable models is in having this conversation across a variety of stakeholders? Omoju:    There's some problems that black box models are very good at solving. However, dependent on context, if it's going to be something that can really harm people, there has to be a way to investigate how we'd come up with the solution. And then sometimes maybe you don't necessarily throw away the black box model entirely. Maybe the black box model helps you come up with a set of features that you use.
    I think the best approach would be to have an ensemble of different kinds. Like, the black box model can help you triage, and then eventually on top of all that have a decision tree or some kind of hybrid of the two. But deep learning has given us so much. Hugo:    It¡¯s powerful. Omoju:    We can't just throw it away. Hugo:    Agreed. Something we've been circling around is how data science can inform us about humans. And I know that a deep evolving interest of yours is anthropology and the uses of data science for the human. So I'm just wondering how you're thinking about this at the moment. Omoju:    I think it's so fascinating because human beings, we think we have an idea of who we are, and we have an idea of our own behaviors and our own biases. And we often don't. Human memory is so fuzzy. The kinds of things you can actually do, sometimes when you think about all the stuff from cognitive science, how you can literally delude yourself into believing something that is completely false by firing a same set of neurons and creating this strong association.
    But if we can actually use data to help you understand your own habits and your own patterns, then you can actually use it to optimize yourself in a certain way. And the thing about it is very interesting to me is often I will find people who are in science, maybe they¡¯re machine learning engineers and things like that, and we're very good at the craft. But we never actually apply it to decision-making for ourselves. We literally just do the work, and we don't apply the same rigor into our own decision-making.
    So I'm one of those people that would like to believe I do more of that, and so it's very, very, very interesting to have a good reckoning of what your strengths are and what your weaknesses, and having data to actually give you the evidence that that is in fact the case and that is in fact true. And the question is what do you do with that knowledge? In my mind, you use that knowledge to make better decisions. Hugo: Absolutely. I love it, and I've actually started doing something along these lines recently. I'll tell you very briefly, I've started doing stand-up comedy, open mics recently in New York where I live. And I've realized that stand-up comedy, I can apply the principles of Bayesian inference to it. So when I write a joke, I've got a prior belief of how funny it is. And then I have to tell that joke maybe 10 times in different settings to get 10 different data points on how funny it actually is for an audience. And I update my prior with respect to the likelihood generated by this data. And then I have some posterior belief of how funny this joke actually is. Omoju:    Exactly, things like that, and just realizing that as a human being, literally everything you do is an opportunity to experience and optimize some kind of data algorithm. Hugo:    Yeah, absolutely. Omoju:    Because your whole life is just a chain of decisions from moment to moment. Hugo: And as you say, before decision-making, though, there's a process of pattern recognition, classification, and understanding of your own behaviors as well. Omoju:    Yeah. And this, it's fascinating to me because the lack of understanding of this is sometimes the reason why we have so many societal ills. And it's a tricky thing to like walk around because we don't even understand how the human mind works. And the understanding that we do have, we don't apply it very well. The human mind is optimized for pattern recognition. If it wasn¡¯t, we will not be able to function adequately.
    And it's also like the first fit model. Like, the first fit is the best fit. You never question the second fit. If it looks like a chair, I'm going to assume it's a chair. You never question, is it actually in fact a chair? Because you have to triage so much information so fast and make decisions. But there are certain times where you want like friction and you want to reconsider the decision you just made. Is it actually a chair or is it a piece of art? And then come up with the heuristic of why you believe it's something you should sit on versus something that you should look at. Hugo:    That's a great example. So how did you get into data science originally? Omoju:    This is one that's a big tricky. I think it goes back to how did I get into computer science. And I got into computer science because of the Internet. I was just fascinated by communication technology, the fact that you could just know whatever you wanted to know. It just seemed like the world was at your fingertips. And then as I continued down that path, I realize, I mean, I did my master's years ago, 2002. And it was all about neural networks and all that kind of stuff, but this was before things like scikit-learn, NumPy, Pandas.
    So writing those algorithms in C++ was not a pleasant experience, and then on top of that, we didn't have tons of data. So it was almost like we had all this knowledge, and it was ... We couldn't use it. The power was not there. So I kind of abandoned it and went from symbolic logic. Well, then I realize years later, oh, now we actually have the cell phones, and we have tons of data. And now is the time you can actually use all this knowledge, all that knowledge you acquired years ago, and you can apply it today, and there are all this frameworks & tooling to help do it this thing faster. So I just decided that the time was right.    And so during my dissertation at Berkeley in computer science education, I collected all this data during the data analysis, and I just ... I was like, ""Oh, I really, really love this because it's fascinating. It's just so fascinating. You can just keep on knowing more and more and more things about people through the things that you can find in their data. And it seemed to me like the most interesting and fun thing to do. And I was just like, ""Yeah, I guess this is what I'm going to do for the rest of my life."" I'm very actually happy that it is a thing now. Hugo:    And you're not merely a data scientist, but you specialize in machine learning. You're a machine learning data scientist. Omoju:    Yeah. Hugo:    What does that actually mean, and what's even the distinction there? Omoju:    The biggest distinction that I see is that we have like data science is such a broad umbrella too, but data science encompasses decision support, so organizations like analytics often gathering data to support a decision versus shipping data products, so building data products, something like a learning control system for an autonomous vehicle is a data product. It's not necessarily doing decision support within an organization. So it's just different users of the same tool set. But on the machine learning routes, what ends up happening is you probably have vast more amounts of data, and the kinds of approaches are very, very ... They go further. I think that's the biggest difference. Hugo:    I want to know what you actually do at GitHub. But before that I want to know what your colleagues think that you do. Omoju:    I think people outside of machine learning often don't know what we do. They ... Magic. And it's not magic. We don't do magic. They don't have a clue. They actually don't understand. They know there's data. Something happens. Decisions get made somehow. So they literally don't have a clue. Hugo:    Is that dangerous? Omoju:    It's very, very dangerous because people think it's magic, or they think that it's a knowledge that is so far out there that the gap between them and the attainment and understanding of that knowledge is too wide. And that is a very dangerous thing because it's not true. Hugo:    And are there, for example, product managers that have more of an understanding than other people within the organization? Or- Omoju:    Yes, we do. We have some amazing product managers. We have one in particular that supports our team, and he knows so much about machine learning because he's a voracious reader. But he is very rare, very, very rare. Hugo:    So now knowing what your colleagues don't know about what you do, what do you actually do at GitHub? Omoju:    What I do at GitHub is I build data models, often deep learning models on GitHub data to help GitHub probably build things like a recommendation engine so we can recommend repositories to people. Do things around, understand insecurity, vulnerabilities in code, do things around figuring out what topic a repository is really speaking about. So that's the kinds of things I do. We build data products. So basically getting data, building a data pipeline, coming up with a hypothesis, building a model. If it's a predictive model, see how good you are at making the kind of predictions. If everything goes well, put it in an API and serve that API so that other engineers can use it and use it to support the GitHub platform. Hugo:    So this basically machine learning side of things, when you're making product changes or anything along those lines, do you also do, or does somebody at GitHub do online experiments to decide the direction of these changes? Omoju:    We are beginning to start to have a team that's going to start doing that. Right now, machine learning is very nascent to us. Machine learning is around a year or a year plus. So we are at the beginnings of doing all of those kinds of things. We are not ready yet for full-on online experimentation quite yet. But we do a little bit of that. Hugo:    Really exciting times. Omoju:    Yes, very exciting times. Hugo:    I've heard you describe what you do, and we're speaking about this already, but specifically is you use computation to build products to solve real-life decision-making practical challenges. And I'm wondering what practical business challenges does GitHub face that data science can help to solve? Omoju:    One of the things is a lot of open source is on GitHub. Open source often times is looking for contributors, people to contribute to the open source repository. And the maintainers sometimes get overwhelmed. So one very simple thing is perhaps as an open-source library you really like and you use it all the time. And then you realize, ""Oh my God, this thing is missing from this library,"" maybe something like Pandas. And then you just automatically go to the Pandas repo, and you open an issue: ""Oh, I see that when I do this and that, this x, y, z happened. Wouldn't it be great if you could fix this for me,"" so on and so forth. Open an issue, leave the issue in the repo, right? Hugo:    Mm-hmm (affirmative). Omoju:    There might be other issues that have basically said the same thing. So when the maintainer comes to that repo, triaging all those issues becomes a very, very big challenge so that the challenge of triaging, and finding duplicates, and what has been opened, and what has been closed, what's most relevant, all that kind of stuff, just doing all that decision-making around that is a major problem. And that's a problem that can be solved with machine learning. Hugo:    That's fantastic. So I'm just going to stop you for one second and kind of reiterate what you've said, that this is actually one of the biggest challenges in open-source software development. There's a huge rate of burnout in developers, who were doing a lot of this in their own time after working their full-time jobs, taking time out from their family to do this. And it's a huge challenge faced by the community at large. How do we even think about package maintenance, especially with the expectations that come from the user community? Omoju:    Yes. We, the users, we are so ... We want everything now. So I saw it recently, and I was literally aghast. I was like, ""Oh my God."" So I think it was like matplotlib, NumPy, and maybe Pandas or something, those three SciPy packages for data science pretty much are maintained by 15 people. Hugo:    Yeah. I saw that as well on Twitter. Omoju:    I was like, ""What?"" It was like three of them, five people each. I was like, ""What?"" Omoju:    ""Five people? This is insane."" And the number of people that use those packages, orders of magnitude, millions of people. Five people? Hugo:    Yeah. I was just going to say hopefully those five people are never in the name room at the same time. Omoju:    Exactly. So it's a machine learning problem if as a maintainer you can come to GitHub, and I've already triaged all the issues for you to let you know, all right, maybe you have like 10 contributors. The 10 contributors are available right now. And I know what their skill sets are. I could say contributor number A will be great for issue number B and match them because all I know is this contributor can close this kind of issues very, very fast, so I can just match them automatically for you. I can basically get rid of all the dead issues like the things that we won't fix because it's just out of our control, or it's not on our roadmap.
    I could come up with all the duplicate ones. I can find other issues on the platform that are from other repos that look like your issue that have been closed and figure out what the solution was. So these are all the things that can just help you. Maybe then those five people then become like an army of 300 because they are powered by machine learning. Hugo:    That's fantastic, and I actually, this speaks of automating certain things, which people do constantly and takes up a lot of time. When I spoke with Jake VanderPlas on the podcast, I asked him how he got involved with scikit-learn. And he'd written I'm not sure whether it was PCA, or he'd written something to help him in his astronomy research. And he'd emailed the SciPy users mailing list or something like that. And eventually someone from scikit-learn, I think it was Gael, said, ""Hey, why don't we put this in scikit-learn?"" And that's how Jake got involved in scikit-learn. That took a period of time and a lot of human hours to figure that out, whereas what you're speaking to is developing machine learning, automated systems that do that work for us. Omoju:    Exactly, that do that kind of work for you, that just take the pain out of it. What's the point? You have them at repo. You need contributors. You are a maintainer. What can we automate away to help you get your two, three hours that you're committing to this every four or five days as meaningful as possible? So you're not spending the hours just like, ""Oh my God, all those issues,"" triaging the issues. Triaging issues is a big deal. Machine learning can triage the issues for you. Hugo:    So you also think a lot about building data products, and you build data products ... Is this the type of thing that you'd consider a data product at some point? Omoju:    Oh, yes, absolutely. Everything I've just told you is a bunch of APIs. Hugo:    Okay. Great. What other types of challenges do you think about, or is GitHub interested in, that machine learning can help with? Omoju:    One of the things that I am so excited about is actually gaining deep understanding of computational competencies, like actually understanding the kinds of computation that we realize in code, understanding the complexity of a code base, and building things towards just doing ... getting to the point of doing like automated code review, giving you some kind of thing that maybe you can plug into atom that can go through your code and tell you all the kinds of things. Like imagine a linter on steroids. Instead of just finding all the PEP 8 errors, but can tell you, ""Oh, I see it. It was in this pattern over and over again."" Hugo:    ""Let's refactor."" Omoju:    Refactoring it for you, like highlighting things. Imagine if you can actually look at ... Imagine when you're entering a new code base. A new code base can be so large to just like chew. Where do you even start reading this code base from? What if there's a visualization of that code base as a tree, a different path they can go through. And that tree can then expand and contract based on the kinds of things you need to go down on. Hugo:    Awesome. Omoju:    Just like tools to visualize knowledge, computational knowledge so you can get what you need to get out of it very, very fast. Hugo:    Yeah. And something you mentioned in passing earlier was detecting security vulnerabilities in code also, right? Omoju:    Yes, detecting security vulnerability in code, it's a major thing. What if we can go through your code and find the security vulnerabilities? Number one, we already have a service like this that will alert you if you find certain vulnerabilities in your code so you know what it is. Then the next part will be what if we can tell you what to do to patch that security vulnerability? Like, we found it. Now here's the patch.
    The next level is what if we can actually write the automated pull request for you? That we've found it, we've written a solution for you. All you¡¯ve got to do is hit ""merge."" These are the kinds of things that will save so many human hours, and more importantly have such a major implication on society. Lots and lots of data breaches have so many deleterious effects. All of the people that have been affected by the what's the ... Is it Experian breach? All these kinds of things.
    If you can automatically see that, oh my God, I've seen that you've committed AWS key on a public repo, just alert you before you even like push it, like, ""Ah, stop it. There's a violation here,"" things like that. Hugo:    That's fantastic. Omoju:    Simple things that just make you sleep easier at night. Hugo:    And I know a lot of people, myself included, use GitHub really as a way to share code, to collaborate on code, and to discuss code. But I know that in your mind, GitHub is a far larger ecosystem than this. And I know that in particular you also view it as a social network of sorts. Omoju:    Oh, yes. I use the social coding. I use the social coding aspects in GitHub. When I was on campus at Berkeley, it's easy to collaborate with people or to know what they're thinking about. Even if you're not collaborating with them, like your buddies, what are they thinking about? It's easy for you to do that because you're also in the same lab.
    When everybody graduates, and goes on, and does whatever it is that they want to do, by following them on GitHub, I still have a way of knowing what they're still interested in. So I can go and look at my friends, and I follow my friends in GitHub. And I'm seeing the repositories that they're starting. Or I'm seeing this person is interested in crypto. I'm seeing this person is now interested in this version of a new JS. And I'm seeing or this person that was totally into front end design is now starring things that with machine learning.
    So you're like, ""Oh my God. They are now into machine learning."" You can hit them up and be like, ""So what are you thinking about doing?"" So it's like an opportunity for you to keep in touch computationally of what people's interests are. Hugo:    And to see people, what people are committing to, and even, as you said earlier, let's say you make a contribution to scikit-learn, for example, you may get a message saying, ""Hey, if you made that commit, this is something similar that's raised in an issue on statsmodels that you might want to have a look at."" Omoju:    Absolutely. Exactly. Through it sometimes I look at my dashboard, and I know who's going to what talk. I see they're already preparing their talk. I mean, I can see everything that they're doing, and it's public, it's there, and it's so interesting. And I also use it as a form of discovery because there are so many open-source projects, too much for you to consume that your friends can almost serve like a sort of triage to figure out what is hot, and what are they sharing in common, and things like that. Hugo:    So you've also described GitHub as a platform for work, and not just technical work. And I'm really interested in this. You've told me that everyone who works within GitHub, even non-technical people, use it for communication. And you've described it as a ¡°collaborative work environment centered around humans¡±. How do people use it who aren't technical and not writing code, for example? Omoju:    So I will start off with my use of it, my non-technical use of it. So I keep a blog. I have not updated my blog in a while. And my blog is in Markdown. So when I want to write a new thing and I'm just writing ideas, I literally like write it in Markdown and commit it to GitHub. It's like I just have like a ... Basically it's my own version of how I write to text. Even if it's not Markdown, it's like regular English. I'm just writing it there. I'm just committed to GitHub. It's just my own version of I guess Microsoft Word because I don't like Microsoft Word. So I just write in whatever free text editor. And I just commit it to GitHub, things like that. Hugo:    And it pushes directly to your blog. Omoju:    Yeah. Well, yeah, it pushes the blog because I'm using Jekyll and all that, but even just putting my ideas down and just keeping all those ideas together. So I use GitHub, I used the GitHub platform to help manage writing my dissertation. So I'm writing this dissertation. I'm like, I wrote the dissertation using LaTex. I'm just like committing everything to the GitHub repo. And that just gives me piece of mind just in case something I wrote a while back that I decided I no longer want, I can roll back to that commit. I can go all the way back and be like, ""Oh, 10 months ago what was I thinking? Oh, there it is. This was the state of the dissertation. This is what's going on."" And I like to do that.
    And then also I like to look at some of the insight to see my velocity so I can no longer deceive myself. If I'm working on something and I'm not working on it, it's very evident I did not work on it or I worked on it. So that's just from a perspective. But things like I know people use GitHub for coordinating work. People use GitHub for crisis response. So they will open a repo, and the repo may be around how are we going to coordinate all of our resources together so that when a crisis is ... Let's say a hurricane is coming. Once the hurricane passes, how do we collect all the things we need and all that kind of stuff?
    So we can open a repo and use that repo to track all those kinds of things. You can track it using issues. So just search for this issue, and it's going to have the thread of all the information of this is what we're thinking. So it's basically a way for us to capture our thoughts on paper. And we can always go back to that issue and see this is everything we wanted to do, these are the things that we did. And they can go back and see where did we go wrong? We can almost go and do like your ... You can debrief and go back to all those issues. So you can see everything, and the conversations, and the people who participated, and what they were thinking about.
    So opening issues is one of the ways that GitHub uses GitHub non-technically, just coordinating because GitHub is also distributed. So certain conversations are just better kept in a niche so we have the record of the entire thing. And there's a repo for everything. There's a repo for the finance team. There's a repo for the legal team. There's a repo for the machine learning group. So it's not just technical groups. Everybody has a repo. Everybody has like opened an issue in the repo. If you want to do this, we'll have a repo. Open an issue in our repo, and we'll get back to you, things like that. Hugo:    That's awesome, and I think that the fact that history is always preserved in GitHub speaks ... Like, you close an issue, but it's still always there, and the fact that you- Omoju:    Yeah, you close- Hugo:    ... have versioning around everything means that you have a history of everything, if needed. Omoju:    Literally you have a history of everything. So when somebody gets onboarded and they, ""Okay, so you're here to do x, y, z,"" you can actually see the history that has happened before you got here, see where conversations went, and then you can take it from there. So it's a collaborative work platform. It just so happens that the first major use case was around computation. Hugo:    And I've actually, I can't quite remember the details, I've definitely used infrastructures and plug-ins that allow you to visualize issues. Maybe one's called Zendesk, which allows you to track issues and drag and drop them into particular places to see which stage they're at. Omoju:    Oh, yes. We actually have our own products, like a project board that you can just use within GitHub itself, and you can create different verticals, and you can drag and drop issues. And things can be closed automatically. You can have your Slack integration, and all that kind of stuff. Hugo:    That's really cool. And do you see this becoming more and more a broader use of GitHub as we move into the future? Omoju:    Absolutely, yes. I hope so because having used it in this way, it's ... I started using GitHub this way before I actually got to GitHub. Hugo:    Yeah, that's really cool. Omoju:    It just seems like a natural way to do things, because it's like several products on the same platform. You don't have to keep jumping from one thing to the next. It's the same that can do all that for you. Hugo:    So I want to talk about accessibility of Git for a second, and what I mean by that is I've taught Git in various places. And I know that you're very interested in education. One thing I've always found, Git is incredibly difficult to teach for a number of reasons. GitHub has made it a lot easier, I think, but one example is I try to motivate Git and tell people that versioning is important. Then I tell them that, ""Oh, we're using Python, so let's use Jupyter Notebooks,"" right? Omoju:    Yeah. Hugo:    So it's an inherent challenge and barrier to entry for Git. So how do you see this progressing as we move into the future? Omoju:    As we move into the future, we have a lot more to do to actually abstract away a lot of the pain points in Git and create ... I think one of our killer opportunities is the GitHub Desktop app. I use the GitHub Desktop app probably 90% of the time. Hugo:    As opposed to using your Unix shell, or ...?
Omoju:    Yeah, as opposed to using terminal because the app, I make less mistakes because there are so many things that are evident to me. I automatically know what branch I am on. It is right there. I don't have to worry what branch I was on. The branch may not be there. I can see that. I can see my diffs visually. I can see the difference. And I like going back into the history in the GitHub Desktop and actually going through.
    So it helps make it slightly easier, and as that app becomes better and better, itself will become the learning tool, and then also GitHub, we just launched something recently, the GitHub Learning Lab, which is a bot that is on the GitHub site itself that will teach you how to do all these things. So we're beginning to build all these learning tools to help people cross over that chasm of Git. It's not all the way there yet, but we are nowhere near where we were three, four years ago. Hugo:    Great, and has the Learning Lab gone live in some form? Omoju:    Oh, it's live. It's already out there. People are using it. They've already built several classes, and they are using it. Hugo:    That's great. We'll definitely link to the Learning Lab in the show notes.
Omoju:    Yes. Absolutely. Hugo:    So you spoke to a really interesting principle in terms of saying GitHub for desktop, you can use it visually. And I think thinking in terms of design principles of products, essentially if we're a lot of the time forcing people to use terminal to interact with such systems, we're losing a huge portion of the population, right, people who actually prefer to do things visually. Omoju: Yes, people who prefer to do things visually, and then also one of the easier ways to do things, one of the benefits of doing things visually versus by typing or reading is that pictures are not necessarily as language-dependent. So if an arrow is pointing to the right and it's saying something like go from left to right, that arrow saying left to right or the symbol of a bathroom or something at an airport, it is a universal. It is not language-dependent. And because we're sitting in San Francisco and we're often English speakers, we are so tied to the English scripts that we don't leverage all the other things you can do without language, without using text-based language. So I often would rather default to a visual medium, what is the visual way of doing this thing, before even thinking about anything text-based. Hugo: Interesting. That actually made me think of something. I had Greg Wilson on, a colleague of mine whom you know and who is well known for his original work with the software and data carpentry, but he's very provocative in a lot of respects. And he made a statement that he thinks the future of data science in decades may be people using Scratch for data science. Omoju:    Yeah. Why not? Drag and drop. Hugo:    Exactly. So you think that's a viable future as opposed to us writing code in Emacs or whatever it may be? Omoju:    Yes because what is the problem you're trying to solve? Maybe you're trying to build a thing that can give you like ranking--like, that is the problem. You're trying to do prediction, for example. Do you honestly care what way you use to solve the problem of prediction? I don't think so. I think you care about solving prediction.
    Tying yourself to one medium makes no sense. If there's a medium that is easier and faster and less error-prone, why not adapt to that medium? It has nothing to do with the solution to the problem; it is just the ... I thought it's just the way you're realizing the solution. We don't need to tie ourselves cheek to jowl to only one form of realization of a solution. Hugo:    I agree, and this is actually why I have such an allergic reaction to the language wars when aspiring data scientists, for example, say to me, ""Oh, which is better, Python or R?"" Omoju:    Who cares? Hugo:    And I immediately say, yeah, ""Who cares? And what are you trying to do? Let's talk about the problem you're having. See where we go."" Omoju:    Exactly. What is the problem we're trying to solve? What is the best tool for that problem? And then let's do that. And then in certain kind of environments, you have to ask yourself, how will this thing scale? Maybe one language is easy for you to just build a prototype, and you use the language that is easier for you to prototype. Maybe another language is better for you to actually build a full-scale application and production--use that. It doesn't matter.
    I think we're eventually going to start moving more towards automated tooling and things that are like drag and drop and they are not as language dependent. And there'll always be an API that you can call and still connect back to Emacs if that is what you want to do. Hugo:    Great. And I think this also speaks to something that I know you're deeply invested in, which is lowering the barrier of entry to these types of things. Omoju:    Absolutely. What I am designing, who I am designing for, my perfect person I'm designing for is that person that has a solution in their head of like, ""Oh my God, there's this problem, and I could see this solution. But, ugh, I can't code."" There are so many people who fall into that bucket. What if with the assistance of intelligent agents powered by machine learning, we can help those people realize a prototype of their solution so they can at least get the proof of concept out there. And if the thing has legs, then it maybe can go raise money and then hire engineers to build full-on computationally robust version of that prototype. Hugo:    What does the future of data science look like to you? Omoju:    I think it will still exist, but it's not going to exist in the way that we think it is now. It's going to be more of like a real discipline with so many branches. You have decision support. You have some versions with its optimization. You have ethics. You have things that are like linear algebra on steroids, like numerical computational methods. It will be all these kinds of things, but we'll have deeper understanding of what it is.
    When you think of data science, it will be like, oh, I am in medicine, right? And then you say there will be people who will be like practitioners, who are like internal medicine practitioners, and then there will be people who will be like oral surgeons or people who will be like cardiologists. There will be all these specializations. And it'll be a product discipline with like a committee or boards of like saying, ""Okay, this is what it means to be this kind of data scientist. More importantly, this is the kind of knowledge you need to be able to solve this kinds of problems. So if you're interested in this kind of problem, this is the path that you go down."" Because right now we don't really have that. Hugo:    No, and we have a problem then with career paths and even job listings. You look at job listings, and it's like we want 10 years of experience with distributed computing and five years of statistical inference and these types of things. I think it promulgates this stereotype of the unicorn existing. Omoju:    Exactly. Hugo:    Specialization will be a restorative force there. Omoju:    Will be a restorative force because what do you need 10 years of distributed computing experience for? What are you trying to solve? And a lot of these things are over the top. Some of the things that they're doing, you probably can write an SQL script. You don't need learning for everything. And some of this stuff, you don't need this. And then some other things you're like, maybe you actually don't need a machine learning ... Maybe what you need more is a data engineer.
    Some problems are just pure infrastructure problems. They don't require any kind of extraordinary numerical computational methods. It's more just infrastructure. And there are some things that I've, ""Oh, this is truly the bleeding edge. You really need advanced computing, and you really need advanced numerical methods to understand the space of hypothesis."" Just we don't even have a lot of that kind of stuff yet.   Hugo:    So we haven't got too technical, and I don't want to, but before we end, I'd like to know what's just one of your favorite data sciencey techniques or methodologies, just something you love to do.   Omoju:    You know what I love to do? I really like exploratory data analysis. I actually like that stuff a lot. I don't even get to do it as much as I would like to because I have things I need to build, but I just like worming through data.   Hugo:    It's a lot of fun, isn't it?   Omoju:    Yeah, and I like creating my pretty pictures.   Hugo:    Yeah. That's playful.   Omoju:    It's playful. That's the thing: It's actually just playful. It's playful.   Hugo:    Yeah. It's like a first date. Or it's like the first few dates, right-   Omoju:    Yeah.   Hugo:    ... when you don't know someone, and, yeah, you're discovering things always.   Omoju:    Yeah. I really like that because I think it's interesting. Nothing might come of it. It might just be something that nothing comes of it.   Hugo:    For sure. And something you mentioned to me last time we spoke, and this is something I've been thinking about ever since actually, is that reading code is so important. So if you want to think about how ... Don't read the code you'll [inaudible 00:53:47] for scikit-learn's random forest, right? How much fun is that? Omoju:    Yeah. I really like doing that. I love doing that, and I like to read the ... So I would like to read the person's dissertation, like understand how to write in English, and then go see how they write in code. Hugo:    You actually said something to me that stuck with me. You said to me, ""You don't only learn to write by writing, you learn to write by reading a lot as well."" Omoju:    Exactly. Hugo:    And you can think about that in terms of code. Omoju:    Yeah. Try and read tons of code, and just go through it. Instead of thumbing through Instagram, read some code. Hugo:    I wonder if there's some sort of Instagram for code like some- Omoju:    Oh- Hugo:    ... product you could have- Omoju:    ... my God Hugo:    ... in GitHub, right? Omoju:    I know. We're totally getting nerdy. But that is so awesome. Like, one of the problems is understanding ... After understanding how the human mind works, and how the brain works, and how the human brain actually acquires computation, there are so many ways to game it and to become a master of certain kinds of things. Hugo:    And I just want to circle back to a comment you made that you love exploratory data analysis or EDA. I just want to say that's really heartening to hear from a machine learning data scientist, in particular because a lot of a lot of aspiring data scientists say to me, ""What's the best model to use on data?"" And I'm like, ""What are you talking about? What does your data look like?"" Let's actually spend a couple of hours just looking at your data to get a feel for it, to understand its contours and its dimensions, right, before thinking about throwing blended extreme gradient boosting at it."" Omoju:    Exactly. And what is the best model for your data? Often the simplest thing will do. You have lots and lots and lots and lots of data. Hugo:    So Omoju, do you have a final call to action for our listeners out there? Omoju:    Yes, one that is quiet self-serving. GitHub is hiring. GitHub infrastructure is hiring. We are hiring in machine learning. So look at the career pages. Hugo:    And we'll put it in the show notes as well. Omoju:    Yeah, and the only other call to action that I have for people is to actually challenge yourself to learn. Like, this machine learning is not magic, it is basically mathematics, it's applied mathematics. And if you really want to understand it, take the time required. It might take you two years, it might take you three years. It's absolutely worth it because this is the future. And I want more people to have an understanding of what it is so they can ask stringent questions of us, and keep us ethically sound, and force us to actually use our knowledge to create solutions to the problems that they have because they will know that we are able to do x, y, and z. They know what our capabilities are. And they can hold our feet to the fire and say, ""I want to fund a company that does x, y, and z. The fund is dedicated to this."" So it forces us to go build the solutions. Hugo:    Thanks, Omoju. It's been such a pleasure having you on the show. Omoju:    It's been absolutely a pleasure, Hugo. Thank you for having me on the show.    "
"datacamp",2018-06-27,"New Project: A Visual History of Nobel Prize Winners","https://www.datacamp.com/community/blog/history-nobel-prize-winners","Well, let's find out! In this Project, you get to explore patterns and trends in over 100 years worth of Nobel Prize winners. What characteristics do the prize winners have? Which country gets it most often? And has anybody gotten it twice? It's up to you to figure this out. Before taking on this Project, we recommend that you have completed Introduction to the Tidyverse. The dataset used in this Project was retrieved from The Nobel Foundation on Kaggle."
"datacamp",2018-06-27,"New Course: Marketing Analytics in R","https://www.datacamp.com/community/blog/marketing-analytics-r","This is your chance to dive into the worlds of marketing and business analytics using R. Day by day, there are a multitude of decisions that companies have to face. With the help of statistical models, you're going to be able to support the business decision-making process based on data, not your gut feeling. Let us show you what a great impact statistical modeling can have on the performance of businesses. You're going to learn about and apply strategies to communicate your results and help them make a difference. How can you decide which customers are most valuable for your business? Learn how to model the customer lifetime value using linear regression. Predicting if a customer will leave your business, or churn, is important for targeting valuable customers and retaining those who are at risk. Learn how to model customer churn using logistic regression. Learn how to model the time to an event using survival analysis. This could be the time until next order or until a person churns. Learn how to reduce the number of variables in your data using principal component analysis. Not only does this help to get a better understanding of your data. PCA also enables you to condense information to single indices and to solve multicollinearity problems in a regression analysis with many intercorrelated variables. If you are interested in learning more about marketing and data science, check out this tutorial for Python, Data Science for Search Engine Marketing. The following R courses are prerequisites to take Marketing Analytics in R: Statistical Modeling."
"datacamp",2018-06-25,"Organizing Data Science Teams (Transcript)","https://www.datacamp.com/community/blog/organizing-data-science-teams-transcript","Hugo:    Hi Jonathan, and welcome to DataFramed. Jonathan:    Hi, how's it going? Hugo:    Really good. Really excited to have you on the show, and before we jump into our conversation, I've got a quick question for you. My first question is, in terms of skills that data scientists need to work on a daily basis, do you think it's more important to be able to develop sophisticated machine learning models or to be able to give a PowerPoint presentation? Jonathan:    Oh. I'm gonna say PowerPoint presentation, but I feel like that's a controversial answer, but I actually feel really strongly about this. Hugo:    Tell me a bit more about that. Jonathan:    When you think about the job of a data scientist, usually what that entails is taking data, understanding it, processing it, really trying to figure out what's going on, and then, once you've figured out what's going on, trying to convince someone else of what they should do, or what's there, or what your feeling is, but there's this really important part that is the convincing part, and knowing how to make a good PowerPoint presentation is really important in a business setting to be able to convey all those thoughts and ideas you've learned. 
    Having the ability to do that is way more important than being able to use the most advanced machine learning models, right? If you can use a linear regression and you can give a good PowerPoint, that can often be much more powerful than being able to do deep learning recurrent neural networks, but not being able to take what you've learned and convey it to other people. Hugo:    That's great, and I love that you mentioned regression, because this is an example where you can show people relatively straightforwardly, even non-technical people, why the model does what it does. You can explain if you tweak one parameter, why the output that they're interested in changes. Jonathan:    Yeah, and I think that's a thing that is often undervalued. I think there's kind of traditional notion in data science that the higher accuracy, the better, and that's the most important, you know? I think like things like Kaggle competitions really emphasize this, that the more accurate you can get, the higher your R-squared, the better things are going, but there's actually a lot to doing models, right? 
    There's understanding what are the things that are important within the model. There's the convincing people that your model is good. There's lots of things that go on, and often that stuff is more important than necessarily getting the technically most accurate answer, and so something like a logistic regression, it's really great because it's really easy to understand what's important, it's easy to explain to other people what's going on, and having things not be a black box is often really useful. Hugo:    Absolutely, and I also think ... You mentioned that a data scientist's job is to convince people of the power of their models and to help make business decisions, and another point is not only do you wanna make PowerPoint presentations, but you wanna, for example, you know, have figures that people understand, use colors that people like, as well. Jonathan:    Right. If you're making a plot, and there's something that's good that could come out of the plot and there's something that's bad, right? There's a bar for opportunity and a bar for risk, and if you color the bars where the good one is green and the bad one is red, when someone's gonna look at that chart, they're gonna much more quickly understand what's going on than if you made them two different shades of blue, and that's really small, but if you think about making good visualizations, good presentations, there are giant sets of these small decisions that all together compile into ¡°do people quickly understand what you're talking about and accept it, or do people feel confused and uncertain and don't necessarily wanna listen to what you're saying?¡± Hugo:    The other thing of course is, and I think you've written about it, if I recall correctly, is even a data scientist needs to be able to choose their meeting times correctly. For example, it's probably better to try to convince someone of something at 11AM than directly after lunch. Jonathan:    Yeah, there's that famous study that I'm gonna butcher, but it was around seeing who gets parole, I think, and the prisoners who go their parole right before lunch did a lot worse than the people who got it right after lunch, and so just having your meeting at the right time can often be influential in the decision, and that's really infuriating when you think about data science, right, because data science is all about getting the best evidence and using the best techniques to try and really understand what's going on, so to find a really cool finding and then have it not be listened to because your meeting was at the wrong time, that's just unfortunate, but there's lots of these sorts of decisions that happen, and being thoughtful of them is really valuable. Hugo:    You mentioned R-squared, and very recently you wrote a post on Medium which actually the first image in it is R-squared equals 0.042, and this is a great post about why data science projects may not work, and how we should feel when they don't work, and I thought maybe you could give us a bit of insight into your feelings about this. Jonathan:    Yeah, so this post was really kind of a culmination of a thing that's been happening a lot in my career, which was I would be given, or I would have some new idea for a project, right? Maybe it's trying to predict which customers are likely to churn at a company. I'd say, ""Hey, we can use data science here at this company to try and predict which customers are gonna churn, and we can use the customers' transactions, and we can use when they called the call center, and we can put all these things in, and make a machine learning model, and guess which customers will churn,"" and I come up with this idea, and people would say, ""Okay, great. Go try it,"" and they'd be all excited, and then I'd go try and do it, and it wouldn't work. 
    Maybe the data really wasn't quite there, or maybe I would actually have the data, and I make the model, but for whatever reason, it actually didn't work very well. Maybe it did turn out that knowing what transactions happened doesn't really actually tell you much about which customers are likely to churn, and what would happen to me in these situations is I'd get really depressed, and I'd think, ""Oh, if I was only a better data scientist, I could have made this work. If I had had a few more techniques, or if I had had a few more projects under my belt so I'd known what to look out for, if only I was better, this thing would have succeeded."" Hugo:    In fact, the top highlight on your Medium post is ... So you say, ""Each time I feel awful about myself, comma,"" and then the top highlight is, ""with a lingering belief that the project would have worked if only I had been a better data scientist,"" and what that says to me is a lot of people identify with this, or consider it very important to them. Jonathan:    Yeah, and I think, if you go and sample a hundred random blog posts about data science, I'm pretty sure that at least 95 would be talking about how great data science is, and how it's gonna change the world, and cure cancer, and you can use data to improve your company, and data's the new oil, and there are all these optimistic things. To create this environment where this feeling is is that data science is easy and it's gonna be very fruitful, and so when you get people trying to do these things in practice, the reality is you're doing a new, risky venture, right? 
    No one's tried predicting churn with transactions before. No one's tried clustering these customers before. You're doing all these things that no one has actually tried before, and so naturally it makes sense that most of the time, it doesn't work. If it did work, people would have probably tried it already, and so because of that we have this environment where the hype is that everything is easy and good, and the reality is that it's difficult and hard, and the natural inclination when people don't succeed at problems is they don't wanna talk about it, and so I feel like most people run into this situation where they try doing machine learning model, they try doing some data science, and it doesn't work, but then they can't really talk to anyone else about it, and so you just assume, well, since no one else is talking about how they fail all of the time, it must just be that I'm bad, but I don't think that's true. I think that happens to most of us, and dare I say all of us. Hugo:    As data scientists, we should be absolutely aware that statistically, it will happen to most of us some of the time. Jonathan:    Yeah. It should happen to some of us all the time, and when you see those news articles about those companies doing really cool data science and everything going great, they don't make articles about all the times things didn't work, so there's just a huge selection bias going on. Hugo:    This happens in basic research as well, right, that negative results aren't publishable, for the most part. Jonathan:    Yeah, and I feel like there's a movement to start publishing them more, and just getting more open about what people try that doesn't work, and I think data science is an especially good field where we would be especially receptive to that kind of philosophy. Hugo:    In this post, you mention several reasons why you may not be successful on any given task. Two in particular you mention is the data just doesn't contain a signal in it, and you give the example of it'd be ridiculous to predict the weather based on rolling die, right? But you also mention that a signal may exist, but your model isn't right, but then you chop down that particular possibility, really stating that if a signal exists, when you try a bunch of models, you'll find it, even if it's weak, so it's usually the fact that the data just doesn't contain the signal you're interested in. Jonathan:    Yeah, and that's really from my personal experience, is that if there's some relationship, you know, if it turns out that transactions can help you predict churn, then trying even a linear regression will pick up some sort of correlation, you know? You'll get some sort of success, and then you can try using better techniques, and choosing better features, and you can do a lot of things to improve it, but usually the very simple approach will still work. 
    In fact, I think this comes from this thing I've been thinking about a lot, which is this notion that machine learning models very rarely will pick up something that a human couldn't detect themselves, and so what do I mean by that? I mean that if you took customer data, you know, if you took a bunch of history of transactions, and you said, ""I'm gonna personally take a guess if this customer, number 27, if they will churn or not."" If you as a human can do that pretty well, then a machine learning algorithm can probably do it, too, right? Maybe it's that, well, maybe if you haven't made any transactions in a year, that's a sign you're not gonna come back, or maybe it's, if your transaction are getting less in value, it's coming back, but if you can kind of just explain, as a human, by looking at a couple of data points, what's happening, then your model will probably work. 
    Conversely, if you can't, even as a human, if you can't look at the data points and try and predict what's gonna happen, then a machine learning algorithm probably won't either. Let me give you an example from my career. At one point, I was working for a software company, and this software company would put out software, and before the software was released, it would have to go through testing, and so what would happen is people would use the software a lot, and it'd create a lot of in-app telemetry, so it'd create a lot of data around, well, then someone clicked here, and then someone did this, and then the app was a little sluggish, and things like that, and it'd create tons and tons of logs of telemetry. 
    The idea was, hey, what if we use machine learning and data science to try and make it so that we didn't need a human to tell if the software passed the test or not? What if we could just look at that telemetry and tell if then, by looking at the telemetry, tell if it will pass or not. The company I was working for ended up spending two years and millions of dollars trying to do all these different data science and machine learning techniques, only to realize that it just wasn't gonna work, and intuitively, if you had looked at the very beginning and said, ""Okay, well, here's a terabyte of telemetry data collected from the app. Tell if it's gonna pass the test or not."" 
    Like, the tests were really complex things, right? They're like, the app must feel responsive, and it must have no grammatical errors, and things like that, and things you would never be able to tell just by looking at telemetry. So as a human, if you couldn't look at the telemetry and tell if it would pass or not, there'd have been no way, or it's very unlikely that a machine learning algorithm is gonna suddenly detect a thing that you wouldn't have noticed, either. 
    Let me bring this all back. Then going back to why do the simple approaches work, well, it's like if you with your eye can figure out right away, ""Oh, this is what the rules are here. You know, I could think this customer is going to churn, or I think this software is going to pass the test,"" if you can do that, then a simple algorithm can probably pick it up, too. Hugo:    Essentially, then, a more complex algorithm won't just, I think as you say in this post, won't just give you signal where a simple algorithm did not, but a more complex algorithm will give you some sort of lift, or it'll turn a good algorithm into a great algorithm, but it won't just produce something from nothing. Jonathan:    Right, exactly. It may catch the edge cases, right? Your linear and logistic regressions, they may not be able to notice all the different possible rules and things like that, but they can pick up on the base idea of what's going on, and so if they can't pick up on anything, then it is very unlikely that by putting in a super hard algorithm, you're actually gonna get a huge success. Hugo:    This has been a whirlwind introduction to, I think, a lot of your interests, what you think about. I wanna step back a bit and consider you historically, in some sense. I'd like to know how you got into data science originally, what your trajectory was. Jonathan:    I would say my story starts with me going to college, and I went to a college not knowing what I wanted to study, and after my first semester I realized, ""You know, I really like mathematics. I'm gonna get a degree in that."" I had no idea what kind of things you could do with a math degree or anything like that. I knew intellectually that people said, ""Oh, businesses hire math majors,"" but I didn't actually know what that was like, or what the job would look like. I didn't really know anything, but I figured, ""I like math. It'll all work itself out."" 
    I ended up getting an undergrad degree, and I got a master's degree in mathematics, too, because at some point during my undergrad and master's, I decided, ""Oh, I wanna be a professor,"" and then later, during that undergrad and master's, I realized, ""Oh, I hate math research. I don't wanna do that at all."" Then I ended up working at a company called Vistaprint, and this was before data science was a term, so at the time, this was a role called business analytics, and I didn't really know what that meant, but it sounded interesting, and so there I ended up doing a bunch of cool stuff around creating forecasting algorithms for sales, and helping them optimize their recommendation engine, and things like that. 
    I ended up doing that for a while, and then I realized, ""Wow, having a degree in math and having all this applied math knowledge isn't that helpful without knowing statistics, and knowing how to work with data, and all those things,"" so I ended up going back for a doctorate in industrial engineering, and my particular research had to do with how do you optimize electrical vehicle route networks, so if you're Tesla, where do you put charging stations, and if you have electric buses, where do you have them stop and recharge their batteries? 
    All these sorts of cool math problems, but not super related to the data science, but I was finishing my degree, and then I started getting into consulting, and I realized there's actually this huge opportunity where lots of these companies have all this data, and they just need help figuring out what to do with it, and so I think by that point, data science had decided to be a term, and so at that point I became a data scientist, and did that for quite a few years. 
    Eventually, that consulting company I was with went under. That was unrelated to me, and then I ended up being a director of analytics at a consulting firm called Lenati, where I started the analytics teams from scratch, started as just me, ended up growing to a team of seven, and then I decided that I wanted to do something new. That team was pretty much running itself, and so I became an independent consultant, and so I am working for Nolis LLC. Hugo:    Great. What do you do generally in your independent consultancy work? Jonathan:    There are lots of companies that are trying to start getting into data science, machine learning, AI, and if you don't have any of that, trying to start having that is not easy, and so I help companies with that kind of process. Right now I'm helping T-Mobile looking into growing their AI, and how can you use AI and natural language processing within call centers, and things like that, and really growing that space up there. Hugo:    That sounds really exciting. T-Mobile of course is telecommunications, and we see that data science can have a huge impact in telecommunications. I'm wondering more generally what verticals do you currently see data science having the most impact in or being capable of the most impact? Jonathan:    I'm gonna pivot that question slightly, if you don't mind. Hugo:    No, not at all. I've got no idea where this is gonna go, so I'm excited. Jonathan:    Okay. The question of which verticals are right for data science. I think, in some way, it's more a question of which verticals are furthest along in the data science journey. You imagine tech companies, Google, Microsoft, Amazon, they're pretty far in. They really understand data science, they have big teams, and some of the other verticals, you can imagine they're much earlier. Retail is getting there, healthcare in some ways is getting there, in some ways it's just barely starting. It really depends, and when I think about data science, I really think it actually breaks into three separate fields, and I think for each one of those subfields, the different companies and different verticals can approach them differently. Allow me to talk about those three different fields. Hugo:    I'd love that. Jonathan:    When I think of data science, I think it's really three topics. One topic is business intelligence, and that's really around taking data that the company has and getting it in front of the right people. That can mean taking data and putting it in dashboards, or weekly reports, or even in emails that get sent out every day, but really taking data and getting it to the right place, and business intelligence generally doesn't have that much analysis of the data. As a BI person, your job is not to try and figure out what it all means, your job is to get it to the right people so they can figure out what it means. 
    The second area of data science is what I call decision science, and I say what I call, but I didn't coin the term, and I don't know who did, and it's killing me, so if someone's listening to this and they know who coined the term decision science, please email me. Hugo:    Get in touch. Jonathan:    Yeah. Decision science is really around taking data and using it to help a company make a decision. For instance, that could be trying to figure out, ""Hey, which of our products is the right product ... You know, which of the products should we stop stocking, or we are noticing that this segment of customers is churning. Can you help us figure out why, or even what is the best way to split up our customers so that we market to them differently?"" This is really around the creating PowerPoints, and really trying to get people to understand what is happening from what we see in the data. 
    Then the last field is what I think about as machine learning, and so this is the area of data science that's how can we take data science models and put them continuously into production. For instance, making a websites recommendations engine, or creating the model that chooses what price are we gonna quote you when you look at our website, or trying to predict each day, trying to decide which customers are gonna get the email. All these things that are continuously running, that's really the machine learning part of data science, and each one of these fields is different, and each one is important, and I think for different verticals and industries, they've progressed differently along each one of those. Hugo:    There are interactions as well, right? For example, machine learning can impact decision science. Jonathan:    Yeah, and they really kind of overlap quite a bad, because you could imagine that the decision science work can also influence what you're gonna put in your dashboard, which relates to BI, and the machine learning models can influence what you report out in the decision science, and the decision science folks can really influence what ... Does it even make sense to put a machine learning model in place in the first place? I think from a skill sets perspective, each one is different, but they have a fair amount of overlap, so when you think about business intelligence, that's much more around understanding storing data in databases, it's understanding Tableau, Power BI, and the right visualization approaches. 
    When it comes to decision science, that's much more around using Python and R to be able to take a big dataset and get some meaning out of it, and then put it into a meaningful report, and then the machine learning component is much more around the software engineering, so getting it so that you create a model, you test that it works, that you deploy that code into a test environment, and you deploy it into production, and so it's by far the closest to software engineering. 
    I think when you realize that data science has these three distinctions, it makes it a lot more approachable, because I think if you lump them all together, you kind of think, ""Wow, I need to understand software engineering, and I need to understand visualization approaches, and I need to understand how to make an effective report. How can I, a lowly human being, ever be able to do all of this and become a data scientist?"" But because it's actually much more specialized, it's much easier for any one person to enter this field. 
    Usually which part of this field you're best fit for, you will just naively fall into, so people who really have a good business understanding, and think about people, and that sort of component, are often the ones who end up as the decision scientists, and the people who really like software engineering and thinking about, well, what's the right way to store this data in a big cluster? You end up doing that kind of work, so it's often naturally sorting. Hugo:    Exactly, and I think that will help a lot of newcomers who, as you say, can find the world of data science incredibly overwhelming, get started. Jonathan:    Yeah, I saw that there's this discussion people have been having online around those like ""This is what a data scientist looks like"" infographic that has a hundred thing listed that the person knows, and it's like, no, any one data scientist maybe knows four of those, and that's plenty. Hugo:    Exactly. The unicorn is rare. Jonathan, we've seen clearly that data science can be highly impactful, but as with any endeavor, I'd like to invoke a healthy skepticism, and I'm wondering, to your mind, what can't data science do, or what isn't data science capable of? Jonathan:    I think that there's this naive assumption that if you use data in a decision, then that decision will be better. So if you are deciding where to locate a new factory, then having data on every possible bit of information you can know about all the different places will intrinsically cause you to make a better decision, or, for instance, in my last company, we did a lot of loyalty program design, so having all the historic data on a company's customers before designing the new loyalty program will intrinsically get you a better-designed program than not having that information. 
    There's that assumption that more information, more data, is better, and therefore data science should make every decision better, but often in practice, data can sometimes make decisions harder, or get a less good solution, right? For instance, if the data you have is sufficiently limited, then it could be that using data actually causes you to make a less informed decision. 
    For instance, if you are designing a new loyalty program for a company, so let's say you're designing for Starbucks a new program for you get a certain number of coffees, you get a free coffee. Knowing everything you possibly could about how customers behaved in the old program doesn't actually tell you very much about how they will behave once a new program's available, but if you assume that how they behave in the old program will be just like how they behave in the new program, you may make a decision that is actually less good than had you not thought about data at all. Hugo:    Is this speaking to the idea that an underlying assumption of data science is that the past will be a good predictor of the future? Jonathan:    Yes, exactly, that the past would be a good predictor of the future, or if you have customers that look similar to other customers, how they will all behave similarly. There's lots of actually, if you really step back and think about it, when you're doing a data science analysis, you're often making a ton of assumptions, and those assumptions often they have the possibility of leading you to a bad place, which is, if you think about it, can get really dark, because then it's like, why do anything ever? Why should everyone even care about data science? But no, it's still good, but you just have to be careful. 
    Here's just another example, right? Suppose you're doing an analysis for a company, and you find that customers who buy product X end up spending way more after that, so then you now have this bit of knowledge, that the customers who have bought product x spend more in the future. That piece of information doesn't necessarily tell you that much, right, because from it you can infer that A, if we give everyone else product X, they will then buy more, or B, you know, you could say, ""Hey, all those customers who buy product x, they're high-value customers. Maybe we could get rid of product X and they would still be high-value customers,"" right? Or maybe if you took product X and gave it to the people who didn't, you know, maybe you give it to the people who didn't buy product X, maybe they'd be annoyed because product X doesn't work for them at all, right? 
    There's all these actual different things that could come out of the analysis, that customers who bought product X buy more in the future, and that finding that if you buy product X, or the people who have bought product X buy more in the future, that finding could be valuable, but it could also be incredibly dangerous if you make an assumption off it that pulls not true. That doesn't mean finding that initial analysis wasn't valuable in the first place, it's just you have to be very thoughtful of what are the conclusions you draw from it. Hugo:    Absolutely, and then what decisions and business decisions to make afterwards, because that's what's not clear in this case as well. Jonathan:    Right, exactly, and I think most of the time, when we use data science to make decisions, it is not necessarily clear what is the exact implication you can pull from it, which is not to say that you shouldn't pull the data at all, it's just that it is not sufficient to have good data to then make a good decision. Back to the original question of what can't data science do, is it often can't tell you the final conclusion you should draw, it can just present evidence. Hugo:    Fantastic. We're here today to talk, among other things, about organizing data science teams and good organizational structure for data science teams, but before we do this, I'd like to know what are the most common pitfalls that you've seen people make when organizing data science teams. Jonathan:    I think there are a couple big areas people struggle with. For one, there's a big question of how should you organize data science within the bigger company? One approach people take is they build what's called a center of excellence, right, where you take all of your data scientists, you put them together in a space, and you say, ""Well, the data scientists will talk together, they'll share what they've learned, they'll really grow, and that's the best way to organize our data scientists."" 
    Another way you can do it is you can say, ""I'm gonna distribute the data scientists. I'm gonna put data scientists in each part of the company. Finance is gonna get data scientists, and marketing's gonna get data scientists, and the supply chain's gonna get data scientists,"" and you say, ""Well, this is the best way to organize because those data scientists will be really focused on what's important to them, and really be able to help out that particular part of the organization."" 
    These are actually two totally different ways of organizing, and they can have really different results, so thinking about what's the right way to do it for your company is difficult to do, especially because often data science grows organically, and so to be able to coordinate hiring and distribution is just not an easy thing to do. Hugo:    It may change as a function or time and as a function of industry, and the actual business needs of the company in question. Jonathan:    Yeah, it's really not a one size fits all. For some companies, one approach works, and for some it's really about being distributed, and things like that. Hugo:    Are there any other pitfalls that you commonly see? Jonathan:    Yeah. I think similar to that is how do you actually store the knowledge that the data science team generates? People often don't think very hard about how you're gonna store the knowledge that you gain from data science. For instance, if you learn that customers who buy product X spend more in the future, how are you gonna store that so that people remember it, right? How are you gonna keep that knowledge around so that it's not lost the moment the person who did the analysis has left, or the person who is the recipient of that analysis has left, and if you think about it, there's actually a lot of things that have to get stored, right? There's the things that you've learned from the analysis, there's the code to do the analysis, there's the knowledge of how do you actually execute that code to do the analysis?
    The best data science teams are the ones that can really handle the ability to have people changes, the ability to have changes in who's working on what, and have all that knowledge be stored, and the worst data science teams are the ones where all that knowledge is stored in one person, and if that one person quits or whatever, then that knowledge is lost. If you think about it, that is hugely expensive for an organization to have someone do lots of different analysis, and help out a company a lot, and then just suddenly lose all of that the moment the person quits. Hugo:    Interesting. In a word, essentially, it's how to take the knowledge gained and how to store it, but all the distribution of the knowledge. Jonathan:    Exactly, so companies that do this really well, they will make knowledge sharing hubs, right? Places where when you do analysis, you drop it right in, and they will come up with standards for their code, so code always has to be stored in this particular way, and I think that I really get caught up a lot is when you do an analysis, make sure that someone else can run it by pressing a single button.
    If you're thinking very practically, if you're writing R code and you have to run this script, and then that creates a dataset that you then run this script, and then you run this script, and you have to remember all those steps, no one else is gonna be able to do that, so to be able to enforce a rule that, hey, any time you do an analysis, it has to be that someone can run one script in one place, and it runs everything else correctly. Those things can be really helpful to an organization and a data science team, but isn't something that everyone always does. Hugo:    Agreed, and not only other people may not be able to run it, but me in a week or less may not be able to run what I've been doing, right? Jonathan:    Yeah, that's how I learned this lesson. It only takes you having getting burnt yourself by this a couple of times, and also I think 10 years ago, when I started in this field, we didn't have the tools to do it that we do now. In R, for instance, you can have R code that automatically pulls from your database, and runs an analysis, and creates a Word doc with all of your output. A single R script can do all of those things, and that just wasn't possible 10 years ago. Hugo:    No, that's right. Having talked about the common pitfalls, in your mind, what is the ideal organizational structure for a data science team? Jonathan:    For me, I've found that what works best is what I think of as distributing the data science within the business, but culturally being centered as a single group. By that I mean if you're gonna have lots of different data scientists, some who have to work on supply chain and some who work on marketing, really have those people embedded in those parts of the organization. Really have them get to know the problems and the people and what's going on there, but that being said, make sure that all the data scientists culturally feel like they're connected together, so have even small things, like team lunches, and quarterly outings where we talk about our career goals, and things like that that get them to feel like they are part of the data science team. 
    That combination I think really works the best, so distribute the work, and on a day to day basis have people embedded, but make sure you have everyone coming back as a single group culturally to keep people feeling like one core team. Hugo:    Yeah, that's cool, and I'm wondering whether even this idea of storing and distributing knowledge in a knowledge repository, for example, or a wiki, can actually help with this kind of culture of a single group, that they're checking each other's code every now and then, seeing what each other are working on in this kind of a knowledge base. Jonathan:    Yeah, exactly, and if we're culturally in one group, then when that group is meeting once a week or quarter or whatever, when we're meeting together, we'll be talking, and I'll say, ""Hey, let me tell you about this analysis I did."" They're like, ""Oh wow, that would have been great over here,"" and you build those bridges, and you get things continuing to work together, and that kind of cultural cohesion is really valuable, especially to your point of keeping the knowledge around. Hugo:    Yeah, and in this particular model, would ... Let me get this right. Would a data scientist on a growth team report directly to the VP of growth, or would they also report to a chief data scientist or something like that? Jonathan:    My heart tells me they would report to the chief data scientist, but dotted line to the other group, but I think in practice it really depends on the actual organization. You said ideal, and I'm kind of punting by saying, ""Well, it depends,"" but I think that kind of exactly who reports to who is much more around the culture of the company you're working in. Hugo:    Is there an inherent challenge here, though, that if you report to someone, but there are dotted lines also, that you get crossed lines, and it can actually get confusing. Jonathan:    Yeah, I think the problem is is because of the field of data science, you're always gonna get dotted lines, right? If you only report just to a data science team, you will never actually do work for people who care, right? You need some sort of dotted line to get direction on what is important to the business, and if you report just to that part of the business, then you will kind of be isolated, and your data science work won't be coordinated with other parts of the company, which can be really problematic. Yeah, I think just more than other fields, like software development, we really, in data science, have to deal with the ambiguity of who reports to who. Hugo:    Yeah, I agree completely. How does your distinction between the different types of data science play into this ideal org structure? To be very specific, the distinction between what we discussed earlier. Business intelligence work, decision science, machine learning. Jonathan:    Yeah. I think those are really three different types of work, and I think data scientists listening to this podcast would likely agree that business intelligence, decision science and machine learning are really quite different, but people outside of this field often may struggle to understand the difference, and many times those are the people doing the hiring, and just to our point we were talking about, giving those dotted lines of work, so it's very natural that a person who feels like they're best at decision science may be asked to do BI work, or a person who is really thoughtful about machine learning may suddenly find themselves doing decision science work. 
    This cross-work happens all the time, but the more you can kind of, as a team, try and enforce a structure that keeps that work separate, the better, because I've really seen teams struggle where people are doing kinds of work that they don't like, which ends up causing them to be really dissatisfied, which causes the team's productivity to go down, and everyone's just unhappy, and that's just not a good way to run a team. Hugo:    Yeah, so I think something worth circling around is the fact that data science teams and data science individuals can actually sometimes be stuck between a rock and a hard place, in the sense that we do have a lot of dotted lines everywhere, and in all honesty, a lot of the time you'll have engineers on one side that you're waiting on stuff to be implemented, and marketeers and BizOps on the other, for example. I'm just wondering what are practical deals with this unique position for data scientists and data science teams. Jonathan:    Yeah, and I really love that question, because it gets to the thing ... Just as we were talking about, and just as I was talking about the beginning, whereas to me, it feels like data science is much less about understanding complex mathematical approaches or working with giant, massive, big datasets, and it's much more about how do you get people to work together and make decisions using data, and yeah, on the day-to-day basis, as a data scientist, you're gonna be given questions from marketing, and BizOps, and you're gonna be given questions from the engineering teams, and you're gonna have people not wanting to do things you really wish they would do, and trying to manage all of that is difficult, but it's very much a part of the job, and the more, as a field, we think bigger than just what is the next, newest technique, and more about how do we handle these kinds of approaches, I think the better we will be. 
    The question is ¡°what is the actual practical way of dealing with this?¡±, so I have two answers to that. One is kind of a cop out, and the kind of a cop out answer is hire people who are good at the stuff in the first place. When it comes to hiring, if you are the person on the data science team who does hiring, really try and hire people who you think would be effective at communicating with people outside of data science, because if you're given the choice to hire someone who knows the best techniques or knows how to communicate, the person with the communication skills is gonna really be the person who helps your team out, because just as we were talking about at the beginning of this podcast, data science is about getting people to make decisions effectively, and so a core component of that is communication. 
    That's the kind of cop out answer, because most of us don't actually have the ability to choose who's hired, so a more practical answer. For me, I've found that the most effective thing is really just trusting that everyone you're working with is trying to do what they believe is in the group's best interest. When the engineering people who you're working with are telling you they can't do a thing, it's not because they're malicious or don't like you or don't respect you, it's because they are trying to do what they think is best, and the same with marketing, and the business teams, and so the more you trust other people to make good decisions, I find the better things go. 
    Another way of wording this is have empathy for people outside of data science. Really have empathy and understand what are the struggles of the business person? What is that poor engineering team struggling with that they are having difficulty getting you the data? The more empathy you can employ in a situation, the easier these sorts of decisions end up being. Hugo:    I think what you referred to as the cop out answer actually plays into that a great deal, because if you hire people who are good at working with other people, you can trust everybody as much as possible. Jonathan:    Yeah, and oftentimes, when you hire people are good at the communicating and good at more than just purely getting an optimal answer to a textbook's math problem, you're gonna get more different perspectives, you're gonna get more viewpoints, you'll have a more diverse team, and it just ends up being much more successful than hiring around technically who has the technically best technical skills. Hugo:    Yeah, and actually, this discussion reminded me of several points in ... You've got a fantastic series of posts on Medium. There are four parts of how your hiring process, and how to hire data scientists. Listeners, definitely check out all of Jonathan's hiring data scientists posts. The reason I mention this now is that you have a section on the take home challenge, which is a real business problem that you give people during the interview process, and you encourage, almost actually force them, I think, to email someone on your team at some point, to even see how they frame interacting with collaborators in that type of workspace. Jonathan:    Yeah, because, you know, when we're interviewing, you really wanna understand how is the person going to do at the job you are hiring them for, and oftentimes that job involves communicating with others. As part of the case studies we would give out at Lenati, I had written it so that the person taking it had to ... They needed a data dictionary to be able to do almost anything, and I didn't give them a data dictionary, and I said in the instructions, ""If you want a data dictionary, please just email me,"" and so they almost universally would, and that email was very informative on how do they think about handling communication, and did they make their request clear, and so that was very helpful, and that email was very much like what they would have to do on the job. Hugo:    In a previous discussion we had, talking about the distinctions between decision scientists and data scientists, you raised an interesting point, which is you asked the question ¡°is your product, as a data scientist or a decision scientist, a model, or is it an idea?¡± I'm wondering if you could unpack that slightly, because I think that's a wonderful question that perhaps we need to think about more. Jonathan:    Yeah. It's very easy as a data scientist to think what you're building and what you're doing is making models, so for instance, maybe you're making a segmentation model, or maybe you're making a customer lifetime value model, or whatever. You're making models, and for some people, that is their job. Really, it's just literally make a model, put it in production, and call it a day. Those are the people I've referred to being in machine learning, but for most of us, the job is much more around delivering an actual idea, right? 
    It's not just creating a survival model and survival curves, it's really saying, ""Okay, I have learned that these are the indicators that a customer is not likely to survive, and so I need to convey the idea of hey, these are the signs that someone's not gonna survive,"" and by survive, I don't mean die, I mean stop being a customer. ""These are the signs someone's not gonna be a customer, and so when you're thinking about doing marketing, you should try and market around these particular ideas."" When you pivot from thinking about delivering a model to an idea, then a lot of things come into more focus, right? Just like we were talking about before, you know, how do you give the right presentation to convey that idea? How do you get people convinced that you are a trustworthy person, that your ideas are sound? There are lots of things that come into convincing someone of something, and it's more than just a single model that does that. Hugo:    We've spoken a lot about the different types of data science work that can be done, and you spoke to the fact that it's important to recognize whether people really are interested and adept and skilled in business intelligence, decision science, machine learning. However, when you were director of insights and analytics at Lenati, you made it very clearly that you were hiring generalist data scientists, not specialists, and I'd like you to speak a bit more to the role of generalists and specialists in data science as a whole today, and how you see this evolving in the future. Jonathan:    I think to the point earlier about there behind business intelligence work, decision science work, machine learning work, I think it is very reasonable, acceptable and wise to focus in one of those three areas. For me, most of my work has been in the decision science space. I've done a fair amount in BI and machine learning, but most of my work is in decision science, and so for each person, having a specialization is generally good. But, just as we were discussing earlier, what will happen is oftentimes, for whatever reason, your company doesn't have that much, or your team doesn't have that much of that type of work to do right now, you know? 
    It's like, ""Oh, we don't have any decision science work to do, but we really could use someone to make this visualization,"" and so having the ability to switch between one or the other, it's not something you have to be able to do, but you should be able to want to try it, and for me, a lot of my career growth has just come from a situation where someone asked me to do something that I didn't know how to do before, and I just said, ""Well, I'll try,"" and I learned it. 
    When I hire, I really look for people who are generalists because I think it's important that they feel comfortable, that if they were given something new, they would be able to learn it. When you think about running a team, right, you could have a team where everyone's literally a generalist, and technically anyone can do anything, right? If we have BI work, anyone can do it. If we have decision science work, anyone can do it. In some ways, it's very good for running a team because you don't have to worry about having ... If anyone's available to do work, they should be able to do it, and as a employee, it's very good to be able to work in that space, because it means that if something new happens, you can feel comfortable trying to do it.
    Generalization is good, but the downside of that is that people like doing the stuff they like doing, right? Some people really like making dashboards, and some people really like building machine learning models and coding them so they can run in production, and some people like giving presentations, and so if you can get your team to specialize, then you get to give people what they like to do, and then if you like people, you should like to give them what they like to do. By allowing for more specialization, you can then make your team happier, but then the downside of specialization is that, say you have someone on your team who likes making the presentations, yeah? Someone who likes loading the data. You have all these different specialist roles. 
    Well, now anytime you need to do one task, you have to have, like, five people involved, right? You have to have the data loader, and then the person who does the analysis, and then the visualizer, and then the presenter, and each time you have a new person involved, getting knowledge from one person to the other is hard, it takes time, that person loses context, and so that can make your team run less efficiently. 
    It's like, I was constantly in the struggle to try and decide, well, how much should I allow people to specialize versus how much should I say, ""Everyone here technically could do everything, so I will give someone a piece of work even if they're not the best at it, and they will learn how to do it."" At the end of the day, I really landed on, ""I'm gonna do that generalist approach of I'm gonna assume anyone can do everything, and while some people are better at things than others, I'm not gonna feel uncomfortable giving something to someone a little bit out of their comfort zone,"" but I'm not sure that's the right approach, and so if a listener has some evidence that, ""No, here's a situation where our team really highly specialized and it worked extremely well,"" I would love to hear about it. Hugo:    Yeah, as would I, and look, Jonathon, embedded in that narrative I think is one of the most important points for people kind of getting into data science, is really to demonstrate the willingness and ability to learn new things on the fly almost constantly, right? Jonathan:    Yeah, and in fact, let me tell you a story from my very first job at Vistaprint. I was a month out of school, out of a master's. First real job, and I was working at the company, and they had this situation where the company, there was a bug on the website, and they lost a fair amount of money, and they only realized there was a bug because the marketing dashboard went down a bit, and so marketing noticed, and then eventually they found the bug.
    A marketing executive came to me and he said, ""You're on the forecasting team."" The bug was on a Tuesday, and he said, ""You're on the forecasting team. Can you pull the last five Tuesdays worth of data, because I show that we can analyze Tuesdays and see that this issue occurred,"" and so at the time, I realized, hey, this is actually a deeply complex mathematical problem of if you have a sales forecast, and every day you have a certain amount of sales come in, how can you know when sales are so much lower than baseline that it's a problem? In fact, how can you even know what the baseline is? There's weekly seasonality, there's yearly seasonality, our sales are going up each day because our company is growing, so there's all these different factors, and how can you tell, okay, this is so bad, we should do something about it. 
    I realized that problem was way out of my comfort zone, no one's ever tackled it before, and all of the executive wants is these five Tuesdays, so do I give him the five Tuesdays or do I make this problem way more complicated and complain that we're not doing it the right way? As you can imagine, I did the latter, and I complained a lot, and eventually, this ultimately led to me running a team where we built this tool that would actually, every day, analyze all these different components, and try and figure out, okay, is today a day that we're considering sufficiently bad that we need to do something? 
    To do that, I had to learn a ton about statistical quality control, I had to learn a lot about different forecasting methods, and I had to really get out of my comfort zone, but because I did that and because I had learned how to learn, I really succeeded in that environment, and I think that's what makes the best data scientists, is the people who have learned how to learn, which is not easy, but if you can learn how to do it, it's very valuable. Hugo:    Agreed. We've talked a lot about what data science looks like today. What does the future of data science look like to you? Jonathan:    That's a good question. I see a couple of different things happening. I think more and more companies are going to get business intelligence people and decision science people, right? It's just going to be a common narrative for executives, and business people, that they're gonna wanna see what the data looks like at any time, which means they need BI to help support that, and they're gonna then look at that data and be like, ""So what should I infer from this? What's important and what sort of decisions should I make?"" And you need a decision scientist to do that, so I think those groups are really gonna flourish. 
    I think the machine learning work is gonna kind of diverge, where a lot of the things that used to be really common data science work, like building a churn model or segmenting customers, I think that stuff's gonna really start to be commoditized, and I think you can already start to see that happening, where it's getting to the point where it makes sense for a company to just buy an off-the-shelf customer lifetime value model rather than building their own, because building your own means you have to maintain it, your team may not have a customer lifetime value model expert, and if you buy one off the shelf, that model is trained on other people's data, too, so it should be, in theory, work better. 
    I think a lot of that's gonna be commoditized, but I think also there's gonna continue to be companies who really just continue to do ML work in terms of really trying to do cutting edge natural language processing, and AI, so I think those jobs are gonna continue to exist, but I think it's gonna become more and more special applications as opposed to just general, ""Hey, I'm at a retail company, and I'm the retail company's machine learning engineer."" 
    I think, on a related note, something I've noticed a lot in the last year or so is I've seen a real divergence in that I've seen half the people I know saying, ""Data science is such an easy field to get a job in,"" and the other half saying, ""Data science is an impossible field to get a job in,"" and the difference is the people in the first group are all senior people who have worked in data science, and companies are begging to get experienced data scientists on their teams. 
    But there are lots and lots and lots of people who are doing data science bootcamps, getting data science degrees, reading about data science and wanting to get into it, and so there's a large group of people who are less experienced, and I think companies are not hiring those people at the rate that they're being generated, and I suspect that that trend's gonna continue to get worse, where people are gonna continue to want senior data scientists even more than they do today, but the wave of people trying to get into data science is gonna get harder, and so it's gonna be more difficult for a new person to get in. Hugo: As a final question, I'm wondering if you have a final call to action to our listeners out there. Jonathan:    Yeah, I would say I've got two calls of action, which is maybe breaking it, because you're only supposed to have one, so two. One, if you are interested in having me consult for you or your organization around data science, in particular around how do you grow, expand your team, are hiring or potentially coming in with new approaches, please reach out to me. If you not in a position where you're looking to hire a consultant, but you still find me mildly entertaining, you should check me out on Twitter, and my Twitter account is @skyetetra, and that's Skye with an E at the end, so S-K-Y-E T-E-T-R-A. Hugo:    We'll also link to you on Twitter in the show notes. All right, Jonathan, thank you so much for coming on the show, and the wonderful chat. Jonathan:    Thank you. It's been a lot of fun."
"mastery",2018-07-11,"A Gentle Introduction to Effect Size Measures in Python","https://machinelearningmastery.com/effect-size-measures-in-python/","Statistical hypothesis tests report on the likelihood of the observed results given an assumption, such as no association between variables or no difference between groups. Hypothesis tests do not comment on the size of the effect if the association or difference is statistically significant. This highlights the need for standard ways of calculating and reporting [¡¦] Correlation is a measure of the association between two variables. It is easy to calculate and interpret when both variables have a well understood Gaussian distribution. When we do not know the distribution of the variables, we must use nonparametric rank correlation methods. In this tutorial, you will discover rank correlation methods for quantifying the [¡¦] There is an ocean of books on statistics; where do you start? A big problem in choosing a beginner book on statistics is that a book may suffer one of two common problems. It may be a mathematical textbook filled with derivations, special cases, and proofs for each statistical method with little idea for the [¡¦] The use of randomness is an important part of the configuration and evaluation of machine learning algorithms. From the random initialization of weights in an artificial neural network, to the splitting of data into random train and test sets, to the random shuffling of a training dataset in stochastic gradient descent, generating random numbers and [¡¦] Tom Mitchell¡¯s classic 1997 book ¡°Machine Learning¡± provides a chapter dedicated to statistical methods for evaluating machine learning models. Statistics provides an important set of tools used at each step of a machine learning project. A practitioner cannot effectively evaluate the skill of a machine learning model without using statistical methods. Unfortunately, statistics is an [¡¦] The machine learning practitioner has a tradition of algorithms and a pragmatic focus on results and model skill above other concerns such as model interpretability. Statisticians work on much the same type of modeling problems under the names of applied statistics and statistical learning. Coming from a mathematical background, they have more of a focus [¡¦] Statistics is a collection of tools that you can use to get answers to important questions about data. You can use descriptive statistical methods to transform raw observations into information that you can understand and share. You can use inferential statistical methods to reason from small samples of data to whole domains. In this post, [¡¦] Statistics and machine learning are two very closely related fields. In fact, the line between the two can be very fuzzy at times. Nevertheless, there are methods that clearly belong to the field of statistics that are not only useful, but invaluable when working on a machine learning project. It would be fair to say [¡¦] Systematic experimentation is a key part of applied machine learning. Given the complexity of machine learning methods, they resist formal analysis methods. Therefore, we must learn about the behavior of algorithms on our specific problems empirically. We do this using controlled experiments. In this tutorial, you will discover the important role that controlled experiments play [¡¦] Comparing machine learning methods and selecting a final model is a common operation in applied machine learning. Models are commonly evaluated using resampling methods like k-fold cross-validation from which mean skill scores are calculated and compared directly. Although simple, this approach can be misleading as it is hard to know whether the difference between mean [¡¦] Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning. Read More"
"mastery",2018-07-09,"How to Calculate Nonparametric Rank Correlation in Python","https://machinelearningmastery.com/how-to-calculate-nonparametric-rank-correlation-in-python/","Statistical hypothesis tests report on the likelihood of the observed results given an assumption, such as no association between variables or no difference between groups. Hypothesis tests do not comment on the size of the effect if the association or difference is statistically significant. This highlights the need for standard ways of calculating and reporting [¡¦] Correlation is a measure of the association between two variables. It is easy to calculate and interpret when both variables have a well understood Gaussian distribution. When we do not know the distribution of the variables, we must use nonparametric rank correlation methods. In this tutorial, you will discover rank correlation methods for quantifying the [¡¦] There is an ocean of books on statistics; where do you start? A big problem in choosing a beginner book on statistics is that a book may suffer one of two common problems. It may be a mathematical textbook filled with derivations, special cases, and proofs for each statistical method with little idea for the [¡¦] The use of randomness is an important part of the configuration and evaluation of machine learning algorithms. From the random initialization of weights in an artificial neural network, to the splitting of data into random train and test sets, to the random shuffling of a training dataset in stochastic gradient descent, generating random numbers and [¡¦] Tom Mitchell¡¯s classic 1997 book ¡°Machine Learning¡± provides a chapter dedicated to statistical methods for evaluating machine learning models. Statistics provides an important set of tools used at each step of a machine learning project. A practitioner cannot effectively evaluate the skill of a machine learning model without using statistical methods. Unfortunately, statistics is an [¡¦] The machine learning practitioner has a tradition of algorithms and a pragmatic focus on results and model skill above other concerns such as model interpretability. Statisticians work on much the same type of modeling problems under the names of applied statistics and statistical learning. Coming from a mathematical background, they have more of a focus [¡¦] Statistics is a collection of tools that you can use to get answers to important questions about data. You can use descriptive statistical methods to transform raw observations into information that you can understand and share. You can use inferential statistical methods to reason from small samples of data to whole domains. In this post, [¡¦] Statistics and machine learning are two very closely related fields. In fact, the line between the two can be very fuzzy at times. Nevertheless, there are methods that clearly belong to the field of statistics that are not only useful, but invaluable when working on a machine learning project. It would be fair to say [¡¦] Systematic experimentation is a key part of applied machine learning. Given the complexity of machine learning methods, they resist formal analysis methods. Therefore, we must learn about the behavior of algorithms on our specific problems empirically. We do this using controlled experiments. In this tutorial, you will discover the important role that controlled experiments play [¡¦] Comparing machine learning methods and selecting a final model is a common operation in applied machine learning. Models are commonly evaluated using resampling methods like k-fold cross-validation from which mean skill scores are calculated and compared directly. Although simple, this approach can be misleading as it is hard to know whether the difference between mean [¡¦] Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning. Read More"
"mastery",2018-07-06,"Statistics in Plain English for Machine Learning","https://machinelearningmastery.com/statistics-in-plain-english-for-machine-learning/","Statistical hypothesis tests report on the likelihood of the observed results given an assumption, such as no association between variables or no difference between groups. Hypothesis tests do not comment on the size of the effect if the association or difference is statistically significant. This highlights the need for standard ways of calculating and reporting [¡¦] Correlation is a measure of the association between two variables. It is easy to calculate and interpret when both variables have a well understood Gaussian distribution. When we do not know the distribution of the variables, we must use nonparametric rank correlation methods. In this tutorial, you will discover rank correlation methods for quantifying the [¡¦] There is an ocean of books on statistics; where do you start? A big problem in choosing a beginner book on statistics is that a book may suffer one of two common problems. It may be a mathematical textbook filled with derivations, special cases, and proofs for each statistical method with little idea for the [¡¦] The use of randomness is an important part of the configuration and evaluation of machine learning algorithms. From the random initialization of weights in an artificial neural network, to the splitting of data into random train and test sets, to the random shuffling of a training dataset in stochastic gradient descent, generating random numbers and [¡¦] Tom Mitchell¡¯s classic 1997 book ¡°Machine Learning¡± provides a chapter dedicated to statistical methods for evaluating machine learning models. Statistics provides an important set of tools used at each step of a machine learning project. A practitioner cannot effectively evaluate the skill of a machine learning model without using statistical methods. Unfortunately, statistics is an [¡¦] The machine learning practitioner has a tradition of algorithms and a pragmatic focus on results and model skill above other concerns such as model interpretability. Statisticians work on much the same type of modeling problems under the names of applied statistics and statistical learning. Coming from a mathematical background, they have more of a focus [¡¦] Statistics is a collection of tools that you can use to get answers to important questions about data. You can use descriptive statistical methods to transform raw observations into information that you can understand and share. You can use inferential statistical methods to reason from small samples of data to whole domains. In this post, [¡¦] Statistics and machine learning are two very closely related fields. In fact, the line between the two can be very fuzzy at times. Nevertheless, there are methods that clearly belong to the field of statistics that are not only useful, but invaluable when working on a machine learning project. It would be fair to say [¡¦] Systematic experimentation is a key part of applied machine learning. Given the complexity of machine learning methods, they resist formal analysis methods. Therefore, we must learn about the behavior of algorithms on our specific problems empirically. We do this using controlled experiments. In this tutorial, you will discover the important role that controlled experiments play [¡¦] Comparing machine learning methods and selecting a final model is a common operation in applied machine learning. Models are commonly evaluated using resampling methods like k-fold cross-validation from which mean skill scores are calculated and compared directly. Although simple, this approach can be misleading as it is hard to know whether the difference between mean [¡¦] Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning. Read More"
"mastery",2018-07-04,"How to Generate Random Numbers in Python","https://machinelearningmastery.com/how-to-generate-random-numbers-in-python/","Statistical hypothesis tests report on the likelihood of the observed results given an assumption, such as no association between variables or no difference between groups. Hypothesis tests do not comment on the size of the effect if the association or difference is statistically significant. This highlights the need for standard ways of calculating and reporting [¡¦] Correlation is a measure of the association between two variables. It is easy to calculate and interpret when both variables have a well understood Gaussian distribution. When we do not know the distribution of the variables, we must use nonparametric rank correlation methods. In this tutorial, you will discover rank correlation methods for quantifying the [¡¦] There is an ocean of books on statistics; where do you start? A big problem in choosing a beginner book on statistics is that a book may suffer one of two common problems. It may be a mathematical textbook filled with derivations, special cases, and proofs for each statistical method with little idea for the [¡¦] The use of randomness is an important part of the configuration and evaluation of machine learning algorithms. From the random initialization of weights in an artificial neural network, to the splitting of data into random train and test sets, to the random shuffling of a training dataset in stochastic gradient descent, generating random numbers and [¡¦] Tom Mitchell¡¯s classic 1997 book ¡°Machine Learning¡± provides a chapter dedicated to statistical methods for evaluating machine learning models. Statistics provides an important set of tools used at each step of a machine learning project. A practitioner cannot effectively evaluate the skill of a machine learning model without using statistical methods. Unfortunately, statistics is an [¡¦] The machine learning practitioner has a tradition of algorithms and a pragmatic focus on results and model skill above other concerns such as model interpretability. Statisticians work on much the same type of modeling problems under the names of applied statistics and statistical learning. Coming from a mathematical background, they have more of a focus [¡¦] Statistics is a collection of tools that you can use to get answers to important questions about data. You can use descriptive statistical methods to transform raw observations into information that you can understand and share. You can use inferential statistical methods to reason from small samples of data to whole domains. In this post, [¡¦] Statistics and machine learning are two very closely related fields. In fact, the line between the two can be very fuzzy at times. Nevertheless, there are methods that clearly belong to the field of statistics that are not only useful, but invaluable when working on a machine learning project. It would be fair to say [¡¦] Systematic experimentation is a key part of applied machine learning. Given the complexity of machine learning methods, they resist formal analysis methods. Therefore, we must learn about the behavior of algorithms on our specific problems empirically. We do this using controlled experiments. In this tutorial, you will discover the important role that controlled experiments play [¡¦] Comparing machine learning methods and selecting a final model is a common operation in applied machine learning. Models are commonly evaluated using resampling methods like k-fold cross-validation from which mean skill scores are calculated and compared directly. Although simple, this approach can be misleading as it is hard to know whether the difference between mean [¡¦] Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning. Read More"
"mastery",2018-07-02,"Statistics for Evaluating Machine Learning Models","https://machinelearningmastery.com/statistics-for-evaluating-machine-learning-models/","Statistical hypothesis tests report on the likelihood of the observed results given an assumption, such as no association between variables or no difference between groups. Hypothesis tests do not comment on the size of the effect if the association or difference is statistically significant. This highlights the need for standard ways of calculating and reporting [¡¦] Correlation is a measure of the association between two variables. It is easy to calculate and interpret when both variables have a well understood Gaussian distribution. When we do not know the distribution of the variables, we must use nonparametric rank correlation methods. In this tutorial, you will discover rank correlation methods for quantifying the [¡¦] There is an ocean of books on statistics; where do you start? A big problem in choosing a beginner book on statistics is that a book may suffer one of two common problems. It may be a mathematical textbook filled with derivations, special cases, and proofs for each statistical method with little idea for the [¡¦] The use of randomness is an important part of the configuration and evaluation of machine learning algorithms. From the random initialization of weights in an artificial neural network, to the splitting of data into random train and test sets, to the random shuffling of a training dataset in stochastic gradient descent, generating random numbers and [¡¦] Tom Mitchell¡¯s classic 1997 book ¡°Machine Learning¡± provides a chapter dedicated to statistical methods for evaluating machine learning models. Statistics provides an important set of tools used at each step of a machine learning project. A practitioner cannot effectively evaluate the skill of a machine learning model without using statistical methods. Unfortunately, statistics is an [¡¦] The machine learning practitioner has a tradition of algorithms and a pragmatic focus on results and model skill above other concerns such as model interpretability. Statisticians work on much the same type of modeling problems under the names of applied statistics and statistical learning. Coming from a mathematical background, they have more of a focus [¡¦] Statistics is a collection of tools that you can use to get answers to important questions about data. You can use descriptive statistical methods to transform raw observations into information that you can understand and share. You can use inferential statistical methods to reason from small samples of data to whole domains. In this post, [¡¦] Statistics and machine learning are two very closely related fields. In fact, the line between the two can be very fuzzy at times. Nevertheless, there are methods that clearly belong to the field of statistics that are not only useful, but invaluable when working on a machine learning project. It would be fair to say [¡¦] Systematic experimentation is a key part of applied machine learning. Given the complexity of machine learning methods, they resist formal analysis methods. Therefore, we must learn about the behavior of algorithms on our specific problems empirically. We do this using controlled experiments. In this tutorial, you will discover the important role that controlled experiments play [¡¦] Comparing machine learning methods and selecting a final model is a common operation in applied machine learning. Models are commonly evaluated using resampling methods like k-fold cross-validation from which mean skill scores are calculated and compared directly. Although simple, this approach can be misleading as it is hard to know whether the difference between mean [¡¦] Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning. Read More"
"mastery",2018-06-29,"The Close Relationship Between Applied Statistics and Machine Learning","https://machinelearningmastery.com/relationship-between-applied-statistics-and-machine-learning/","Statistical hypothesis tests report on the likelihood of the observed results given an assumption, such as no association between variables or no difference between groups. Hypothesis tests do not comment on the size of the effect if the association or difference is statistically significant. This highlights the need for standard ways of calculating and reporting [¡¦] Correlation is a measure of the association between two variables. It is easy to calculate and interpret when both variables have a well understood Gaussian distribution. When we do not know the distribution of the variables, we must use nonparametric rank correlation methods. In this tutorial, you will discover rank correlation methods for quantifying the [¡¦] There is an ocean of books on statistics; where do you start? A big problem in choosing a beginner book on statistics is that a book may suffer one of two common problems. It may be a mathematical textbook filled with derivations, special cases, and proofs for each statistical method with little idea for the [¡¦] The use of randomness is an important part of the configuration and evaluation of machine learning algorithms. From the random initialization of weights in an artificial neural network, to the splitting of data into random train and test sets, to the random shuffling of a training dataset in stochastic gradient descent, generating random numbers and [¡¦] Tom Mitchell¡¯s classic 1997 book ¡°Machine Learning¡± provides a chapter dedicated to statistical methods for evaluating machine learning models. Statistics provides an important set of tools used at each step of a machine learning project. A practitioner cannot effectively evaluate the skill of a machine learning model without using statistical methods. Unfortunately, statistics is an [¡¦] The machine learning practitioner has a tradition of algorithms and a pragmatic focus on results and model skill above other concerns such as model interpretability. Statisticians work on much the same type of modeling problems under the names of applied statistics and statistical learning. Coming from a mathematical background, they have more of a focus [¡¦] Statistics is a collection of tools that you can use to get answers to important questions about data. You can use descriptive statistical methods to transform raw observations into information that you can understand and share. You can use inferential statistical methods to reason from small samples of data to whole domains. In this post, [¡¦] Statistics and machine learning are two very closely related fields. In fact, the line between the two can be very fuzzy at times. Nevertheless, there are methods that clearly belong to the field of statistics that are not only useful, but invaluable when working on a machine learning project. It would be fair to say [¡¦] Systematic experimentation is a key part of applied machine learning. Given the complexity of machine learning methods, they resist formal analysis methods. Therefore, we must learn about the behavior of algorithms on our specific problems empirically. We do this using controlled experiments. In this tutorial, you will discover the important role that controlled experiments play [¡¦] Comparing machine learning methods and selecting a final model is a common operation in applied machine learning. Models are commonly evaluated using resampling methods like k-fold cross-validation from which mean skill scores are calculated and compared directly. Although simple, this approach can be misleading as it is hard to know whether the difference between mean [¡¦] Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning. Read More"
"mastery",2018-06-27,"What is Statistics (and why is it important in machine learning)?","https://machinelearningmastery.com/what-is-statistics/","Statistical hypothesis tests report on the likelihood of the observed results given an assumption, such as no association between variables or no difference between groups. Hypothesis tests do not comment on the size of the effect if the association or difference is statistically significant. This highlights the need for standard ways of calculating and reporting [¡¦] Correlation is a measure of the association between two variables. It is easy to calculate and interpret when both variables have a well understood Gaussian distribution. When we do not know the distribution of the variables, we must use nonparametric rank correlation methods. In this tutorial, you will discover rank correlation methods for quantifying the [¡¦] There is an ocean of books on statistics; where do you start? A big problem in choosing a beginner book on statistics is that a book may suffer one of two common problems. It may be a mathematical textbook filled with derivations, special cases, and proofs for each statistical method with little idea for the [¡¦] The use of randomness is an important part of the configuration and evaluation of machine learning algorithms. From the random initialization of weights in an artificial neural network, to the splitting of data into random train and test sets, to the random shuffling of a training dataset in stochastic gradient descent, generating random numbers and [¡¦] Tom Mitchell¡¯s classic 1997 book ¡°Machine Learning¡± provides a chapter dedicated to statistical methods for evaluating machine learning models. Statistics provides an important set of tools used at each step of a machine learning project. A practitioner cannot effectively evaluate the skill of a machine learning model without using statistical methods. Unfortunately, statistics is an [¡¦] The machine learning practitioner has a tradition of algorithms and a pragmatic focus on results and model skill above other concerns such as model interpretability. Statisticians work on much the same type of modeling problems under the names of applied statistics and statistical learning. Coming from a mathematical background, they have more of a focus [¡¦] Statistics is a collection of tools that you can use to get answers to important questions about data. You can use descriptive statistical methods to transform raw observations into information that you can understand and share. You can use inferential statistical methods to reason from small samples of data to whole domains. In this post, [¡¦] Statistics and machine learning are two very closely related fields. In fact, the line between the two can be very fuzzy at times. Nevertheless, there are methods that clearly belong to the field of statistics that are not only useful, but invaluable when working on a machine learning project. It would be fair to say [¡¦] Systematic experimentation is a key part of applied machine learning. Given the complexity of machine learning methods, they resist formal analysis methods. Therefore, we must learn about the behavior of algorithms on our specific problems empirically. We do this using controlled experiments. In this tutorial, you will discover the important role that controlled experiments play [¡¦] Comparing machine learning methods and selecting a final model is a common operation in applied machine learning. Models are commonly evaluated using resampling methods like k-fold cross-validation from which mean skill scores are calculated and compared directly. Although simple, this approach can be misleading as it is hard to know whether the difference between mean [¡¦] Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning. Read More"
"mastery",2018-06-25,"10 Examples of How to Use Statistical Methods in a Machine Learning Project","https://machinelearningmastery.com/statistical-methods-in-an-applied-machine-learning-project/","Statistical hypothesis tests report on the likelihood of the observed results given an assumption, such as no association between variables or no difference between groups. Hypothesis tests do not comment on the size of the effect if the association or difference is statistically significant. This highlights the need for standard ways of calculating and reporting [¡¦] Correlation is a measure of the association between two variables. It is easy to calculate and interpret when both variables have a well understood Gaussian distribution. When we do not know the distribution of the variables, we must use nonparametric rank correlation methods. In this tutorial, you will discover rank correlation methods for quantifying the [¡¦] There is an ocean of books on statistics; where do you start? A big problem in choosing a beginner book on statistics is that a book may suffer one of two common problems. It may be a mathematical textbook filled with derivations, special cases, and proofs for each statistical method with little idea for the [¡¦] The use of randomness is an important part of the configuration and evaluation of machine learning algorithms. From the random initialization of weights in an artificial neural network, to the splitting of data into random train and test sets, to the random shuffling of a training dataset in stochastic gradient descent, generating random numbers and [¡¦] Tom Mitchell¡¯s classic 1997 book ¡°Machine Learning¡± provides a chapter dedicated to statistical methods for evaluating machine learning models. Statistics provides an important set of tools used at each step of a machine learning project. A practitioner cannot effectively evaluate the skill of a machine learning model without using statistical methods. Unfortunately, statistics is an [¡¦] The machine learning practitioner has a tradition of algorithms and a pragmatic focus on results and model skill above other concerns such as model interpretability. Statisticians work on much the same type of modeling problems under the names of applied statistics and statistical learning. Coming from a mathematical background, they have more of a focus [¡¦] Statistics is a collection of tools that you can use to get answers to important questions about data. You can use descriptive statistical methods to transform raw observations into information that you can understand and share. You can use inferential statistical methods to reason from small samples of data to whole domains. In this post, [¡¦] Statistics and machine learning are two very closely related fields. In fact, the line between the two can be very fuzzy at times. Nevertheless, there are methods that clearly belong to the field of statistics that are not only useful, but invaluable when working on a machine learning project. It would be fair to say [¡¦] Systematic experimentation is a key part of applied machine learning. Given the complexity of machine learning methods, they resist formal analysis methods. Therefore, we must learn about the behavior of algorithms on our specific problems empirically. We do this using controlled experiments. In this tutorial, you will discover the important role that controlled experiments play [¡¦] Comparing machine learning methods and selecting a final model is a common operation in applied machine learning. Models are commonly evaluated using resampling methods like k-fold cross-validation from which mean skill scores are calculated and compared directly. Although simple, this approach can be misleading as it is hard to know whether the difference between mean [¡¦] Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning. Read More"
"mastery",2018-06-22,"Controlled Experiments in Machine Learning","https://machinelearningmastery.com/controlled-experiments-in-machine-learning/","Statistical hypothesis tests report on the likelihood of the observed results given an assumption, such as no association between variables or no difference between groups. Hypothesis tests do not comment on the size of the effect if the association or difference is statistically significant. This highlights the need for standard ways of calculating and reporting [¡¦] Correlation is a measure of the association between two variables. It is easy to calculate and interpret when both variables have a well understood Gaussian distribution. When we do not know the distribution of the variables, we must use nonparametric rank correlation methods. In this tutorial, you will discover rank correlation methods for quantifying the [¡¦] There is an ocean of books on statistics; where do you start? A big problem in choosing a beginner book on statistics is that a book may suffer one of two common problems. It may be a mathematical textbook filled with derivations, special cases, and proofs for each statistical method with little idea for the [¡¦] The use of randomness is an important part of the configuration and evaluation of machine learning algorithms. From the random initialization of weights in an artificial neural network, to the splitting of data into random train and test sets, to the random shuffling of a training dataset in stochastic gradient descent, generating random numbers and [¡¦] Tom Mitchell¡¯s classic 1997 book ¡°Machine Learning¡± provides a chapter dedicated to statistical methods for evaluating machine learning models. Statistics provides an important set of tools used at each step of a machine learning project. A practitioner cannot effectively evaluate the skill of a machine learning model without using statistical methods. Unfortunately, statistics is an [¡¦] The machine learning practitioner has a tradition of algorithms and a pragmatic focus on results and model skill above other concerns such as model interpretability. Statisticians work on much the same type of modeling problems under the names of applied statistics and statistical learning. Coming from a mathematical background, they have more of a focus [¡¦] Statistics is a collection of tools that you can use to get answers to important questions about data. You can use descriptive statistical methods to transform raw observations into information that you can understand and share. You can use inferential statistical methods to reason from small samples of data to whole domains. In this post, [¡¦] Statistics and machine learning are two very closely related fields. In fact, the line between the two can be very fuzzy at times. Nevertheless, there are methods that clearly belong to the field of statistics that are not only useful, but invaluable when working on a machine learning project. It would be fair to say [¡¦] Systematic experimentation is a key part of applied machine learning. Given the complexity of machine learning methods, they resist formal analysis methods. Therefore, we must learn about the behavior of algorithms on our specific problems empirically. We do this using controlled experiments. In this tutorial, you will discover the important role that controlled experiments play [¡¦] Comparing machine learning methods and selecting a final model is a common operation in applied machine learning. Models are commonly evaluated using resampling methods like k-fold cross-validation from which mean skill scores are calculated and compared directly. Although simple, this approach can be misleading as it is hard to know whether the difference between mean [¡¦] Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning. Read More"
"vidhya",2018-07-09,"DataHack Radio #4 <U+2013> Data Privacy, Women in Data Science and More with Carla Gentry","https://www.analyticsvidhya.com/blog/2018/07/datahack-radio-episode-4-carla-gentry/","Carla Gentry is one of most popular social media influencers in the data science field. She has over 300,000 followers on LinkedIn and 48k followers on Twitter. Her experience in this field is unparalleled and we are grateful for leaders like her who consistently give back to the community. She has been a regular reader of Analytics Vidhya¡¯s articles for quite a while now. It was a pleasure to have her appear on the podcast and to hear her views on data privacy, how the domain has changed in the last few years, her advice for women in data science, and a whole host of other topics. This article contains highlights of Carla¡¯s conversation with Kunal Jain. You can listen to the podcast by clicking on the above SoundCloud link or on our iTunes channel. Happy listening! You can subscribe to DataHack Radio and listen to this, and all previous episodes, on any of the below platforms: Carla holds a number of degrees including one in advanced economics, one in advanced mathematics, among others. Right after college she got an internship at one of the few econometrics firms in the United States. There she worked with terabytes of data (this was well before ¡®Big Data¡¯ was a buzzword). Her experience started with tools and platforms like SAS, Pico, etc. Her role was working with credit card data but the nature of the work was such that it became monotonous after a while. From there, she moved to the Weinstein Organization as a Senior Analyst and Data Specialist. Here Carla¡¯s role expanded to include direct marketing experience. This was followed by a year at the University of Chicago Booth School of Business as a Research Support Analyst where she taught Ph.D students how to work with data, how to do data mining, etc. Carla then moved on to work as the Marketing Information Manager at Career Education Corporation for the next 4 years. Post that she spent the next 3.5 years in the marketing and data field at PromoWorks, Tandus and Area203. She is now the owner and data scientist at Analytical Solution, where she has worked with clients like Kellogg, Johnson & Johnson, among various other organizations. Carla considers data mining a critical aspect in any data driven project. It helps you understand trends, see patterns, interpret reasons why a person was denied a loan or credit card, etc. It¡¯s at the core of the business and should be respected as such. But she stressed on the importance of privacy and the need to be clear with your clients and customers about where you are going to use their data, for what purposes, and how it might impact them (if at all). GDPR has of course changed the game in Europe with regards to being transparent with users but Carla feels everyone, regardless of laws, should have this as a best practice. With the amount of data that¡¯s being generated in the world, from websites to social media, it¡¯s critical to have that level of sensitivity. Data scientists have a responsibility to be unbiased, have integrity and use their experience to add a positive background to the dataset, rather than let their feelings cloud the model building exercise. Carla recalled that if a business had terabytes of data in the 90s, running a program on that was next to impossible because the mainframe would have crashed. Now a mainframe isn¡¯t even required! If you have a good database architecture set up, you can access millions and billions of rows in a fraction of the time it used to take previously. COBOL, PASCAL, C++, SAS, Mathematica, MATLAB <U+2013> these were the only programs available back when Carla started her journey. Of course now we have much more robust tools like R, Visual Studio (SQL), IBM¡¯s suite of tools, etc. One of the biggest reasons why the older programs have been phased out is because of their inability to handle gigantic datasets, which the new tools can. Of course with the rise of this data wave, and the advent of the digital era, the number of hackers and cyber thieves has also risen. So as things have gotten better in many ways, they have also gotten worse when it comes to security of your data. Carla expects more laws like GDPR to come to action in the next few years that will dictate how organizations collect and deal with your data. She has a warning for businesses that abuse data <U+2013> people will leave and look for ways to go incognito, which will leave your business with no data at all. ¡°We have got to get rid of the thinking that it¡¯s always been this way, so it should stay that way.¡± Carla is a champion of women in data science. She strongly believes that in order to incorporate more females into this field, the change has to start from the top. The CEO should have an obligation to encourage diversity into the organization by going to the HR department and digging deeper to understand the percentage of women in the company, and how to further improve upon that. Her advice to aspiring female data scientists was to the point <U+2013> stand your ground, be confident in yourself, find mentors, keep going and keep learning. You will find the perfect fit for you as long as you continue to believe in yourself and your abilities. Carla is an avid social media user and a HUGE influencer, especially in the data science field. It¡¯s very time consuming (2-3 hours per day at times) but if what she shares helps even one person, she definitely considers it worth that investment. She feels it¡¯s her responsibility to give back to the community, given that she has gotten so much from it over the years. There are some awesome rapid fire questions at the end of the podcast <U+2013> ensure you listen to those as well! *P.S. <U+2013> All views expressed by the guests on DataHack Radio are their own, and not of Analytics Vidhya."
"vidhya",2018-07-05,"Learn and Test your Machine Learning Skills with AV¡¯s New Practice Problems and Free Courses!","https://www.analyticsvidhya.com/blog/2018/07/learn-and-test-your-machine-learning-skills-with-avs-new-practice-problems-and-free-courses/","¡°Knowledge is of no value unless you put it into practice.¡± <U+2013> Anton Chekhov Gaining knowledge of new concepts is a critical aspect of data science and machine learning. But the real gold lies in putting these concepts into practice. The more you practice, the better your concepts become! I am excited to announce that Analytics Vidhya has launched two brand new practice problems for both machine learning and deep learning enthusiasts and experts. We have also added three new courses to our burgeoning training portal. These courses cover a variety of challenges machine learning folks will find useful. We believe in providing only top class content for our community and our trainings, hackathons, articles and practice problems reflect that commitment. Let¡¯s look at these practice problems and training courses in a bit more detail. Analytics Vidhya¡¯s practice problems bring out the data scientist within you. Our collection of practice problems span varying domains <U+2013> performing sentiment analysis, building recommendation systems, prediction loan default, identifying digits from images, estimating the age of Indian actors, among a whole host of other challenges. We are excited to launch two new practice problems: This is an intriguing computer vision problem which has been recently gained a lot of traction in the deep learning community. The dataset we are providing for this is called ¡®Fashion MNIST¡¯. It¡¯s inspired from MNIST, a very popular dataset in the machine learning community (you can check out the MNIST practice problem in our<U+00A0>¡®Identify the digits¡¯<U+00A0>challenge). In ¡®Identify the apparels¡¯, instead of digits the images show a type of apparel, e.g. tee-shirt, trousers, bag, etc. The dataset used in this problem was created by Zalando Research. This practice problem is meant for beginners in deep learning. Intermediate or experts in this field can also work on this to refresh their concepts. This is quite a unique practice problem.<U+00A0>You are challenged with predicting the ratings for jokes given by the users provided the ratings provided by the same users for another set of jokes. This dataset has been taken from the famous jester online Joke Recommender system dataset. This practice problem is meant for everyone in the machine learning field <U+2013> from beginners to experts. I recommend getting familiar with recommendation systems to get the most from this challenge. Analytics Vidhya¡¯s aim has always been to build and help data scientists all over the globe by providing top notch training resources. So, we have expanded our training catalogue exponentially this year. We launched the ¡®Introduction to Data Science¡® course which has quickly become our most popular training. We also have trainings on Excel and problem solving using data, and of course a comprehensive learning path to become a data scientist. We have recently added three more exciting trainings to this list! Time Series forecasts come in handy for creating simple forecasts like number of airline passengers, website traffic, etc. This course is a comprehensive guide to getting you started in this vast and intriguing domain. Time Series forecasting is a skill every data scientist should have in their skillset so ensure you take this course! This course is meant for newcomers in data science and machine learning. Predicting the sales of a business is one of the most common challenges in this field and this course will give you a very good idea of how to approach this challenge. This course will equip you with the skills and techniques required for solving a regression problem in R. This course is designed for people who want to learn how to solve binary classification problems. In this course, you will solve a real life case study of Dream Housing Finance. The company wants to automate the loan eligibility process (real-time) based on the customer details provided through an online application form. By the end of the course, you will have a solid understanding of classification problems and various approaches to solve them."
"vidhya",2018-07-05,"Using the Power of Deep Learning for Cyber Security","https://www.analyticsvidhya.com/blog/2018/07/using-power-deep-learning-cyber-security/","The majority of the deep learning applications that we see in the community are usually geared towards fields like marketing, sales, finance, etc. We hardly ever read articles or find resources about deep learning being used to protect these products, and the business, from malware and hacker attacks. While the big technology companies like Google, Facebook, Microsoft, and Salesforce have already embedded deep learning into their products, the cybersecurity industry is still playing catch up.<U+00A0>It¡¯s a challenging field but one that needs our full attention. In this article, we briefly introduce Deep Learning (DL) along with a few existing Information Security (hereby referred to as InfoSec) applications it enables. We then deep dive into the interesting problem of anonymous tor traffic detection and also present a DL-based solution to detect TOR traffic. The target audience for this article is data science professionals who are already working on machine learning projects. The content of this article assumes that you have foundation knowledge of machine learning and are currently either a beginner, or are exploring, deep learning and it¡¯s use cases. The below pre-reads are highly recommended to get the most out of this article: Deep learning is not a silver bullet that can solve all the InfoSec problems because it needs extensive labeled datasets. Unfortunately, no such labeled datasets are readily available. However, there are several InfoSec use cases where the deep learning networks are making significant improvements to the existing solutions. Malware detection and network intrusion detection are two such areas where deep learning has shown significant improvements over the rule-based and classic machine learning-based solutions. Network intrusion detection systems are typically rule-based and signature-based controls that are deployed at the perimeter to detect known threats. Adversaries change the malware signatures and easily evade the traditional network intrusion detection systems. Quamar et al. [1], in their IEEE transaction paper, showed deep learning (DL)-based systems using self-taught learning to be promising in detecting unknown network intrusions. Traditional security use cases such as malware detection and spyware detection have been tackled with deep neural net-based systems [2]. The generalization power of DL-based techniques is better compared to traditional ML-based approaches. Jung et al.¡¯s [3] DL based system can even detect zero-day malware. Daniel Gibert [2], a Ph.D. graduate from the University of Barcelona, has done extensive work related to convolutional neural networks (CNN, a type of DL architecture) and malware detection. In his Ph.D. thesis, he says that CNNs can detect even polymorphic malware. The DL-based neural nets are now getting used in User and Entity Behaviour Analytics (UEBA). Traditionally, UEBA employs anomaly detection and machine learning algorithms which distill the security events to profile and baseline every user and network element in the enterprise IT environment. Any significant deviations from the baselines were triggered as anomalies that further raised alerts to be investigated by the security analysts. UEBA enhanced the detection of insider threats, albeit to a limited extent. Now, deep learning-based systems are used to detect many other types of anomalies. Pawe©© Kobojek from Warsaw university, Poland [4] uses keystroke dynamics to verify the user using an LSTM network. Jason Trost, director of security data engineering at Capital One, has published several blogs [5] that have a list of technical papers and talks on applying deep learning in InfoSec. The artificial neural network is inspired from the biological neural network. Neurons are the atomic unit of a biological neural network. Each neuron consists of dendrites, nucleus, and axons. It receives signals through dendrites and is carried out through axons (Figure 1 below). The computations are performed in the nucleus. The entire network is made up of a chain of neurons. AI researchers borrowed this idea to develop the artificial neural network (ANN). In this setting, each neuron accomplishes three actions: Each neuron thus can classify whether a set of inputs belong to one class or another. This power is limited when only a single neuron is used. However, coining a set of neurons makes it a powerful machinery for classification and sequence labelling tasks. Figure 1: Greatest inspiration that we can get is from the nature <U+2013> figure depicts a biological neuron and an artificial neuron. A set of neuron layers can be used to create a neural network. The network architecture differs based on the objective it needs to achieve. A common network architecture is a Feed Forward Neural Network (FFN). Neurons are arranged linearly without any cycles to form a FFN. It is called feed forward because information travels in the forward direction inside the network, first through the input neurons layer, then through the hidden neuron layers, and the output neurons layer (Figure 2 below). Figure 2: A feed forward network with two hidden layers  Like any supervised machine learning model, the FFN needs to be trained using labeled data. The training is in the form of optimizing the parameters by reducing the error between the output value and the true value. One such important parameter to optimize is the weight each neuron gives to each of its input signals. For a single neuron, the weight can be easily computed using the error. However, when a set of neurons are collated in multiple layers, it is challenging to optimize the neuron weights in multiple layers based on the error computed at the output layer. The backpropagation algorithm helps to address this issue [6]. Backpropagation is an old technique which comes under the branch of computer algebra. Here, automatic differentiation is used<U+00A0>to calculate the<U+00A0>gradient<U+00A0>that is needed in the calculation of the<U+00A0>weights<U+00A0>to be used in the network. In a<U+00A0>FFN, based on activation of each linked neuron, the output is obtained. The error is propagated layer by layer. Based on the correctness of the output with the final outcome, the error is calculated. This error is then in turn back propagated to fix errors of internal neurons. For each data instance, the parameters are optimized by going through multiple iterations.  The primary goal of cyber-attacks is to steal the enterprise customer data, sales data, intellectual property documents, source codes and software keys. The adversaries exfiltrate the stolen data to remote servers in encrypted traffic along with the regular traffic. Most often adversaries use an anonymous network that makes it difficult for the security defenders to trace the traffic. Moreover, the exfiltrated data is typically encrypted, rendering rule-based network intrusion tools and firewalls to be ineffective. Recently, anonymous networks have also been used for C&C by specific variants of ransomware/malware. For instance, Onion Ransomware [7] uses the TOR network to communicate with its C&C. Figure 3: An illustration of TOR communication between Alice and a destination server. The communication starts with Alice requesting a path to the server. TOR network gives the path which is AES encrypted. The randomization of the path happens inside the TOR network. The encrypted path of the packet is shown in red. Upon reaching the exit node, which is the periphery node of the TOR network, the plain packet is transferred to the server.  Anonymous network/traffic can be accomplished through various means. They can be broadly classified into: Among them, TOR is one of the more popular choices. TOR is a free software that enables anonymous communication over the internet through a specialized routing protocol known as the onion routing protocol [9]. The protocol depends on redirecting internet traffic over various freely hosted relays across the world. During the relay, like the layers of an onion peel, each HTTP packet is encrypted using the public key of the receiver. At each receiver point, the packet can be decrypted using the private key. Upon decryption, the next destination relay address is revealed. This carries on until the exit node of the TOR network is met, where the decryption of the packet ends, and a plain HTTP packet is forwarded to the original destination server. An example routing scheme between Alice and the server is depicted in the above Figure 3 for illustration.  The original intent of launching TOR was to safeguard the privacy of users. However, adversaries have hijacked the good Samaritan objective to use it for various nefarious means instead. As of 2016, around 20% of the Tor traffic accounts for illegal activities. In an enterprise network, TOR traffic is curtained by not allowing the installation of the TOR client or blocking the Guard or Entry node IP address. However, there are numerous means through which adversaries and malware can get access to the TOR network to transfer data and information. The IP blocking strategy is not a sound strategy. Adversaries can spawn different IPs to carry out the communication. A bad bot landscape report by distil networks [5] shows that 70% of automated attacks in 2015 used multiple IPs, and 20% of automated attacks used over 100 IPs. TOR traffic can be detected by analyzing the traffic packets. This analysis can be on the TOR node, or in between the client and the entry node. The analysis is done on a single flow of packet. Each flow constitutes a tuple of source address, source port, destination address, and destination port. Network flows for different time intervals are extracted and analysis is carried on them. G. He et al. in their paper ¡°Inferring Application Type Information from Tor Encrypted Traffic¡± extracted burst volumes and directions to create a HMM model to detect the TOR applications that might be generating that traffic. Most of the popular works in this area leverage time-based features along with other features like size and port information<U+00A0>to detect TOR traffic. We take inspiration from Habibi et al¡¯s ¡°<U+00A0>Characterization of Tor Traffic using Time based Features¡± paper and follow a time-based approach over extracted network flow to detect TOR traffic for this article. However, our architecture uses a plethora of other meta-information that can be obtained to classify the traffic. This is inherently due to the Deep Learning architecture that has been chosen to solve this problem. We obtained the data from Habibi Lashkari et al. [11] at the University of<U+00A0>New Brunswick<U+00A0>for the data experiments done in this article. Their data consists of features extracted from the analysis of the university internet traffic. Extracted meta information from the data is given in the table below: Table 1: Meta information parameters obtained from [1] Apart from these parameters, other flow-based parameters are also included. A sample instance from the dataset is shown in Figure 4 below: Figure 4: An instance of the dataset used for this article. Please note that source IP/port and destination IP/port, along with the protocol field, have been removed from the instance as they overfit the model. We process all other features using a deep feed forward neural network with N hidden layers. The architecture of the neural network is shown in Figure 5 below. Figure 5: Deep learning network representation used for TOR traffic detection. The hidden layers vary between 2 to 10. We found N=5 to be optimal. For activation, Relu is used for all the hidden layers. Each layer of the Hidden layers is dense in nature and of dimension 100. Figure 6: A Python Code Snippet of the FFN in Keras.  The output node is activated by a sigmoid function. This was used as the output is a binary classification <U+2013> Tor or Non-Tor.  We used<U+00A0>Keras with Tensorflow in the backend to train the DL module. Binary cross entropy loss was used for optimizing the FFN. <U+00A0>The model was trained for different epochs. Figure 7 below shows training simulation for a run depicting the increasing performance and decreasing loss value as the number of epochs increase. Figure 7: Tensorboard generated statics depicting the network training process The results of the deep learning system were compared with various other estimators. Standard classification metrics of Recall, Precision and F-Score were used to measure the efficacy of the estimators. Our DL-based system was able to detect the TOR class well. However, it is the Non-Tor class that we need to give more importance to. It is seen that a Deep Learning-based system can reduce the false positive cases for Non-Tor category samples. The results are shown in the table below: Table 2: The output of ML and DL Models for the Tor Traffic Detection experiment Among various classifiers, Random Forest and Deep learning based approaches perform better than the rest. The result shown is based on 55,000 training instances. The dataset used in this experiment is comparatively smaller than typical DL-based systems. As the training data increases, performance would increase further for both DL-based and Random forest classifier. However, for large datasets, a DL-based classifier typically outperforms other classifiers, and it can be generalised for similar types of applications. For example, if one needs to train a classifier to detect the application used by TOR, then only the output layer needs retraining, and all the other layers can be kept the same. Whereas other ML-classifiers will need to be retrained for the entire dataset. Keep in mind that retraining the model may take significant computing resources for large datasets. Anonymized traffic detection is a nuanced challenge that every enterprise faces. The adversaries use TOR channels to exfiltrate data in anonymous mode. Current approaches by tor traffic detection vendors depend on blocking known entry nodes of the TOR network. This is not a scalable approach and can be easily bypassed. A generic method is to use deep learning-based techniques. In this article, we presented a deep learning-based system to detect the TOR traffic with high recall and precision. Let us know your take on the current state of deep learning, or if you have any alternate approaches, in the comments section below. Dr. Satnam Singh, Chief Data Scientist <U+2013> Acalvio Technologies Dr Satnam Singh is currently leading security data science development at Acalvio Technologies. He has more than a decade of work experience in successfully building data products from concept to production in multiple domains. In 2015, he was named as one of the top 10 data scientists in India.<U+00A0> To his credit, he has 25+ patents and 30+ journal and conference publications. Apart from holding a PhD degree in ECE from University of Connecticut, Satnam also holds a Masters in ECE from University of Wyoming. Satnam is a senior IEEE member and a regular speaker in various Big Data and Data Science conferences. Balamurali A R, Member Technical Staff (Data Science) at Acalvio Balamurali A R is a member of the data science team at Acalvio. He is a graduate from IIT Mumbai, holds a Ph.D in Computer Science and has previously worked with companies like Samsung and IBM."
"vidhya",2018-07-02,"The Top GitHub Repositories & Reddit Threads Every Data Scientist should know (June 2018)","https://www.analyticsvidhya.com/blog/2018/07/top-github-reddit-data-science-machine-learning-june-2018/","Half the year has flown by and that brings us to the June edition of our popular series <U+2013> the top GitHub repositories and Reddit threads from last month. During the course of writing these articles, I have learned so much about machine learning from either open source codes or invaluable discussions among the top data science brains in the world. What makes GitHub special is not just it¡¯s code hosting and social collaboration features for data scientists. It has lowered the entry barrier into the open source world and has played a MASSIVE role in spreading knowledge and expanding the machine learning community. We saw some amazing open source code being released in June. One of the most intriguing repositories was ¡®NLP Progress¡¯ <U+2013> created with the aim of keeping everyone updated regarding the latest updates in this field. Facebook also released the code for it¡¯s popular DensePose framework which could be a game changer in the pose estimation field. When it comes to Reddit, it is so rich in knowledge and perspective from data scientists and ML experts from around the globe. In this article you¡¯ll see discussions on reinforcement learning applications, machine learning setups, a wonderful computer vision example, and much more. I highly recommend participating in this discussions to enhance your skillset. You can check out the top GitHub repositories and top Reddit discussions (from April onwards) for the last five months below: Human pose estimation has garnered a lot of attention in the deep learning community this year. And Facebook took things to a new level when they open sourced the code to ¡®DensePose¡¯, their popular pose estimation framework. This technique identifies more than 5000 nodes in the human body (for context, other approaches operate with 10 or 20 joints). You can get an idea of this node mapping technique in the above image. DensePose has been created in the Detectron framework and is powered by Caffe2. Apart from the code, this repository also contains notebooks to visualize the DensePose-COCO dataset. Read more details about this release here. Natural Language Processing (NLP) is an often difficult field to get into, despite it¡¯s attractions. There is tons of unstructured text lying around which you have to work with, and that is no easy task. This repository has been created especially to track the progress in the NLP field. It¡¯s a very informative list of datasets and current state-of-the-art tasks, like dependency parsing, part-of-speech tagging, reading comprehension, etc. Make sure you star this and follow the progress if you¡¯re even vaguely interested in NLP. There is still a lot that can (and will) be added to this list, like information extraction, relation extraction, grammatical error correction, etc. If you have anything to contribute to this repository, the creator is open to ideas and suggestions so feel free to do that. Getting your model into production is one of the biggest challenges data scientists face when they enter this field. Designing and building the model is what attracts most people to machine learning but if you can¡¯t get that model into production, it essentially becomes just a piece of useless code. So Databricks (founded by the Apache Spark creators) decided to build and open source a solution to all ML framework challenges. Called MLflow, it is a platform that manages the entire machine learning lifecycle (from start to production) and has been designed to work with any library. Ever since it¡¯s release, it has gained a huge following (1,355 stars on GitHub) and you can check out our coverage of the library here. Another NLP entry in this article! When it comes to NLP tasks like sentiment analysis, or machine translation, the norm has been to build models specific to that task. Have you ever built a sentiment analysis model that can also do semantic parsing and question answering at the same time? That¡¯s what Salesforce researchers intend to do with this repository. They have published a research paper that outlines a model which can do 10 different NLP tasks at the same time. In this paper, they have thrown a chalenge (which they are calling decaNLP) to the community <U+2013> can you build such a model and improve on the approach they¡¯ve provided? The model Salesforce have built is being called the ¡°Swiss Army Knife for Natural Language Processing¡±. Read more details about this on AV¡¯s post here. Reinforcement learning is becoming popular by the day and so is the open source community for it. This repository is a collection of reinforcement learning algorithms from Richard Sutton and Andrew Barto¡¯s book and other research papers. These algorithms are presented in the form pf Python notebooks. The creator of the repository recommends using these notebooks when you read the book as they will significantly enhance your understand of what¡¯s being presented. The notes are detailed and anyone entering this field should definitely refer to this collection. Source: Wikipedia Your interest in this thread will be piqued by the above video, which is put together and presented really neatly. It sent the machine learning subreddit into overdrive and received almost 100 comments! The thread has a lot of useful information on how the technique was created (there¡¯s a step-by-step explanation from the developer), how long it took, what kind of other things it can do, etc. You¡¯ll learn a lot about computer vision in this thread. The creator of this technique and video has also open sourced his code on GitHub. So open your Jupyter notebooks and get cracking! OpenAI Five is a group of 5 neural networks designed and developed to beat human opponents in the popular Dota 2 game. It has been developed by the Elon Musk co-founded OpenAI venture, which explains the immediate popularity it has received since it¡¯s release. Why I¡¯m recommending this thread is the rich discussion around what else data scientists want to see from this technique, it¡¯s comparison with the popular DeepMind AlphaGo algorithm, and how much computational power it required to pull this off. There is a lot of perspective in this thread that will greatly benefit you. Additionally, you can also read our article on OpenAI Five here. If this topic didn¡¯t get your attention, the first few comments surely will. This discussion is like a wish list of what data scientists and machine learning practitioners want to see from the community. This thread made my list because of the discussion each idea spawned. Once a person added their idea to the thread, multiple folks replied with their ideas on how to implement it and if similar research was already present. This is a MUST-READ discussion <U+2013> for both enthusiasts and practitioners. Take some time out to go through it and you¡¯ll come out with a lot of knowledge (and perhaps even more questions). What hardware you use for machine learning plays a critical role in determining how good your model will perform, especially when the amount of data to be trained is huge. Read this thread to find out what other data scientists use for building their ML processes and models. The original poster has listed down a structured list of questions which helped keep the thread neat and understandable. There questions are as below: You can also take part in the discussion or use the comments section below this article to let us know your setup! As I mentioned above, reinforcement learning is a popular field these days. But due to the complex nature of the work, most of the research and use cases are limited to games and lab environments. In this thread, people already working in this field give their take on where they see RL penetrating in the near future. Some of the comments are of a more skeptical nature but are worth reading to understand what experts and enthusiasts feel about RL. Phew! So much to read and learn in this past month. This list has something for everybody <U+2013> NLP, reinforcement learning, open source code you can download and start working on, computer vision, discussions on various machine learning related things, and much more. Use the comments section below to let us know which repository and/or discussion you found the most interesting! Thank you Pranav for this very informative article."
