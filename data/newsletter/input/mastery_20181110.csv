"site","date","headline","url_address","text"
"mastery",2018-11-09,"How to Develop Multilayer Perceptron Models for Time Series Forecasting","https://machinelearningmastery.com/how-to-develop-multilayer-perceptron-models-for-time-series-forecasting/","Multilayer Perceptrons, or MLPs for short, can be applied to time series forecasting. A challenge with using MLPs for time series forecasting is in the preparation of the data. Specifically, lag observations must be flattened into feature vectors. In this tutorial, you will discover how to develop a suite of MLP models for a range of standard time series forecasting problems. The objective of this tutorial is to provide standalone examples of each model on each type of time series problem as a template that you can copy and adapt for your specific time series forecasting problem. In this tutorial, you will discover how to develop a suite of Multilayer Perceptron models for a range of standard time series forecasting problems. After completing this tutorial, you will know: Let¡¯s get started. How to Develop Multilayer Perceptron Models for Time Series ForecastingPhoto by Bureau of Land Management, some rights reserved. This tutorial is divided into four parts; they are: Multilayer Perceptrons, or MLPs for short, can be used to model univariate time series forecasting problems. Univariate time series are a dataset comprised of a single series of observations with a temporal ordering and a model is required to learn from the series of past observations to predict the next value in the sequence. This section is divided into two parts; they are: Before a univariate series can be modeled, it must be prepared. The MLP model will learn a function that maps a sequence of past observations as input to an output observation. As such, the sequence of observations must be transformed into multiple examples from which the model can learn. Consider a given univariate sequence: We can divide the sequence into multiple input/output patterns called samples, where three time steps are used as input and one time step is used as output for the one-step prediction that is being learned. The split_sequence() function below implements this behavior and will split a given univariate sequence into multiple samples where each sample has a specified number of time steps and the output is a single time step. We can demonstrate this function on our small contrived dataset above. The complete example is listed below. Running the example splits the univariate series into six samples where each sample has three input time steps and one output time step. Now that we know how to prepare a univariate series for modeling, let¡¯s look at developing an MLP model that can learn the mapping of inputs to outputs. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course A simple MLP model has a single hidden layer of nodes, and an output layer used to make a prediction. We can define an MLP for univariate time series forecasting as follows. Important in the definition is the shape of the input; that is what the model expects as input for each sample in terms of the number of time steps. The number of time steps as input is the number we chose when preparing our dataset as an argument to the split_sequence() function. The input dimension for each sample is specified in the input_dim argument on the definition of first hidden layer. Technically, the model will view each time step as a separate feature instead of separate time steps. We almost always have multiple samples, therefore, the model will expect the input component of training data to have the dimensions or shape: Our split_sequence() function in the previous section outputs the X with the shape [samples, features] ready to use for modeling. The model is fit using the efficient Adam version of stochastic gradient descent and optimized using the mean squared error, or ¡®mse¡®, loss function. Once the model is defined, we can fit it on the training dataset. After the model is fit, we can use it to make a prediction. We can predict the next value in the sequence by providing the input: And expecting the model to predict something like: The model expects the input shape to be two-dimensional with [samples, features], therefore, we must reshape the single input sample before making the prediction, e.g with the shape [1, 3] for 1 sample and 3 time steps used as input features. We can tie all of this together and demonstrate how to develop an MLP for univariate time series forecasting and make a single prediction. Running the example prepares the data, fits the model, and makes a prediction. Your results may vary given the stochastic nature of the algorithm; try running the example a few times. We can see that the model predicts the next value in the sequence. Multivariate time series data means data where there is more than one observation for each time step. There are two main models that we may require with multivariate time series data; they are: Let¡¯s take a look at each in turn. A problem may have two or more parallel input time series and an output time series that is dependent on the input time series. The input time series are parallel because each series has an observation at the same time step. We can demonstrate this with a simple example of two parallel input time series where the output series is the simple addition of the input series. We can reshape these three arrays of data as a single dataset where each row is a time step and each column is a separate time series. This is a standard way of storing parallel time series in a CSV file. The complete example is listed below. Running the example prints the dataset with one row per time step and one column for each of the two input and one output parallel time series. As with the univariate time series, we must structure these data into samples with input and output samples. We need to split the data into samples maintaining the order of observations across the two input sequences. If we chose three input time steps, then the first sample would look as follows: Input: Output: That is, the first three time steps of each parallel series are provided as input to the model and the model associates this with the value in the output series at the third time step, in this case 65. We can see that, in transforming the time series into input/output samples to train the model, that we will have to discard some values from the output time series where we do not have values in the input time series at prior time steps. In turn, the choice of the size of the number of input time steps will have an important effect on how much of the training data is used. We can define a function named split_sequences() that will take a dataset as we have defined it with rows for time steps and columns for parallel series and return input/output samples. We can test this function on our dataset using three time steps for each input time series as input. The complete example is listed below. Running the example first prints the shape of the X and y components. We can see that the X component has a three-dimensional structure. The first dimension is the number of samples, in this case 7. The second dimension is the number of time steps per sample, in this case 3, the value specified to the function. Finally, the last dimension specifies the number of parallel time series or the number of variables, in this case 2 for the two parallel series. We can then see that the input and output for each sample is printed, showing the three time steps for each of the two input series and the associated output for each sample. Before we can fit an MLP on this data, we must flatten the shape of the input samples. MLPs require that the shape of the input portion of each sample is a vector. With a multivariate input, we will have multiple vectors, one for each time step. We can flatten the temporal structure of each input sample, so that: Becomes: First, we can calculate the length of each input vector as the number of time steps multiplied by the number of features or time series. We can then use this vector size to reshape the input. We can now define an MLP model for the multivariate input where the vector length is used for the input dimension argument. When making a prediction, the model expects three time steps for two input time series. We can predict the next value in the output series proving the input values of: The shape of the 1 sample with 3 time steps and 2 variables would be [1, 3, 2]. We must again reshape this to be 1 sample with a vector of 6 elements or [1, 6] We would expect the next value in the sequence to be 100 + 105 or 205. The complete example is listed below. Running the example prepares the data, fits the model, and makes a prediction. There is another more elaborate way to model the problem. Each input series can be handled by a separate MLP and the output of each of these submodels can be combined before a prediction is made for the output sequence. We can refer to this as a multi-headed input MLP model. It may offer more flexibility or better performance depending on the specifics of the problem that are being modeled. This type of model can be defined in Keras using the Keras functional API. First, we can define the first input model as an MLP with an input layer that expects vectors with n_steps features. We can define the second input submodel in the same way. Now that both input submodels have been defined, we can merge the output from each model into one long vector, which can be interpreted before making a prediction for the output sequence. We can then tie the inputs and outputs together. The image below provides a schematic for how this model looks, including the shape of the inputs and outputs of each layer. Plot of Multi-Headed MLP for Multivariate Time Series Forecasting This model requires input to be provided as a list of two elements, where each element in the list contains data for one of the submodels. In order to achieve this, we can split the 3D input data into two separate arrays of input data: that is from one array with the shape [7, 3, 2] to two 2D arrays with the shape [7, 3] These data can then be provided in order to fit the model. Similarly, we must prepare the data for a single sample as two separate two-dimensional arrays when making a single one-step prediction. We can tie all of this together; the complete example is listed below. Running the example prepares the data, fits the model, and makes a prediction. An alternate time series problem is the case where there are multiple parallel time series and a value must be predicted for each. For example, given the data from the previous section: We may want to predict the value for each of the three time series for the next time step. This might be referred to as multivariate forecasting. Again, the data must be split into input/output samples in order to train a model. The first sample of this dataset would be: Input: Output: The split_sequences() function below will split multiple parallel time series with rows for time steps and one series per column into the required input/output shape. We can demonstrate this on the contrived problem; the complete example is listed below. Running the example first prints the shape of the prepared X and y components. The shape of X is three-dimensional, including the number of samples (6), the number of time steps chosen per sample (3), and the number of parallel time series or features (3). The shape of y is two-dimensional as we might expect for the number of samples (6) and the number of time variables per sample to be predicted (3). Then, each of the samples is printed showing the input and output components of each sample. We are now ready to fit an MLP model on this data. As with the previous case of multivariate input, we must flatten the three dimensional structure of the input data samples to a two dimensional structure of [samples, features], where lag observations are treated as features by the model. The model output will be a vector, with one element for each of the three different time series. We can now define our model, using the flattened vector length for the input layer and the number of time series as the vector length when making a prediction. We can predict the next value in each of the three parallel series by providing an input of three time steps for each series. The shape of the input for making a single prediction must be 1 sample, 3 time steps and 3 features, or [1, 3, 3]. Again, we can flatten this to [1, 6] to meet the expectations of the model. We would expect the vector output to be: We can tie all of this together and demonstrate an MLP for multivariate output time series forecasting below. Running the example prepares the data, fits the model, and makes a prediction. As with multiple input series, there is another, more elaborate way to model the problem. Each output series can be handled by a separate output MLP model. We can refer to this as a multi-output MLP model. It may offer more flexibility or better performance depending on the specifics of the problem that is being modeled. This type of model can be defined in Keras using the Keras functional API. First, we can define the input model as an MLP with an input layer that expects flattened feature vectors. We can then define one output layer for each of the three series that we wish to forecast, where each output submodel will forecast a single time step. We can then tie the input and output layers together into a single model. To make the model architecture clear, the schematic below clearly shows the three separate output layers of the model and the input and output shapes of each layer. Plot of Multi-Output MLP for Multivariate Time Series Forecasting When training the model, it will require three separate output arrays per sample. We can achieve this by converting the output training data that has the shape [7, 3] to three arrays with the shape [7, 1]. These arrays can be provided to the model during training. Tying all of this together, the complete example is listed below. Running the example prepares the data, fits the model, and makes a prediction. In practice, there is little difference to the MLP model in predicting a vector output that represents different output variables (as in the previous example) or a vector output that represents multiple time steps of one variable. Nevertheless, there are subtle and important differences in the way the training data is prepared. In this section, we will demonstrate the case of developing a multi-step forecast model using a vector model. Before we look at the specifics of the model, let¡¯s first look at the preparation of data for multi-step forecasting. As with one-step forecasting, a time series used for multi-step time series forecasting must be split into samples with input and output components. Both the input and output components will be comprised of multiple time steps and may or may not have the same number of steps. For example, given the univariate time series: We could use the last three time steps as input and forecast the next two time steps. The first sample would look as follows: Input: Output: The split_sequence() function below implements this behavior and will split a given univariate time series into samples with a specified number of input and output time steps. We can demonstrate this function on the small contrived dataset. The complete example is listed below. Running the example splits the univariate series into input and output time steps and prints the input and output components of each. Now that we know how to prepare data for multi-step forecasting, let¡¯s look at an MLP model that can learn this mapping. The MLP can output a vector directly that can be interpreted as a multi-step forecast. This approach was seen in the previous section were one time step of each output time series was forecasted as a vector. With the number of input and output steps specified in the n_steps_in and n_steps_out variables, we can define a multi-step time-series forecasting model. The model can make a prediction for a single sample. We can predict the next two steps beyond the end of the dataset by providing the input: We would expect the predicted output to be: As expected by the model, the shape of the single sample of input data when making the prediction must be [1, 3] for the 1 sample and 3 time steps (features) of the input and the single feature. Tying all of this together, the MLP for multi-step forecasting with a univariate time series is listed below. Running the example forecasts and prints the next two time steps in the sequence. In the previous sections, we have looked at univariate, multivariate, and multi-step time series forecasting. It is possible to mix and match the different types of MLP models presented so far for the different problems. This too applies to time series forecasting problems that involve multivariate and multi-step forecasting, but it may be a little more challenging, particularly in preparing the data and defining the shape of inputs and outputs for the model. In this section, we will look at short examples of data preparation and modeling for multivariate multi-step time series forecasting as a template to ease this challenge, specifically: Perhaps the biggest stumbling block is in the preparation of data, so this is where we will focus our attention. There are those multivariate time series forecasting problems where the output series is separate but dependent upon the input time series, and multiple time steps are required for the output series. For example, consider our multivariate time series from a prior section: We may use three prior time steps of each of the two input time series to predict two time steps of the output time series. Input: Output: The split_sequences() function below implements this behavior. We can demonstrate this on our contrived dataset. The complete example is listed below. Running the example first prints the shape of the prepared training data. We can see that the shape of the input portion of the samples is three-dimensional, comprised of six samples, with three time steps and two variables for the two input time series. The output portion of the samples is two-dimensional for the six samples and the two time steps for each sample to be predicted. The prepared samples are then printed to confirm that the data was prepared as we specified. We can now develop an MLP model for multi-step predictions using a vector output. The complete example is listed below. Running the example fits the model and predicts the next two time steps of the output sequence beyond the dataset. We would expect the next two steps to be [185, 205]. It is a challenging framing of the problem with very little data, and the arbitrarily configured version of the model gets close. A problem with parallel time series may require the prediction of multiple time steps of each time series. For example, consider our multivariate time series from a prior section: We may use the last three time steps from each of the three time series as input to the model and predict the next time steps of each of the three time series as output. The first sample in the training dataset would be the following. Input: Output: The split_sequences() function below implements this behavior. We can demonstrate this function on the small contrived dataset. The complete example is listed below. Running the example first prints the shape of the prepared training dataset. We can see that both the input (X) and output (Y) elements of the dataset are three dimensional for the number of samples, time steps, and variables or parallel time series respectively. The input and output elements of each series are then printed side by side so that we can confirm that the data was prepared as we expected. We can now develop an MLP model to make multivariate multi-step forecasts. In addition to flattening the shape of the input data, as we have in prior examples, we must also flatten the three-dimensional structure of the output data. This is because the MLP model is only capable of taking vector inputs and outputs. The complete example is listed below. Running the example fits the model and predicts the values for each of the three time steps for the next two time steps beyond the end of the dataset. We would expect the values for these series and time steps to be as follows: We can see that the model forecast gets reasonably close to the expected values. In this tutorial, you discovered how to develop a suite of Multilayer Perceptron, or MLP, models for a range of standard time series forecasting problems. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hello  Jason, Really good intro to MLP Neural Networks, just wondering if you had any training or tutorials on the same thing but on R instead of Python? Sorry, I only have examples in Python. Comment  Name (required)  Email (will not be published) (required)  Website"
"mastery",2018-11-07,"How to Use the TimeseriesGenerator for Time Series Forecasting in Keras","https://machinelearningmastery.com/how-to-use-the-timeseriesgenerator-for-time-series-forecasting-in-keras/","Time series data must be transformed into a structure of samples with input and output components before it can be used to fit a supervised learning model. This can be challenging if you have to perform this transformation manually. The Keras deep learning library provides the TimeseriesGenerator to automatically transform both univariate and multivariate time series data into samples, ready to train deep learning models. In this tutorial, you will discover how to use the Keras TimeseriesGenerator for preparing time series data for modeling with deep learning methods. After completing this tutorial, you will know: Let¡¯s get started. How to Use the TimeseriesGenerator for Time Series Forecasting in KerasPhoto by Chris Fithall, some rights reserved. This tutorial is divided into six parts; they are: Note: This tutorial assumes that you are using Keras v2.2.4 or higher. Time series data requires preparation before it can be used to train a supervised learning model, such as a deep learning model. For example, a univariate time series is represented as a vector of observations: A supervised learning algorithm requires that data is provided as a collection of samples, where each sample has an input component (X) and an output component (y). The model will learn how to map inputs to outputs from the provided examples. A time series must be transformed into samples with input and output components. The transform both informs what the model will learn and how you intend to use the model in the future when making predictions, e.g. what is required to make a prediction (X) and what prediction is made (y). For a univariate time series interested in one-step predictions, the observations at prior time steps, so-called lag observations, are used as input and the output is the observation at the current time step. For example, the above 10-step univariate series can be expressed as a supervised learning problem with three time steps for input and one step as output, as follows: You can write code to perform this transform yourself; for example, see the post: Alternately, when you are interested in training neural network models with Keras, you can use the TimeseriesGenerator class. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course Keras provides the TimeseriesGenerator that can be used to automatically transform a univariate or multivariate time series dataset into a supervised learning problem. There are two parts to using the TimeseriesGenerator: defining it and using it to train models. You can create an instance of the class and specify the input and output aspects of your time series problem and it will provide an instance of a Sequence class that can then be used to iterate across the inputs and outputs of the series. In most time series prediction problems, the input and output series will be the same series. For example: Technically, the class is not a generator in the sense that it is not a Python Generator and you cannot use the next() function on it. In addition to specifying the input and output aspects of your time series problem, there are some additional parameters that you should configure; for example: You must define a length argument based on your designed framing of the problem. That is the desired number of lag observations to use as input. You must also define the batch size as the batch size of your model during training. If the number of samples in your dataset is less than your batch size, you can set the batch size in the generator and in your model to the total number of samples in your generator found via calculating its length; for example: There are also other arguments such as defining start and end offsets into your data, the sampling rate, stride, and more. You are less likely to use these features, but you can see the full API for more details. The samples are not shuffled by default. This is useful for some recurrent neural networks like LSTMs that maintain state across samples within a batch. It can benefit other neural networks, such as CNNs and MLPs, to shuffle the samples when training. Shuffling can be enabled by setting the ¡®shuffle¡® argument to True. This will have the effect of shuffling samples returned for each batch. At the time of writing, the TimeseriesGenerator is limited to one-step outputs. Multi-step time series forecasting is not supported. Once a TimeseriesGenerator instance has been defined, it can be used to train a neural network model. A model can be trained using the TimeseriesGenerator as a data generator. This can be achieved by fitting the defined model using the fit_generator() function. This function takes the generator as an argument. It also takes a steps_per_epoch argument that defines the number of samples to use in each epoch. This can be set to the length of the TimeseriesGenerator instance to use all samples in the generator. For example: Similarly, the generator can be used to evaluate a fit model by calling the evaluate_generator() function, and using a fit model to make predictions on new data with the predict_generator() function. A model fit with the data generator does not have to use the generator versions of the evaluate and predict functions. They can be used only if you wish to have the data generator prepare your data for the model. We can make the TimeseriesGenerator concrete with a worked example with a small contrived univariate time series dataset. First, let¡¯s define our dataset. We will choose to frame the problem where the last two lag observations will be used to predict the next value in the sequence. For example: For now, we will use a batch size of 1, so that we can explore the data in the generator. Next, we can see how many samples will be prepared by the data generator for this time series. Finally, we can print the input and output components of each sample, to confirm that the data was prepared as we expected. The complete example is listed below. Running the example first prints the total number of samples in the generator, which is eight. We can then see that each input array has the shape [1, 2] and each output has the shape [1,]. The observations are prepared as we expected, with two lag observations that will be used as input and the subsequent value in the sequence as the output. Now we can fit a model on this data and learn to map the input sequence to the output sequence. We will start with a simple Multilayer Perceptron, or MLP, model. The generator will be defined so that all samples will be used in each batch, given the small number of samples. We can define a simple model with one hidden layer with 50 nodes and an output layer that will make the prediction. We can then fit the model with the generator using the fit_generator() function. We only have one batch worth of data in the generator so we¡¯ll set the steps_per_epoch to 1. The model will be fit for 200 epochs. Once fit, we will make an out of sample prediction. Given the inputs [9, 10], we will make a prediction and expect the model to predict [11], or close to it. The model is not tuned; this is just an example of how to use the generator. The complete example is listed below. Running the example prepares the generator, fits the model, and makes the out of sample prediction, correctly predicting a value close to 11. We can also use the generator to fit a recurrent neural network, such as a Long Short-Term Memory network, or LSTM. The LSTM expects data input to have the shape [samples, timesteps, features], whereas the generator described so far is providing lag observations as features or the shape [samples, features]. We can reshape the univariate time series prior to preparing the generator from [10, ] to [10, 1] for 10 time steps and 1 feature; for example: The TimeseriesGenerator will then split the series into samples with the shape [batch, n_input, 1] or [8, 2, 1] for all eight samples in the generator and the two lag observations used as time steps. The complete example is listed below. Again, running the example prepares the data, fits the model, and predicts the next out of sample value in the sequence. The TimeseriesGenerator also supports multivariate time series problems. These are problems where you have multiple parallel series, with observations at the same time step in each series. We can demonstrate this with an example. First, we can contrive a dataset of two parallel series. It is a standard structure to have multivariate time series formatted such that each time series is a separate column and rows are the observations at each time step. The series we have defined are vectors, but we can convert them into columns. We can reshape each series into an array with the shape [10, 1] for the 10 time steps and 1 feature. We can now horizontally stack the columns into a dataset by calling the hstack() NumPy function. We can now provide this dataset to the TimeseriesGenerator directly. We will use the prior two observations of each series as input and the next observation of each series as output. Each sample will then be a three-dimensional array of [1, 2, 2] for the 1 sample, 2 time steps, and 2 features or parallel series. The output will be a two-dimensional series of [1, 2] for the 1 sample and 2 features. The first sample will be: The complete example is listed below. Running the example will first print the prepared dataset, followed by the total number of samples in the dataset. Next, the input and output portion of each sample is printed, confirming our intended structure. The three-dimensional structure of the samples means that the generator cannot be used directly for simple models like MLPs. This could be achieved by first flattening the time series dataset to a one-dimensional vector prior to providing it to the TimeseriesGenerator and set length to the number of steps to use as input multiplied by the number of columns in the series (n_steps * n_features). A limitation of this approach is that the generator will only allow you to predict one variable. You almost certainly may be better off writing your own function to prepare multivariate time series for an MLP than using the TimeseriesGenerator. The three-dimensional structure of the samples can be used directly by CNN and LSTM models. A complete example for multivariate time series forecasting with the TimeseriesGenerator is listed below. Running the example prepares the data, fits the model, and makes a prediction for the next value in each of the input time series, which we expect to be [110, 115]. There are multivariate time series problems where there are one or more input series and a separate output series to be forecasted that is dependent upon the input series. To make this concrete, we can contrive one example with two input time series and an output series that is the sum of the input series. Where values in the output sequence are the sum of values at the same time step in the input time series. This is different from prior examples where, given inputs, we wish to predict a value in the target time series for the next time step, not the same time step as the input. For example, we want samples like: We don¡¯t want samples like the following: Nevertheless, the TimeseriesGenerator class assumes that we are predicting the next time step and will provide data as in the second case above. For example: Running the example prints the input and output portions of the samples with the output values for the next time step rather than the current time step as we may desire for this type of problem. We can therefore modify the target series (out_seq) and insert an additional value at the beginning in order to push all observations down by one time step. This artificial shift will allow the preferred framing of the problem. The complete example with this shift is provided below. Running the example shows the preferred framing of the problem. This approach will work regardless of the length of the input sample. A benefit of neural network models over many other types of classical and machine learning models is that they can make multi-step forecasts. That is, that the model can learn to map an input pattern of one or more features to an output pattern of more than one feature. This can be used in time series forecasting to directly forecast multiple future time steps. This can be achieved either by directly outputting a vector from the model, by specifying the desired number of outputs as the number of nodes in the output layer, or it can be achieved by specialized sequence prediction models such as an encoder-decoder model. A limitation of the TimeseriesGenerator is that it does not directly support multi-step outputs. Specifically, it will not create the multiple steps that may be required in the target sequence. Nevertheless, if you prepare your target sequence to have multiple steps, it will honor and use them as the output portion of each sample. This means the onus is on you to prepare the expected output for each time step. We can demonstrate this with a simple univariate time series with two time steps in the output sequence. You can see that you must have the same number of rows in the target sequence as you do in the input sequence. In this case, we must know values beyond the values in the input sequence, or trim the input sequence to the length of the target sequence. The complete example is listed below. Running the example prints the input and output portions of the samples showing the two lag observations as input and the two steps as output in the multi-step forecasting problem. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to use the Keras TimeseriesGenerator for preparing time series data for modeling with deep learning methods. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hi Jason
Great tutorials.
When I run your code. The univariate one step problem with lstm.
there is an error when running model.fit Traceback (most recent call last):
  File ¡°¡±, line 2, in
  File ¡°C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\keras\legacy\interfaces.py¡±, line 91, in wrapper
    return func(*args, **kwargs)
  File ¡°C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\keras\engine\training.py¡±, line 1415, in fit_generator
    initial_epoch=initial_epoch)
  File ¡°C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\keras\engine\training_generator.py¡±, line 177, in fit_generator
    generator_output = next(output_generator)
  File ¡°C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\keras\utils\data_utils.py¡±, line 793, in get
    six.reraise(value.__class__, value, value.__traceback__)
  File ¡°C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\six.py¡±, line 693, in reraise
    raise value
  File ¡°C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\keras\utils\data_utils.py¡±, line 658, in _data_generator_task
    generator_output = next(self._generator)
TypeError: ¡®TimeseriesGenerator¡¯ object is not an iterator Something you know how to overcome? Are you able to confirm that you Keras library is up to date?  E.g. v2.2.4 or higher? Hi Jason Thank you for your fast reply.
No, I was running v2.2.2 but have updated now to 2.2.4 and it works <U+0001F642> Thank you Glad to hear it. I have added a note to the tutorial. Hi Jason,
Thanks for the tutorials, it helps me a lot.
But I still have a huge problem when I deal with my dataset. I think it¡¯s a multi-site multivariate time series forecasting dataset (the trajectory data of multiple vehicles over a period of time). I don¡¯t know how to pre-process it before using the TimeseriesGenerator.
Could you give me some advice?
Thanks a lot. Before pre-processing, perhaps start with a strong framing of the problem <U+2014> it will guide you as to how to prepare the data. Thanks for your advice. But what is a frame? Like a model? I don¡¯t quite understand. Could you give a simple example or a link?
Thank you in advance. A frame is the choice of the type of problem (classification/regression) and the choice of inputs and outputs. More here:http://machinelearningmastery.com/how-to-define-your-machine-learning-problem/ I have read another article titled ¡®A Standard Multivariate, Multi-Step, and Multi-Site Time Series Forecasting Problem¡¯ on your blog. I have some kind of understanding the framing problem.
My project is quite like the ¡®Air Quality Prediction¡¯ project. Do you think it¡¯s possible to use LSTM for the global forecasting problem? Perhaps try it and see. Great tutorial Jason. I was wondering how you could include other independent variables to forecast the output. In my case I want to forecast the air pollution for a set of measurement stations. Besides the historical pollution I also have other variables like termperature and humidity. Do you know how to deal with that? Thanks! I show how to use it for multivariate data with a dependent series, does that help? Hey, thanks for the great tutorial. I¡¯m getting an error in the multivariate case. When I insert the 0 the input and output series¡¯ aren¡¯t the same length anymore: ValueError: Data and targets have to be of same length. Data length is 10 while target length is 11 Hmmm, perhaps post to stackoverflow or Keras help:https://machinelearningmastery.com/get-help-with-keras/ Comment  Name (required)  Email (will not be published) (required)  Website"
"mastery",2018-11-05,"A Gentle Introduction to LSTM Autoencoders","https://machinelearningmastery.com/lstm-autoencoders/","An LSTM Autoencoder is an implementation of an autoencoder for sequence data using an Encoder-Decoder LSTM architecture. Once fit, the encoder part of the model can be used to encode or compress sequence data that in turn may be used in data visualizations or as a feature vector input to a supervised learning model. In this post, you will discover the LSTM Autoencoder model and how to implement it in Python using Keras. After reading this post, you will know: Let¡¯s get started. A Gentle Introduction to LSTM AutoencodersPhoto by Ken Lund, some rights reserved. This post is divided into six sections; they are: An autoencoder is a neural network model that seeks to learn a compressed representation of an input. They are an unsupervised learning method, although technically, they are trained using supervised learning methods, referred to as self-supervised. They are typically trained as part of a broader model that attempts to recreate the input. For example: The design of the autoencoder model purposefully makes this challenging by restricting the architecture to a bottleneck at the midpoint of the model, from which the reconstruction of the input data is performed. There are many types of autoencoders, and their use varies, but perhaps the more common use is as a learned or automatic feature extraction model. In this case, once the model is fit, the reconstruction aspect of the model can be discarded and the model up to the point of the bottleneck can be used. The output of the model at the bottleneck is a fixed length vector that provides a compressed representation of the input data. Input data from the domain can then be provided to the model and the output of the model at the bottleneck can be used as a feature vector in a supervised learning model, for visualization, or more generally for dimensionality reduction. Sequence prediction problems are challenging, not least because the length of the input sequence can vary. This is challenging because machine learning algorithms, and neural networks in particular, are designed to work with fixed length inputs. Another challenge with sequence data is that the temporal ordering of the observations can make it challenging to extract features suitable for use as input to supervised learning models, often requiring deep expertise in the domain or in the field of signal processing. Finally, many predictive modeling problems involving sequences require a prediction that itself is also a sequence. These are called sequence-to-sequence, or seq2seq, prediction problems. You can learn more about sequence prediction problems here: Recurrent neural networks, such as the Long Short-Term Memory, or LSTM, network are specifically designed to support sequences of input data. They are capable of learning the complex dynamics within the temporal ordering of input sequences as well as use an internal memory to remember or use information across long input sequences. The LSTM network can be organized into an architecture called the Encoder-Decoder LSTM that allows the model to be used to both support variable length input sequences and to predict or output variable length output sequences. This architecture is the basis for many advances in complex sequence prediction problems such as speech recognition and text translation. In this architecture, an encoder LSTM model reads the input sequence step-by-step. After reading in the entire input sequence, the hidden state or output of this model represents an internal learned representation of the entire input sequence as a fixed-length vector. This vector is then provided as an input to the decoder model that interprets it as each step in the output sequence is generated. You can learn more about the encoder-decoder architecture here: An LSTM Autoencoder is an implementation of an autoencoder for sequence data using an Encoder-Decoder LSTM architecture. For a given dataset of sequences, an encoder-decoder LSTM is configured to read the input sequence, encode it, decode it, and recreate it. The performance of the model is evaluated based on the model¡¯s ability to recreate the input sequence. Once the model achieves a desired level of performance recreating the sequence, the decoder part of the model may be removed, leaving just the encoder model. This model can then be used to encode input sequences to a fixed-length vector. The resulting vectors can then be used in a variety of applications, not least as a compressed representation of the sequence as an input to another supervised learning model. One of the early and widely cited applications of the LSTM Autoencoder was in the 2015 paper titled ¡°Unsupervised Learning of Video Representations using LSTMs.¡± LSTM Autoencoder ModelTaken from ¡°Unsupervised Learning of Video Representations using LSTMs¡± In the paper, Nitish Srivastava, et al. describe the LSTM Autoencoder as an extension or application of the Encoder-Decoder LSTM. They use the model with video input data to both reconstruct sequences of frames of video as well as to predict frames of video, both of which are described as an unsupervised learning task. The input to the model is a sequence of vectors (image patches or features). The encoder LSTM reads in this sequence. After the last input has been read, the decoder LSTM takes over and outputs a prediction for the target sequence. <U+2014> Unsupervised Learning of Video Representations using LSTMs, 2015. More than simply using the model directly, the authors explore some interesting architecture choices that may help inform future applications of the model. They designed the model in such a way as to recreate the target sequence of video frames in reverse order, claiming that it makes the optimization problem solved by the model more tractable. The target sequence is same as the input sequence, but in reverse order. Reversing the target sequence makes the optimization easier because the model can get off the ground by looking at low range correlations. <U+2014> Unsupervised Learning of Video Representations using LSTMs, 2015. They also explore two approaches to training the decoder model, specifically a version conditioned in the previous output generated by the decoder, and another without any such conditioning. The decoder can be of two kinds <U+2013> conditional or unconditioned. A conditional decoder receives the last generated output frame as input [¡¦]. An unconditioned decoder does not receive that input. <U+2014> Unsupervised Learning of Video Representations using LSTMs, 2015. A more elaborate autoencoder model was also explored where two decoder models were used for the one encoder: one to predict the next frame in the sequence and one to reconstruct frames in the sequence, referred to as a composite model. ¡¦ reconstructing the input and predicting the future can be combined to create a composite [¡¦]. Here the encoder LSTM is asked to come up with a state from which we can both predict the next few frames as well as reconstruct the input. <U+2014> Unsupervised Learning of Video Representations using LSTMs, 2015. LSTM Autoencoder Model With Two DecodersTaken from ¡°Unsupervised Learning of Video Representations using LSTMs¡± The models were evaluated in many ways, including using encoder to seed a classifier. It appears that rather than using the output of the encoder as an input for classification, they chose to seed a standalone LSTM classifier with the weights of the encoder model directly. This is surprising given the complication of the implementation. We initialize an LSTM classifier with the weights learned by the encoder LSTM from this model. <U+2014> Unsupervised Learning of Video Representations using LSTMs, 2015. The composite model without conditioning on the decoder was found to perform the best in their experiments. The best performing model was the Composite Model that combined an autoencoder and a future predictor. The conditional variants did not give any significant improvements in terms of classification accuracy after fine-tuning, however they did give slightly lower prediction errors. <U+2014> Unsupervised Learning of Video Representations using LSTMs, 2015. Many other applications of the LSTM Autoencoder have been demonstrated, not least with sequences of text, audio data and time series. Creating an LSTM Autoencoder in Keras can be achieved by implementing an Encoder-Decoder LSTM architecture and configuring the model to recreate the input sequence. Let¡¯s look at a few examples to make this concrete. The simplest LSTM autoencoder is one that learns to reconstruct each input sequence. For these demonstrations, we will use a dataset of one sample of nine time steps and one feature: We can start-off by defining the sequence and reshaping it into the preferred shape of [samples, timesteps, features]. Next, we can define the encoder-decoder LSTM architecture that expects input sequences with nine time steps and one feature and outputs a sequence with nine time steps and one feature. Next, we can fit the model on our contrived dataset. The complete example is listed below. The configuration of the model, such as the number of units and training epochs, was completely arbitrary. Running the example fits the autoencoder and prints the reconstructed input sequence. The results are close enough, with very minor rounding errors. A plot of the architecture is created for reference. LSTM Autoencoder for Sequence Reconstruction We can modify the reconstruction LSTM Autoencoder to instead predict the next step in the sequence. In the case of our small contrived problem, we expect the output to be the sequence: This means that the model will expect each input sequence to have nine time steps and the output sequence to have eight time steps. The complete example is listed below. Running the example prints the output sequence that predicts the next time step for each input time step. We can see that the model is accurate, barring some minor rounding errors. A plot of the architecture is created for reference. LSTM Autoencoder for Sequence Prediction Finally, we can create a composite LSTM Autoencoder that has a single encoder and two decoders, one for reconstruction and one for prediction. We can implement this multi-output model in Keras using the functional API. You can learn more about the functional API in this post: First, the encoder is defined. Then the first decoder that is used for reconstruction. Then the second decoder that is used for prediction. We then tie the whole model together. The complete example is listed below. Running the example both reconstructs and predicts the output sequence, using both decoders. A plot of the architecture is created for reference. Composite LSTM Autoencoder for Sequence Reconstruction and Prediction Regardless of the method chosen (reconstruction, prediction, or composite), once the autoencoder has been fit, the decoder can be removed and the encoder can be kept as a standalone model. The encoder can then be used to transform input sequences to a fixed length encoded vector. We can do this by creating a new model that has the same inputs as our original model, and outputs directly from the end of encoder model, before the RepeatVector layer. A complete example of doing this with the reconstruction LSTM autoencoder is listed below. Running the example creates a standalone encoder model that could be used or saved for later use. We demonstrate the encoder by predicting the sequence and getting back the 100 element output of the encoder. Obviously, this is overkill for our tiny nine-step input sequence. A plot of the architecture is created for reference. Standalone Encoder LSTM Model This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered the LSTM Autoencoder model and how to implement it in Python using Keras. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Long Short-Term Memory Networks with Python It provides self-study tutorials on topics like:CNN LSTMs, Encoder-Decoder LSTMs, generative models, data preparation, making predictions and much more¡¦ Skip the Academics. Just Results. Click to learn more. Nice Explained¡¦¡¦¡¦. Thanks. Thanks for the great posts! I have learn a lot from them.
Can this approach for classification problems such as sentiment analysis? Perhaps. Hi Jason,
Thanks for the posts, I really enjoy reading this.
I¡¯m trying to use this method to do time series data anomaly detection and I got few questions here:
When you reshape the sequence into [samples, timesteps, features], samples and features always equal to 1. What is the guidance to choose the value here? If the input sequences have variable length, how to set timesteps, always choose max length? Also, if the input is two dimension tabular data with each row has different length, how will you do the reshape or normalization?
Thanks in advance! The time steps should provide enough history to make a prediction, the features are the observations recorded at each time step. More on preparing data for LSTMs here:https://machinelearningmastery.com/faq/single-faq/how-do-i-prepare-my-data-for-an-lstm Hi, I am wondering why the output of encoder has a much higher dimension(100), since we usually use encoders to create lower dimensions! Could you please bring examples if I am wrong? And what about variable length of samples? You keep saying that LSTM is useful for variable length. So how does it deal with a training set like: dataX[0] = [1,2,3,4]
dataX[1] = [2,5,7,8,4]
dataX[2] = [0,3] I am really confused with my second question and I¡¯d be very thankful for your help! <U+0001F642> The model reproduces the output, e.g. a 1D vector with 9 elements. You can pad the variable length inputs with 0 and use a masking layer to ignore the padded values. I really likes your posts and they are important.I got a lot of knowledge from your post.
Today, am going to ask your help. I am doing research on local music classifications. the key features of the music is it sequence and it uses five keys out of the seven keys, we call it scale. 1.  C <U+2013> E <U+2013> F <U+2013> G <U+2013> B. This is a major 3rd, minor 2nd, major 2nd, major 3rd, and minor 2nd
2.  C <U+2013> Db <U+2013> F <U+2013> G <U+2013> Ab. This is a minor 2nd, major 3rd, major 2nd, minor 2nd, and major 3rd.
3.  C <U+2013> Db <U+2013> F <U+2013> Gb <U+2013> A. This is a minor 2nd, major 3rd, minor 2nd, minor 3rd, and a minor 3rd.
4.  C <U+2013> D <U+2013> E <U+2013> G <U+2013> A. This is a major 2nd, major 2nd, minor 3rd, major 2nd, and a minor 3rd
it is not dependent on range, rythm, melody and other features. This key has to be in order. Otherwise it will be out of scale. So, which tools /algorithm do i need to use for my research purpose and also any sampling mechanism to take 30 sec sample music from each track without affecting the sequence of the keys ? Regards Perhaps try a suite of models and discover what works best for your specific dataset. More here:https://machinelearningmastery.com/faq/single-faq/what-algorithm-config-should-i-use Hi, can you please explain the use of repeat vector between encoder and decoder?
Encoder is encoding 1-feature time-series into fixed length 100 vector. In my understanding, decoder should take this 100-length vector and transform it into 1-feature time-series.
So, encoder is like many-to-one lstm, and decoder is one-to-many (even though that ¡®one¡¯ is a vector of length 100). Is this understanding correct? The RepeatVector repeats the internal representation of the input n times for the number of required output steps. Which model is most suited for stock market prediction None, a time series of prices is a random walk as far as I¡¯ve read. More here:https://machinelearningmastery.com/faq/single-faq/can-you-help-me-with-machine-learning-for-finance-or-the-stock-market Hi, thanks for the instructive post! I am trying to repeat your first example (Reconstruction LSTM Autoencoder) using a different syntax of Keras; here is the code: import numpy as np
from keras.layers import Input, LSTM, RepeatVector
from keras.models import Model timesteps = 9
input_dim = 1
latent_dim = 100 # input placeholder
inputs = Input(shape=(timesteps, input_dim)) # ¡°encoded¡± is the encoded representation of the input
encoded = LSTM(latent_dim,activation=¡¯relu¡¯)(inputs) # ¡°decoded¡± is the lossy reconstruction of the input
decoded = RepeatVector(timesteps)(encoded)
decoded = LSTM(input_dim, activation=¡¯relu¡¯, return_sequences=True)(decoded) sequence_autoencoder = Model(inputs, decoded)
encoder = Model(inputs, encoded) # compile model
sequence_autoencoder.compile(optimizer=¡¯adadelta¡¯, loss=¡¯mse¡¯) # run model
sequence_autoencoder.fit(sequence,sequence,epochs=300, verbose=0) # prediction
sequence_autoencoder.predict(sequence,verbose=0) I did not know why, but I always get a poor result than the model using your code.
So my question is: is there any difference between the two method (syntax) under the hood? or they are actually the same ? Thanks. Comment  Name (required)  Email (will not be published) (required)  Website"
