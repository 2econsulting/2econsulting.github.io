"site","date","headline","url_address","text"
"vidhya",2018-08-16,"Top 7 Sectors where Data Science can Transform India (with Free Datasets)","https://www.analyticsvidhya.com/blog/2018/08/top-7-sectors-where-data-science-can-transform-india-with-free-datasets/","

DATA DAYS DISCOUNT: Get FLAT 33% OFF on All Courses | Use COUPON CODE: DATA33 |
OFFER ENDS: 26th Aug 23:59 hrs IST | 
Valid for first 33 users(daily) | Buy Now 

 Data science is a powerful tool and has the ability to transform the world. We are already seeing massive changes to the way industries work in the Western world and data science is powering that change. But when it comes to India, we are still a long, long way from reaching that stage. As Dr. Avik mentioned in his insightful DataHack Radio podcast, most of the data we collect is highly unstructured and difficult to make sense of. But optimism abounds <U+2013> there is a growing expectation that people are realizing how crucial data is to the economy. In this article, we look at the top sectors in this wonderful nation of ours that are ripe for applying data science. We have also provided resource links for each sector with the hope that our AV community can take up the challenge and make this country a better place, all with the help of data science! The agriculture industry needs the use of data science more than any other right now. 40% of our population is employed in this field but unfortunately, agriculture¡¯s contribution to the nation¡¯s economy is a paltry 16% of the overall GPD. Given how critical this sector is, should that number not be significantly higher? There are a lot of facets in agriculture that can be worked upon <U+2013> predicting monthly/quarterly/annual yield, forecasting demand, analyzing weather patterns to decide when to sow, predicting the prices of vegetables so as to pick which crop to sow, etc. Open datasets on agriculture: There are three datasets on this page <U+2013> two are monthly and one is annual. These deal with the stock of different food grains in a year, the production of these food grains, and the central statistics of food and beverages. Dataset on the crop production in India. It¡¯s a fairly straightforward dataset but excellent for producing visualizations and basic insights. Rainfall in India dataset. Another crucial aspect to agriculture, and one that decides the livelihood of farmers. Predicting rainfall is essential to farming, and with this dataset, you can do just that! It contains monthly rainfall data from 1901-2015. This article, written by Shweta Gupta, is an excellent resource on the state of the agriculture industry in India, challenges that we face right now, and how we can use data to improve it. It should be a mandatory read for all Indians who are into data science, it¡¯s just that important! The average electricity use in India during the 2016-17 FY was a staggering 1,122 kWh per capita. Out of this, the industrial consumption was 40%, followed by residential consumption at 24%. This is all to say that the power demand is surging beyond expectations as the population increases year-on-year. Predicting power supply and demand, understanding the consumption pattern of households, classifying this by region/district/blocks, etc. are just some of the ways we can use data science in this sector. The resources I have mentioned below are enough to get you started and even go beyond that. A collection of open datasets by the Government. Datasets on pattern of electricity consumption, per capita consumption, consumption by sectors, and more are available here. Check it out! This Wikipedia article has up-to-date statistics on the electricity sector in India. This should be a compulsory read for all Indians, from students to working professionals. It is an eye opener to the fact that we are consuming power at a never-before-seen rate. It also contains granular details about rural areas. If you know even the basics of web scraping, this page is a goldmine. Dataset on individual household electric power consumption. While this isn¡¯t strictly Indian data, it sheds light on the kind of data we do need to collect in the first place. I encourage you to download this dataset, play around with it and come up with solutions as to how we, as a community, can utilize and maximize power consumption to our benefit. The most critical resource of all, and one of the most misused in India. It seems we see a drought every summer in quite a lot of rural areas, and the situation does not seem to be improving. The water usage is increasing each year and unless we properly assess the usage, it could end up turning into a crisis very soon. You can predict things like the predicted water level, the usage in certain areas in order to send adequate water supply tanks there in time, etc. You can come up with more ideas as you think about the challenges in this sector. Open datasets from the Central Ground Water Board. This contains granular information about water levels in every district in India. The variables it includes are the district name, latitude and longitude, type of site, the year the data was observed, and the water level during, after and before monsoons. It¡¯s a good place for you to understand the kind of data collected by the Government, and even work on it on your own! Open datasets on water quality in 2014. Plenty of datasets to download and work with here. It is available for different states so pick and choose as per your interest. Did you know that the Indian constitution guarantees free healthcare for all citizens? And that¡¯s the practice Government hospitals follow, at least for those who are below the poverty line. But the truth is that the private healthcare sector takes care of the majority of the healthcare business in the country. With the amount of people populating government hospital, it is not easy to get proper attention there. Which is why people who can afford it tend to turn to the private hospitals. They prefer paying from their own pockets than putting themselves through the rigors of a government hospital. According to Wikipedia, 58% of the hospitals in India are private along with a mind-boggling 81% doctors. The current infrastructure is just not good enough to handle the growing demands and the surging population. This is where data science can step in and ease the burden. Predicting things like how many days will a patient be admitted so as to calculate the proper allotment of beds, child mortality rate, heart issues, diabetes, etc. are some of the points you can work with for starters. The NITI Aayog initiative is already working on quite a lot of these points. Dataset on key indicators of annual health survey. These are survey results for nine Indian states from 2012-13. It is a very comprehensive dataset and contains 1,287 columns. If you are serious about analyzing and working with Indian healthcare data, this is as good a place as any to start. Multiple datasets on the government¡¯s data site. If you wish to analyze the state of healthcare at a more granular level, check out this link. It contains all sorts of information about the various aspects of healthcare, from OPD attendance to the comparison of various health indicators around the country. Datasets curated by the World health Organization. This is a treasure trove of data on healthcare in India, collected by WHO. It contains datasets on infant mortality rate, life expectancy at birth, hospital beds, etc. The state of education in India is appalling, to say the least. While more Indians are enrolled in schools than ever before, they are not really being educated. Outside the cream of the crop private schools, there is no proper structure, focus or attention given to the majority of children in rural areas. Almost 95% children have enrolled in primary school, 69% in secondary and a shockingly meager 25% in post secondary. Where is it all going wrong? Why can¡¯t one of the biggest school systems in the world improve upon this? What is the expected years of schooling education? Using data from national surveys, you can analyze and try to find answers to these pressing questions. As with any data science project, curiosity will help you a lot. This is a field that¡¯s very close to me so any progress, however minor, has the potential to start a ripple effect. Comprehensive district-level dataset. This is a really detailed dataset covering the length and breadth of report card information categorized by district. It contains 439 columns with zero milling values. What a great place to begin! Open datasets from the government. These are not so neat and tidy. You will require a bit of preprocessing and research to work with these properly, but they highlight the true nature of education here, including data on teaching staff and education loans. Ah, one of the most frustrating things we encounter on an almost daily basis. As more and more people flock to metro cities, the state of traffic on the roads is getting worse. Long traffic jams are an accepted part of our lives, but should they be? The NITI Aayog team is working on understanding why this happens, and how to deal with them. Aspects like choke points, narrow or broken roads, lack of traffic personnel, and failure of traffic lights, are just some of the features you can look at when trying to solve this problem. Cities like Kuala Lumpur and Toronto are already being converted into smart cities, with CCTV cameras and sensors everywhere to monitor traffic and imediaetely solve the problem. India is a fair way off that, though we saw earlier this year how the Kolkata police is trying to use Google Maps with the aim of dealing with long jams. Another aspect of road transport is the number of accidents on the road. India records some of the world¡¯s largest road fatalities every year. According to an Economic Times article, more than 150,000 people are killed in these accidents every year! This is a terribly distressing number and I hope data science can be used to analyze patterns and take immediate action on this. Datasets on road accidents. These are quite a few in number and cover features like accidents due to intake of alcohol/drugs, overspeeding, over crowding, over loading of trucks, etc. Accidents in India by month (2001-2014) dataset. You will need to carefully import this data but it¡¯s a good starting point for analyzing and extracting any patterns, if you can. Traffic Data. Unfortunately there¡¯s no single resource that contains the traffic data for India. Thankfully there are simple ways to get it. You can head over to Google Maps and export the data in a jiffy. Check out this article which explains how to do it. Once you download it, you can get to analyzing where the traffic jams regularly occur, at what time that happens, and can come up with ways to mitigate it. The possibilities are vast and making your city a smart one is now in your hands! Anyone with access to news will be aware how bad the air pollution levels are in certain parts on India. It is beyond the ¡°out of hand¡± stage. Despite taking precautions and trying out different measures, the pollution level has not really come down to a manageable state. According to a WHO report from 2016, 11 of the top 12 most polluted cities come from India (Kanpur leads the way). The Environment Performance Index ranked India 141 out of 180 nations. All this is to say that the problem is grave, and we need a permanent solution to this in double-quick time. Variables like crop burning, pollution from vehicles, industry fuel and biomass burning, etc. are major contributors to the alarming rise in air pollution. While there have been recent studies done using data science on the topic, none have so far been able to bring down the numbers. Air Quality Data for India. This contains historical data on India¡¯s air pollution levels and has spawned many a projects. It¡¯s a brilliant starting point for anyone looking to work with this kind of data. Daily Ambient Air Quality Data. Coming from the Government itself, this is a location wise dataset measuring the air quality in 2015. You can also check out their entire catalog on air pollution here if you so wish. It¡¯s a little unstructured so patience is key! Air Quality Info Site. A really cool website displaying the different statistics associated with air quality indices in India. It has forecasts for daily, monthly, and hourly numbers. Bookmark this site! The resources I have mentioned here are enough to get you started in each sector. There are other datasets and resources out there which you can get your hands on to practice more. There are government sites where you can request more data, if required. There is so much scope for improving each of these sectors with the help of data science. I¡¯m looking forward to our community making a huge impact (in a positive way of course!) soon! If there are any other datasets you are aware of and want to share with the community, please feel free to do so in the comments section below. helpful Hey guys. Tried to access data at the following link <U+2013> Dataset on individual household electric power consumption. (https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption). Received the following error <U+2013> You don¡¯t have permission to access /ml/datasets/individual+household+electric+power+consumption on this server. Just thought i¡¯d mention, for it could be Browser related or maybe a genuine error in itself. This is my way of saying, thanks for the great job you all do with the articles and knowledge sharing. It¡¯s very much appreciated. Hi Michael, Thanks for pointing this out. It seems the Machine Learning UCI repository site is down and hence you¡¯re unable to access the link. It should be back up and functioning soon."
"vidhya",2018-08-14,"DataHack Radio Episode #7: Tackling Data Science Challenges in India with NITI Aayog¡¯s Dr. Avik Sarkar (Independence Day Special!)","https://www.analyticsvidhya.com/blog/2018/08/datahack-radio-episode-7-dr-avik-sarkar/","

DATA DAYS DISCOUNT: Get FLAT 33% OFF on All Courses | Use COUPON CODE: DATA33 |
OFFER ENDS: 26th Aug 23:59 hrs IST | 
Valid for first 33 users(daily) | Buy Now 

 Data science is still a very nascent field in India, despite the recent surge in interest. From agriculture to healthcare, there are a plethora of challenges the Government faces on a day-to-day basis, and that was the primary reason for founding a data science department under the NITI Aayog initiative. On this Independence Day, we thought what better way to acquaint our community with these challenges and how our Goverment is using data science to tackle them, than bring NITI Aayog¡¯s Head of Data Science straight to you? It was a thrilling experience to have one of India¡¯s foremost data science leaders, Dr. Avik Sarkar, on our DataHack Radio podcast. He is an eloquent speaker and he talked about various topics, from his love of mathematics to his master¡¯s and Ph.D thesis techniques. He also provided details about the work performed by the data science team under the NITI Aayog initiative, a must-listen for all Indians. In this article, we look at the top key points Dr. Avik made during his conversation with Kunal. Happy listening! You can subscribe to DataHack Radio and listen to this, and all previous episodes, on any of the below platforms: Dr. Avik¡¯s penchant for numbers can be traced back to his childhood. He was interested in mathematics since his school days and that led him to do his bachelor¡¯s in statistics and master¡¯s (from IIT-Bombay) in applied statistics and informatics. He also holds a Ph.D in computer science and statistics. As you can surmise from this, he was the perfect candidate for data science! Before he joined NITI Aayog as the Head of the Data Science cell, Dr. Avik worked in senior roles at companies like Accenture, IBM and Nokia Siemens, among others. A trend emerges when you look at his profile <U+2013> he worked with data well before data science become a buzzword, and thus has a very strong background in this domain. When Dr. Avik was learning and working on Artificial Intelligence, it was a different experience to how we see AI these days. This is what he had to say about how quickly the world of data science, machine learning and AI is advancing: ¡°In this domain, learning new things is something you have to do every year. It¡¯s a rapidly evolving field <U+2013> new technologies, new platforms and new coding languages come every year, so getting acquainted with these is very important.¡± The subject of Dr. Avik¡¯s master¡¯s thesis was around multi-topic text classification. He took this up because it was an important topic due to the hierarchical information arrangement that was prevalent at that time (early 2000s). The main aim of the hierarchy was to arrange whatever text data you had into categories <U+2013> it could be news articles, blogs, etc. The internet was getting democratized as more and more Indians (and global users) started getting online in the late 90s/early 2000s. So, suddenly we went from seeing a few editors putting content online to a plethora of writers gaining access to the internet. The amount of content spiked, nothing close to what we see now, but enough to ensure that one could not manually categorize the articles into a hierarchy. Dr. Avik saw a need for an automatic classification system that would identify these topics and put them into a hierarchy model. The more challenging problem, which he took up, was that some articles might be relevant for multiple topics. His Ph.D was in text mining and statistical modeling on text distribution. If you are interested in NLP, do listen to this section where Dr. Avik explains why and how he took up this topic.<U+00A0>He discusses the nuances of various techniques he used and how they helped him build up his study. It makes for fascinating listening! ¡°We are trying to make sense of the operational data to get a good picture about the state of the economy.¡± The data science team at NITI Aayog, as Dr. Avik put it, is more of a horizontal organization. The type of analytics he and his team perform are vast in nature. Even though he had over fifteen years of experience working with data prior to joining the Government, this was an almost new body of work for him. There is a lot of simulation and scenario modeling that they need to perform. He gave some really intuitive examples of how the team thinks about certain industries (like oil and automobiles), and the variables to consider when forecasting production and manufacturing. This qualifies as long term forecasting. The team also uses analytics for short-term challenges as well, which are operational in nature. For example, malnutrition is a major problem in India (and has been for decades). They extract insights about which districts need more funds to deal with the issue and this has helped the people on the ground. There are other aspects where data science is helping the government tackle long standing challenges. Taking the example of surveys, Dr. Avik explained how there is a lag of 2-3 years between initiating surveys and finally extracting meaningful insights from them. His current team at NITI Aayog is trying to do more of a real-time analysis of these things, especially critical fields like healthcare, education and agriculture. ¡°80% of my day goes in phone calls!¡± Data collection, as Kunal pointed out, would be a major obstacle for Dr. Avik¡¯s team. All of the things they are doing are fairly new from an Indian perspective and nothing so far has been done in a systematic or structured manner. As the above quote summarized, he spends most of his day trying to convince people to share their data. Often there are data quality issues. Since most of the data is operational, people assume it might not be used anywhere and hence it¡¯s stored in a very unfocused manner. A lot of the fields need to be dropped because of the serious gaps in data quality. The hope is that with time, as Dr. Avik continues his work, departments will soon realize the need to properly store this data. A lack of data also inevitably leads to biases in the model you build. Unfortunately this is a problem India faces in almost all sectors. Mitigating these issues has become a big challenge as well and Dr. Avik pointed out this is the biggest obstacle he had to deal with. For energy modeling, a long term initiative (takes up to 1-2 years), ¡®Message Models¡¯ and ¡®Times Markel Model¡¯ are the team¡¯s tools of choice. For generating visualizations and dashboards to be shared with state governments, the team uses popular tools like: Different countries have their unique challenges when it comes to adopting AI. For India, Dr. Avik believes it¡¯s the obstacle of inclusion, or ¡°AI for all¡±. This is what his team is piloting throughout the country. Taking the example of healthcare, he explained how automating certain parts of a nurse and/or doctor¡¯s job will help cut down on the time it takes to make a diagnosis, as well as spread the benefits of healthcare to rural places. Intriguing is the only word I can think of to describe the task Dr. Avik and his team are dealing with."
"vidhya",2018-08-13,"Complete tutorial on Text Classification using Conditional Random Fields Model (in Python)","https://www.analyticsvidhya.com/blog/2018/08/nlp-guide-conditional-random-fields-text-classification/","

DATA DAYS DISCOUNT: Get FLAT 33% OFF on All Courses | Use COUPON CODE: DATA33 |
OFFER ENDS: 26th Aug 23:59 hrs IST | 
Valid for first 33 users(daily) | Buy Now 

 The amount of text data being generated in the world is staggering. Google processes more than 40,000 searches EVERY second!<U+00A0> According to a Forbes report, every single minute we send 16 million text messages and post 510,00 comments on Facebook. For a layman, it is difficult to even grasp the sheer magnitude of data out there? News sites and other online media alone generate tons of text content on an hourly basis. Analyzing patterns in that data can become daunting if you don¡¯t have the right tools. Here we will discuss one such approach, using entity recognition, called Conditional Random Fields (CRF). This article explains the concept and python implementation of conditional random fields on a self-annotated dataset. This is a really fun concept and I¡¯m sure you¡¯ll enjoy taking this ride with me! Entity recognition has seen a recent surge in adoption with the interest in Natural Language Processing (NLP). An entity can generally be defined as a part of text that is of interest to the data scientist or the business. Examples of frequently extracted entities are names of people, address, account numbers, locations etc. These are only simple examples and one could come up with one¡¯s own entity for the problem at hand. To take a simple application of entity recognition, if there¡¯s any text with ¡°London¡± in the dataset, the algorithm would automatically categorize or classify that as a location (you must be getting a general idea of where I¡¯m going with this). Let¡¯s take a simple case study to understand our topic in a better way. Suppose that you are part of an analytics team in an insurance company where each day, the claims team receives thousands of emails from customers regarding their claims. The claims operations team goes through each email and updates an online form with the details before acting on them. Source: mugo.ca You are asked to work with the IT team to automate the process of pre-populating the online form. For this task, the analytics team needs to build a custom entity recognition algorithm. To identify entities in text, one must be able to identify the pattern. For example, if we need to identify the claim number, we can look at the words around it such as ¡°my id is¡± or ¡°my number is¡±, etc. Let us examine a few approaches mentioned below for identifying the patterns. The bag of words (BoW) approach works well for multiple text classification problems. This approach assumes that presence or absence of word(s) matter more than the sequence of the words. However, there are problems such as entity recognition, part of speech identification where word sequences matter as much, if not more. Conditional Random Fields (CRF) comes to the rescue here as it uses word sequences as opposed to just words.  Let us now understand how CRF is formulated. Below is the formula for CRF where Y is the hidden state (for example, part of speech) and X is the observed variable (in our example this is the entity or other words around it).  Broadly speaking, there are 2 components to the CRF formula:  Now that you are aware of the CRF model, let us curate the training data. The first step to doing this is annotation.<U+00A0> Annotation is a process of tagging the word(s) with the corresponding tag. For simplicity, let us suppose that we only need 2 entities to populate the online form, namely the claimant name and the claim number. The following is a sample email received as is. Such emails need to be annotated so that the CRF model can be trained. The annotated text needs to be in an XML format. Although you may choose to annotate the documents in your way, I¡¯ll walk you through the use of the GATE architecture to do the same.  Email received: ¡°Hi,  I am writing this email to claim my insurance amount. My id is abc123 and I claimed it on 1st January 2018. I did not receive any acknowledgement. Please help.  Thanks, randomperson¡± Annotated Email: ¡°<document>Hi, I am writing this email to claim my insurance amount. My id is <claim_number>abc123</claim_number> and I claimed on 1st January 2018. I did not receive any acknowledgement. Please help. Thanks, <claimant>randomperson</claimant></document>¡± Let us understand how to use the General Architecture for Text Engineering (GATE). Please follow the below steps to install GATE. Once the installation is complete, you are ready to train and build your own CRF module. Let¡±s do this! Let¡¯s define and build a few functions. Now we will import the annotated training data. Generate features. These are the default features that NER algorithm uses in nltk. One can modify it for customization. Now we¡¯ll build features and create train and test data frames. Let¡¯s test our model. You can inspect any predicted value by selecting the corresponding row number ¡°i¡±. Check the performance of the model. Print out the classification report. Based on the model performance, build better features to improve the performance. By now, you would have understood how to annotate training data, how to use Python to train a CRF model, and finally how to identify entities from new text. Although this algorithm provides some basic set of features, you can come up with your own set of features to improve the accuracy of the model. To summarize, here are the key points that we have covered in this article:"
