"","site","date","headline","url_address","text","keyword"
"1","datacamp",2018-11-19,"Wes McKinney discusses data science tool building.","https://www.datacamp.com/community/blog/data-science-tool-building","Hugo Bowne-Anderson, the host of DataFramed, the DataCamp podcast, recently interviewed Wes McKinney, creator of the pandas project for data analysis tools in Python and author of Python for Data Analysis, among many other things. Here is the podcast link. Hugo:    Hi there, Wes, and welcome to DataFramed. Wes:    Thank you. Thanks for having me. Hugo:    Great pleasure to have you on the show, and I'm really excited to have you here today to talk about open source software development, to talk about your work at Ursa Labs, Apache Arrow, a bunch of other things to do with tool building, but first I'd like to find out a bit about you. Perhaps you could open by telling me what you're known for in the data community? Wes:    Sure, I'm best known for being the original author of the Python Pandas project, which I understand that a lot of people use. So, I started building that as a closed source library a little over ten years ago, and I've been working on a lot of different open source projects for the Python data science world and beyond. I also wrote a book called Python for Data Analysis, which is now in its second edition. I think that's become a pretty ubiquitous resource for people that are getting into the field of data science and are wanting to learn how to use Pandas and get their feet wet with working with data. Hugo:    And, congrats on the second edition. That was in the past year or so, that that was released, right? Wes:    Yeah, it was just about a year ago, the end of 2017. Hugo:    How did you get into data science tool building originally? Because, I'm aware that your background wasn't in CS, per se. Wes:    Right, I was a mathematician, so I studied pure math at MIT. I did a little bit of computer science. I had had some exposure to the world of machine learning in that I was aware that it existed, but MIT didn't have a statistics program, so data analysis and statistics wasn't very familiar to me when I was entering the working world. I got a job at an investment manager called AQR Capital Management, which is based in Greenwich, Connecticut. Wes:    There were a number of MIT grads that had gone to work there. Some of them were math majors, and they kind of sold me on the idea of getting experience with applied math, and then maybe I would go back to grad school later on. I found that in my job there, that rather than doing very much applied math, that I was really doing a lot of data munging. I was writing SQL. I was using Excel. Really, I just found that I wasn't as productive and efficient working with the data as I felt like I should have been. Wes:    Part of it was, well, I'm just starting out my career. I'm 22 years old. What do I know? But, I looked around, even at people who were a lot more senior to me and a lot more experienced. It seemed like they weren't very productive either, and they were spending a lot of time, obviously their skill with Excel and Excel shortcuts and so forth, keyboard shortcuts, was a lot better than mine, but still it seemed like there was just something missing to working with data. Wes:    I started to learn R at the end of 2007, beginning of 2008. There were, at that point in time, the R ecosystem was a lot less mature. It felt like an interesting, valuable language for doing statistics and data analysis, but we also needed to build software. So, I learned a little bit of Python and thought, wow, this is a really easy to use programming language. I had done some Java programming and thought that I just wasn't very good at Java. Wes:    So, I thought maybe I'm just not cut out for building software. But, I decided to have a tinker with building some data manipulation tools in Python. That was March, April, 2008, and just went down the rabbit hole from there. Once I had made myself more productive working with data, I started evangelizing the tools I was building to my colleagues. I kept pulling on one thread and ended up becoming more of a software engineer than a finance or math person. Hugo:    Yeah, there are a lot of interesting touch points there. For example, your background in pure math and that you're in Connecticut, I actually was working in pure math and ended up doing applied math in a biology lab in New Haven, Connecticut, not in Greenwich, but at that point, I actually started dealing with data a lot as well. That's when I started getting into data science also. It's also interesting that Pandas, when you first developed it, was closed source, but before we get there, you've spoken a bit to why you chose Python. Could you explain a bit more about what was attractive about Python then? Because, of course, a lot of the attractive things for researchers and data scientists now about Python is the data science stack, Pandas, scikit-learn, NumPy, all of these things. What made you really like it back in the day? Wes:    Yeah, at that point in time, 2007, 2008, in terms of doing statistical computing, Python was not ... Let's think of it as a promising world that has not yet been terraformed, think that there were kind of the nuts and bolts of a really interesting environment. I learned about the IPython project and said, ""Okay, here's a really nice interactive shell where you can plot things. It has tab completion and really basic interactive affordances that really help out a lot."" You had the nuts and bolts of doing all of the analytical computing that you need to do for data manipulation. Wes:    NumPy had its 1.0 release, I think, in 2006 and had become a mature project and the scientific Python world was de-fragmenting itself after the numarray/numeric rift, which had persisted for several years. Travis Oliphant had worked to bring those communities together. Really, I think what attracted me to the language was the accessibility and the fact that it was really very suited for interactive and exploratory computing, that you didn't have to set up an elaborate development environment, an IDE, to be able to get up and running doing some really basic things. Having had experience with Java, I think one of the things that put me off about Java was the elaborateness of the environment that you need to really be productive. Wes:    You really need to set up an IDE. There's all this tooling that you need to do, whereas with Python, you could do some pretty complex things with a few lines of code in a text file, then you run the script. So, that kind of interactive scripting feel of doing exploratory computing was really compelling to me at the time. But, obviously, Python was missing a lot of tools. So, it was a bit daunting to start the process of building some of those tools from scratch. Hugo:    Yeah, and you mention IPython, NumPy, and Travis, and I suppose this was the time where John Hunter was working a lot on matplotlib and working with Fernando to incorporate it with IPython. There was a lot of close collaboration. I suppose this speaks to the idea of community as well. Did you find the scientific Python community something that was also attractive? Wes:    Yeah, I didn't have much interaction with the community until much later. I think the first person ... There's two people that I met from the Python community who were my first point of contact with that world. One person is Eric Jones who is a founder of Enthought, which is the original Python scientific computing company based in Austin, Texas. Hugo:    And, they also run the SciPy conference. Wes:    Yeah, they run SciPy. Enthought was doing a lot of consulting work in New York City with financial firms that were getting big into Python during that era, training, and custom development. I got in touch with Eric some time during 2009 and gave him the very first external demo of Pandas. This was right around the time that we were getting ready to publish the Pandas bits on PyPI and so forth, the first open source version of the project. The second person I met was John Hunter himself from matplotlib. I met him in Chicago in January, 2010. At that point, I was looking around for how to engage with the Python world having just open sourced Pandas, and because John was working, he worked for Tradelink up until his death in 2012. Wes:    But he was a quant there, having been a neuroscientist and had been building matplotlib for many years. He took me under his wing. He was my mentor for a couple of years and helped me enter and get involved in the community. I definitely feel that I found it a very warm and very inviting community, very collaborative and collegial. I think I was attracted to that feeling. It didn't seem like a lot of people competing with each other. It was really just a lot of pragmatic software developers looking to build tools that were useful and to help each other succeed. Hugo:    Yeah, and you actually still get the sense of that when you go to SciPy in Austin, Texas every July or every second year. You still get a strong sense of community and people just loving building the tools together. Wes:    Yeah, totally, totally. Obviously, the community has grown much bigger. I think the ratio of project developers, people working on the open source projects, to the users, that ratio has certainly changed a lot in that there are a lot more users now than there are developers. I think the very first SciPy conference was probably the majority of people there were people who were the developers of open source projects. But still, I think it's a great community, and I think that's helping continue to bring people into the ecosystem. Hugo:    Actually, I had Brian Granger on the podcast recently, and we discussed this. Several people are discussing at the moment that we're now entering a phase transition from having individual users spread across orgs and spread across the globe of a lot of open source packages that actually having large scale institutional adoption, right? I'm wondering, in terms of Pandas starting off as a project, I'm under the impression it was started as a tool to be used in finance. Is that the case? Wes:    Yeah, it was focused. You can go back and download Pandas 0.1, which was published to PyPI in December, 2009 and see what was in the library. Compared with now, the functionality was a lot more geared towards time series data and the kinds of problems that we were dealing with back at AQR. I wouldn't say that it necessarily is finance specific. It was very general data manipulation. It was a pretty small project back then, but it was just about dealing with tabular data, dealing with messy data, data munging, data alignment, essentially all those kind of really basic wrangling and data integration problems. It wasn't really until 2011, 2012 that the project got built. I built the project out and created a more comprehensive set, relational algebra facilities. It didn't have complete joins, all the different kinds of basic joins until late 2011. So, its feature set was certainly skewed by use cases that we had in front of us back at AQR. Hugo:    How did you get the project off the ground? I know that's a relatively ill formed question, but just in terms of hours and people and resources? Wes:    Well, you smelt metal or you forged weapons. You have to get the crucible really, really hot. We open sourced the project at the end of 2009, and I think we had deliberated whether or not to open source at all for about six months or so, and ultimately, powers that be decided that we would open source Pandas and see what would happen. I gave my very first talk about Pandas, you can still find online. It's PyCon 2010 in Atlanta, and the subject of the talk was about using Python in quantitative finance, but the project didn't really go anywhere after that. So, it was hosted on Google Code. GitHub existed, but it was a Ruby thing at that time. I left AQR to go back to grad school. I went to Duke to start a PhD in statistics, statistical science, it's called there. I continued to do a little bit of contract work developing Pandas for AQR. Wes:    Somewhere, I think the catalyst for me was in early 2011 I started to get contacted by more companies that were exploring using Python for data analysis use cases. They had seen my talk at PyCon and were interested in getting my perspective on statistical computing. I just had this feeling that the ecosystem was facing a sort of existential crisis about whether or not it was going to become truly relevant for doing statistics. It was clear to me that Pandas was promising, but really had not reached a level of functional completeness or usefulness to be the foundation of a statistical computing ecosystem in Python. Wes:    So, I guess I felt that feeling so strongly that I sort of had an epiphany where it wasn't quite like shouting, ""Eureka,"" and jumping out of the bathtub, but I emailed my advisor and said, ""Hey, I would like to take a year off from my PhD and go explore this Python programming stuff, and we'll see how it goes."" I had some money saved from my first job, and I moved back to New York into a tiny apartment in the East Village which had mice and stuff. Really, not the best place I've ever lived, but I essentially was like, ""I'm just going to work full time on Pandas for a while and build it out and see what happens."" Wes:    I think that's when, as soon I started socializing the functionality of Pandas and filling in feature gaps, implementing joins and fixing some of the internal issues ... Of Course, I created other internal problems, but there were definitely some design problems in the early versions of Pandas that got fixed in the summer of 2011. But, as soon as Pandas could read CSV files pretty reliably and could do joins and a lot of the basic stuff that you need to be productive working with multiple data sets, I think that's when it started to catch people's eye toward the end of 2011 and start to take off the ground. Wes:    So, around the same time, I pitched the idea of a data analysis book in Python to O'Reilly and they agreed to do a book, which thinking back on it was a bit risky, because who knows what would have become of... Pandas was not at all obviously going to be successful back in 2011. They decided to take a bet and so much so that I asked them later why they didn't put a panda on the cover, but they said, ""Well, we're saving the panda for something really big."" So, it wasn't even clear then that Python and Pandas and everything was going to be a popular thing. So, it's important to have that kind of perspective. Hugo:    So, when leaving in the East Village, supporting yourself to build out the package, did you have any inkling that it would achieve the growth and wide scale adoption that it has? Wes:    Not really. Obviously, I had the belief that the Python ecosystem had a lot of potential and that projects like Pandas were necessary to help the language and the community realize the potential. I think there was a lot of computational firepower in the NumPy world and all the tooling, Cython, and tools for interoperability with the native code. I just wanted to help realize that potential, but I didn't really have a sense of where it would go. Wes:    There were some other significant kind of confluence of things that happened, particularly when you consider the development of statsmodels and Scikit-learn, which brought meaningful analytical functionality to Python. I think if Pandas, really, the big thing that made Pandas successful was the fact that it could read CSV files reliably. So, it became a first port of entry for data into Python and for data cleaning and data preparation. If you wanted to do machine learning in Scikit-learn or you wanted to use stats models for statistics and econometrics, you needed to clean data first. Wes:    So, using Pandas was the obvious choice for that. But, yeah, it wasn't obvious. I recruited a couple of my former colleagues from AQR who worked with me on Pandas, and we explored starting a company around financial analytics in Python powered by Pandas, but we were focused on building out Pandas as an open source project while we explored that startup idea. Ultimately, we didn't pursue that startup, but it was clear that by mid-2012, that we'd sort of crossed the critical horizon of people being interested in Python as a language for data analysis. Hugo:    Since then, you've found certain institutions which have employed you in order to work on Pandas, right? Wes:    I wouldn't say that. Outside of my time at AQR, when I was building Pandas initially, I've never been employed directly to work on Pandas. I started a company called DataPad with Chang She. It was a venture backed company, and we were building a visual analytics that was powered by Pandas and other Python ... DataPad was acquired by Cloudera at the end of 2014, so Chang and I landed there to work on ... My role at Cloudera was to look holistically at the big data world and figure out how to forge a better path for Python and data science tools in general in the context of the big data world. Wes:    That's the Hadoop ecosystem and Spark and all the technology which was largely Java based, which had been developed since 2006 or 2008. But, I wasn't working on Pandas in particular at that point. I sort of had taken stock of the structural and infrastructural problems that Pandas had. I gave a talk at the end of 2013 at PyData in New York. The title of the talk was ""Practical Medium Data Analytics in Python."" The subtitle of the talk was ""Ten Things I Hate About Pandas."" Hugo:    I remember that. Wes:    I had, in the background, this feeling that Pandas was built on a fantastic platform for scientific computing and numerical computing, so if you're doing particle physics or HPC work in a national lab with a supercomputer, Python is really great, and that's how the ecosystem develop in the late 90s, early 2000s, but for statistical computing and big data and analytics, the fact that strings and categorical data wasn't a first class citizen in that world made things a lot harder. Missing data was not a first class citizen. So, there were a lot of problems that had accumulated. At that point, I started to look beyond Pandas as it was implemented then into how we could build technology to advance the whole ecosystem and beyond the Python world as well. Hugo:    I think a through line in this is really encapsulated by a statement you made earlier, which is you wanted to build technologies and tools that are truly relevant for doing statistics or working with data. I know as a tool builder, you're committed to developing human interfaces to data to make individuals more productive. I think that actually provides a really nice segue into a lot of what you're thinking about now, in particular, the Apache Arrow project. I'm wondering if you can tell me about Apache Arrow and how you feel it can facilitate data science work. Wes:    Yeah, I got involved in what became the Apache Arrow project as part of my work at Cloudera. One problem that had plagued me as a Python programmer was the fact that when you arrive at foreign data and foreign systems that you want to plug into, whether those are other kinds of ways of storing data or accessing data or accessing computational systems, that we were in a position of having to build custom data connectors for Python for Pandas or whatever Python library you're using. Wes:    I felt that we were losing a lot of energy to building custom connectors into all of these different things. This problem isn't unique to Python. So, if you look at all of the number of different pair-wise adapters that are available to convert between one data format and another, or serialized data from one programming language to another programming language, sharing data was something that had caused me a lot of pain. Also, sharing code and algorithms was a big problem. Wes:    So, the way that Pandas is implemented internally, it has its own custom way of representing data that's layered on top of NumPy arrays, but we had to essentially re-implement all of our own algorithms and data access layers from scratch. We had implemented our own CSV reader, our own interfaces to HDF5 files, our own interfaces to JSON data. We have pretty large libraries of code in Pandas for doing in memory analytics, aggregating arrays, performing groupby operations. If you look across other parts of the big data world, you see the same kinds of things implemented in many different ways in many different programming languages. In R you have the same thing, many of the same things implemented in R. So, I was kind of trying to make sense of all of that energy loss to sharing data and sharing code and thinking about how I could help enable the data world to become a lot less fragmented. People building systems, people like me who build tools for people, how to make people like me who are building tools a lot more productive and able to build better and more efficient data processing tools in the future. Wes:    This was just kind of feelings that I had, so I started to poke around Cloudera and see if other people felt the same way. So, I was working with folks on the Impala team, people like Marcel Kornacker who started the Impala project, Todd Lipcon who started the Apache Kudu project. It's now Apache Impala, joined the Apache Foundation. So, there were a lot of people at Cloudera that essentially agreed with me, and we thought about what kind of technology we could build to help improve interoperability. Wes:    We sort of centered on the problem of representing DataFrames and tabular data. As we looked outside of Cloudera, we saw that there were other groups of developers who concurrently were thinking about the exact same problem, so we bumped into folks from the Apache Drill project, which is a SQL on Hadoop system. They were also thinking about the tabular data interoperability problem. How can we move around tabular data sets and reuse algorithms and code and data without so much conversion and energy loss? Wes:    Very quickly, we got 20, 25 people in the room representing 12 or 13 open source projects with a general consensus that we should build some technology to proverbially tie the room together. That became Apache Arrow, but it took all of 2015 to put the project together. Now, how is all this relevant to data science? Well, what the Arrow project provides is a way of representing data and memory that is language agnostic and standardized and portable. You can think of it as being a language independent DataFrame. Wes:    If you create Arrow based DataFrames in Python, you can share them with any system, whether that's written in C or C++ or JavaScript or Java or Rust or Go. As long as they implement the Arrow columnar format, they can interact with that data without having to convert it or serialize to some kind of intermediate representation like you usually have. The goal of the project, in addition to providing high quality libraries for building data sciences tools and building databases, is also to improve the portability of code and data between languages. Wes:    Outside of the interoperability side of the project, there's also the goal within the walls of the particular data processing system to provide a platform of algorithms and tools for memory management and data access that can accelerate large scale data processing. We wanted the Arrow columnar format to support working with much larger quantities of data, the single node scale, data that is particularly data that does not fit into memory. Hugo:    I love this idea of tying the room together, as you put it, because, essentially, it speaks to the idea of breaking down the walls between all these silos that exist as well, right? Wes:    Yeah, yeah. I know, I think if you look across, just within the data science world, even though functionally we're solving many of the same problems, there's very little collaboration that happens between the communities, whether collaborating at the software design level or at the code level. As a result, people point fingers and accuse people of reinventing wheels or not wanting to collaborate, but really it's if your data is different in memory, there's just no basis for code sharing in most cases. So, the desire to create an open standard for DataFrames is just ... If you want to share code, it's essential. You have to standardize the representation in RAM or on the GPU or essentially at the byte or the bit level agreeing on what the data looks like once you load it off disk or once you parse it out of a CSV file, is the basis of collaboration amongst multiple programming languages or amongst different data science languages that are ultimately based in C or C++. Hugo:    I remember actually Fernando Perez spoke to this as well in his keynote where you also keynoted, the inaugural JupyterCon, saying, ""We welcome so many contributions, but we need to agree on some things, right? These are some things that we've all agreed upon. So, if you're going to contribute, let's build on these particular things."" Wes:    Right, yeah. I think the Jupyter project certainly socialized this idea of open standards by developing the kernel protocol providing a way ... Here's the abstract notion of a computational notebook. Here's how if you want to build a kernel, add a new language to the Jupyter ecosystem, here's how you do it. That certainly has played out beautifully with ... I think it's over 40 languages have kernel implementation for Jupyter. But, I think in general, I think people are appreciating more the value of having open standards that are community developed and that are developed on the basis of consensus where there's just broad buy in. It's not one developer or one isolated group of people building some technology and then trying to get people to adopt it. I think Jupyter is unique in the sense that it started out in the Python world, but I think it's there: they set out with the goal of embracing a much broader community of users and developers. That's played out in really exciting ways. Hugo:    I really like the descriptions you gave and the inspiration behind the Arrow project, in particular the need for interoperability, the importance of these portable DataFrames. I don't want to go too far down the rabbit hole. I can't really help myself, though. I'd like you to speak just a bit more to your thoughts behind the challenge of working in the big data limit. For example, that we have computers and hard drives that can store a lot of stuff, but we don't actually have languages that can interact with ... unless we parallelize it, right? Wes:    Right. A common thing that I've heard over the years from ... People will say, ""Wes, I just want to write Pandas code, but I want it to work with big data."" It's a complicated thing, because the way that a lot of these libraries are designed, the way that Pandas is designed, and a lot of libraries that are similar to Pandas, it's the implementation and the computational model, when computation happens, what are the semantics of the code that you're writing? Wes:    There's a lot of built in assumptions around the idea that data fits in memory and that when you write A plus B that A plus B is evaluated immediately and materialized in memory. So, if you want to scale out, scale up computing to DataFrame libraries, you essentially have to re-architect around the idea of deferred evaluation and essentially defining a rich enough algebra or intermediate representation of analytical computation where you can actually use a proper query engine or a query planner to execute operations. Wes:    So, really, what is needed is to make libraries like Pandas internally more like analytic databases. If you look at all the innovation that has happened in the analytic database world over the last 20 years, of columnar databases and things that have happened in the big data world, very little of that innovation in scalable data processing has made its way into the hands of data scientists. Wes:    So, really, one of my major goals with my involvement in the Arrow project is to provide the basis for collaboration between the database and analytic database world and the data science world, which is just not something that's happened before. Ultimately, the goal is to create an embedded analytic database that is language independent and can be used in Python, can be used in R, that can work with much larger quantities of data. Wes:    But, it's going to take a different approach in terms of the user API because I think this idea of magically retrofitting Pandas or essentially retrofitting Pandas with the ability to work with hundreds of gigabytes of data or terabytes of data ... I hate to say, it's a little bit of a pipe dream. I think it's going to require some breaking changes and some kind of different approaches to the problem. That's not to say that Pandas is going away. Pandas is not going anywhere, and I think certainly is occupying the sweet spot of being the ultimate Swiss army knife for data sets under a few gigabytes. Hugo:    So, does this conversation relate to the murmurings we've heard of a potential Pandas 2 in the pipeline? Wes:    Yeah, at the end of 2015, I started a discussion in the Pandas community. Just FYI, I think people are often thanking ... I see people out in the community. They're like, ""Wes, thanks so much for Pandas."" I have to remind them to go out of your way and thank Jeff Reback, and Joris Van den Bosche, and Phillip Cloud, and Tom Augspurger, and the other Pandas core developers that have really been driving the project forward over the last five years. I haven't been very involved in the day to day development since some time in 2013, but at the end of 2015, I started spending some more time with the Pandas developers that had been building this project for ... It's a little over seven years old, the code base. Are there things that we would like to fix? What are we going to do about the performance and memory use and scalability issues? Wes:    I don't think at that point, I don't know that Dask DataFrame existed. So, Dask has provided an alternative route to scaling Pandas by using Pandas as is, but essentially re-implementing Pandas operations using a Dask computation graph. But, looking at the single node scale, the in memory side of Pandas, we looked at what we'd like to fix about the Pandas internals. That was what we described as the Pandas two initiative. Around that time, we were just getting ready to kick off the Apache Arrow project. Wes:    So, I wouldn't say that we reached a fully baked game plan in terms of how to create a quote/unquote Pandas 2, but I think we reached some consensus that we would like to build an evolved DataFrame library that is a lot simpler in its functionality, so shedding some of the baggage of multi-indexes and some of the things in Pandas that can be a bit complex and also don't lend themselves very well to out of core, very large, don't fit into memory, data sets. But, to something that's focused on dealing with the very large data sets at the single node scale. So, large out of core just big data sets on a laptop. We are working on that, and I think the project itself is not going to be called Pandas 2, just to not confuse people. Wes:    I think the Pandas project, we all got together, the Pandas team, we all got together in Austin over the summer. This was one of the topics that we're going to continue to grow and innovate and evolve the current Pandas project as it is right now, but my goal is to grow a parallel kind of companion project which is powered by the Apache Arrow ecosystem and provides the Pandas-like user experience in terms of usability and functionality, but is really focused on powering through very large on disk data sets. Hugo:    I'd like to step back a bit and think about open source software development in general. I suppose, spoiler alert, where I want this to go is to talk about one of your latest ventures, Ursa Labs. But, I'm wondering in your mind what the biggest challenges for open source software development are at this point in time? Wes:    Well, we could have a whole podcast just about this topic. Hugo:    And, of course, it depends on the stage of a project. Wes:    The way that I frame the problem when I talk to people is that I think open source projects face funding and sustainability problems of different kinds depending on the stage of the project. So, I think in the early stages of projects, when you're building something new or you're essentially solving a known problem in a different way, it can be hard to get support from other developers or financial support to sponsor individuals to work on the project because it's hard to build consensus around something new. Wes:    There might be even competing approaches to the same problem. So, if we're talking about the kind of funding that can support full time software developers, it can be a lot of money. So, committing a lot of money to support a risky venture into building a new open source project which may or may not become successful can be a tough pill to swallow for a potential financial backer. Wes:    Later on, as projects become wider adopted, they start becoming, particularly projects that are foundational, and you can call them ... I think the popular term is open source infrastructure. There was a report, Nadia Eghbal wrote the report called Roads and Bridges about open source infrastructure with the Ford Foundation, and sort of is about this idea of thinking about open source software as a public good, roads and bridges and public infrastructure that everyone uses. With public infrastructure, it's great, because it's supported by tax dollars, but we don't exactly have a open source tax. Wes:    I could get behind one, but we don't have that same kind of mentality around funding critical open source infrastructure. I think that as projects become really successful and they become something that people can't live without, they end up facing the classic tragedy of the commons problem where people feel like, well, they derive a lot of value from the project, but because everyone uses this as a project, they don't want to foot the bill of supporting and maintaining the software project. Wes:    So, whether you're on the early side of a project or in the early stage or a late stage, I think there's different kinds of funding and sustainability challenges. In all cases, I think open source developers end up particularly as projects become more successful, you end up quite over burdened and a burnout risk and I know I've experienced a burnout many times and many other open source developers have experienced periods of significant burnout. Hugo:    So, what can listeners who are working or aspiring data scientists or data analysts in organizations or C-level people within organizations do for the open source ... What would you like to see them do more for the open source? Wes:    I think users and other folks can help with ... People like me, I guess I've recently been working on putting myself in a situation where I am able to raise money and put to work money that is donated for direct open source development. The best way a lot of people can help is by selling the idea of supporting and either through development work or through direct funding, supporting the open source projects that you rely on. So, I think a lot of companies and a lot of developers are passive participants in open source projects. Wes:    So finding a way to contribute, whether it's through money or time, it is difficult because many open source projects, particularly ones that are systems related to infrastructure, they don't necessarily lend themselves to casual, quote/unquote, ¡°casual contributions¡±. So, if it's your five percent project or your 20% project, it can be hard as an individual to make a meaningful contribution to a project which may have a steep learning curve or just require a lot of intense focus. So, I think for a lot of organizations, the best way to help projects can be to donate money directly. Hugo:    So, I think this provides a nice segue into your work at Ursa Labs. I'd love for you to just give us a rundown of Ursa Labs, in particular, how it frames the challenges of open source software development. Wes:    Ursa Labs is an organization: I partnered with Hadley Wickham from the R community and RStudio to found Ursa Labs earlier this year. The raison d'etre of Ursa Labs was to build shared infrastructure for data science, in particular, building out the Arrow ecosystem, the Apache Arrow ecosystem, as it relates to data science and making sure that we have high quality consistent support for all of that new technology in the Python and R world and beyond, and improving interoperability for data scientists that use all of those programming languages, but the particular logistical details of Ursa Labs is that we wanted to be able to effectively put together an industry consortium type model where we can raise money from corporations and use that money to hire full time open source developers. Wes:    So, at the moment, Ursa Labs is being supported by RStudio, by Two Sigma where I used to work right up until the founding of Ursa Labs, and it's now being funded by Nvidia, the makers of graphics cards. So, we're working actively on bringing in more sponsors to build a larger team of developers. I think it's really confronting that challenge that I think for an engineer at a company as a part time contributor to an open source project may not be as effective or nearly as effective as a full time developer. Wes:    So, I want to make sure I'm able to build an organization that is full of outstanding engineers who are working full time on open source software and making sure that we are able to do that in a scalable and sustainable way, and is organized for the benefit of the open source data science world. Anyway, having been through the consulting path and the startup path and working for single companies, I think a consortium type model where it's being funded by multiple organizations and where we're not building a product of some kind, it's kind of a new model for doing open source development, but one that I'm excited to pursue and see how things go. Hugo:    Yeah, I think it's really exciting as well, because it does approach a lot of the different challenges. One in particular, it's a trope, it's a common problem, right, of developers being employed by organizations and being given a certain amount of time to wok on open source software development, but that time being eaten away because of different incentives within organization, essentially. Wes:    Yeah. I think there have been a ton of contributions to Pandas and Apache Arrow from developers that work at corporations, and those contributions mean a lot, so definitely still looking for companies to collaborate on the roadmap and to work together to build new computational infrastructure for data science. I think it's tough when a developer might show up and be spending a lot of time for a month or two, then based on their priorities within where the company where they work they might disappear for six months. That's just the nature of things. I think the kinds of developers that make big contributions to open source can often be more senior or tend to be very important developers in their respective organizations, so frequently get called in to prioritize closed source or internal projects. That's just kind of the ebb and flow of corporate environment. Hugo:    So, I've got a relatively general question for you: what does the future of data science tooling look like to you? Wes:    Well, speculative, of course, but I think by spending my time on the Arrow project, my objective and what I would like to see happen in data science tooling is a de-fragmenting of data and code. So, to have increased standardization and adoption of open standards like the Arrow columnar format, storage formats like Parquet and Orc, protocols for messaging like GRPC. I think that in the future, I believe that things will be a lot more standardized and a lot less fragmented. Wes:    Kind of a slightly crazy idea, I don't know how crazy it is, but I think also in the future that programming languages are going to diminish in importance relative to data itself and common computational libraries. This is kind of a self serving opinion, but I do think that to be able to leave data in place and to be able to choose the user interface, namely the programming languages, the programming language that best suits your needs in terms of interactivity or software development or so forth, that you can use multiple programming languages to build an application or pick the programming language that you prefer while utilizing common libraries of algorithms, common query engines for processing that data. So, I think we're beginning to see murmurings of this de-fragmentation happening, and I think the Arrow project is ... kicking along this process and socialize the idea of what a more de-fragmented and more consistent user experience for data scientists, what that might look like. Hugo:    That's a very exciting future. My last question for you, Wes, is do you have a final call to action for our listeners out there? Wes:    Yeah, I would say my call to action would be to find some meaningful way to contribute to the open source world, whether it's sharing your ideas or sharing your use cases about what parts of the open source stack are working well for you or what parts you think could serve you better. If you are able to contribute to projects, whether through discussions on mailing lists or GitHub or commenting on the roadmap or so forth, that's all very valuable. Wes:    I think a lot of people think that code is the only real way to contribute to open source projects, but actually I spent a lot of my time as not writing code. It's reviewing code and steering discussions about design and roadmap and feature scope. I think the more voices and the more people involved to help build consensus and help prioritize the work that's happening in open source projects helps make healthier and more productive communities. If you do work in an organization that has the ability to donate money to open source projects, I would love to see worldwide corporations effectively tithing a portion of profits to fund open source infrastructure. Wes:    I think if corporations gave a fraction of one percent of their profits to open source projects, the funding and sustainability crisis that we have now would essentially go away. Obviously, I guess that might be a lot to ask, but I can always hope, so I think corporations can lead by example. Certainly, if you do donate money to open source projects, you should make a show of that and make sure that other corporations know that you're a good citizen and you're helping support the work of open source developers. Hugo:    I couldn't agree more. Wes, it's been an absolute pleasure having you on the show. Wes:    Thanks, Hugo. It's been fun.","Keyword(freq): pandas(60), project(22), developer(20), tool(15), lab(10), language(10), library(8), problem(8), set(7), statistics(7)"
"2","mastery",2018-11-23,"A Gentle Introduction to Weight Constraints to Reduce Generalization Error in Deep Learning","https://machinelearningmastery.com/introduction-to-weight-constraints-to-reduce-generalization-error-in-deep-learning/","Weight regularization methods like weight decay introduce a penalty to the loss function when training a neural network to encourage the network to use small weights. Smaller weights in a neural network can result in a model that is more stable and less likely to overfit the training dataset, in turn having better performance when making a prediction on new data. Unlike weight regularization, a weight constraint is a trigger that checks the size or magnitude of the weights and scales them so that they are all below a pre-defined threshold. The constraint forces weights to be small and can be used instead of weight decay and in conjunction with more aggressive network configurations, such as very large learning rates. In this post, you will discover the use of weight constraint regularization as an alternative to weight penalties to reduce overfitting in deep neural networks. After reading this post, you will know: Let¡¯s get started. A Gentle Introduction to Weight Constraints to Reduce Generalization Error in Deep LearningPhoto by Dawn Ellner, some rights reserved. Large weights in a neural network are a sign of overfitting. A network with large weights has very likely learned the statistical noise in the training data. This results in a model that is unstable, and very sensitive to changes to the input variables. In turn, the overfit network has poor performance when making predictions on new unseen data. A popular and effective technique to address the problem is to update the loss function that is optimized during training to take the size of the weights into account. This is called a penalty, as the larger the weights of the network become, the more the network is penalized, resulting in larger loss and, in turn, larger updates. The effect is that the penalty encourages weights to be small, or no larger than is required during the training process, in turn reducing overfitting. A problem in using a penalty is that although it does encourage the network toward smaller weights, it does not force smaller weights. A neural network trained with weight regularization penalty may still allow large weights, in some cases very large weights. An alternate solution to using a penalty for the size of network weights is to use a weight constraint. A weight constraint is an update to the network that checks the size of the weights, and if the size exceeds a predefined limit, the weights are rescaled so that their size is below the limit or between a range. You can think of a weight constraint as an if-then rule checking the size of the weights while the network is being trained and only coming into effect and making weights small when required. Note, for efficiency, it does not have to be implemented as an if-then rule and often is not. Unlike adding a penalty to the loss function, a weight constraint ensures the weights of the network are small, instead of mearly encouraging them to be small. It can be useful on those problems or with networks that resist other regularization methods, such as weight penalties. Weight constraints prove especially useful when you have configured your network to use alternative regularization methods to weight regularization and yet still desire the network to have small weights in order to reduce overfitting. One often-cited example is the use of a weight constraint regularization with dropout regularization. Although dropout alone gives significant improvements, using dropout along with [weight constraint] regularization, [¡¦] provides a significant boost over just using dropout. <U+2014> Dropout: A Simple Way to Prevent Neural Networks from Overfitting, 2014. A constraint is enforced on each node within a layer. All nodes within the layer use the same constraint, and often multiple hidden layers within the same network will use the same constraint. Recall that when we talk about the vector norm in general, that this is the magnitude of the vector of weights in a node, and by default is calculated as the L2 norm, e.g. the square root of the sum of the squared values in the vector. Some examples of constraints that could be used include: The maximum norm, also called max-norm or maxnorm, is a popular constraint because it is less aggressive than other norms such as the unit norm, simply setting an upper bound. Max-norm regularization has been previously used [¡¦] It typically improves the performance of stochastic gradient descent training of deep neural nets ¡¦ <U+2014> Dropout: A Simple Way to Prevent Neural Networks from Overfitting, 2014. When using a limit or a range, a hyperparameter must be specified. Given that weights are small, the hyperparameter too is often a small integer value, such as a value between 1 and 4. ¡¦ we can use max-norm regularization. This constrains the norm of the vector of incoming weights at each hidden unit to be bound by a constant c. Typical values of c range from 3 to 4. <U+2014> Dropout: A Simple Way to Prevent Neural Networks from Overfitting, 2014. If the norm exceeds the specified range or limit, the weights are rescaled or normalized such that their magnitude is below the specified parameter or within the specified range. If a weight-update violates this constraint, we renormalize the weights of the hidden unit by division. Using a constraint rather than a penalty prevents weights from growing very large no matter how large the proposed weight-update is. <U+2014> Improving neural networks by preventing co-adaptation of feature detectors, 2012. The constraint can be applied after each update to the weights, e.g. at the end of each mini-batch. This section provides a few cherry-picked examples from recent research papers where a weight constraint was used. Geoffrey Hinton, et al. in their 2012 paper titled ¡°Improving neural networks by preventing co-adaptation of feature detectors¡± used a maxnorm constraint on CNN models applied to the MNIST handwritten digit classification task and ImageNet photo classification task. All layers had L2 weight constraints on the incoming weights of each hidden unit. Nitish Srivastava, et al. in their 2014 paper titled ¡°Dropout: A Simple Way to Prevent Neural Networks from Overfitting¡± used a maxnorm constraint with an MLP on the MNIST handwritten digit classification task and with CNNs on the streetview house numbers dataset with the parameter configured via a holdout validation set. Max-norm regularization was used for weights in both convolutional and fully connected layers. Jan Chorowski, et al. in their 2015 paper titled ¡°Attention-Based Models for Speech Recognition¡± use LSTM and attention models for speech recognition with a max norm constraint set to 1. We first trained our models with a column norm constraint with the maximum norm 1 ¡¦ This section provides some tips for using weight constraints with your neural network. Weight constraints are a generic approach. They can be used with most, perhaps all, types of neural network models, not least the most common network types of Multilayer Perceptrons, Convolutional Neural Networks, and Long Short-Term Memory Recurrent Neural Networks. In the case of LSTMs, it may be desirable to use different constraints or constraint configurations for the input and recurrent connections. It is a good general practice to rescale input variables to have the same scale. When input variables have different scales, the scale of the weights of the network will, in turn, vary accordingly. This introduces a problem when using weight constraints because large weights will cause the constraint to trigger more frequently. This problem can be done by either normalization or standardization of input variables. The use of a weight constraint allows you to be more aggressive during the training of the network. Specifically, a larger learning rate can be used, allowing the network to, in turn, make larger updates to the weights each update. This is cited as an important benefit to using weight constraints. Such as the use of a constraint in conjunction with dropout: Using a constraint rather than a penalty prevents weights from growing very large no matter how large the proposed weight-update is. This makes it possible to start with a very large learning rate which decays during learning, thus allowing a far more thorough search of the weight-space than methods that start with small weights and use a small learning rate. <U+2014> Improving neural networks by preventing co-adaptation of feature detectors, 2012. Explore the use of other weight constraints, such as a minimum and maximum range, non-negative weights, and more. You may also choose to use constraints on some weights and not others, such as not using constraints on bias weights in an MLP or not using constraints on recurrent connections in an LSTM. This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered the use of weight constraint regularization as an alternative to weight penalties to reduce overfitting in deep neural networks. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. Hola Jason, Interesting post!  the ideas and presentations combine perfectly with weigh regularization, dropouts , normalizing, ¡¦well introduced in other post¡¦ Now I imagine you are thinking about, we would need to go trough specific implementations of those ideas via particular codes addressing some ML/DL problems, in order to explicitly known how Keras or scikit_Learn or other APIs libraries of functions operate specifically with those weighs constraint implementation¡¦ Thank you very much for your free work and big effort on behalf of this big and universal community to approach these techniques and ideas for developers and enthusiats of ML/DL techniques ! we  owe a lot to you ! Thanks.  Yes, I¡¯m working on a whole series of neural net regularization posts that will culminate in a new book. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): weight(37), constraint(13), network(12), model(5), method(4), variable(4), detector(3), idea(3), layer(3), penalty(3)"
"3","mastery",2018-11-21,"How to Reduce Overfitting of a Deep Learning Model with Weight Regularization","https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/","Weight regularization provides an approach to reduce the overfitting of a deep learning neural network model on the training data and improve the performance of the model on new data, such as the holdout test set. There are multiple types of weight regularization, such as L1 and L2 vector norms, and each requires a hyperparameter that must be configured. In this tutorial, you will discover how to apply weight regularization to improve the performance of an overfit deep learning neural network in Python with Keras. After completing this tutorial, you will know: Let¡¯s get started. How to Reduce Overfitting in Deep Learning With Weight RegularizationPhoto by Seabamirum, some rights reserved. This tutorial is divided into three parts; they are: Keras provides a weight regularization API that allows you to add a penalty for weight size to the loss function. Three different regularizer instances are provided; they are: The regularizers are provided under keras.regularizers and have the names l1, l2 and l1_l2. Each takes the regularizer hyperparameter as an argument. For example: By default, no regularizer is used in any layers. A weight regularizer can be added to each layer when the layer is defined in a Keras model. This is achieved by setting the kernel_regularizer argument on each layer. A separate regularizer can also be used for the bias via the bias_regularizer argument, although this is less often used. Let¡¯s look at some examples. The example below sets an l2 regularizer on a Dense fully connected layer: Like the Dense layer, the Convolutional layers (e.g. Conv1D and Conv2D) also use the kernel_regularizer and bias_regularizer arguments to define a regularizer. The example below sets an l2 regularizer on a Conv2D convolutional layer: Recurrent layers like the LSTM offer more flexibility in regularizing the weights. The input, recurrent, and bias weights can all be regularized separately via the kernel_regularizer, recurrent_regularizer, and bias_regularizer arguments. The example below sets an l2 regularizer on an LSTM recurrent layer: It can be helpful to look at some examples of weight regularization configurations reported in the literature. It is important to select and tune a regularization technique specific to your network and dataset, although real examples can also give an idea of common configurations that may be a useful starting point. Recall that 0.1 can be written in scientific notation as 1e-1 or 1E-1 or as an exponential 10^-1, 0.01 as 1e-2 or 10^-2 and so on. Weight regularization was borrowed from penalized regression models in statistics. The most common type of regularization is L2, also called simply ¡°weight decay,¡± with values often on a logarithmic scale between 0 and 0.1, such as 0.1, 0.001, 0.0001, etc. Reasonable values of lambda [regularization hyperparameter] range between 0 and 0.1. <U+2014> Page 144, Applied Predictive Modeling, 2013. The classic text on Multilayer Perceptrons ¡°Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks¡± provides a worked example demonstrating the impact of weight decay by first training a model without any regularization, then steadily increasing the penalty. They demonstrate graphically that weight decay has the effect of improving the resulting decision function. ¡¦ net was trained [¡¦] with weight decay increasing from 0 to 1E-5 at 1200 epochs, to 1E-4 at 2500 epochs, and to 1E-3 at 400 epochs. [¡¦] The surface is smoother and transitions are more gradual <U+2014> Page 270, Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999. This is an interesting procedure that may be worth investigating. The authors also comment on the difficulty of predicting the effect of weight decay on a problem. ¡¦ it is difficult to predict ahead of time what value is needed to achieve desired results. The value of 0.001 was chosen arbitrarily because it is a typically cited round number <U+2014> Page 270, Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999. Weight regularization does not seem widely used in CNN models, or if it is used, its use is not widely reported. L2 weight regularization with very small regularization hyperparameters such as (e.g. 0.0005 or 5 x 10^<U+2212>4) may be a good starting point. Alex Krizhevsky, et al. from the University of Toronto in their 2012 paper titled ¡°ImageNet Classification with Deep Convolutional Neural Networks¡± developed a deep CNN model for the ImageNet dataset, achieving then state-of-the-art results reported: ¡¦and weight decay of 0.0005. We found that this small amount of weight decay was important for the model to learn. In other words, weight decay here is not merely a regularizer: it reduces the model¡¯s training error. Karen Simonyan and Andrew Zisserman from Oxford in their 2015 paper titled ¡°Very Deep Convolutional Networks for Large-Scale Image Recognition¡± develop a CNN for the ImageNet dataset and report: The training was regularised by weight decay (the L2 penalty multiplier set to 5 x 10^<U+2212>4) Francois Chollet from Google (and author of Keras) in his 2016 paper titled ¡°Xception: Deep Learning with Depthwise Separable Convolutions¡± reported the weight decay for both the Inception V3 CNN model from Google (not clear from the Inception V3 paper) and the weight decay used in his improved Xception for the ImageNet dataset: The Inception V3 model uses a weight decay (L2 regularization) rate of 4e<U+2212>5, which has been carefully tuned for performance on ImageNet. We found this rate to be quite suboptimal for Xception and instead settled for 1e<U+2212>5. It is common to use weight regularization with LSTM models. An often used configuration is L2 (weight decay) and very small hyperparameters (e.g. 10^<U+2212>6). It is often not reported what weights are regularized (input, recurrent, and/or bias), although one would assume that both input and recurrent weights are regularized only. Gabriel Pereyra, et al. from Google Brain in the 2017 paper titled ¡°Regularizing Neural Networks by Penalizing Confident Output Distributions¡± apply a seq2seq LSTMs models to predicting characters from the Wall Street Journal and report: All models used weight decay of 10^<U+2212>6 Barret Zoph and Quoc Le from Google Brain in the 2017 paper titled ¡°Neural Architecture Search with Reinforcement Learning¡± use LSTMs and reinforcement learning to learn network architectures to best address the CIFAR-10 dataset and report: weight decay of 1e-4 Ron Weiss, et al. from Google Brain and Nvidia in their 2017 paper titled ¡°Sequence-to-Sequence Models Can Directly Translate Foreign Speech¡± develop a sequence-to-sequence LSTM for speech translation and report: L2 weight decay is used with a weight of 10^<U+2212>6 In this section, we will demonstrate how to use weight regularization to reduce overfitting of an MLP on a simple binary classification problem. This example provides a template for applying weight regularization to your own neural network for classification and regression problems. We will use a standard binary classification problem that defines two semi-circles of observations: one semi-circle for each class. Each observation has two input variables with the same scale and a class output value of either 0 or 1. This dataset is called the ¡°moons¡± dataset because of the shape of the observations in each class when plotted. We can use the make_moons() function to generate observations from this problem. We will add noise to the data and seed the random number generator so that the same samples are generated each time the code is run. We can plot the dataset where the two variables are taken as x and y coordinates on a graph and the class value is taken as the color of the observation. The complete example of generating the dataset and plotting it is listed below. Running the example creates a scatter plot showing the semi-circle or moon shape of the observations in each class. We can see the noise in the dispersal of the points making the moons less obvious. Scatter Plot of Moons Dataset With Color Showing the Class Value of Each Sample This is a good test problem because the classes cannot be separated by a line, e.g. are not linearly separable, requiring a nonlinear method such as a neural network to address. We have only generated 100 samples, which is small for a neural network, providing the opportunity to overfit the training dataset and have higher error on the test dataset: a good case for using regularization. Further, the samples have noise, giving the model an opportunity to learn aspects of the samples that don¡¯t generalize. We can develop an MLP model to address this binary classification problem. The model will have one hidden layer with more nodes that may be required to solve this problem, providing an opportunity to overfit. We will also train the model for longer than is required to ensure the model overfits. Before we define the model, we will split the dataset into train and test sets, using 30 examples to train the model and 70 to evaluate the fit model¡¯s performance. Next, we can define the model. The model uses 500 nodes in the hidden layer and the rectified linear activation function. A sigmoid activation function is used in the output layer in order to predict class values of 0 or 1. The model is optimized using the binary cross entropy loss function, suitable for binary classification problems and the efficient Adam version of gradient descent. The defined model is then fit on the training data for 4,000 epochs and the default batch size of 32. Finally, we can evaluate the performance of the model on the test dataset and report the result. We can tie all of these pieces together; the complete example is listed below. Running the example reports the model performance on the train and test datasets. We can see that the model has better performance on the training dataset than the test dataset, one possible sign of overfitting. Your specific results may vary given the stochastic nature of the neural network and the training algorithm. Because the model is severely overfit, we generally would not expect much, if any, variance in the accuracy across repeated runs of the model on the same dataset. Another sign of overfitting is a plot of the learning curves of the model for both train and test datasets while training. An overfit model should show accuracy increasing on both train and test and at some point accuracy drops on the test dataset but continues to rise on the test dataset. We can update the example to plot these curves. The complete example is listed below. Running the example creates line plots of the model accuracy on the train and test sets. We can see an expected shape of an overfit model where test accuracy increases to a point and then begins to decrease again. Line Plots of Accuracy on Train and Test Datasets While Training We can add weight regularization to the hidden layer to reduce the overfitting of the model to the training dataset and improve the performance on the holdout set. We will use the L2 vector norm also called weight decay with a regularization parameter (called alpha or lambda) of 0.001, chosen arbitrarily. This can be done by adding the kernel_regularizer argument to the layer and setting it to an instance of l2. The updated example of fitting and evaluating the model on the moons dataset with weight regularization is listed below. Running the example reports the performance of the model on the train and test datasets. We can see no change in the accuracy on the training dataset and an improvement on the test dataset. We would expect that the telltale learning curve for overfitting would also have been changed through the use of weight regularization. Instead of the accuracy of the model on the test set increasing and then decreasing again, we should see it continually rise during training. The complete example of fitting the model and plotting the train and test learning curves is listed below. Running the example creates line plots of the train and test accuracy for the model for each epoch during training. As expected, we see the learning curve on the test dataset rise and then plateau, indicating that the model may not have overfit the training dataset. Line Plots of Accuracy on Train and Test Datasets While Training Without Overfitting Once you can confirm that weight regularization may improve your overfit model, you can test different values of the regularization parameter. It is a good practice to first grid search through some orders of magnitude between 0.0 and 0.1, then once a level is found, to grid search on that level. We can grid search through the orders of magnitude by defining the values to test, looping through each and recording the train and test performance. Once we have all of the values, we can graph the results as a line plot to help spot any patterns in the configurations to the train and test accuracies. Because parameters jump orders of magnitude (powers of 10), we can create a line plot of the results using a logarithmic scale. The Matplotlib library allows this via the semilogx() function. For example: The complete example for grid searching weight regularization values on the moon dataset is listed below. Running the example prints the parameter value and the accuracy on the train and test sets for each evaluated model. The results suggest that 0.01 or 0.001 may be sufficient and may provide good bounds for further grid searching. A line plot of the results is also created, showing the increase in test accuracy with larger weight regularization parameter values, at least to a point. We can see that using the largest value of 0.1 results in a large drop in both train and test accuracy. Line Plot of Model Accuracy on Train and Test Datasets With Different Weight Regularization Parameters This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to apply weight regularization to improve the performance of an overfit deep learning neural network in Python with Keras. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): model(8), result(8), value(8), dataset(6), network(6), set(6), epoch(4), example(4), kera(4), moon(4)"
"4","mastery",2018-11-19,"Use Weight Regularization to Reduce Overfitting of Deep Learning Models","https://machinelearningmastery.com/weight-regularization-to-reduce-overfitting-of-deep-learning-models/","Neural networks learn a set of weights that best map inputs to outputs. A network with large network weights can be a sign of an unstable network where small changes in the input can lead to large changes in the output. This can be a sign that the network has overfit the training dataset and will likely perform poorly when making predictions on new data. A solution to this problem is to update the learning algorithm to encourage the network to keep the weights small. This is called weight regularization and it can be used as a general technique to reduce overfitting of the training dataset and improve the generalization of the model. In this post, you will discover weight regularization as an approach to reduce overfitting for neural networks. After reading this post, you will know: Let¡¯s get started. A Gentle Introduction to Weight Regularization to Reduce Overfitting for Deep Learning ModelsPhoto by jojo nicdao, some rights reserved. When fitting a neural network model, we must learn the weights of the network (i.e. the model parameters) using stochastic gradient descent and the training dataset. The longer we train the network, the more specialized the weights will become to the training data, overfitting the training data. The weights will grow in size in order to handle the specifics of the examples seen in the training data. Large weights make the network unstable. Although the weight will be specialized to the training dataset, minor variation or statistical noise on the expected inputs will result in large differences in the output. Large weights tend to cause sharp transitions in the node functions and thus large changes in output for small changes in the inputs. <U+2014> Page 269 Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999. Generally, we refer to this model as having a large variance and a small bias. That is, the model is sensitive to the specific examples, the statistical noise, in the training dataset. A model with large weights is more complex than a model with smaller weights. It is a sign of a network that may be overly specialized to training data. In practice, we prefer to choose the simpler models to solve a problem (e.g. Occam¡¯s razor). We prefer models with smaller weights. ¡¦ given some training data and a network architecture, multiple sets of weight values (multiple models) could explain the data. Simpler models are less likely to over-fit than complex ones. A simple model in this context is a model where the distribution of parameter values has less entropy <U+2014> Page 107, Deep Learning with Python, 2017. Another possible issue is that there may be many input variables, each with different levels of relevance to the output variable. Sometimes we can use methods to aid in selecting input variables, but often the interrelationships between variables is not obvious. Having small weights or even zero weights for less relevant or irrelevant inputs to the network will allow the model to focus learning. This too will result in a simpler model. The learning algorithm can be updated to encourage the network toward using small weights. One way to do this is to change the calculation of loss used in the optimization of the network to also consider the size of the weights. Remember, that when we train a neural network, we minimize a loss function, such as the log loss in classification or mean squared error in regression. In calculating the loss between the predicted and expected values in a batch, we can add the current size of all weights in the network or add in a layer to this calculation. This is called a penalty because we are penalizing the model proportional to the size of the weights in the model. Many regularization approaches are based on limiting the capacity of models, such as neural networks, linear regression, or logistic regression, by adding a [¡¦] penalty to the objective function. <U+2014> Page 230, Deep Learning, 2016. Larger weights result in a larger penalty, in the form of a larger loss score. The optimization algorithm will then push the model to have smaller weights, i.e. weights no larger than needed to perform well on the training dataset. Smaller weights are considered more regular or less specialized and as such, we refer to this penalty as weight regularization. When this approach of penalizing model coefficients is used in other machine learning models such as linear regression or logistic regression, it may be referred to as shrinkage, because the penalty encourages the coefficients to shrink during the optimization process. Shrinkage. This approach involves fitting a model involving all p predictors. However, the estimated coefficients are shrunken towards zero [¡¦] This shrinkage (also known as regularization) has the effect of reducing variance <U+2014> Page 204, An Introduction to Statistical Learning: with Applications in R, 2013. The addition of a weight size penalty or weight regularization to a neural network has the effect of reducing generalization error and of allowing the model to pay less attention to less relevant input variables. 1) It suppresses any irrelevant components of the weight vector by choosing the smallest vector that solves the learning problem. 2) If the size is chosen right, a weight decay can suppress some of the effect of static noise on the targets. <U+2014> A Simple Weight Decay Can Improve Generalization, 1992. There are two parts to penalizing the model based on the size of the weights. The first is the calculation of the size of the weights, and the second is the amount of attention that the optimization process should pay to the penalty. Neural network weights are real-values that can be positive or negative, as such, simply adding the weights is not sufficient. There are two main approaches used to calculate the size of the weights, they are: L1 encourages weights to 0.0 if possible, resulting in more sparse weights (weights with more 0.0 values). L2 offers more nuance, both penalizing larger weights more severely, but resulting in less sparse weights. The use of L2 in linear and logistic regression is often referred to as Ridge Regression. This is useful to know when trying to develop an intuition for the penalty or examples of its usage. In other academic communities, L2 regularization is also known as ridge regression or Tikhonov regularization. <U+2014> Page 231, Deep Learning, 2016. The weights may be considered a vector and the magnitude of a vector is called its norm, from linear algebra. As such, penalizing the model based on the size of the weights is also referred to as a weight or parameter norm penalty. It is possible to include both L1 and L2 approaches to calculating the size of the weights as the penalty. This is akin to the use of both penalties used in the Elastic Net algorithm for linear and logistic regression. The L2 approach is perhaps the most used and is traditionally referred to as ¡°weight decay¡± in the field of neural networks. It is called ¡°shrinkage¡± in statistics, a name that encourages you to think of the impact of the penalty on the model weights during the learning process. This particular choice of regularizer is known in the machine learning literature as weight decay because in sequential learning algorithms, it encourages weight values to decay towards zero, unless supported by the data. In statistics, it provides an example of a parameter shrinkage method because it shrinks parameter values towards zero. <U+2014> Page 144-145, Pattern Recognition and Machine Learning, 2006. Recall that each node has input weights and a bias weight. The bias weight is generally not included in the penalty because the ¡°input¡± is constant. The calculated size of the weights is added to the loss objective function when training the network. Rather than adding each weight to the penalty directly, they can be weighted using a new hyperparameter called alpha (a) or sometimes lambda. This controls the amount of attention that the learning process should pay to the penalty. Or put another way, the amount to penalize the model based on the size of the weights. The alpha hyperparameter has a value between 0.0 (no penalty) and 1.0 (full penalty). This hyperparameter controls the amount of bias in the model from 0.0, or low bias (high variance), to 1.0, or high bias (low variance). If the penalty is too strong, the model will underestimate the weights and underfit the problem. If the penalty is too weak, the model will be allowed to overfit the training data. The vector nor of the weights is often calculated per-layer, rather than across the entire network. This allows more flexibility in the choice of the type of regularization used (e.g. L1 for inputs, L2 elsewhere) and flexibility in the alpha value, although it is common to use the same alpha value on each layer by default. In the context of neural networks, it is sometimes desirable to use a separate penalty with a different a coefficient for each layer of the network. Because it can be expensive to search for the correct value of multiple hyperparameters, it is still reasonable to use the same weight decay at all layers just to reduce the size of search space. <U+2014> Page 230, Deep Learning, 2016. This section provides some tips for using weight regularization with your neural network. Weight regularization is a generic approach. It can be used with most, perhaps all, types of neural network models, not least the most common network types of Multilayer Perceptrons, Convolutional Neural Networks, and Long Short-Term Memory Recurrent Neural Networks. In the case of LSTMs, it may be desirable to use different penalties or penalty configurations for the input and recurrent connections. It is generally good practice to update input variables to have the same scale. When input variables have different scales, the scale of the weights of the network will, in turn, vary accordingly. This introduces a problem when using weight regularization because the absolute or squared values of the weights must be added for use in the penalty. This problem can be addressed by either normalizing or standardizing input variables. It is common for larger networks (more layers or more nodes) to more easily overfit the training data. When using weight regularization, it is possible to use larger networks with less risk of overfitting. A good configuration strategy may be to start with larger networks and use weight decay. It is common to use small values for the regularization hyperparameter that controls the contribution of each weight to the penalty. Perhaps start by testing values on a log scale, such as 0.1, 0.001, and 0.0001. Then use a grid search at the order of magnitude that shows the most promise. Rather than trying to choose between L1 and L2 penalties, use both. Modern and effective linear regression methods such as the Elastic Net use both L1 and L2 penalties at the same time and this can be a useful approach to try. This gives you both the nuance of L2 and the sparsity encouraged by L1. The use of weight regularization may allow more elaborate training schemes. For example, a model may be fit on training data first without any regularization, then updated later with the use of a weight penalty to reduce the size of the weights of the already well-performing model. Do you have any tips for using weight regularization?
Let me know in the comments below. This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered weight regularization as an approach to reduce overfitting for neural networks. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. Another possible type of weight penalty is ¡°Exp[Abs[w]] <U+2013> 1¡± which would encourage zero weights although not so strongly as L1, while at the same time strongly penalising large weights similarly to L2 (but stronger). Very nice! Thanks. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): weight(45), network(12), value(10), model(7), variable(7), input(5), change(4), penalty(4), approach(3), coefficient(3)"
"5","vidhya",2018-11-19,"4 Secrets for a Future Ready Career in Data Science","https://www.analyticsvidhya.com/blog/2018/11/4-secrets-for-a-future-ready-career-in-data-science/","Automation has impacted, and will continue impacting, jobs in many domains. Every single job on this planet is subject to a risk of job replacement by bots <U+2013> just the intensity might differ. Automation makes running a business more efficient on one hand, and on the other, it keeps on changing the skill set required to stay relevant in the industry. This inevitably leads to unemployment due to mismatches in the skill set. Let me take you through a few scenarios to illustrate my thoughts. You are an HR professional in the year 2000, when most of the company employee documents were on paper. You are very efficient in sorting documents and retrieving them when needed and have been a star performer for more than 5 years because of these skills. Given that the HR processes did not change much over time, you did not pick up computer skills over the next 18 years. However, the way industries work have changed a lot from 2000 to 2018, and now all the employee documentations are on the cloud or a private server. So, your most sell-able skills are now suddenly not that important. You might face difficulties finding a job unless you upgrade yourself for today¡¯s evolved industry. Note that your skill set mismatch was not because of the evolution of HR specific processes, but the dynamically changing business processes that you support. You work as a news reader on the radio in the era when there was no television. You are very well informed about current affairs and hence you were a strong performer. But after television became mainstream, radios almost went out of business. Your radio employer had to let you go because they were sustaining heavy losses. Now, given your skill set, you can still try to get a job as a TV news reader but you need to work on your body language and the crippling fear of facing the camera. The good news? You have been hanging out with people who work in the TV news industry and hence you know your opportunity areas and have been actively working on them. Note that this time neither your profession evolved, nor your industry. It¡¯s just that the customer started preferring an alternate product/service to the business you support, making your skill a mismatch (or obsolete) in the industry. In the scenarios above, we witnessed that the changes around us are making businesses easy to run but at the same time are<U+00A0> creating job skill mismatches, leading to unemployment in specific domains. Below are the three main reasons of job skill shifts in the industry: It is no surprise that automation and changing business domains have disrupted many jobs. An important questions now is: Will some jobs be impacted more than others? Even though no one really knows what jobs will be more/less impacted by automation, here is a framework that helps understand the broad idea.<U+00A0>Machines are not good at learning from too few examples and machines are not good at being creative. So if your job has these two attributes, you should be just fine. For instance, driving a car is a very repetitive process and does not involve a lot of creativity. Hence, cab drivers are at a high risk of their job facing automation. In this ever-evolving AI-led world, we (data scientists) are definitely on the better side of the deal. Where does the role of a data scientist fall in the graph shown above? As a data scientist, we do a varied set of jobs to help businesses grow. Each of these jobs fall at a different place in the graph above. The below image shows my thoughts on the different sub-jobs we do as data scientists (proportion might vary with individual roles): As you can see, Not all the parts of a data scientist¡¯s job come with a 10 year warranty. Depending on your specific role and proportion of work that is difficult to automate, you can estimate your risk of automation. Consider a data scientist in 2010. Key skill sets were knowing logistic and linear regression, and conversant with base SAS and MS Excel. Now, if we bring this data scientist of 2018 without any significant upgrades on tools and technique, he/she can face hard time finding data scientist job.<U+00A0> With good certainty it can be said that even though the data science stream will stay up and running for long term, the roles and responsibilities of these jobs are up for big changes. People who have challenges upgrading to these new roles and responsibilities will face strong setbacks in progressing in career. Given the young workforce in data science field, skill set match is not a concern over short term as most of the people working in this field have recently picked up knowledge in latest tools and technique. However, as the field gets old so does the workforce and skill set mismatch within data science domain is definitely possible if this workforce is not able to upgrade their skill set while managing their daily jobs. Four things I would recommend for data scientists in any kind of role to build a future proof profile: With a high focus on data-driven strategies across domains, data scientists are kept busy with their job at hand. Not staying updated on each of the 4 pointers mentioned above can<U+00A0>be dangerous in the long run. To fill this gap in the industry, Analytics Vidhya has handcrafted a four day conference <U+2013><U+00A0>DataHack Summit 2018. After the success of the Summit last year, we have further optimized the schedule to pack it with everything you need to know to come up to speed in terms of tools, technologies, and business domains. Sounds too good an opportunity to pass up? Good! Tickets are almost sold on, so grab yours<U+00A0>here<U+00A0>TODAY!","Keyword(freq): job(9), domain(5), scientist(5), process(3), role(3), skill(3), tool(3), business(2), change(2), document(2)"
"6","vidhya",2018-11-19,"Reinforcement Learning: Introduction to Monte Carlo Learning using the OpenAI Gym Toolkit","https://www.analyticsvidhya.com/blog/2018/11/reinforcement-learning-introduction-monte-carlo-learning-openai-gym/","What¡¯s the first thing that comes to your mind when you hear the words ¡°reinforcement learning¡±? The most common thought is <U+2013> too complex with way too much math. But I¡¯m here to assure you that this is quite a fascinating field of study <U+2013> and I aim to break down these techniques in my articles into easy-to-understand concepts. I¡¯m sure you must have heard of OpenAI and DeepMind. These are two leading AI organizations who have made significant progress in this field. A team of OpenAI bots was able to defeat a team of amateur gamers in Dota 2, a phenomenally popular and complex battle arena game. Do you think it¡¯s feasible to build a bot using dynamic programming for something as complex as Dota 2? It¡¯s unfortunately a no-go. There are just too many states (millions and millions), and collecting all the specifics of DOTA 2 is an impossible task. This is where we enter the realm of reinforcement learning or more specifically model-free learning. In this article, we will try to understand the basics of Monte Carlo learning. It¡¯s used when there is no prior information of the environment and all the information is essentially collected by experience. We¡¯ll use the OpenAI Gym toolkit in Python to implement this method as well. Let¡¯s get the ball rolling! If you¡¯re a beginner in this field or need a quick refresher of some basic reinforcement learning terminologies, I highly recommend going through the below articles to truly maximize your learning from this post: We know that dynamic programming is used to solve problems where the underlying model of the environment is known beforehand (or more precisely, model-based learning). Reinforcement Learning is all about learning from experience in playing games. And yet, in none of the dynamic programming algorithms, did we actually play the game/experience the environment. We had a full model of the environment, which included all the state transition probabilities. However, in most real life situations as we saw in the introduction, the transition probabilities from one state to another (or the so called model of the environment) are not known beforehand. It is not even necessary that the task follows a Markov property. Let¡¯s say we want to train a bot to learn how to play chess. Consider converting the chess environment into an MDP. Now, depending on the positioning of pieces, this environment will have many states (more than 1050), as well as a large number of possible actions. The model of this environment is almost impossible to design! One potential solution could be to repeatedly play a complete game of chess and receive a positive reward for winning, and a negative reward for losing, at the end of each game. This is called learning from experience. Any method which solves a problem by generating suitable random numbers, and observing that fraction of numbers obeying some property or properties, can be classified as a Monte Carlo method. Let¡¯s do a fun exercise where we will try to find out the value of pi using pen and paper. Let¡¯s draw a square of unit length and draw a quarter circle with unit length radius. Now, we have a helper bot C3PO with us. It is tasked with putting as many dots as possible on the square randomly 3,000 times, resulting in the following figure: C3PO needs to count each time it puts a dot inside a circle. So, the value of pi will be given by: where N is the number of times a dot was put inside the circle. As you can see, we did not do anything except count the random dots that fall inside the circle and then took a ratio to approximate the value of pi. The Monte Carlo method for reinforcement learning learns directly from episodes of experience without any prior knowledge of MDP transitions. Here, the random component is the return or reward. One caveat is that it can only be applied to episodic MDPs. Its fair to ask why, at this point. The reason is that the episode has to terminate before we can calculate any returns. Here, we don¡¯t do an update after every action, but rather after every episode. It uses the simplest idea <U+2013> the value is the mean return of all sample trajectories for each state. Recalling the idea from multi-armed bandits discussed in this article, every state is a separate multi-armed bandit problem and the idea is to behave optimally for all multi-armed bandits at once. Similar to dynamic programming, there is a policy evaluation (finding the value function for a given random policy) and policy improvement step (finding the optimum policy). We will cover both these steps in the next two sections. The goal here, again, is to learn the value function vpi(s) from episodes of experience under a policy pi. Recall that the return is the total discounted reward: S1, A1, R2, ¡¦.Sk ~ pi Also recall that the value function is the expected return: We know that we can estimate any expected value simply by adding up samples and dividing by the total number of samples: The question is how do we get these sample returns? For that, we need to play a bunch of episodes and generate them. For every episode we play, we¡¯ll have a sequence of states and rewards. And from these rewards, we can calculate the return by definition, which is just the sum of all future rewards. First Visit Monte Carlo:<U+00A0>Average returns only for first time s is visited in an episode. Here¡¯s a step-by-step view of how the algorithm works: Every visit Monte Carlo:<U+00A0>Average returns for every time s is visited in an episode. For this algorithm, we just change step #3.1 to ¡®Add to a list<U+00A0>the return received after every occurrence of this state¡¯. Let¡¯s consider a simple example to further understand this concept. Suppose there¡¯s an environment where we have 2 states <U+2013> A and B. Let¡¯s say we observed 2 sample episodes: A+3 => A indicates a transition from state A to state A, with a reward +3. Let¡¯s find out the value function using both methods: It is convenient to convert the mean return into an incremental update so that the mean can be updated with each episode and we can understand the progress made with each episode. We already<U+00A0>learnt this when solving the multi-armed bandit problem. We update v(s) incrementally after episodes. For each state St, with return Gt: In non-stationary problems, it can be useful to track a running mean, i.e., forget old episodes: V(St) ¡ç V(St) + ¥á (Gt <U+2212> V(St)) Similar to dynamic programming, once we have the value function for a random policy, the important task that still remains is that of finding the optimal policy using Monte Carlo. Recall that the formula for policy improvement in DP required the model of the environment as shown in the following equation: This equation finds out the optimal policy by finding actions that maximize the sum of rewards. However, a major caveat here is that it uses transition probabilities, which is not known in the case of model-free learning. Since we do not know the state transition probabilities p(s¡¯,r/s,a), we can¡¯t do a look-ahead search like DP.<U+00A0>Hence, all the information is obtained via experience of playing the game or exploring the environment. Policy improvement is done by making the policy greedy with respect to the current value function.<U+00A0>In this case, we have an action-value function, and therefore no model is needed to construct the greedy<U+00A0>policy. A greedy policy (like the above mentioned one) will always favor a certain action if most actions are not explored properly. There are two solutions for this: Monte Carlo with exploring starts All the state action pairs have non-zero probability of being the starting pair, in this algorithm. This will ensure each episode which is played will take the agent to new states and hence, there is more exploration of the environment. Monte Carlo with epsilon-Soft What if there is a single start point for an environment (for example, a game of chess)? Exploring starts is not the right option in such cases. Recall here that in a multi-armed bandit problem, we discussed the<U+00A0>epsilon-greedy approach. Simplest idea for ensuring continual exploration all actions are tried with non-zero probability 1 <U+2013> epsilon choose the action which maximises the action value function and with probability epsilon choose an action at random. Now that we understand the basics of Monte Carlo Control and Prediction, let¡¯s implement the algorithm in Python. We will import the frozen lake environment from the popular OpenAI Gym toolkit. The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile. The surface is described using a grid like the following: (S: starting point, safe),<U+00A0><U+00A0>(F: frozen surface, safe),<U+00A0>(H: hole, fall to your doom),<U+00A0>(G: goal) The idea is to reach the goal from the starting point by walking only on a frozen surface and avoiding all the holes. Installation details and documentation for the OpenAI Gym are available at this<U+00A0>link. Let¡¯s begin! ¡©¡© First, we will define a few helper functions to set up the Monte Carlo algorithm. Create Environment Function for Random Policy Dictionary for storing the state action value Function to play episode Function to test policy and print win percentage First Visit Monte Carlo Prediction and Control Now, it is time to run this algorithm to solve an 8¡¿8 frozen lake environment and check the reward: On running this for 50,000 episodes, we get a score of 0.9. And with more episodes, it eventually reaches the optimal policy. The story of Monte Carlo learning does not end here. There is another set of algorithms under this which are called off policy Monte Carlo methods. Off policy methods try to learn an optimal policy using returns generated from another policy.","Keyword(freq): episode(8), return(5), state(5), action(4), probability(4), reward(4), method(3), algorithm(2), article(2), bandit(2)"
