site,date,headline,url_address,text
datacamp,2018-07-11,New Course: Machine Learning with Tree-Based Models in Python,https://www.datacamp.com/community/blog/course-machine-learning-with-tree-based-models-in-python,"Decision trees are supervised learning models used for problems involving classification and regression. Tree models present a high flexibility that comes at a price: on one hand, trees are able to capture complex non-linear relationships; on the other hand, they are prone to memorizing the noise present in a dataset. By aggregating the predictions of trees that are trained differently, ensemble methods take advantage of the flexibility of trees while reducing their tendency to memorize noise. Ensemble methods are used across a variety of fields and have a proven track record of winning many machine learning competitions. In this course, you'll learn how to use Python to train decision trees and tree-based models with the user-friendly scikit-learn machine learning library. You'll understand the advantages and shortcomings of trees and demonstrate how ensembling can alleviate these shortcomings, all while practicing on real-world datasets. Finally, you'll also understand how to tune the most influential hyperparameters in order to get the most out of your models. Classification and Regression Trees (CART) are a set of supervised learning models used for problems involving classification and regression. In this chapter, you'll be introduced to the CART algorithm. The bias-variance tradeoff is one of the fundamental concepts in supervised machine learning. In this chapter, you'll understand how to diagnose the problems of overfitting and underfitting. You'll also be introduced to the concept of ensembling where the predictions of several models are aggregated to produce predictions that are more robust. Bagging is an ensemble method involving training the same algorithm many times using different subsets sampled from the training data. In this chapter, you'll understand how bagging can be used to create a tree ensemble. You'll also learn how the random forests algorithm can lead to further ensemble diversity through randomization at the level of each split in the trees forming the ensemble. Boosting refers to an ensemble method in which several models are trained sequentially with each model learning from the errors of its predecessors. In this chapter, you'll be introduced to the two boosting methods of AdaBoost and Gradient Boosting. The hyperparameters of a machine learning model are parameters that are not learned from data. They should be set prior to fitting the model to the training set. In this chapter, you'll learn how to tune the hyperparameters of a tree-based model using grid search cross-validation. Supervised Learning with scikit-learn"
datacamp,2018-07-11,New Course: Python for R Users,https://www.datacamp.com/community/blog/course-python-r-users,"Python and R have seen immense growth in popularity in the ""Machine Learning Age"". They both are high-level languages that are easy to learn and write. The language you use will depend on your background and field of study and work. R is a language made by and for statisticians, whereas Python is a more general purpose programming language. Regardless of the background, there will be times when a particular algorithm is implemented in one language and not the other, a feature is better documented, or simply, the tutorial you found online uses Python instead of R. In either case, this would require the R user to work in Python to get his/her work done, or try to understand how something is implemented in Python for it to be translated into R. This course helps you cross the R-Python language barrier. Learn about some of the most important data types (integers, floats, strings, and booleans) and data structures (lists, dictionaries, numpy arrays, and pandas DataFrames) in Python and how they compare to the ones in R. This chapter covers control flow statements (if-else if-else), for loops and shows you how to write your own functions in Python! In this chapter you will learn more about one of the most important Python libraries, Pandas. In addition to DataFrames, pandas provides several data manipulation functions and methods. You will learn about the rich ecosystem of visualization libraries in Python. This chapter covers matplotlib, the core visualization library in Python along with the pandas and seaborn libraries. As a final capstone, you will apply your Python skills on the NYC Flights 2013 dataset. Writing Functions in R"
mastery,2018-07-13,A Gentle Introduction to Statistical Power and Power Analysis in Python,https://machinelearningmastery.com/statistical-power-and-power-analysis-in-python/,"The statistical power of a hypothesis test is the probability of detecting an effect, if there is a true effect present to detect. Power can be calculated and reported for a completed experiment to comment on the confidence one might have in the conclusions drawn from the results of the study. It can also be used as a tool to estimate the number of observations or sample size required in order to detect an effect in an experiment. In this tutorial, you will discover the importance of the statistical power of a hypothesis test and now to calculate power analyses and power curves as part of experimental design. After completing this tutorial, you will know: Let¡¯s get started. A Gentle Introduction to Statistical Power and Power Analysis in PythonPhoto by Kamil Porembi<U+0144>ski, some rights reserved. This tutorial is divided into four parts; they are: Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course A statistical hypothesis test makes an assumption about the outcome, called the null hypothesis. For example, the null hypothesis for the Pearson¡¯s correlation test is that there is no relationship between two variables. The null hypothesis for the Student¡¯s t test is that there is no difference between the means of two populations. The test is often interpreted using a p-value, which is the probability of observing the result given that the null hypothesis is true, not the reverse, as is often the case with misinterpretations. In interpreting the p-value of a significance test, you must specify a significance level, often referred to as the Greek lower case letter alpha (a). A common value for the significance level is 5% written as 0.05. The p-value is interested in the context of the chosen significance level. A result of a significance test is claimed to be ¡°statistically significant¡± if the p-value is less than the significance level. This means that the null hypothesis (that there is no result) is rejected. Where: We can see that the p-value is just a probability and that in actuality the result may be different. The test could be wrong. Given the p-value, we could make an error in our interpretation. There are two types of errors; they are: In this context, we can think of the significance level as the probability of rejecting the null hypothesis if it were true. That is the probability of making a Type I Error or a false positive. Statistical power, or the power of a hypothesis test is the probability that the test correctly rejects the null hypothesis. That is, the probability of a true positive result. It is only useful when the null hypothesis is rejected. ¡¦ statistical power is the probability that a test will correctly reject a false null hypothesis. Statistical power has relevance only when the null is false. <U+2014> Page 60, The Essential Guide to Effect Sizes: Statistical Power, Meta-Analysis, and the Interpretation of Research Results, 2010. The higher the statistical power for a given experiment, the lower the probability of making a Type II (false negative) error. That is the higher the probability of detecting an effect when there is an effect. In fact, the power is precisely the inverse of the probability of a Type II error. More intuitively, the statistical power can be thought of as the probability of accepting an alternative hypothesis, when the alternative hypothesis is true. When interpreting statistical power, we seek experiential setups that have high statistical power. Experimental results with too low statistical power will lead to invalid conclusions about the meaning of the results. Therefore a minimum level of statistical power must be sought. It is common to design experiments with a statistical power of 80% or better, e.g. 0.80. This means a 20% probability of encountering a Type II area. This different to the 5% likelihood of encountering a Type I error for the standard value for the significance level. Statistical power is one piece in a puzzle that has four related parts; they are: All four variables are related. For example, a larger sample size can make an effect easier to detect, and the statistical power can be increased in a test by increasing the significance level. A power analysis involves estimating one of these four parameters given values for three other parameters. This is a powerful tool in both the design and in the analysis of experiments that we wish to interpret using statistical hypothesis tests. For example, the statistical power can be estimated given an effect size, sample size and significance level. Alternately, the sample size can be estimated given different desired levels of significance. Power analysis answers questions like ¡°how much statistical power does my study have?¡± and ¡°how big a sample size do I need?¡±. <U+2014> Page 56, The Essential Guide to Effect Sizes: Statistical Power, Meta-Analysis, and the Interpretation of Research Results, 2010. Perhaps the most common use of a power analysis is in the estimation of the minimum sample size required for an experiment. Power analyses are normally run before a study is conducted. A prospective or a priori power analysis can be used to estimate any one of the four power parameters but is most often used to estimate required sample sizes. <U+2014> Page 57, The Essential Guide to Effect Sizes: Statistical Power, Meta-Analysis, and the Interpretation of Research Results, 2010. As a practitioner, we can start with sensible defaults for some parameters, such as a significance level of 0.05 and a power level of 0.80. We can then estimate a desirable minimum effect size, specific to the experiment being performed. A power analysis can then be used to estimate the minimum sample size required. In addition, multiple power analyses can be performed to provide a curve of one parameter against another, such as the change in the size of an effect in an experiment given changes to the sample size. More elaborate plots can be created varying three of the parameters. This is a useful tool for experimental design. We can make the idea of statistical power and power analysis concrete with a worked example. In this section, we will look at the Student¡¯s t test, which is a statistical hypothesis test for comparing the means from two samples of Gaussian variables. The assumption, or null hypothesis, of the test is that the sample populations have the same mean, e.g. that there is no difference between the samples or that the samples are drawn from the same underlying population. The test will calculate a p-value that can be interpreted as to whether the samples are the same (fail to reject the null hypothesis), or there is a statistically significant difference between the samples (reject the null hypothesis). A common significance level for interpreting the p-value is 5% or 0.05. The size of the effect of comparing two groups can be quantified with an effect size measure. A common measure for comparing the difference in the mean from two groups is the Cohen¡¯s d measure. It calculates a standard score that describes the difference in terms of the number of standard deviations that the means are different. A large effect size for Cohen¡¯s d is 0.80 or higher, as is commonly accepted when using the measure. We can use the default and assume a minimum statistical power of 80% or 0.8. For a given experiment with these defaults, we may be interested in estimating a suitable sample size. That is, how many observations are required from each sample in order to at least detect an effect of 0.80 with an 80% chance of detecting the effect if it is true (20% of a Type II error) and a 5% chance of detecting an effect if there is no such effect (Type I error). We can solve this using a power analysis. The statsmodels library provides the TTestIndPower class for calculating a power analysis for the Student¡¯s t test with independent samples. Of note is the TTestPower class that can perform the same analysis for the paired Student¡¯s t test. The function solve_power() can be used to calculate one of the four parameters in a power analysis. In our case, we are interested in calculating the sample size. We can use the function by providing the three pieces of information we know (alpha, effect, and power) and setting the size of argument we wish to calculate the answer of (nobs1) to ¡°None¡°. This tells the function what to calculate. A note on sample size: the function has an argument called ratio that is the ratio of the number of samples in one sample to the other. If both samples are expected to have the same number of observations, then the ratio is 1.0. If, for example, the second sample is expected to have half as many observations, then the ratio would be 0.5. The TTestIndPower instance must be created, then we can call the solve_power() with our arguments to estimate the sample size for the experiment. The complete example is listed below. Running the example calculates and prints the estimated number of samples for the experiment as 25. This would be a suggested minimum number of samples required to see an effect of the desired size. We can go one step further and calculate power curves. Power curves are line plots that show how the change in variables, such as effect size and sample size, impact the power of the statistical test. The plot_power() function can be used to create power curves. The dependent variable (x-axis) must be specified by name in the ¡®dep_var¡® argument. Arrays of values can then be specified for the sample size (nobs), effect size (effect_size), and significance (alpha) parameters. One or multiple curves will then be plotted showing the impact on statistical power. For example, we can assume a significance of 0.05 (the default for the function) and explore the change in sample size between 5 and 100 with low, medium, and high effect sizes. The complete example is listed below. Running the example creates the plot showing the impact on statistical power (y-axis) for three different effect sizes (es) as the sample size (x-axis) is increased. We can see that if we are interested in a large effect that a point of diminishing returns in terms of statistical power occurs at around 40-to-50 observations. Power Curves for Student¡¯s t Test Usefully, statsmodels has classes to perform a power analysis with other statistical tests, such as the F-test, Z-test, and the Chi-Squared test. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered the statistical power of a hypothesis test and how to calculate power analyses and power curves as part of experimental design. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦by writing lines of code in python Discover how in my new Ebook:Statistical Methods for Machine Learning It provides self-study tutorials on topics like:Hypothesis Tests, Correlation, Nonparametric Stats, Resampling, and much more¡¦ Skip the Academics. Just Results. Click to learn more. nIce article thank u Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning."
mastery,2018-07-11,A Gentle Introduction to Effect Size Measures in Python,https://machinelearningmastery.com/effect-size-measures-in-python/,"Statistical hypothesis tests report on the likelihood of the observed results given an assumption, such as no association between variables or no difference between groups. Hypothesis tests do not comment on the size of the effect if the association or difference is statistically significant. This highlights the need for standard ways of calculating and reporting a result. Effect size methods refer to a suite of statistical tools for quantifying an the size of an effect in the results of experiments that can be used to complement the results from statistical hypothesis tests. In this tutorial, you will discover effect size and effect size measures for quantifying the magnitude of a result. After completing this tutorial, you will know: Let¡¯s get started. A Gentle Introduction to Effect Size Measures in PythonPhoto by scott1346, some rights reserved. This tutorial is divided into three parts; they are: Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course Once practitioners become versed in statistical methods, it is common to become focused on quantifying the likelihood of a result. This is often seen with the calculation and presentation of the results from statistical hypothesis tests in terms of p-value and the significance level. One aspect that is often neglected in the presentation of results is to actually quantify the difference or relationship, called the effect. It can be easy to forget that the intention of an experiment is to quantify an effect. The primary product of a research inquiry is one or more measures of effect size, not P values. <U+2014> Things I have learned (so far), 1990. The statistical test can only comment on the likelihood that there is an effect. It does not comment on the size of the effect. The results of an experiment could be significant, but the effect so small that it has little consequence. It is possible, and unfortunately quite common, for a result to be statistically significant and trivial. It is also possible for a result to be statistically nonsignificant and important. <U+2014> Page 4, The Essential Guide to Effect Sizes: Statistical Power, Meta-Analysis, and the Interpretation of Research Results, 2010. The problem with neglecting the presentation of the effect is that it may be calculated using ad hoc measures or even ignored completely and left to the reader to interpret. This is a big problem as quantifying the size of the effect is essential to interpreting results. An effect size refers to the size or magnitude of an effect or result as it would be expected to occur in a population. The effect size is estimated from samples of data. Effect size methods refers to a collection of statistical tools used to calculate the effect size. Often the field of effect size measures is referred to as simply ¡°effect size¡°, to note the general concern of the field. It is common to organize effect size statistical methods into groups, based on the type of effect that is to be quantified. Two main groups of methods for calculating effect size are: An effect can be the result of a treatment revealed in a comparison between groups (e.g. treated and untreated groups) or it can describe the degree of association between two related variables (e.g. treatment dosage and health). <U+2014> Page 5, The Essential Guide to Effect Sizes: Statistical Power, Meta-Analysis, and the Interpretation of Research Results, 2010. The result of an effect size calculation must be interpreted, and it depends on the specific statistical method used. A measure must be chosen based on the goals of the interpretation. Three types of calculated result include: Thus, effect size can refer to the raw difference between group means, or absolute effect size, as well as standardized measures of effect, which are calculated to transform the effect to an easily understood scale. Absolute effect size is useful when the variables under study have intrinsic meaning (eg, number of hours of sleep). <U+2014> Using Effect Size<U+2014>or Why the P Value Is Not Enough, 2012. It may be a good idea to report an effect size using multiple measures to aide the different types of readers of your findings. Sometimes a result is best reported both in original units, for ease of understanding by readers, and in some standardized measure for ease of inclusion in future meta-analyses. <U+2014> Page 41, Understanding The New Statistics: Effect Sizes, Confidence Intervals, and Meta-Analysis, 2011. The effect size does not replace the results of a statistical hypothesis test. Instead, the effect size complements the test. Ideally, the results of both the hypothesis test and the effect size calculation would be presented side-by-side. The calculation of an effect size could be the calculation of a mean of a sample or the absolute difference between two means. It could also be a more elaborate statistical calculation. In this section, we will look at some common effect size calculations for both associations and differences. The examples of methods is not complete; there may be 100s of methods that can be used to calculate an effect size. The association between variables is often referred to as the ¡°r family¡± of effect size methods. This name comes from perhaps the most common method for calculating the effect size called Pearson¡¯s correlation coefficient, also called Pearson¡¯s r. The Pearson¡¯s correlation coefficient measures the degree of linear association between two real-valued variables. It is a unit-free effect size measure, that can be interpreted in a standard way, as follows: The Pearson¡¯s correlation coefficient can be calculated in Python using the pearsonr() SciPy function. The example below demonstrates the calculation of the Pearson¡¯s correlation coefficient to quantify the size of the association between two samples of random Gaussian numbers where one sample has a strong relationship with the second. Running the example calculates and prints the Pearson¡¯s correlation between the two data samples. We can see that the effect shows a strong positive relationship between the samples. Another very popular method for calculating the association effect size is the r-squared measure, or r^2, also called the coefficient of determination. It summarizes the proportion of variance in one variable explained by the other. The difference between groups is often referred to as the ¡°d family¡± of effect size methods. This name comes from perhaps the most common method for calculating the difference between the mean value of groups, called Cohen¡¯s d. Cohen¡¯s d measures the difference between the mean from two Gaussian-distributed variables. It is a standard score that summarizes the difference in terms of the number of standard deviations. Because the score is standardized, there is a table for the interpretation of the result, summarized as: The Cohen¡¯s d calculation is not provided in Python; we can calculate it manually. The calculation of the difference between the mean of two samples is as follows: Where d is the Cohen¡¯s d, u1 is the mean of the first sample, u2 is the mean of the second sample, and s is the pooled standard deviation of both samples. The pooled standard deviation for two independent samples can be calculated as follows: Where s is the pooled standard deviation, n1 and n2 are the size of the first sample and second samples and s1^2 and s2^2 is the variance for the first and second samples. The subtractions are the adjustments for the number of degrees of freedom. The function below will calculate the Cohen¡¯s d measure for two samples of real-valued variables. The NumPy functions mean() and var() are used to calculate the sample mean and variance respectively. The example below calculates the Cohen¡¯s d measure for two samples of random Gaussian variables with differing means. The example is contrived such that the means are different by one half standard deviation and both samples have the same standard deviation. Running the example calculates and prints the Cohen¡¯s d effect size. We can see that as expected, the difference between the means is one half of one standard deviation interpreted as a medium effect size. Two other popular methods for quantifying the difference effect size are: This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered effect size and effect size measures for quantifying the magnitude of a result. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦by writing lines of code in python Discover how in my new Ebook:Statistical Methods for Machine Learning It provides self-study tutorials on topics like:Hypothesis Tests, Correlation, Nonparametric Stats, Resampling, and much more¡¦ Skip the Academics. Just Results. Click to learn more. Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning."
mastery,2018-07-09,How to Calculate Nonparametric Rank Correlation in Python,https://machinelearningmastery.com/how-to-calculate-nonparametric-rank-correlation-in-python/,"Correlation is a measure of the association between two variables. It is easy to calculate and interpret when both variables have a well understood Gaussian distribution. When we do not know the distribution of the variables, we must use nonparametric rank correlation methods. In this tutorial, you will discover rank correlation methods for quantifying the association between variables with a non-Gaussian distribution. After completing this tutorial, you will know: Let¡¯s get started. This tutorial is divided into 4 parts; they are: Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course Correlation refers to the association between the observed values of two variables. The variables may have a positive association, meaning that as the values for one variable increase, so do the values of the other variable. The association may also be negative, meaning that as the values of one variable increase, the values of the others decrease. Finally, the association may be neutral, meaning that the variables are not associated. Correlation quantifies this association, often as a measure between the values -1 to 1 for perfectly negatively correlated and perfectly positively correlated. The calculated correlation is referred to as the ¡°correlation coefficient.¡± This correlation coefficient can then be interpreted to describe the measures. See the table below to help with interpretation the correlation coefficient. Table of Correlation Coefficient Values and Their InterpretationTaken from ¡°Nonparametric Statistics for Non-Statisticians: A Step-by-Step Approach¡±. The correlation between two variables that each have a Gaussian distribution can be calculated using standard methods such as the Pearson¡¯s correlation. This procedure cannot be used for data that does not have a Gaussian distribution. Instead, rank correlation methods must be used. Rank correlation refers to methods that quantify the association between variables using the ordinal relationship between the values rather than the specific values. Ordinal data is data that has label values and has an order or rank relationship; for example: ¡®low¡®, ¡®medium¡®, and ¡®high¡®. Rank correlation can be calculated for real-valued variables. This is done by first converting the values for each variable into rank data. This is where the values are ordered and assigned an integer rank value. Rank correlation coefficients can then be calculated in order to quantify the association between the two ranked variables. Because no distribution for the values is assumed, rank correlation methods are referred to as distribution-free correlation or nonparametric correlation. Interestingly, rank correlation measures are often used as the basis for other statistical hypothesis tests, such as determining whether two samples were likely drawn from the same (or different) population distributions. Rank correlation methods are often named after the researcher or researchers that developed the method. Four examples of rank correlation methods are as follows: In the following sections, we will take a closer look at two of the more common rank correlation methods: Spearman¡¯s and Kendall¡¯s. Before we demonstrate rank correlation methods, we must first define a test problem. In this section, we will define a simple two-variable dataset where each variable is drawn from a uniform distribution (e.g. non-Gaussian) and the values of the second variable depend on the values of the first value. Specifically, a sample of 1,000 random floating point values are drawn from a uniform distribution and scaled to the range 0 to 20. A second sample of 1,000 random floating point values are drawn from a uniform distribution between 0 and 10 and added to values in the first sample to create an association. The complete example is listed below. Running the example generates the data sample and graphs the points on a scatter plot. We can clearly see that each variable has a uniform distribution and the positive association between the variables is visible by the diagonal grouping of the points from the bottom left to the top right of the plot. Scatter Plot of Associated Variables Drawn From a Uniform Distribution Spearman¡¯s rank correlation is named for Charles Spearman. It may also be called Spearman¡¯s correlation coefficient and is denoted by the lowercase greek letter rho (p). As such, it may be referred to as Spearman¡¯s rho. This statistical method quantifies the degree to which ranked variables are associated by a monotonic function, meaning an increasing or decreasing relationship. As a statistical hypothesis test, the method assumes that the samples are uncorrelated (fail to reject H0). The Spearman rank-order correlation is a statistical procedure that is designed to measure the relationship between two variables on an ordinal scale of measurement. <U+2014> Page 124, Nonparametric Statistics for Non-Statisticians: A Step-by-Step Approach, 2009. The intuition for the Spearman¡¯s rank correlation is that it calculates a Pearson¡¯s correlation (e.g. a parametric measure of correlation) using the rank values instead of the real values. Where the Pearson¡¯s correlation is the calculation of the covariance (or expected difference of observations from the mean) between the two variables normalized by the variance or spread of both variables. Spearman¡¯s rank correlation can be calculated in Python using the spearmanr() SciPy function. The function takes two real-valued samples as arguments and returns both the correlation coefficient in the range between -1 and 1 and the p-value for interpreting the significance of the coefficient. We can demonstrate the Spearman¡¯s rank correlation on the test dataset. We know that there is a strong association between the variables in the dataset and we would expect the Spearman¡¯s test to find this association. The complete example is listed below. Running the example calculates the Spearman¡¯s correlation coefficient between the two variables in the test dataset. The statistical test reports a strong positive correlation with a value of 0.9. The p-value is close to zero, which means that the likelihood of observing the data given that the samples are uncorrelated is very unlikely (e.g. 95% confidence) and that we can reject the null hypothesis that the samples are uncorrelated. Kendall¡¯s rank correlation is named for Maurice Kendall. It is also called Kendall¡¯s correlation coefficient, and the coefficient is often referred to by the lowercase Greek letter tau (t). In turn, the test may be called Kendall¡¯s tau. The intuition for the test is that it calculates a normalized score for the number of matching or concordant rankings between the two samples. As such, the test is also referred to as Kendall¡¯s concordance test. The Kendall¡¯s rank correlation coefficient can be calculated in Python using the kendalltau() SciPy function. The test takes the two data samples as arguments and returns the correlation coefficient and the p-value. As a statistical hypothesis test, the method assumes (H0) that there is no association between the two samples. We can demonstrate the calculation on the test dataset, where we do expect a significant positive association to be reported. The complete example is listed below. Running the example calculates the Kendall¡¯s correlation coefficient as 0.7, which is highly correlated. The p-value is close to zero (and printed as zero), as with the Spearman¡¯s test, meaning that we can confidently reject the null hypothesis that the samples are uncorrelated. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered rank correlation methods for quantifying the association between variables with a non-Gaussian distribution. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦by writing lines of code in python Discover how in my new Ebook:Statistical Methods for Machine Learning It provides self-study tutorials on topics like:Hypothesis Tests, Correlation, Nonparametric Stats, Resampling, and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hello Jason! I¡¯m starting to make some projects using machine learning and i have a doubt. Can I use these ranking correlations to select atributes for a machine learning project?
What is the best for this use? Thanks! Your blog is helping me a lot to get improved at the machine learning area! Yes, try it.  There are many ways to select features for ML, try a few and go with the method that results in a model with the best performance.  There is no best, instead, there are many different methods to try for your problem. Thank you! Hi Jason!
i got really lots of information from your articales. do you have examples or articles about reinforcement? Not at this stage, perhaps in the future. Hi Jason, Your articles about machine learning and data Science is helping me improve on my data coding skills and data Science generally.
Many thanks Yusuf I¡¯m glad to hear that. Hey Jason, Keep up the good work man.
I love ¡¯em your blogs.
It helps me a lot. Thanks,
Yaser Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning."
vidhya,2018-07-12,13 Common Mistakes Amateur Data Scientists Make and How to Avoid Them?,https://www.analyticsvidhya.com/blog/2018/07/13-common-mistakes-aspiring-fresher-data-scientists-make-how-to-avoid-them/,"So you¡¯ve decided data science is the field for you. More and more businesses are becoming data driven, the world is increasingly becoming more connected and looks like every business will need a data science practice. So, the demand for data scientists is huge. Even better,<U+00A0>everyone acknowledges the shortfall of talent in the industry. However, becoming a data scientist does not come easy. It needs a mix of problem solving, structured thinking, coding and various technical skills among others to be truly successful. If you are from a non-technical and non-mathematical background, there¡¯s a good chance a lot of your learning happens through books and video courses. Most of these resources don¡¯t teach you what the industry is looking for in a data scientist. This is one of the reasons why aspiring data scientists are struggling to bridge the gap between self education and real-world jobs. In this article, I discuss the top mistakes amateur data scientists make (I have made some of them myself). I have also provided resources wherever applicable with the aim of helping you avoid these pitfalls on your data science journey. Source: Cognitive Class <U+2013> YouTube As I mentioned in my article on AV¡¯s practice problems<U+00A0><U+2013> it¡¯s good to get a grasp of the theory behind machine learning techniques. But if you don¡¯t apply them, they are only theoretical concepts. When I started out learning data science, I made the same mistake <U+2013> I studied books and online courses but didn¡¯t always apply them to solve a problem. So when I was faced with a challenge or problem where I had the chance to apply all that I had learned, I couldn¡¯t remember half of it! There¡¯s so much to learn <U+2013> algorithms, derivations, research papers, etc. There¡¯s a high chance you¡¯ll lose your motivation halfway through and give up. I have personally seen this happen to a lot of people who attempt to enter this field. It¡¯s imperative that your learning process should be a healthy balance between theoretical and practical. As soon as you learn a concept, head over to Google and find a dataset or problem where you can use it. You¡¯ll find that you are retaining that concept way better than before. You can also use AV¡¯s DataHack platform to take part in practice problems and ongoing competitions. You will have to accept that you cannot learn everything in one go. Fill in the gaps as you practice and you will learn a whole lot more! Source: Imperial College London <U+2013> YouTube The majority of folks who want to become a data scientist are inspired by videos of robots, or awesome predictive models, and in some cases even the high salaries. Sadly (sorry to disappoint!), there is a long road you need to travel, before you reach there. You should get to know how techniques work before you apply them in a problem.<U+00A0>Learning this will help you understand how an algorithm works, what you can do to fine tune it, and will also help you build on existing techniques. Mathematics plays an important role here so it¡¯s always helpful to know certain concepts. In a day-to-day corporate data scientist role you may not need to know advanced calculus, but having a high-level overview definitely helps. In case you have a curious mind, or want to get into a research role, the four key components you need to know before diving into core machine learning are: Just as a house is built brick-by-brick, a data scientist is also the sum of all the individual parts. There are tons of resources out there which will help you learn these topics. I have mentioned one resource of each topic below which should get you started: You can also check out Analytics Vidhya¡¯s ¡®Introduction to Data Science¡® course which includes a comprehensive module on statistics and probability. Source: CIO.com Ah, the pet-peeve of hiring managers and recruiters. Ever since data science became ultra popular, certifications and degrees have cropped up just about everywhere. A glance through my LinkedIn feed shows up at least 5 certification images proudly being displayed. While achieving that certification is no easy feat, relying solely on it is a recipe for disaster. There are too many of these courses online being poured over and completed by thousands upon thousands of aspiring data scientists. If they ever added a unique value to your data science CV, that is no longer the case. Hiring managers do not care much for these pieces of paper <U+2013> they place far more emphasis on your knowledge, and how you¡¯ve applied it in real-life practical situations. This is because dealing with clients, handling deadlines, understanding how a data science project lifecycle works, how to design your model to fit into the existing business framework <U+2013> these are just some of the things you will need to know to succeed as a data scientist. Just a certification or degree will not qualify you for it. Don¡¯t get me wrong <U+2013> certifications are valuable, but only when you apply that knowledge outside the classroom and put it out in the open. Use real-world datasets and whatever analysis you do, make sure you write about it. Create your own blog, post it on LinkedIn, and ask for feedback from the community. This shows that you are willing to learn and are flexible enough to ask for suggestions and work them into your projects. You should be open to the idea of internships (regardless of your experience level). You will learn a lot about how a data science team works, which will benefit you when you sit for another interview. If you¡¯re looking for that next project, you¡¯ve come to the right place. We have an awesome list of projects here divided by the degree of difficulty. Get started NOW. This is one of the biggest misconceptions aspiring data scientists have these days. Competitions and hackathons provide us with datasets that are clean and spotless (okay <U+2013> I went a little overboard, but you get the hang of it). You download them, and start working on the problem. Even those datasets that have columns with missing values don¡¯t require you to work your brain cells off <U+2013> figure out an imputation technique and fill in the blanks. Unfortunately, real-world projects don¡¯t work like that. There is an end-to-end pipeline which involves working with a bunch of people. You will almost always have to work with messy and unclean data. The old saying about spending 70-80% of your time just collecting and cleaning data is true. It¡¯s the grueling part and you will (most likely) not enjoy but it¡¯s something that eventually becomes part of a routine. Also, and we will cover this in more detail in the next point, the simpler model will win precedence over any complex stacked ensemble model. Accuracy isn¡¯t always the end goal, and this is one of the most contrasting things you¡¯ll learn on the job. One of the key factors to negate this misunderstanding is, ironically, experience. The more experience you gain (internships help a lot in this case), the better you¡¯ll be able to distinguish between the two. This is where social media comes in handy <U+2013> reach out to data scientists and ask them their experience. Additionally, I suggest going through this Quora thread where data scientists from around the world provide their input on this exact question. Getting a good score on a competition leaderboard is excellent for measuring your learning progress, but interviewers will want to know how you can optimize your algorithm for impact, not for optimization. Learn about how a data science project works, what different types of roles a team has (from a data engineer to a data architect), and structure your answer in that sense. Go through this LinkedIn post which explains the standard methodology for analytical models. Source: Design Shack As mentioned above, accuracy isn¡¯t always what the business is after. Sure a model that predicts loan default with 95% accuracy is good, but if you can¡¯t explain how the model got there, which features led it there, and what your thinking was when building the model, your client will reject it. You will rarely, if ever, find a deep neural network being used in commercial applications. It¡¯s just not possible to explain to the client how a neural network (let alone a deep one) worked with hidden layers, convolutions layers, etc. The first preference is, and will always be, on ensuring that we are able to understand what¡¯s going on underneath the model. If you can¡¯t tell whether age, or number of family members, or previous credit history went into rejecting a loan application, how will the business run? Another key aspect is whether your model will fit within the organization¡¯s existing framework. Using 10 different types of tools and libraries will fail spectacularly if the production environment cannot support it. You will have to redesign and retrain the model from scratch with a simpler approach. The best way to prevent yourself from making this mistake is speaking to people working in the industry. There is no better teacher than experience. Pick a domain (finance, HR, marketing, sales, operations, etc.) and reach out to people to understand how their project works. Apart from that, practice making simpler models and then explaining them to non-technical people. Then add complexity to your model and keep doing this until even you don¡¯t understand what¡¯s going on beneath. This will teach you when to stop, and why simple models are always given preference in real-life applications. If you have done this before, you will know what I¡¯m talking about. If your resume currently has this problem, rectify it immediately! You may know a plethora of techniques and tools but simply listing them down will turn off potential hiring managers. Your resume is a profile of what you have accomplished and how you did it <U+2013> not a list of things to simply jot down. When a recruiter looks at your resume, he/she wants to understand your background and what all you have accomplished in a neat and summarized manner. If half the page is filled with vague data science terms like linear regression, XGBoost, LightGBM, without any explanation, your resume might not clear the screening round. The simplest way to eliminate resume clutter is to use bullet points. Only list the techniques which you have used to accomplish something (could be a project or a competition). Write a line about how you used it <U+2013> this helps the recruiter understand your thinking. When you¡¯re applying for fresher or entry-level jobs, your resume needs to reflect what potential impact you can add to the business. You will be applying to roles in different domains so perhaps having a set template will help <U+2013> just change the story to relfect your interest in that particular industry. This article by Kunal Jain is an excellent resource for preparing an outstanding CV for data science roles. Source: Data Science Lab Let¡¯s take an example to understand why this is a mistake. Imagine you¡¯ve been given a dataset on house prices and you need to predict the value of future real estate. There are over 200 variables, including number of buildings, rooms, number of tenants, family size, size of the courtyard, whether faucets are available, etc. There¡¯s a good chance you might not be aware of what some variables mean. You can still build a model with a good accuracy, but you have no idea why a certain variable was dropped. As it turns out, that variable was a crucial element in a real-world scenario. It¡¯s a calamitous mistake. Having a solid knowledge of tools and libraries is excellent, but it will only take you so far. Combining that knowledge with the business problem posed by the domain is where a true data scientist steps in. You should be aware of at least the basic challenges in the industry you are interested in (or are applying to). There are plenty of options to explore here: Data visualization is such a wonderful facet of data science, yet a lot of aspiring data scientists prefer to skim over it and get to the model building stage. This approach might work out in competitions, but is bound to fail in a real job. Understanding the data you¡¯re given is the single most important thing you will do, and your model¡¯s results will reflect that. By spending time on getting to know the dataset and trying out different charts, you will gain a deeper knowledge of the challenge or problem you¡¯ve been tasked with solving. You¡¯d be surprised to know how much insight you can gain just by doing this! Pattern and trends emerge, stories are told and the best part? Visualizations are the best way to present your findings to the client. As a data scientist, you need to be inherently curious. It¡¯s one of the great things about data science <U+2013> the more curious you are, the more questions you¡¯ll ask. This leads to a much better understanding of the data you are given and also helps solve problems you didn¡¯t know existed in the first place! Practice! Next time you work on a dataset, spend more time on this step. You will be stunned at the amount of insight it will generate for you. Ask questions! Ask your manager, ask domain experts, search for solutions on the internet and if you don¡¯t find any, ask on social media. So many options! To help you get started, I have mentioned a few resources below which you should refer to: Source: MindMatters.co.in Structured thinking helps a data scientist in many ways: There are many more reasons why having a structured thinking mindset helps. As you can imagine, not having a structured thinking mindset is counter intuitive. Your work and approach to a problem will be haphazard, you will lose track of your own steps when faced with a complex problem, etc. When you go for a data science interview, you will inevitably be given a case study, guess estimate and puzzle problem(s). Because of the pressure filled atmosphere in an interview room and the time constraint, the interviewer looks at how well you structure your thoughts to arrive at a final result. In many cases, this can be a deal breaker or deal sealer for getting the job. You can acquire a structured thinking mindset through simple training and and a disciplined approach. I have listed a few articles below which will help you get started on this crucial aspect: I¡¯ve seen this one too many times. Because of the dilemma and the unique features each tool offers, people tend to attempt learning all the tools at once. This is a bad idea <U+2013> you will end up mastering none of them. Tools are a means to perform data science, they are not the end goal. Pick one tool and stick to it until you have mastery over it. If you¡¯ve already started learning R, then don¡¯t be tempted by Python (yet). Stick with R, learn it end-to-end and only then try to incorporate another tool into your skillset. You will learn more with this approach. Each tool has a great user community which you can tap into whenever you get stuck. Use our discussion forum to ask questions, search stuff online, and don¡¯t give up. The aim is to learn data science through the tool, not the tool through data science. If you are still undecided on which tool you should use, check out this wonderful article which lists down each tool¡¯s advantages and shortcomings (it also includes SAS in case you are interested in that). Source: The Brooks Group This one applies to all data scientists, not just freshers. We have a tendency to get distracted easily. We study for a period of time (say, a month), then we give it a break for the next 2 months. Trying to get back into the groove of things after that is a nightmare. Most of the earlier concepts are forgotten, notes are lost and it feels like we just wasted the last few months. I have personally experienced this as well. Due to various things we have going on, we find excuses and reasons not to get back to studying. But this is eventually our loss <U+2013> if data science was as easy as opening a text book and cramming everything, everyone would be a data scientist today. It demands consistent effort and learning, something which people don¡¯t appreciate until it¡¯s too late. Set goals for yourself. Map out a time table and stick it on your wall. Plan how and what you want to study and set deadlines for yourself. For example, when I wanted to learn about neural networks, I gave myself a couple of weeks and then tested what I¡¯d learned by competing in a hackathon. You have decided to become a data scientist so you should be ready to put in the hours. If you continually keep finding excuses not to study, this might not be the field for you. Source: Interview Skills Consulting This is a combination of a few things we¡¯ve seen in the above points. Aspiring data scientists tend to shy away from posting their analysis online in fear of being criticized. But if you don¡¯t receive feedback from the community, you will not grow as a data scientist. Data science is a field where discussions, ideas and brainstorming is of utter importance. You cannot sit in a silo and work <U+2013> you need to collaborate and understand other data scientists¡¯ perspective. Similarly, people don¡¯t take part in competitions because they feel they won¡¯t win. This is a wrong mindset! You participate in these competitions to learn, not to win. Winning is a bonus, learning is the goal. It¡¯s fairly straightforward <U+2013> start participating in discussions and competitions! It¡¯s okay to not come in the top 5%. If you learn a new technique out of the whole thing, you have won in your own right. Source: Jim Harvey Communications skills are one of the most under-rated and least talked about aspects a data scientist absolutely MUST possess. I am yet to come across a course that places a solid emphasis on this. You can learn all the latest techniques, master multiple tools and make the best graphs, but if you cannot explain your analysis to your client, you will fail as a data scientist. And not just clients, you will also be working with team members who are not well versed with data science <U+2013> IT, HR, finance, operations, etc.<U+00A0>You can be sure that the interviewer will be monitoring this aspect throughout. Assume you¡¯ve built a credit risk model using logistic regression. As a thought exercise, take a minute to think how you would explain to a non-technical person how you came to the final conclusion. If you used any technical words, you need to work on this ASAP! Most data scientists these days are coming from a computer science background so I understand this can be a daunting skill to acquire. But to become a successful data scientist and climb up the ladder, you don¡¯t have a choice but to polish this part of your personality. One of the things I find most helpful is explaining data science terms to a non-technical person. It helps me gauge how well I have articulated the problem. If you¡¯re working in a small to medium-sized company, find a person in the marketing or sales department and do this exercise with them. It will help you immensely in the long term. There are plenty of free resources available on the internet to get you started but remember, practice is key when it comes to soft skills. Ensure you start doing this TODAY. This is most definitely NOT an exhaustive list <U+2013> there are plenty of other mistakes aspiring data scientists tend to make. But these were the most common ones I have seen and my aim, as stated earlier, is to help others avoid it (as much as possible). I would love to hear your thoughts on these pointers, and also your personal experience with similar problems. Use the comments section below to let me know! Great observations Pranav and the way you have penned down the same is just amazing. Keep up the good work. Thank you pranav¡¦ Now this will help me a lot This article is too helpful for pioneers Great Thank you very much for this article Really awesome article, thanks! Excellent article. I will revisit again and again..! Really very helpful article.. Thanks pranav Nicely written article, very useful tips, especially no. 10 and 11, I have been the victim of both. Excellent. I see number 2 constantly! Good summary and nicely written! Thanks"
vidhya,2018-07-09,"DataHack Radio #4 <U+2013> Data Privacy, Women in Data Science and More with Carla Gentry",https://www.analyticsvidhya.com/blog/2018/07/datahack-radio-episode-4-carla-gentry/,"Carla Gentry is one of most popular social media influencers in the data science field. She has over 300,000 followers on LinkedIn and 48k followers on Twitter. Her experience in this field is unparalleled and we are grateful for leaders like her who consistently give back to the community. She has been a regular reader of Analytics Vidhya¡¯s articles for quite a while now. It was a pleasure to have her appear on the podcast and to hear her views on data privacy, how the domain has changed in the last few years, her advice for women in data science, and a whole host of other topics. This article contains highlights of Carla¡¯s conversation with Kunal Jain. You can listen to the podcast by clicking on the above SoundCloud link or on our iTunes channel. Happy listening! You can subscribe to DataHack Radio and listen to this, and all previous episodes, on any of the below platforms: Carla holds a number of degrees including one in advanced economics, one in advanced mathematics, among others. Right after college she got an internship at one of the few econometrics firms in the United States. There she worked with terabytes of data (this was well before ¡®Big Data¡¯ was a buzzword). Her experience started with tools and platforms like SAS, Pico, etc. Her role was working with credit card data but the nature of the work was such that it became monotonous after a while. From there, she moved to the Weinstein Organization as a Senior Analyst and Data Specialist. Here Carla¡¯s role expanded to include direct marketing experience. This was followed by a year at the University of Chicago Booth School of Business as a Research Support Analyst where she taught Ph.D students how to work with data, how to do data mining, etc. Carla then moved on to work as the Marketing Information Manager at Career Education Corporation for the next 4 years. Post that she spent the next 3.5 years in the marketing and data field at PromoWorks, Tandus and Area203. She is now the owner and data scientist at Analytical Solution, where she has worked with clients like Kellogg, Johnson & Johnson, among various other organizations. Carla considers data mining a critical aspect in any data driven project. It helps you understand trends, see patterns, interpret reasons why a person was denied a loan or credit card, etc. It¡¯s at the core of the business and should be respected as such. But she stressed on the importance of privacy and the need to be clear with your clients and customers about where you are going to use their data, for what purposes, and how it might impact them (if at all). GDPR has of course changed the game in Europe with regards to being transparent with users but Carla feels everyone, regardless of laws, should have this as a best practice. With the amount of data that¡¯s being generated in the world, from websites to social media, it¡¯s critical to have that level of sensitivity. Data scientists have a responsibility to be unbiased, have integrity and use their experience to add a positive background to the dataset, rather than let their feelings cloud the model building exercise. Carla recalled that if a business had terabytes of data in the 90s, running a program on that was next to impossible because the mainframe would have crashed. Now a mainframe isn¡¯t even required! If you have a good database architecture set up, you can access millions and billions of rows in a fraction of the time it used to take previously. COBOL, PASCAL, C++, SAS, Mathematica, MATLAB <U+2013> these were the only programs available back when Carla started her journey. Of course now we have much more robust tools like R, Visual Studio (SQL), IBM¡¯s suite of tools, etc. One of the biggest reasons why the older programs have been phased out is because of their inability to handle gigantic datasets, which the new tools can. Of course with the rise of this data wave, and the advent of the digital era, the number of hackers and cyber thieves has also risen. So as things have gotten better in many ways, they have also gotten worse when it comes to security of your data. Carla expects more laws like GDPR to come to action in the next few years that will dictate how organizations collect and deal with your data. She has a warning for businesses that abuse data <U+2013> people will leave and look for ways to go incognito, which will leave your business with no data at all. ¡°We have got to get rid of the thinking that it¡¯s always been this way, so it should stay that way.¡± Carla is a champion of women in data science. She strongly believes that in order to incorporate more females into this field, the change has to start from the top. The CEO should have an obligation to encourage diversity into the organization by going to the HR department and digging deeper to understand the percentage of women in the company, and how to further improve upon that. Her advice to aspiring female data scientists was to the point <U+2013> stand your ground, be confident in yourself, find mentors, keep going and keep learning. You will find the perfect fit for you as long as you continue to believe in yourself and your abilities. Carla is an avid social media user and a HUGE influencer, especially in the data science field. It¡¯s very time consuming (2-3 hours per day at times) but if what she shares helps even one person, she definitely considers it worth that investment. She feels it¡¯s her responsibility to give back to the community, given that she has gotten so much from it over the years. There are some awesome rapid fire questions at the end of the podcast <U+2013> ensure you listen to those as well! *P.S. <U+2013> All views expressed by the guests on DataHack Radio are their own, and not of Analytics Vidhya."
