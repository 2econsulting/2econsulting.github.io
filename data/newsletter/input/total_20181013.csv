"","site","date","headline","url_address","text","keyword"
"1","datacamp",2018-10-11,"Our Content Library Growth in the Past 3 Months","https://www.datacamp.com/community/blog/content-q3","In this post, we want to give you a more detailed overview of all what happened in the past 3 months at DataCamp related to the release of new content material, content quality improvements and getting to better feedback messages. At DataCamp we work according to the Learn-Practice-Apply model. You learn via courses, you practice via our practice modes, and you apply via our interactive projects. In all 3 areas, we have expanded our content library significantly in the past 3 months, having now over 1000 hours of learning material available to our learners. In the third quarter of the year, our content teams broke all records in terms of new course launches: 42 new courses in total!  Some exciting milestones we passed along the way: These new courses have something for everyone, covering topics such as machine learning, data visualizations, reporting, and much more. This quarter our internal curriculum team also made considerable progress on a more diverse instructor field (but we are not there yet). Today 43 of our courses are taught by women and/or non-binary, and within our biggest curriculum (R) 30% of instructors are women and/or non-binary. Increasing the diversity of our instructor base (in all dimensions) remains a work in progress, but we are committed to continuing our efforts here. If you want to take advantage of all these new courses hurry to our course library. This quarter we have been directing our efforts towards (i) making our practice environment mobile first, and (ii) on making practice a much bigger part of your learning experience. As a result, our practice library has grown with 10 new practice modules, all linked to our most popular courses on the platform.      Want to experience all these new practice modes? Download or open the DataCamp mobile app and start practicing.   Where Courses teach you new data science skills and Practice Mode helps you sharpen them, building Projects gives you hands-on experience solving real-world problems. In the past 3 months, our projects library has grown with 13 new R and Python projects, bringing the total project library to 35+. See how best to apply your skills in our projects library.   At DataCamp, we pride ourselves on having the best platform and the best curriculum for learning data science and therefore we put a lot of effort into ensuring that the quality of our content materials stay at the highest level. To do this, our newly formed Content Quality team works with the instructors to improve our existing courses.   In the past 3 months, our content quality team has reworked in total over 50 existing courses representing  100s of exercises. Some of the major improvements that were done are: With the increase of the content library, we see content quality and course maintenance taking up a growing role over time in our content creation process.    Our past 3 months were great. We never delivered more content to our learners. However, we obviously will not stop here. So what can you expect in the next quarter?   Stay tuned! Interested in creating your own learning material? Apply here.
Interested in joining our team? Apply here. New DataCamp Courses Launched Quarter 3 2018: New DataCamp Projects Launched Quarter 3 2018:","Keyword(freq): project(6), effort(2), improvement(2), instructor(2), learner(2), mode(2), skill(2), woman(2), dimension(1), exercise(1)"
"2","datacamp",2018-10-10,"How is content created at DataCamp? ","https://www.datacamp.com/community/blog/how-content-is-created-datacamp","Do you ever wonder how these instructors go about creating their courses and projects, though? If you think it would be difficult to create a consistent learning experience with instructors that have all different kinds of backgrounds, you're right. An academic's instincts for creating content are different than an industry expert's are different than an open-source software developer's. There are also cultural, generational, and programming language backgrounds to consider. To solve this problem, DataCamp has a Content team in-house comprised of Instructor Recruiters, Curriculum Leads, Content Developers, and Content Quality Analysts. Here's what each of those roles entails. If you're interested in becoming an instructor, you can apply here for courses and here for projects. DataCamp instructors are subject matter experts with a passion for teaching, excellent communication skills, and a strong network of their own. If someone in the community fits this profile, it is the Instructor Recruiter's job to make sure they know who we are and why they should consider working with us. If their skills align with our current curriculum goals, an Instructor Recruiter invites them to apply for an instructor role, leads them through the application process, and assesses their application alongside our Curriculum Leads. Once approved, they also guide instructors through our contracting and onboarding processes. Meet Jen, an Instructor Recruiter at DataCamp. Once a contract is signed and onboarded with the help of one of our Instructor Recruiters, instructors are handed off to one of our Curriculum Leads (CL). The CL's primary responsibility is to work closely with instructors to create detailed specifications for new courses. The initial planning phase, if you will. They train instructors on our state-of-the-art authoring tools and best practices, as well as set and hold instructors accountable for deliverable deadlines. A typical day for a CL is filled with check-ins with instructors as they manage 15-20 courses at a time. A CL also owns the data science curriculum roadmap for their portion of the curriculum (Python, R, SQL, Spreadsheets, etc.) and uses internal and external data to make informed curriculum decisions. Meet Chester, Hillary, and Mona, Curriculum Leads for courses. Following the Curriculum Lead's initial planning phase, it is the responsibility of the Content Developer (CD) lead instructors through the course development process from post-planning to launch. Their first touch point with instructors is to provide critical feedback on course specifications. CDs then act as reviewers, thought partners, and (most importantly) advocates for our students. They are the main source of review on the text and code our instructors write, delivering feedback, primarily in writing via GitHub Issue, to make courses as interactive and engaging as possible. They keep their 15-20 instructors on schedule and act as a director during the filming of their course's videos. CDs also write automated tests in Python and R to provide personalized feedback for students and make data-driven suggestions for course improvements based on student feedback and engagement data. Meet Adrian, Amy, Becca, David, Hadrien, Sara, and Sumedh, Content Developers, and Yashas, Content Development Lead at DataCamp. DataCamp prides itself on having the best quality data science curriculum around. The Content Quality team makes sure that statement stays true. A Content Quality Analyst's role is to work with our instructors to maintain and improve their content, based upon feedback from students and course performance data. They review text and code to ensure they are high quality, and write new automated tests in Python and R to provide personalized feedback for students. Meet Richie and Kaelen, Head of Content Quality and Content Quality Analyst. Practice at DataCamp lives in the browser and in a mobile app, where you Practice the skills you Learned in courses. Practice is a relatively new product at DataCamp with one Curriculum Lead that performs all of the roles above. DataCamp projects are where you Apply the several of skills you Learned in courses in a real-world, end-to-end data analysis. Like practice, the projects team is currently a team of one: a Curriculum Lead that performs all of the roles above. Meet David (me!), Curriculum Lead for projects. Though instructors tend to not interact with them, the Content team is fearlessly led by a Curriculum Director and a VP of Content. Meet Mari and Martijn (who is also a co-founder of the company). And that's the team! Because of these folks, instructors get first-class treatment as they are guided through the content creation process. For more information on the benefits of becoming an instructor, read the FAQs here. If you want to become an instructor, apply here to create a course and here to create a project. DataCamp's Learn, Practice, Apply philosophy.","Keyword(freq): instructor(16), project(5), skill(4), student(4), role(3), background(2), developer(2), perform(2), recruiter(2), specification(2)"
"3","datacamp",2018-10-08,"Andrew Gelman  discusses election forecasting and polling. (Transcript)","https://www.datacamp.com/community/blog/election-forecasting-polling","Here is the podcast link. Hugo:               Hi there, Andy, and welcome to DataFramed. Andrew:           Hello. Hugo:               Such a pleasure to have you on the show and I'm really excited to have you here today to talk about polling and election forecasting, but before that I'd like to find out a bit about you just to set the scene. My first question is, what are you known for in the data community? Andrew:           What is the data community? Hugo:               The data community, I think, is the rough broad collection of people working with data analytic techniques, working with data science, and working with large and messy datasets these days. Andrew:           I'm probably best known as one the of authors of the book Bayesian Data Analysis, which came out in 1995 but then we've had two more editions since then. So that was a book, I like to think of it as the first applied Bayesian Statistics book. So, a lot of people who have gotten into Bayesian Statistics have gotten there through our book, or used the book as a reference. Hugo:               Great. And maybe you can tell us a bit more about Bayesian Statistics in general, just by way of introduction. And I suppose there are two types of statistics in general that we talk about which are Bayesian Statistics and Frequentist Statistics, right? Andrew:           So, in Bayesian Statistics, all of your unknowns, all of your unknown parameters and predictions are associated with a probability distribution. So the way you solve a problem using Bayesian Inference is, you put all of your knowns and all of your unknowns into a joint probability distribution and then use the laws of probability to make statements about the unknowns, given the knowns. Hugo:               And so, you've actually done a lot of work on implementing a lot of Bayesian techniques in a language called Stan, right? In fact, a language in which, as you mentioned, probability distributions are the core objects of the Bayesian Statistics. I suppose distributions are first class citizens in Stan and other what are known as probabilistic programming languages, right? Andrew:           Right. Exactly. So, I can give you a simple example. Suppose you're doing an educational innovation, and you want to look at students' test scores after the intervention. So you start with basic statistical ideas, you fit a linear regression model, say predicting the test score given their pre-test score, and given an indicator for whether they got the treatment or the control. So that's regression, that's not Bayesian yet. It's just statistical modeling. Andrew:           It can become more or less difficult, it can become non-linear, you can control for more predictors, not just your pre-test but all sorts of student characteristics. There's a million things you can do. What makes it Bayesian, is that this regression model has parameters, like the effect of a treatment, how much the post-test is predictable from the pre-test. There are parameters for how big your variance is, shapes of distributions, whatever. Andrew:           All of those parameters are assigned a probability distribution. We call it a prior distribution. So, you put that all in Stan, along with your data, and then it gives you a posterior distribution which represents your uncertainty about the parameters after seeing the data. Hugo:               And so, Bayesian data analysis and Bayesian inference, I think, historically, we've seen them to be incredibly powerful but maybe haven't been adopted as widely as Bayesians would have liked. I think a lot of learners, a lot of people learning data science and statistical inference, may find Bayesian data analysis even a bit scary. Firstly, is this right? Secondly, why is that the case and how can we correct that? Andrew:           In Bayesian statistics you kind of make a deal with the devil. You assume a probability model, so you make a big assumption, and having done that, you can make predictions about just about anything. So, I think, maybe it's a little scary in some way because it's so powerful and so easy to use that it's like those 3D printers, people are afraid of them because they can print anything. So, in Bayesian statistics, even if you have weak data, you can get inferences and the inferences then become driven by your prior distribution. There's a saying we have in Bayesian statistics, with great power come great responsibility. What that means is that, in Bayesian inference, it's very important that you check the fit of your model and check the reasonableness of your model. Andrew:           So, in that sense, there's kind of two approaches to statistics. One approach is to make very minimal assumptions, and the other is to make maximal assumptions. The Bayesian approach is really you make maximal assumptions. What I like to say is you create a paper trail going from your assumptions to your conclusions, then if your conclusions don't make sense, you look at what was wrong with your assumptions. What was wrong might be your model for your data. Maybe your sampling was biased and you didn't recognize that. But, whatever it is, somewhere you need to go back and forth, you need to communicate between your assumptions and conclusions. Andrew:           A lot of people would rather work without assumptions and sometimes you can, we can talk about examples, but basically, if you have a clean problem and good data then you don't need to work with a lot of assumptions, except for the assumption that you have good data. As the data quality becomes worse, as your questions become more difficult to answer, you need to put more assumptions in and then Bayesian inference becomes more useful. Hugo:               Absolutely and one of the great things that you mentioned in there was the ability to check your model after the fact and we have enough computational power to do that these days, right? So, for example, once we have our model, we can simulate what the data would actually look like and compare that to the data we actually saw. Andrew:           Exactly. We call that posterior predictive checking. People have been doing this for a long time, they were just not under that name. There was a book from the 1950s by the statistician, Frederick Mosteller, where they were analyzing data from an experiment, it was called a stochastic learning experiment, they were actually giving electric shocks to dogs in cages and seeing how long it took for the dogs to figure out that the shock was coming. So, they had this probabilistic model and then after fitting the model, they simulated fake data and compared the fake data to the real data. Andrew:           In the 1970s, the statistician, Brian Ripley, who was working on spatial statistics, and since has become very famous for his involvement with R, Brian Ripley was fitting spatial models and again did the same thing. He had a model that seemed kind of reasonable, he stimulated replicated data from the model and it didn't look like the real data and that then inspired him to expand his model. So, it was examples like that that motivated us to formalize this idea of model checking. I think people have always checked their model but there's been a sense in which it has been outside the system. It's not that people are embarrassed to check their model but it's almost like people think, ""I'm a good person. I'm a good citizen. So, I check my model."" And it hadn't been formally encompassed into statistics and in the Bayesian framework, you can do that. You can put model checking right in the middle of the process and not feel that its some external thing you're doing. Hugo:               I'm glad you mentioned that cause that was my next point that it is actually baked in to the Bayesian workflow, the idea of model checking. Andrew:           Yes. Hugo:               So, this was Bayesian data analysis. Are there any other things that you're known for in the data community? Andrew:           I'd like to say that I'm known for statistical graphics because in the early 2000s, I did a lot of work trying to integrate statistical graphics with statistical analytics. So, traditionally, there's this idea that exploratory data analysis is looking at your data and finding interesting patterns. Confirmatory data analysis is like crunching the numbers and getting your p values. Exploratory data analysis, again, was kind of on the outside of statistics. Its proponents would often say, ""Forget all this silly modeling stuff, let's just jump to looking at the data."" Andrew:           But, what's interesting, is if you think carefully, exploratory data analysis is finding the unexpected. So, to say I'm finding the unexpected means that's relative to the expected. In fact, exploratory analysis is most powerful when it's tied to models. So, I think exploratory data analysis and statistical graphics, and learning new things from data from visualizations is actually fitting in very well with Bayesian inference and formal statistical modeling. Because you fit the model, the better your model is the more you learn from its falsification. Andrew:           So, way back, Copernicus had the model that the planets were going in circular orbits around the sun and it's easy to falsify that. But then, Kepler moved to the elliptical orbits, so falsifying that became more interesting and so forth. So, every time we have a model, that motivates more sophisticated graphics which allows us to learn more. Hugo:               So, how did you get in to data science and statistics, originally? Andrew:           I always was good at math, ever since I was a baby, and then, I've written about this actually, but anyway, when I was in high school, I did the math Olympiad training program, I found that there were people better at that than I was. We had a very naive view back then, so we didn't know about applied math, we just knew about this thing called math, and we thought ability was uni-dimensional. But anyway, I went to college and studied physics and math and I didn't want to be a pure theoretician. I just felt I wasn't good enough to make useful contributions in that way. I first took a probability class because it was in the evening, it fit my schedule. Andrew:           So, I took probability and stochastic processes and then took statistics and I really liked that. In statistics, there's kind of a continuous connection between everything that I care about. So, there's a connection between things that I can do, like mathematics, and also things like politics, public health, economics, sociology, all those things. There's kind of a continuous thread from these qualitative thoughts about what's going on in our country, what's going on in the world, how do people learn, all sort of things like that, through qualitative thinking, statistical modeling, mathematical analysis, programming, all those things. So, it was really perfect for me. Andrew:           I sometimes think that statistics should be called mathematical engineering. They have electrical engineering and mechanical engineering and statistics is mathematical engineering. Hugo:               I like that and something that you hinted in there or spoke to directly is that it is this marriage of your aptitude and mathematical skills but also your serious deep interest in the political and social sciences. Andrew:           Yeah. In college, I minored in political science and so I found that very interesting. Political science is a funny field because you don't make progress in the same way you do in a technical field. You can say technically, we can do all sorts of things that Gauss couldn't do, whatever, I'm sure he could figure it out when he saw it, but we just know stuff they didn't know. In politics, what do we know that Hobbes didn't know? Well, it's hard to say. A lot of specific things like the size of the incumbency advantage and so forth, but it's a little bit different. It's more like something like architecture. We have buildings now but you're just building things that serve current purposes then maybe the principle of the technology changes. But the general principles aren¡¯t changing. Hugo:               So, before we get in to polling and election forecasting, I just want to speak more generally to data science and statistics. I'm just wondering, it's 2018, moving forward from now, what do you think the biggest challenges facing data science and statistics as disciplines are? Andrew:           Well, speaking generically, I think there are three challenges of statistical inference. The first is generalization from samples to population and that's a problem that's associated with survey sampling but actually arises in nearly every application of statistical inference. People sometimes say, ""Wait, I have data on the 50 states. That's the population. We're not gonna have a 51st state any time soon."" Even then, I would respond, ""Okay, you have data from the 50 states last year and the last 10 years, what you're interested in is the 50 states next year."" So, there's always some generalization that's involved. So, ideas of statistical sampling always come up. Andrew:           The second fundamental challenge of statistics is generalizing from the control group to the treatment group. Much of the time we're interested in the effect of some treatment or intervention and obviously things like drugs, or educational interventions, or business decisions, but all sorts of social science things. Whenever you ask why is something happening, you're implicitly asking what would happen if I change something. With rare exceptions, we don't have a matched control and treatment group. Typically the people who you can do something to are different from the people who didn't get the treatment, and so some adjustment needs to be made. Andrew:           The third is generalizing from observed measurements to the underlying constructs of interest. So, that's most obvious in something like educational testing. You want to know ability but what you get is test score. So, we spend a lot of time designing instruments, designing survey questions, lab measurements. What those people at Theranos, those fraudulent blood testing people, that was all about measurement. So, when you talk about challenges, I think those are the old challenges and they remain the new challenges. Big data tend to be messy data. So, it's not a random sample, it's convenience sample, it's an opt-in sample. You don't have control and treatment group, people choose their own decisions on what to do. Often, you don't have careful measurements of what you care about, you often just have data that are available from another source which you're trying to adapt. Andrew:           For that reason, if you want to get good predictions and sensible answers, and learn, you need to adjust for differences between sample and population. You need to adjust for differences between control and treatment group and you need to model the connection between what you care about and what your measurement is. All that can take a lot of modeling so, typically, we say that you either get good data, or good model, or you have to have a mixture of both. You have to do a little bit of data, a little bit of work, you have to do work on data collection, you have to also work on the model. So, if you have big data and you need big model, then that's going to require a lot of computation and that's going to be expensive. So, you need algorithms for fitting models, approximately fitting models. We have some sort of good things in our corner. For example, as you get a lot of data, often your inferences will become more stable, they won't necessarily converge to the right answer but things might look more normally distributed, that's from the Central Limit Theorem. So, that suggests that certain statistical methods, certain approximations might work well when you have a lot of data. Which is good, cause when you have a lot of data, that's when you need the approximations more. So, there's a lot of things like that, moving between applications and research agendas but the research is to fit these big models and to understand them and that's continually going to be a challenge. Hugo:               So, those are all really important points that we'll actually see focused even more through the lens of polling and election forecasting. Before we get there, this idea of statistical inference and statistical modeling, I'm wondering what it takes to be able to be part of that conversation. I suppose, my question is, as humans, we don't necessarily have good statistical intuition and I'm wondering how you, as an educator and statistician, would like to see statistical and data literacy change in general for a general population? Andrew:           There's different ways of looking at it. Some of this is procedural. So, if there is an expectation that, when you have an analysis, you put your data on GitHub and you put your analysis on GitHub and it's all replicable, I think that alone will help. That won't make peoples analysis better but it will make is easier for people to see what went wrong. It's surprisingly difficult to get people to say or write exactly what they did. I find this with students but even I've been in consulting settings where maybe there's an expert on the other side and they do an analysis and they write up their analysis and you can't understand what they did. They'll photocopy three pages from a textbook and say, ""We did this."" And they don't say where their data come from or anything. I've come to realize that a lot of people don't even know what they did. People don't have a workflow, they just have a bunch of numbers and they start screwing around with the numbers and putting calculations in different places on their spreadsheet, and then at the end they pull a number out and write it down and type it in to their report. So, that famous example that Reinhart and Rogoff Excel error from that econ paper from a few years ago, but lots of published journal articles where not only the the results not replicate, but people have gone back to the articles and found that the numbers in the paper aren't even consistent with themselves. For example, they'll say there is a certain number of cases, and then they'll have a percentage, but the percentage doesn't correspond with any ratio with that denominator, or they have the estimates and the standard errors and the Z-scores but they don't correspond to the same thing. Andrew:           I'm just starting to realize people don't have a workflow at all. Requiring a workflow would help. When it comes to understanding, there's something you might have heard when you were a kid, which is if you have difficulty with a math problem, put a dollar sign in front of it and then it's somehow much harder to be off by orders of magnitude. Psychologists, such as Gerd Gigerenzer and others, have put a lot of work in to understanding our cognitive illusions and how we can fix those problems. One idea is to move away from probability and move towards frequencies. Andrew:           So, there are these classic probability problems like there's a disease, and one percent of the people have the disease, and you do a test and the test for the disease has 98 percent accuracy, somebody tests positive, what's the chance that they have the disease? And it's very hard to do that in your head. But, what you can do is say imagine you have an auditorium with a thousand people in it, well I just told you one percent of the people have the disease, so picture 10 people in the front row of the auditorium. They're the people with the disease. The other 990 don't. Now we're going to do a test with 98 percent accuracy. That's tough because you have to do 98 percent of the 10 people, so then you need higher numbers. Andrew:           Let me rephrase that and say if has 90 percent accuracy, just to keep the algebra simple. The test has 90 percent accuracy. So, then you look at the 10 people in the front row, well 9 of them test positive and one of them is gonna test negative, and you look at the 990 people otherwise, and out of them, 99 are gonna test positive by accident, that's 10 percent, and then the others are gonna be negative. Then you take all the people who test positive, if you have them raise their hand, and you see that we have 9 sick people who tested positive and 99 healthy people who tested positive. So, most of the people tested positive were healthy. So, the amazing thing is, I could do that all by speaking in my head and I couldn't solve the first problem in my head. You could say, well, but I had to screw around with the numbers cause 98 percent didn't work, but that's kind of the point. If you have a one percent disease, and the test has 98 percent accuracy, you really can't solve the problem by thinking of a thousand people. You need a larger population. So, we could think of a city with a million people and now, one percent, so 10 thousand people have the disease, and I'm purposely talking this through to show that you can do it. 10 thousand people have the disease, and 990 thousand don't. You could write this down but you could try it in your head. Then, of those 10 thousand with the disease, 98 percent, so that's gonna be 200. Andrew:           So, I could change the numbers around a little, I could do it in different ways, but the point is having that denominator makes it easier to visualize, it makes all the numbers makes more sense. So, Gigerenzer's argument is that the denominator really is always there and the denominator actually matters. There's a difference between something that happens 10 percent of the time to 10 people compared to something that happens 10 percent of the time to 10 thousand people. It's a different phenomenon. Probability theory is great, so the answer is there are ways of understanding probability better by thinking in terms of frequencies. Hugo:               And this is something we've actually seen in election forecasting, so this'll prove a nice segue. I know 538 and Nate Silver's house model, they won't say we predict that the democrats have a 75 percent chance of getting the house, they'll say a three in four chance because they feel that, heuristically, that helps people formalize it a bit better. They'll know one out of four times the republicans will get it, three out of four the democrats will. And then you can even think in those terms what does one in four mean. That's the frequency of getting two heads in a row which you wouldn't be surprised if that happened, right? Andrew:           Oh, sure, this happened before, I could tell you a story about Nate, but first, before the 2016 election, someone said, ""Well what about this forecast?"" There was some model that gave Clinton a 90 percent chance of winning. Well, 90 percent, how do you think about that? And I said, ""There's a presidential election every four years. 10 percent means something would happen roughly once every 10 elections, so every 40 years. Andrew:           I remember about 40 years ago, in the 1980 election, that it was supposed to be very close and then Regan one by I think about 7 percentage points. So, it was a big surprise. So, yeah, I think it could happen. Sure. Actually Clinton did very close to what she was polled, she was supposed to get 52 percent of the two party votes, and she got 51 percent. So, the polls are better now, in some ways, the forecasts are better now than they were in 1980. But, that's how I calibrate one in 10. As a political scientist, I often say I don't like 95 percent intervals. Because the 95 percent intervals are supposed to be correct 19 times out of 20 for 20 presidential elections that takes 80 years. I think it's ridiculous to try to make a statement that would be valid over an 80 year period because politics changes over 80 years. Andrew:           Now, my story about Nate was in 2012, he was going around, he said, ""Obama has a 65.8 percent chance of reelection"", then next week he'd say it was 63.2 percent, then it was 67.1 percent, and it would jump around. It was meaningless. You can say he has a 60 percent chance, but to say a 65.1 percent, you can do a little bit of mathematics. What you can do is say let's predict his share of the vote. Let's suppose he was predicted to get something like 52 or 53 percent of the vote and there's some uncertainty. You have a little bell-shaped curve and if it's less than 50 percent, let's temporarily forget about the electoral college for a minute, that's not really the concern here. The point is if his electoral votes are predicted to be less than 50 percent then he would lose, otherwise he would win. Andrew:           Let's suppose you say the probability is 65.8 percent. That's gonna correspond to a certain bell-shaped curve with his expected number of votes and uncertainty. It turns out, if you wanted to shift that from 65 percent to 66 percent, that would correspond to shifting his forecast share of the vote from something like, I don't remember the exact numbers, something like 52 percent to 52.01 percent. Something trivial like that. So, it's a meaningless number. It would be like saying Steph Curry is 6 feet 3.81724 inches tall. Andrew:           So, I got on Nate's case and I said, ""I understand, Nate, you're trying to, you need eyeballs. You need news every week. There's not much news. Obama's expected to win, but he might not. Every week, Obama's in the lead, but he might lose. That's what we know. It's hard and one way of creating news is to focus on these noise fluctuations."" So, if he's shifted to saying things like three in four chance, I think that¡¯s a good thing. He might have lost a few clicks that way, but one thing I've always admired for many years about Nate is his integrity. I don't think he would want people to get fooled by noise. So, it's a very good thing that he's done that. Hugo:               So, moving to polling. Polling is generally thought of with respect to election forecasting. I'm wondering what polling is, more generally, and what type of things it can tell us. Andrew:           Survey sampling is whenever you want to learn about the whole from a part. A blood test is like a survey sample. They take a sample of your blood and it's supposed to be representative of your blood. If I interview people on the street and ask them how they're gonna vote, that's supposed to be representative of the general population. Well, it might not be. They do random digit dialing, that's kind of representative of the population except not everybody answers the phone. Most people don't answer the phone, actually. So, it's not at all representative of the population. Andrew:           I was talking in my class and saying how I think it's mildly unethical to do an opinion poll and not pay people. You do a survey, you're making money from your survey, and a lot of pollsters do. Online panels pay people but a lot of your telephone polls they just call people up and you're kind of abusing people's good will by doing that. Then someone said, ""But, what about the kind of people who will only participate in a survey cause you pay. Are they non-representative?"" And I said, ""What do you think about the kind of people who will participate in a survey for free? They're kind of weird people, huh? Most people don't. Most people hang up on pollsters."" So, survey respondents are not representative. Andrew:           We do a lot of work to adjust the sample to the population. We need to because response rates are so low. But, anyway, it's not just election polling, it could be public opinion, blood testing, it could be businesses, they audit their own records and if they want to do an audit they'll take a random sample of records and then audit the random sample and use that to draw conclusions about the whole business et cetera. Hugo:               So, before we move into polling in a bit more detail, I'm just wondering, can you tell us why polling is even important? Andrew:           Well, George Gallup, who was the founder of his poll, wrote a lot about this. He argued that polling is good for democracy. There's two ways of putting it. Bill James, the great baseball analyst, once said something along the lines of, ""The alternative to good statistics is not no statistics, it's bad statistics."" He was arguing that some baseball player was overrated and then he quoted some sports writer saying, ""This Bill James number cruncher doesn't know anything. This batter was amazing. He had 300, all these time, and he got all these..."" And Bill James pointed out, let's look at what the sports writer said. What was his evidence that this guy was such a great athlete? It was a bunch of statistics. He was just using statistics naively, but the guy wasn't being Mr. Qualitative, he started talking about how the baseball player was hitting 300. Andrew:           Now, similarly, suppose you are a legislator and you want to know about public opinion. I think, first, public opinion is relevant. We don't always like when politicians follow public opinion too much but I think we like them to be aware or public opinion. So, if they don't have polls, what are they gonna do? They might very well do informal polls. Canvasing. And that over-represents certain kinds of people. That's gonna be unrepresentative of the people they find hard to reach. Gallup's argument was pretty much that democracy is ultimately based on public opinion and knowing public opinion between the elections is important. A lot of issues come up and it should allow politicians to do a better job, which seems reasonable to me. Andrew:           Beyond all that, of course, surveys are used all the time in marketing. So, business people don't have to apologize for wanting to know what the customer wants. So, it makes sense to do that. Marketing surveys are very interesting in part because you get into this question of connecting the observed measurements to what you really care about because how realistic is a marketing survey? So, if I call you up on the phone and say, ""Will you pay 30 thousand dollars for this sort of electric car?"" You could say yes or no, that doesn't mean that that's gonna actually make it out of the showroom, cause the survey's not realistic. Andrew:           Political surveys are a little easier. Who do you plan to vote for? That's almost the same as being in the damn polling booth and voting. So, the realism of political surveys is much closer than the realism of certain marketing surveys. Hugo:               And I don't know how long his has been the case but we've definitely seen polling affect... there's a feedback loop in to the political, and voting, and election process. I think it was, the primaries, I think, the debates, your position on the stage and whether you're in the debate or not, is actually dependent on your performance in polls, right? Andrew:           Yeah, and Donald Trump, when he would give speeches in the primaries, he would talk about how high his poll ratings were. Hugo:               Until they weren't, and then he said they're unscientific. Andrew:           Well, yeah, but I'm not talking about his approval, but the percentage of people who said they were gonna vote for him. So, he was polling very high even when outside observers didn't seem to give him much of a chance. So, yeah, there is feedback. I'll say one thing, there is a useful feedback, at least for pollsters. Sometimes the question arises, why do people tell the truth to a pollster? And you'll sometimes get, pundits will say, ""Hey, let's all lie to the pollsters. Let's screw them up. I don't like the pollsters. Tell them the opposite of what you think."" And yet, people don't do that. And there's a couple reasons for this. Andrew:           The first is that, as I said, polls are opt in. No one forces you to do a poll. So, if you really hate pollsters, it's likely you won't bother to do it in the first place. But the second thing is that I think people think of a poll is like a way of voting. So, if I survey you and do you approve of Donald Trump's job performance? You think this might get out in the news somewhere, you're motivated: if you approve, you're motivated to say yes, and if you don't, you're motivated to say no. There's a direct motivation to be sincere in your response. Again, that's not true of all surveys. If I ask you, do you take illegal drugs? You might have various motivations not to answer that honestly. Hugo:               I couldn't answer that on air either. Andrew:           Well, there are asymmetries. You could if your answer was no, and you could answer it on air if you want, I'm not asking you. I'm just saying that it's complicated. So, one thing that's not always very well understood about political polling is that the incentives align to actually encourage sincerity in the survey response. That's very important. Hugo:               Now, the other thing you mentioned that I just want to touch on briefly is this idea of polls, and measuring public opinion, and this is more playing devil's advocate, not necessarily trolling. I'm just wondering, public opinion generally is views that are prevalent among the general public. Does public opinion even exist? Andrew:           It's like Heisenberg's uncertainty principle. So, to measure opinion is to change it. You know how you want to measure the position of a particle, you have to look at it, and looking at it means bouncing a light particle off it and that adds energy and it changes its position and momentum? So, similarly, if you want to know what someone thinks, you have to ask them and then that changes it. Now, not always can you observe their behavior. There's other ways. Andrew:           I have a college, Matt Salganic, he's a sociologist at Princeton, he wrote a book around social science data collection recently and he talked about... you can survey people, you can ask them, or you can observe them. Those are different. You can observe someone so inobtrustively it doesn't change their behavior, sometimes. Amazon can look at how you purchase. Arguably, once you know that Amazon's looking, then you might not purchase certain things or not search on things because you don't want them to know about it. Until that happens, you can observe them. Andrew:           Similarly, with the security camera outside of your apartment. If you don't know it's there, then it's observing you pretty well. So, in that sense, if you think of we being measured, we're kind of in a cat and mouse game with those social scientists who are trying to measure us. That they're trying to measure us in ways that don't disturb us and we might want to be aware of how we're being measure. Hugo:               So, now I want to get directly into polling and this is something that's known. I'm going to quote you because you said it so well in an article in Slate magazine that I will link to in the show notes. You wrote, ""The statistical theory of traditional polling is amazing. In the theory, a random sample of a thousand people is enough to estimate public opinion within a margin of error of plus or minus three percentage points."" Could you just give us a run down of what this exactly means? Andrew:           This is the mathematics of drawing balls from an urn. So, if you have a large urn that's full of balls, and 55 percent of the balls are green, and 45 percent are yellow, and you draw a ball at random a thousand times, then most likely, you'll get between 52 percent and 58 percent green balls. And so, it's 55 percent in the urn, and you draw a thousand, each time you draw a ball and throw it back in the urn and shuffle it and draw another one, then the mathematics of probability tell you that the most likely thing you'll see is 55 percent green balls, but it could be anywhere between 52 percent and 58 percent. There's a 95 percent chance, roughly, that it's in that range. So, we call that the margin of error. If you can actually sample people, like drawing them from an urn, you can learn about public opinion pretty accurately. Hugo:               But, of course, this is theoretical, right? And one of the parts of the theory is that its' a random representative sample. I'm wondering what the practical problems and challenges associated with this theory are. Andrew:           In practice, you can't draw people at random from the urn because there's no list of people. You can call phone numbers at random, not everyone has a phone, some people have two phones, some people never answer their phone, et cetera. Also, if you draw a ball, you get to look at it in the urn model, but, when you're sampling people, you draw a ball and what if they don't want to respond to your survey? Then you don't get to see it. Andrew:           So, our surveys are systematically non-representative of the population. So, what we do is we adjust for known differences between sample and population. So, our population is 52 percent female, but our survey is 60 percent female, we adjust for that. Our survey gets too many old people, it gets too many white people, it gets too many people from some states and not others. Different surveys have different biases. Exit polls, I've been told, tend to oversample Democrats, maybe it has to do with who's willing to talk to the exit poll interviewer. The kind of people who are willing to answer the phone may be different. Andrew:           Then, the other thing is you need to worry about getting honest responses or adjusting for inaccuracy in survey responses, which, like I said, is less of an issue for political polling but comes up in other surveys. Hugo:               I'm really interested in this idea of calling people on the telephone because classically, historically, a lot of people had landlines and you could do that. This isn't the case anymore and my understanding is that there's legislation that means you can't automate phone calls to cell phones, is that right? Andrew:           I don't know exactly what the laws are about what you can and can't do. It was all kind of just a window. When Gallup started doing polls, they'd knock on doors because a lot of people didn't have phones back then. So, there was a certain period where a lot of people had phones. In other countries, not everybody had phones either. But, again, even if you could call everybody, so what? The respondents are not representative of the population. Hugo:               So, it's really the adjustment process that is really key as well. Andrew:           Yeah, it's both. You've got to try to get a representative sample even though you're not gonna get it because you want your biases to be correctable. So, if my bias is that I have too many women, I an correct for that. Or too many old people. If my bias is that I have too many conservatives, can I correct for that? Well, maybe cause you can ask people their party affiliation and then you can match it with data on people's party registration. It's more work, right? If I'm asking about health care and my bias is that people with health problems are more likely to respond to the survey. Can I adjust for that? Well, that might be tougher. Andrew:           So, it makes sense to try to get that perfect sample even though you're not gonna get there, to aim for it. Hugo:               And are these correction and adjustment methods relatively sophisticated statistically? Andrew:           Well, they're getting more sophisticated as our data get worse and worse. So, the short story is that there's three reasons they need to get more sophisticated. One is to adjust for inaccurate responses but, as I said, I'm not really gonna focus on that. Second is differences between the sample and the population. You want to adjust for a lot of factors, not just sex and age and ethnicity, party identification, lots of things. So, when you want to adjust for more things, then simple adjustment methods, simple weighting methods, don't do the job. We use a method called multilevel regression and post-stratification, there are other approaches, but you need more sophistication to adjust for more variables. Andrew:           Then, the third thing is that we ask more from our surveys. So, we might want to know not just public opinion, not just do people want to vote for their democrats or republicans, but how does that vote break down among all of the 435 congressional districts? So, even if I have big data, I won't necessarily have a large sample of each congressional district. So, you want to do statistical analysis to get those more focused inferences. So, that's one reason why my colleagues and I have put a lot of effort in to modeling the survey response to be able to estimate subgroup of the population, like rich voters and poor voters within different states. Hugo:               Fantastic. And something that I know you've worked in is thinking outside the box, how to get people involved in surveys, that was an unintentional poor pun, but, because you've actually used gaming technology and Xboxes in order to get survey responses, right? Andrew:           Yes, my colleagues at Microsoft research in New York did that. Microsoft research has some social scientists and my colleagues, David Rothschild and Sharad Goel, who worked there at the time designed a survey so they convinced the Microsoft people to put something on the Xbox in the last months of the 2012 presidential election where people could vote and say who they wanted to vote for. So, every once in a while, you'd get a reminder saying would you like to participate in our poll? And then you'd give some demographics and say who you wanted to vote for. We had a huge sample size, hundreds of thousands of responses, very unrepresentative. Andrew:           An unusual survey because it overrepresented young man and most surveys over represent old women. But after adjustment, first we were able to estimate public opinion very well, in fact, we were able to estimate public opinion more stably than public poll aggregators. That's the good news. The bad news is that we collected the data in 2012, we didn't actually do that analysis until afterwards. So, in theory, it could have been done in real time, but actually, it was a research project and we published it later. Andrew:           So, we didn't beat the polls while it was happening. Not only that, we actually learned something about political science and public opinion. As I said, our estimates were more stable and better than the polling aggregate estimates from the newspapers and online, and it turned out that about two thirds of the variation in the poll, the fluctuations like Romney's doing well, or Obama's doing well, these fluctuations, about two thirds of those fluctuations were actually attributable to differential nonresponse. So, when Romney has some good news, Republicans were more likely to answer the poll. Which makes sense, right? Do you want to participate in a poll? Well, if my candidate is a laughing stock, maybe not. If my candidate is doing great stuff, yes. Andrew:           So, there was this positive feedback mechanism that... negative feedback stabilizes, positive feedback amplifies fluctuations. So, a positive feedback mechanisms which is if a candidate is doing well, more of their supporters will respond to the poll, meaning they look like they're doing even better. And so, you get these big fluctuations from week to week but then when you actually adjust for partisanship you find that the results were much more stable. We found that in 2016 also. You might say, well, maybe people's partisanship was fluctuating too, but we have evidence that that's not really happening. There's various loose ends and the project that we tied up when we wrote our paper, all of that came from this collaboration with these people at Microsoft. Hugo:               I'm glad you mentioned 2016, because, as you stated earlier, the popular vote, the pollsters did pretty well on, within one percent, right? It was 52 instead of 51 was what the pollsters said. But, of course, in the electoral college vote, things were relatively different and I think something you've written about is that's potentially due to the fact that in several key states, people who voted for Trump didn't necessarily respond in the polls. Is that right or do I misremember? Andrew:           There's so much nonresponse, the problem was more with state polls than national polls. That is, there were some people, after the election, some pollsters, Gary Langer and some of his colleagues wrote a paper where they analyzed their national polls by state and they actually found that the state level analyses of the national polls were not that far off. But, some of the state polls in Michigan and other states, didn't do a good enough job of adjusting for non response, so it seems. There was a lot going on but part of it is that the nonresponse adjustments weren't really as complete. It's an issue. Survey response rates keep going down and so the raw survey data, or even the weakly adjusted survey data, are not always enough. Hugo:               In the same Slate article that I mentioned earlier, you also wrote, ""Instead of zeroing in on elections, we should think of polling and public opinion as more of a continuous process for understanding policy."" I find this very attractive and I'm just wondering if you can elucidate this and tell me what you mean by this? Andrew:           Well, this came about, I think it's particularly clear in the Obama administration that there were various issues like the stimulus plan, the health care plan, where public opinion seemed to be very important. There's a lot of both sides rallying public opinion in order to sway certain swing votes in Congress. It's less so right now. Right now, it's like okay Republicans control the House, the Senate, the Presidency, and the Supreme Court, and so, it's sort of up to them what to do. Public opinion doesn't seem to be directly influencing things. They seem to be willing to do various unpopular things to make use of the majority that they have. Andrew:           But, most of the time, politics is at the legislative level it's a bit more transactional. There are swing voters, certainly if one party controls the house and one party control the senate then you get more power to various swing voters. At that point, public opinion can make a difference. So, it's not just about who you're gonna vote for, it's how people are gonna vote once they're in office. So, pollsters are gonna be interested in public opinion throughout the process because it's not really just about who you plan to vote for but also what are your views on various issues, whether it's foreign policy, or health care, or immigration, or trade, or whatever. Hugo:               And how does party allegiance play a role in that, though? Andrew:           Oh, party allegiance is very important and there's a lot of evidence that voters will switch positions based on what their party says. If you look at things like support for wars, there's been big jumps based on the party in power. If you're a Democrat then you support the same policy that you wouldn't support if a Republican was doing, or vice versa. Or how things are labeled. It's like as the economists say, it's exogenist. The pollsters are measuring opinion, but at the same time, politicians are trying to use that opinion. Andrew:           My colleague, Bob Shapiro in the political science department, he and a colleague wrote a book called Politicians Don't Pander, which was based on his study of various political fights, not elections, but legislative fights. He argued that politicians think of public opinion as a tool that... there's a naive view of politicians wanting to do what the public wants but it's actually politicians are often quite confident and they feel that they can sway the voters and they think of public opinion as something that they can manipulate. So, both sides are doing it. To the extent that individual congress members and senators are involved, you also need to know about local public opinion, not just national. Hugo:               So, Andy, what does the future of polling look like to you? Andrew:           I don't have a great sense of what the future will be, if you look traditionally, you'd say lower response rates is the future, paying people to participate, online panels. I think, maybe, in general, we should think of the people who respond to surveys as being more participants, the same way as medical statistics. Instead of thinking that we're measuring people and estimating the effect of the drug and people are just counters that are being moved around, we should actually think of patients as participating in studies and really being involved. Not just because you want to get more compliance, but also because people have a lot of private knowledge that they can share as well as people should be more motivated to help out if they have more connection. Andrew:           So, to me, the future would be something a bit more collaborative. In the other direction, there's just gonna be a lot of passive measurement, things like Amazon measuring your clicks. That's like polling also. So, that's from the opposite direction. So, either, if it's intrusive I think people should be more involved or it's just not intrusive at all. Hugo:               So, Andrew, my final question for you is what's one of your favorite data science and statistical techniques or methodologies? Andrew:           My favorite thing is something I've never done and I read about. This was maybe 10 years ago, supposedly, someone built a machine that you could stick in someone's office and then, if they're typing, 10 minutes later it could be a keylogger. Supposedly, how it works, there's about a hundred keys on your keyboard, so it listens to the sounds and uses some classification algorithm to classify the sounds of the keys into a hundred clusters, and then, having done that, it then uses simple code breaking techniques to estimate which is the space bar, which is the carriage return, which is the letter E, and so forth. Of course, it doesn't have to be perfect, you can use statistical tools and then it can figure out what you're typing. Andrew:           So, I've always wanted to build that. Now, that kind of thing I don't know how to build, it also involves having a microphone and doing sound analysis. I just think it would be really cool. These things are pretty Bayesian, you're using a lot of prior information on especially the second step, the code breaking step. Of course, Alan Turing used Bayesian methods to crack the enigma code in World War II. That's my favorite example even though I've never seen it. I just think it's the coolest. It's not something that I could really do, though. Andrew:           If you want to talk about stuff that I could do, then my favorite technique would be multilevel regression and post-stratification because that's what we use to estimate state level public opinion. It's how we did red state, blue state, and estimated how opinions vary by income in different parts of the country. It allows us to do our best to adjust between differences between sample and population. We could do it in Stan. So, I'll push that. Hugo:               Great. And so, multilevel regression and post-stratification, we'll include some links in the show notes. It's also known as MRP or Mr. P, correct? Andrew:           Exactly. And recently I've started calling it regularized prediction and post-stratification because, strictly speaking, it's modular. So, the first part is you fit a model to do the adjustment, the second part is having done that, you make inferences for the population, which is called post-stratification. So, multilevel regression is one way of doing the model, but generally, you could use the term regularized prediction which includes all sorts of other methods that are out there. Hugo:               It's been such a pleasure having you on the show. Andrew:           Likewise.","Keyword(freq): statistics(29), poll(16), survey(13), pollster(11), number(10), assumption(9), politician(7), state(7), challenge(6), fluctuation(6)"
"4","mastery",2018-10-12,"How to Load, Visualize, and Explore a Complex Multivariate Multistep Time Series Forecasting Dataset","https://machinelearningmastery.com/how-to-load-visualize-and-explore-a-complex-multivariate-multistep-time-series-forecasting-dataset/","Real-world time series forecasting is challenging for a whole host of reasons not limited to problem features such as having multiple input variables, the requirement to predict multiple time steps, and the need to perform the same type of prediction for multiple physical sites. The EMC Data Science Global Hackathon dataset, or the ¡®Air Quality Prediction¡® dataset for short, describes weather conditions at multiple sites and requires a prediction of air quality measurements over the subsequent three days. In this tutorial, you will discover and explore the Air Quality Prediction dataset that represents a challenging multivariate, multi-site, and multi-step time series forecasting problem. After completing this tutorial, you will know: Let¡¯s get started. How to Load, Visualize, and Explore a Complex Multivariate Multistep Time Series Forecasting DatasetPhoto by H Matthew Howarth, some rights reserved. This tutorial is divided into seven parts; they are: The EMC Data Science Global Hackathon dataset, or the ¡®Air Quality Prediction¡® dataset for short, describes weather conditions at multiple sites and requires a prediction of air quality measurements over the subsequent three days. Specifically, weather observations such as temperature, pressure, wind speed, and wind direction are provided hourly for eight days for multiple sites. The objective is to predict air quality measurements for the next three days at multiple sites. The forecast lead times are not contiguous; instead, specific lead times must be forecast over the 72 hour forecast period; they are: Further, the dataset is divided into disjoint but contiguous chunks of data, with eight days of data followed by three days that require a forecast. Not all observations are available at all sites or chunks and not all output variables are available at all sites and chunks. There are large portions of missing data that must be addressed. The dataset was used as the basis for a short duration machine learning competition (or hackathon) on the Kaggle website in 2012. Submissions for the competition were evaluated against the true observations that were withheld from participants and scored using Mean Absolute Error (MAE). Submissions required the value of -1,000,000 to be specified in those cases where a forecast was not possible due to missing data. In fact, a template of where to insert missing values was provided and required to be adopted for all submissions (what a pain). A winning entrant achieved a MAE of 0.21058 on the withheld test set (private leaderboard) using random forest on lagged observations. A writeup of this solution is available in the post: In this tutorial, we will explore this dataset in order to better understand the nature of the forecast problem and suggest approaches for how it may be modeled. The first step is to download the dataset and load it into memory. The dataset can be downloaded for free from the Kaggle website. You may have to create an account and log in, in order to be able to download the dataset. Download the entire dataset, e.g. ¡°Download All¡± to your workstation and unzip the archive in your current working directory with the folder named ¡®AirQualityPrediction¡® You should have five files in the AirQualityPrediction/ folder; they are: Our focus will be the ¡®TrainingData.csv¡® that contains the training dataset, specifically data in chunks where each chunk is eight contiguous days of observations and target variables. The test dataset (remaining three days of each chunk) is not available for this dataset at the time of writing. Open the ¡®TrainingData.csv¡® file and review the contents. The unzipped data file is relatively small (21 megabytes) and will easily fit into RAM. Reviewing the contents of the file, we can see that the data file contains a header row. We can also see that missing data is marked with the ¡®NA¡® value, which Pandas will automatically convert to NumPy.NaN. We can see that the ¡®weekday¡® column contains the day as a string, whereas all other data is numeric. Below are the first few lines of the data file for reference. We can load the data file into memory using the Pandas read_csv() function and specify the header row on line 0. We can also get a quick idea of how much missing data there is in the dataset. We can do that by first trimming the first few columns to remove the string weekday data and convert the remaining columns to floating point values. We can then calculate the total number of missing observations and the percentage of values that are missing. The complete example is listed below. Running the example first prints the shape of the loaded dataset. We can see that we have about 37,000 rows and 95 columns. We know these numbers are misleading given that the data is in fact divided into chunks and the columns are divided into the same observations at different sites. We can also see that a little over 40% of the data is missing. This is a lot. The data is very patchy and we are going to have to understand this well before modeling the problem. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course A good starting point is to look at the data in terms of the chunks. We can group data by the ¡®chunkID¡¯ variable (column index 1). If each chunk is eight days and the observations are hourly, then we would expect (8 * 24) or 192 rows of data per chunk. If there are 37,821 rows of data, then there must be chunks with more or less than 192 hours as 37,821/192 is about 196.9 chunks. Let¡¯s first split the data into chunks. We can first get a list of the unique chunk identifiers. We can then collect all rows for each chunk identifier and store them in a dictionary for easy access. Below defines a function named to_chunks() that takes a NumPy array of the loaded data and returns a dictionary of chunk_id to rows for the chunk. The ¡®position_within_chunk¡® in the data file indicates the order of a row within a chunk. At this stage, we assume that rows are already ordered and do not need to be sorted. A skim of the raw data file seems to confirm this assumption. Once the data is sorted into chunks, we can calculate the number of rows in each chunk and have a look at the distribution, such as with a box and whisker plot. The complete example that ties all of this together is listed below Running the example first prints the number of chunks in the dataset. We can see that there are 208, which suggests that indeed the number of hourly observations must vary across the chunks. A box and whisker plot and a histogram plot of chunk durations is created. We can see that indeed the median is 192, meaning that most chunks have eight days of observations or close to it. We can also see a long tail of durations down to about 25 rows. Although there are not many of these cases, we would expect that will be challenging to forecast given the lack of data. The distribution also raises questions about how contiguous the observations within each chunk may be. Box and whisker plot and Histogram plot of chunk duration in hours It may be helpful to get an idea of how contiguous (or not) the observations are within those chunks that do not have the full eight days of data. One approach to considering this is to create a line plot for each discontiguous chunk and show the gaps in the observations. We can do this on a single plot. Each chunk has a unique identifier, from 1 to 208, and we can use this as the value for the series and mark missing observations within the eight day interval via NaN values that will not appear on the plot. Inverting this, we can assume that we have NaN values for all time steps within a chunk, then use the ¡®position_within_chunk¡® column (index 2) to determine the time steps that do have values and mark them with the chunk id. The plot_discontinuous_chunks() below implements this behavior, creating one series or line for each chunk with missing rows all on the same plot. The expectation is that breaks in the line will help us see how contiguous or discontiguous these incomplete chunks happen to be. The complete example is listed below. Running the example creates a single figure with one line for each of the chunks with missing data. The number and lengths of the breaks in the line for each chunk give an idea of how discontiguous the observations within each chunk happen to be. Many of the chunks do have long stretches of contiguous data, which is a good sign for modeling. There are cases where chunks have very few observations and those observations that are present are in small contiguous patches. These may be challenging to model. Further, not all of these chunks have observations at the end of chunk: the period right before a forecast is required. These specifically will be a challenge for those models that seek to persist recent observations. The discontinuous nature of the series data within the chunks will also make it challenging to evaluate models. For example, one cannot simply split chunk data in half, train on the first half and test on the second when the observations are patchy. At least, when the incomplete chunk data is considered. Line plots of chunks with discontinuous observations The discontiguous nature of the chunks also suggests that it may be important to look at the hours covered by each chunk. The time of day is important in environmental data, and models that assume that each chunk covers the same daily or weekly cycle may stumble if the start and end time of day vary across chunks. We can quickly check this by plotting the distribution of the first hour (in a 24 hour day) of each chunk. The number of bins in the histogram is set to 24 so we can clearly see the distribution for each hour of the day in 24-hour time. Further, when collecting the first hour of the chunk, we are careful to only collect it from those chunks that have all eight days of data, in case a chunk with missing data does not have observations at the beginning of the chunk, which we know happens. The complete example is listed below. Running the example creates a box and whisker plot and a histogram of the first hour within each chunk. We can see a reasonably uniform distribution of the start time across the 24 hours in the day. Further, this means that the interval to be forecast for each chunk will also vary across the 24 hour period. This adds a wrinkle for models that might expect a standard three day forecast period (midnight to midnight). Distribution of first hour in observations within each chunk Now that we have some idea of the chunk-structure of the data, let¡¯s take a closer look at the input variables that describe the meteorological observations. There are 56 input variables. The first six (indexes 0 to 5) are metadata information for the chunk and time of the observations. They are: The remaining 50 describe meteorological information for specific sites; they are: Really, there are only eight meteorological input variables: These variables are recorded across 23 unique sites; they are: The data is beautifully complex. Not all variables are recorded at all sites. There is some overlap in the site identifiers used in the target variables, such as 1, 50, 64, etc. There are site identifiers used in the target variables that are not used in the input variables, such as 4002. There are also site identifiers that are used in the input that are not used in the target identifiers, such as 15. This suggests, at the very least, that not all variables are recorded at all locations. That recording stations are heterogeneous across sites. Further, there might be something special about sites that only collect measures of a given type or collect all measurements. Let¡¯s take a closer look at the data for the input variables. We can start off by looking at the structure and distribution of inputs per chunk. The first few chunks that have all eight days of observations have the chunkId of 1, 3, and 5. We can enumerate all of the input columns and create one line plot for each. This will create a time series line plot for each input variable to give a rough idea of how each varies across time. We can repeat this for a few chunks to get an idea how the temporal structure may differ across chunks. The function below named plot_chunk_inputs() takes the data in chunk format and a list of chunk ids to plot. It will create a figure with 50 line plots, one for each input variable, and n lines per plot, one for each chunk. The complete example is listed below. Running the example creates a single figure with 50 line plots, one for each of the meteorological input variables. The plots are hard to see, so you may want to increase the size of the created figure. We can see that the observations for the first five variables look pretty complete; these are solar radiation, wind speed, and wind direction. The rest of the variables appear pretty patchy, at least for this chunk. Parallel Time Series Line Plots For All Input Variables for 1 Chunks We can update the example and plot the input variables for the first three chunks with the full eight days of observations. Running the example creates the same 50 line plots, each with three series or lines per plot, one for each chunk. Again, the figure makes the individual plots hard to see, so you may need to increase the size of the figure to better review the patterns. We can see that these three figures do show similar structures within each line plot. This is helpful finding as it suggests that it may be useful to model the same variables across multiple chunks. Parallel Time Series Line Plots For All Input Variables for 3 Chunks It does raise the question as to whether the distribution of the variables differs greatly across sites. We can look at the distribution of input variables crudely using box and whisker plots. The plot_chunk_input_boxplots() below will create one box and whisker per input feature for the data for one chunk. The complete example is listed below. Running the example creates 50 boxplots, one for each input variable for the observations in the first chunk in the training dataset. We can see that variables of the same type may have the same spread of observations, and each group of variables appears to have differing units. Perhaps degrees for wind direction, hectopascales for pressure, degrees Celsius for temperature, and so on. Box and whisker plots of input variables for one chunk It may be interesting to further investigate the distribution and spread of observations for each of the eight variable types. This is left as a further exercise. We have some rough ideas about the input variables, and perhaps they may be useful in predicting the target variables. We cannot be sure. We can now turn our attention to the target variables. The goal of the forecast problem is to predict multiple variables across multiple sites for three days. There are 39 time series variables to predict. From the column header, they are: The naming convention for these column headers is: We can convert these column headers into a small dataset of variable ids and site ids with a little regex. The results are as follows: Helpfully, the targets are grouped by variable id. We can see that one variable may have to be predicted across multiple sites; for example, variable 11 predicted at sites 1, 32, 50, and so on: We can see that different variables may need to be predicted for a given site. For example, site 50 requires variables 11, 3, and 4: We can save the small dataset of targets to a file called ¡®targets.txt¡® and load it up for some quick analysis. Running the example prints the number of unique variables and sites. We can see that 39 target variables is far less than (12*14) 168 if we were predicting all variables for all sites. Let¡¯s take a closer look at the data for the target variables. We can start off by looking at the structure and distribution of targets per chunk. The first few chunks that have all eight days of observations have the chunkId of 1, 3, and 5. We can enumerate all of the target columns and create one line plot for each. This will create a time series line plot for each target variable to give a rough idea of how it varies across time. We can repeat this for a few chunks to get a rough idea of how the temporal structure may vary across chunks. The function below, named plot_chunk_targets(), takes the data in chunk format and a list of chunk ids to plot. It will create a figure with 39 line plots, one for each target variable, and n lines per plot, one for each chunk. The complete example is listed below. Running the example creates a single figure with 39 line plots for chunk identifier ¡°1¡±. The plots are small, but give a rough idea of the temporal structure for the variables. We can see that there are more than a few variables that have no data for this chunk. These cannot be forecasted directly, and probably not indirectly. This suggests that in addition to not having all variables for all sites, that even those specified in the column header may not be present for some chunks. We can also see breaks in some of the series for missing values. This suggests that even though we may have observations for each time step within the chunk, that we may not have a contiguous series for all variables in the chunk. There is a cyclic structure to many of the plots. Most have eight peaks, very likely corresponding to the eight days of observations within the chunk. This seasonal structure could be modeled directly, and perhaps removed from the data when modeling and added back to the forecasted interval. There does not appear to be any trend to the series. Parallel Time Series Line Plots For All Target Variables for 1 Chunk We can re-run the example and plot the target variables for the first three chunks with complete data. Running the example creates a figure with 39 plots and three time series per plot, one for the targets for each chunk. The plot is busy, and you may want to increase the size of the plot window to better see the comparison across the chunks for the target variables. For many of the variables that have a cyclic daily structure, we can see the structure repeated across the chunks. This is encouraging as it suggests that modeling a variable for a site may be helpful across chunks. Further, plots 3-to-10 correspond to variable 11 across seven different sites. The string similarity in temporal structure across these plots suggest that modeling the data per variable which is used across sites may be beneficial. Parallel Time Series Line Plots For All Target Variables for 3 Chunks It is also useful to take a look at the distribution of the target variables. We can start by taking a look at the distribution of each target variable for one chuck by creating box and whisker plots for each target variable. A separate boxplot can be created for each target side-by-side, allowing the shape and range of values to be directly compared on the same scale. The complete example is listed below. Running the example creates a figure containing 39 boxplots, one for each of the 39 target variables for the first chunk. We can see that many of the variables have a median close to zero or one; we can also see a large asymmetrical spread for most variables, suggesting the variables likely have a skew with outliers. It is encouraging that the boxplots from 4-10 for variable 11 across seven sites show a similar distribution. This is further supporting evidence that data may be grouped by variable and used to fit a model that could be used across sites. Box and whisker plots of target variables for one chunk We can re-create this plot using data across all chunks to see dataset-wide patterns. The complete example is listed below. Running the example creates a new figure showing 39 box and whisker plots for the entire training dataset regardless of chunk. It is a little bit of a mess, where the circle outliers obscure the main data distributions. We can see that outlier values do extend into the range 5-to-10 units. This suggests there might be some use in standardizing and/or rescaling the targets when modeling. Perhaps the most useful finding is that there are some targets that do not have any (or very much) data regardless of chunk. These columns probably should be excluded from the dataset. Box and whisker plots of target variables for all training data We can investigate the apparent missing data further by creating a bar chart of the ratio of missing data per column, excluding the metadata columns at the beginning (e.g. the first five columns). The plot_col_percentage_missing() function below implements this. The complete example is listed below. Running the example first prints the column id (zero offset) and the ratio of missing data, if the ratio is above 90%. We can see that there are in fact no columns with zero non-NaN data, but perhaps two dozen (12) that have above 90% missing data. Interestingly, seven of these are target variables (index 56 or higher). A bar chart of column index number to ratio of missing data is created. We can see that there might be some stratification to the ratio of missing data, with a cluster below 10%, a cluster around 70%, and a cluster above 90%. We can also see a separation between input variable and target variables where the former are quite regular as they show the same variable type measured across different sites. Such small amounts of data for some target variables suggest the need to leverage other factors besides past observations in order to make predictions. Bar Chart of Percentage of Missing Data Per Column The distribution of the target variables are not neat and may be non-Gaussian at the least, or highly multimodal at worst. We can check this by looking at histograms of the target variables, for the data for a single chunk. A problem with the hist() function in matplotlib is that it is not robust to NaN values. We can overcome this by checking that each column has non-NaN values prior to plotting and excluding the rows with NaN values. The function below does this and creates one histogram for each target variable for one or more chunks. The complete example is listed below. Running the example creates a figure with 39 histograms, one for each target variable for the first chunk. The plot is hard to read, but the large number of bins goes to show the distribution of the variables. It might be fair to say that perhaps none of the target variables have an obvious Gaussian distribution. Many may have a skewed distribution with a long right tail. Other variables have what appears to be quite a discrete distribution that might be an artifact of the chosen measurement device or measurement scale. Histograms for each target variable for one chunk We can re-create the same plot with target variables for all chunks. The complete example is listed below. Running the example creates a figure with 39 histograms, one for each of the target variables in the training dataset. We can see fuller distributions, which are more insightful. The first handful of plots perhaps show a highly skewed distribution, the core of which may or may not be Gaussian. We can see many Gaussian-like distributions with gaps, suggesting discrete measurements imposed on a Gaussian-distributed continuous variable. We can also see some variables that show an exponential distribution. Together, this suggests either the use of power transforms to explore reshaping the data to be more Gaussian, and/or the use of nonparametric modeling methods that are not dependent upon a Gaussian distribution of the variables. For example, classical linear methods may be expected to have a hard time. Histograms for each target variable for the entire training dataset After the end of the competition, the person who provided the data, David Chudzicki, summarized the true meaning of the 12 output variables. This was provided in a form post titled ¡°what the target variables really were¡°, reproduced partially below: This is interesting as we can see that the target variables are meteorological in nature and related to air quality as the name of the competition suggests. A problem is that there are 15 variables and only 12 different types of target variables in the dataset. The cause of this problem is that the same target variable in the dataset may be used to represent different target variables. Specifically: From the names of the variables, the doubling-up of data into the same target variable was done so with variables with differing chemical characters and perhaps even measures, e.g. it appears to be accidental rather than strategic. It is not clear, but it is likely that a target represents one variable within a chunk but may represent different variables across chunks. Alternately, it may be possible that the variables differ across sites within each chunk. In the former case, it means that models that expect consistency in these target variables across chunks, which is a very reasonable assumption, may have difficulty. In the latter, models can treat the variable-site combinations as distinct variables. It may be possible to tease out the differences by comparing the distribution and scales of these variables across chunks. This is disappointing, and depending on how consequential it is to model skill, it may require the removal of these variables from the dataset, which are a lot of the target variables (20 of 39). In this section, we will harness what we have discovered about the problem and suggest some approaches to modeling this problem. I like this dataset; it is messy, realistic, and resists naive approaches. This section is divided into four sections; they are: The problem is generally framed as a multivariate multi-step time series forecasting problem. Further, the multiple variables are required to be forecasted across multiple sites, which is a common structural breakdown for time series forecasting problems, e.g. predict the variable thing at different physical locations such as stores or stations. Let¡¯s walk through some possible framings of the data. A first-cut approach might be to treat each variable at each site as a univariate time series forecasting problem. A model is given eight days of hourly observations for a variable and is asked to forecast three days, from which a specific subset of forecast lead times are taken and used or evaluated. It may be possible in a few select cases, and this could be confirmed with some further data analysis. Nevertheless, the data generally resists this framing because not all chunks have eight days of observations for each target variable. Further, the time series for the target variable can be dramatically discontiguous, if not mostly (90%-to-95%) incomplete. We could relax the expectation of the structure and amount of prior data required by the model, designing the model to make use of whatever is available. This approach would require 39 models per chunk and a total of (208 * 39) or 8,112 separate models. It sounds possible, but perhaps less scalable than we may prefer from an engineering perspective. The variable-site combinations could be modeled across chunks, requiring only 39 models. The target variables can be aggregated across sites. We can also relax what lag lead times are used to make a forecast and present what is available either with zero-padding or imputing for missing values, or even lag observations that disregard lead time. We can then frame the problem as given some prior observations for a given variable, forecast the following three days. The models may have more to work with, but will disregard any variable differences based on site. This may or may not be reasonless and could be checked by comparing variable distributions across sites. There are 12 unique variables. We could model each variable per chunk, giving (208 * 12) or 2,496 models. It might make more sense to model the 12 variables across chunks, requiring only 12 models. Perhaps one or more target variables are dependent on one or more of the meteorological variables, or even on the other target variables. This could be explored by investigating the correlation between each target variable and each input variable, as well as with the other target variables. If such dependencies exist, or could be assumed, it may be possible to not only forecast the variables with more complete data, but also those target variables with above 90% missing data. Such models could use some subset of prior meteorological observations and/or target variable observations as input. The discontiguous nature of the data may require the relaxing of the traditional lag temporal structure for the input variables, allowing the model to use whatever was available for a specific forecast. Depending on the choice of model, the input and target variables may benefit from some data preparation, such as: To address the missing data, in some cases imputing may be required with simple persistence or averaging. In other cases, and depending on the choice of model, it may be possible to learn directly from the NaN values as observations (e.g. XGBoost can do this) or to fill with 0 values and mask the inputs (e.g. LSTMs can do this). It may be interesting to investigate downscaling input to 2, 4, or 12, hourly data or similar in an attempt to fill the gaps in discontiguous data, e.g. forecast hourly from 12 hourly data. Modeling may require some prototyping to discover what works well in terms of methods and chosen input observations. There may be rare examples of chunks with complete data where classical methods like ETS or SARIMA could be used for univariate forecasting. Generally, the problem resists the classical methods. A good choice would be the use of nonlinear machine learning methods that are agnostic about the temporal structure of the input data, making use of whatever is available. Such models could be used in a recursive or direct strategy to forecast the lead times. A direct strategy may make more sense, with one model per required lead time. There are 10 lead times, and 39 target variables, in which case a direct strategy would require (39 * 10) or 390 models. A downside of the direct approach to modeling the problem is the inability of the model to leverage any dependencies between target variables in the forecast interval, specifically across sites, across variables, or across lead times. If these dependencies exist (and some surely do), it may be possible to add a flavor of them in using a second-tier of of ensemble models. Feature selection could be used to discover the variables and/or the lag lead times that may provide the most value in forecasting each target variable and lead time. This approach would provide a lot of flexibility, and as was shown in the competition, ensembles of decision trees perform well with little tuning. Like machine learning methods, deep learning methods may be able to use whatever multivariate data is available in order to make a prediction. Two classes of neural networks may be worth exploring for this problem: CNNs are capable of distilling long sequences of multivariate input time series data into small feature maps, and in essence learn the features from the sequences that are most relevant for forecasting. Their ability to handle noise and feature invariance across the input sequences may be useful. Like other neural networks, CNNs can output a vector in order to predict the forecast lead times. LSTMs are designed to work with sequence data and can directly support missing data via masking. They too are capable of automatic feature learning from long input sequences and alone or combined with CNNs may perform well on this problem. Together with an encoder-decoder architecture, the LSTM network can be used to natively forecast multiple lead times. A naive approach that mirrors that used in the competition might be best for evaluating models. That is, splitting each chunk into train and test sets, in this case using the first five days of data for training and the remaining three for test. It may be possible and interesting to finalize a model by training it on the entire dataset and submitting a forecast to the Kaggle website for evaluation on the held out test dataset. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered and explored the Air Quality Prediction dataset that represents a challenging multivariate, multi-site, and multi-step time series forecasting problem. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): variable(95), chunk(50), observation(43), site(30), plot(24), model(17), value(15), column(10), row(10), time(10)"
"5","mastery",2018-10-10,"How to Develop LSTM Models for Multi-Step Time Series Forecasting of Household Power Consumption","https://machinelearningmastery.com/how-to-develop-lstm-models-for-multi-step-time-series-forecasting-of-household-power-consumption/","Given the rise of smart electricity meters and the wide adoption of electricity generation technology like solar panels, there is a wealth of electricity usage data available. This data represents a multivariate time series of power-related variables that in turn could be used to model and even forecast future electricity consumption. Unlike other machine learning algorithms, long short-term memory recurrent neural networks are capable of automatically learning features from sequence data, support multiple-variate data, and can output a variable length sequences that can be used for multi-step forecasting. In this tutorial, you will discover how to develop long short-term memory recurrent neural networks for multi-step time series forecasting of household power consumption. After completing this tutorial, you will know: Let¡¯s get started. How to Develop LSTM Models for Multi-Step Time Series Forecasting of Household Power ConsumptionPhoto by Ian Muttoo, some rights reserved. This tutorial is divided into nine parts; they are: The ¡®Household Power Consumption¡® dataset is a multivariate time series dataset that describes the electricity consumption for a single household over four years. The data was collected between December 2006 and November 2010 and observations of power consumption within the household were collected every minute. It is a multivariate series comprised of seven variables (besides the date and time); they are: Active and reactive energy refer to the technical details of alternative current. A fourth sub-metering variable can be created by subtracting the sum of three defined sub-metering variables from the total active energy as follows: The dataset can be downloaded from the UCI Machine Learning repository as a single 20 megabyte .zip file: Download the dataset and unzip it into your current working directory. You will now have the file ¡°household_power_consumption.txt¡± that is about 127 megabytes in size and contains all of the observations. We can use the read_csv() function to load the data and combine the first two columns into a single date-time column that we can use as an index. Next, we can mark all missing values indicated with a ¡®?¡® character with a NaN value, which is a float. This will allow us to work with the data as one array of floating point values rather than mixed types (less efficient.) We also need to fill in the missing values now that they have been marked. A very simple approach would be to copy the observation from the same time the day before. We can implement this in a function named fill_missing() that will take the NumPy array of the data and copy values from exactly 24 hours ago. We can apply this function directly to the data within the DataFrame. Now we can create a new column that contains the remainder of the sub-metering, using the calculation from the previous section. We can now save the cleaned-up version of the dataset to a new file; in this case we will just change the file extension to .csv and save the dataset as ¡®household_power_consumption.csv¡®. Tying all of this together, the complete example of loading, cleaning-up, and saving the dataset is listed below. Running the example creates the new file ¡®household_power_consumption.csv¡® that we can use as the starting point for our modeling project. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this section, we will consider how we can develop and evaluate predictive models for the household power dataset. This section is divided into four parts; they are: There are many ways to harness and explore the household power consumption dataset. In this tutorial, we will use the data to explore a very specific question; that is: Given recent power consumption, what is the expected power consumption for the week ahead? This requires that a predictive model forecast the total active power for each day over the next seven days. Technically, this framing of the problem is referred to as a multi-step time series forecasting problem, given the multiple forecast steps. A model that makes use of multiple input variables may be referred to as a multivariate multi-step time series forecasting model. A model of this type could be helpful within the household in planning expenditures. It could also be helpful on the supply side for planning electricity demand for a specific household. This framing of the dataset also suggests that it would be useful to downsample the per-minute observations of power consumption to daily totals. This is not required, but makes sense, given that we are interested in total power per day. We can achieve this easily using the resample() function on the pandas DataFrame. Calling this function with the argument ¡®D¡® allows the loaded data indexed by date-time to be grouped by day (see all offset aliases). We can then calculate the sum of all observations for each day and create a new dataset of daily power consumption data for each of the eight variables. The complete example is listed below. Running the example creates a new daily total power consumption dataset and saves the result into a separate file named ¡®household_power_consumption_days.csv¡®. We can use this as the dataset for fitting and evaluating predictive models for the chosen framing of the problem. A forecast will be comprised of seven values, one for each day of the week ahead. It is common with multi-step forecasting problems to evaluate each forecasted time step separately. This is helpful for a few reasons: The units of the total power are kilowatts and it would be useful to have an error metric that was also in the same units. Both Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) fit this bill, although RMSE is more commonly used and will be adopted in this tutorial. Unlike MAE, RMSE is more punishing of forecast errors. The performance metric for this problem will be the RMSE for each lead time from day 1 to day 7. As a short-cut, it may be useful to summarize the performance of a model using a single score in order to aide in model selection. One possible score that could be used would be the RMSE across all forecast days. The function evaluate_forecasts() below will implement this behavior and return the performance of a model based on multiple seven-day forecasts. Running the function will first return the overall RMSE regardless of day, then an array of RMSE scores for each day. We will use the first three years of data for training predictive models and the final year for evaluating models. The data in a given dataset will be divided into standard weeks. These are weeks that begin on a Sunday and end on a Saturday. This is a realistic and useful way for using the chosen framing of the model, where the power consumption for the week ahead can be predicted. It is also helpful with modeling, where models can be used to predict a specific day (e.g. Wednesday) or the entire sequence. We will split the data into standard weeks, working backwards from the test dataset. The final year of the data is in 2010 and the first Sunday for 2010 was January 3rd. The data ends in mid November 2010 and the closest final Saturday in the data is November 20th. This gives 46 weeks of test data. The first and last rows of daily data for the test dataset are provided below for confirmation. The daily data starts in late 2006. The first Sunday in the dataset is December 17th, which is the second row of data. Organizing the data into standard weeks gives 159 full standard weeks for training a predictive model. The function split_dataset() below splits the daily data into train and test sets and organizes each into standard weeks. Specific row offsets are used to split the data using knowledge of the dataset. The split datasets are then organized into weekly data using the NumPy split() function. We can test this function out by loading the daily dataset and printing the first and last rows of data from both the train and test sets to confirm they match the expectations above. The complete code example is listed below. Running the example shows that indeed the train dataset has 159 weeks of data, whereas the test dataset has 46 weeks. We can see that the total active power for the train and test dataset for the first and last rows match the data for the specific dates that we defined as the bounds on the standard weeks for each set. Models will be evaluated using a scheme called walk-forward validation. This is where a model is required to make a one week prediction, then the actual data for that week is made available to the model so that it can be used as the basis for making a prediction on the subsequent week. This is both realistic for how the model may be used in practice and beneficial to the models allowing them to make use of the best available data. We can demonstrate this below with separation of input data and output/predicted data. The walk-forward validation approach to evaluating predictive models on this dataset is provided below named evaluate_model(). The train and test datasets in standard-week format are provided to the function as arguments. An additional argument n_input is provided that is used to define the number of prior observations that the model will use as input in order to make a prediction. Two new functions are called: one to build a model from the training data called build_model() and another that uses the model to make forecasts for each new standard week called forecast(). These will be covered in subsequent sections. We are working with neural networks, and as such, they are generally slow to train but fast to evaluate. This means that the preferred usage of the models is to build them once on historical data and to use them to forecast each step of the walk-forward validation. The models are static (i.e. not updated) during their evaluation. This is different to other models that are faster to train where a model may be re-fit or updated each step of the walk-forward validation as new data is made available. With sufficient resources, it is possible to use neural networks this way, but we will not in this tutorial. The complete evaluate_model() function is listed below. Once we have the evaluation for a model, we can summarize the performance. The function below named summarize_scores() will display the performance of a model as a single line for easy comparison with other models. We now have all of the elements to begin evaluating predictive models on the dataset. Recurrent neural networks, or RNNs, are specifically designed to work, learn, and predict sequence data. A recurrent neural network is a neural network where the output of the network from one time step is provided as an input in the subsequent time step. This allows the model to make a decision as to what to predict based on both the input for the current time step and direct knowledge of what was output in the prior time step. Perhaps the most successful and widely used RNN is the long short-term memory network, or LSTM for short. It is successful because it overcomes the challenges involved in training a recurrent neural network, resulting in stable models. In addition to harnessing the recurrent connection of the outputs from the prior time step, LSTMs also have an internal memory that operates like a local variable, allowing them to accumulate state over the input sequence. For more information about Recurrent Neural Networks, see the post: For more information about Long Short-Term Memory networks, see the post: LSTMs offer a number of benefits when it comes to multi-step time series forecasting; they are: Further, specialized architectures have been developed that are specifically designed to make multi-step sequence predictions, generally referred to as sequence-to-sequence prediction, or seq2seq for short. This is useful as multi-step time series forecasting is a type of seq2seq prediction. An example of a recurrent neural network architecture designed for seq2seq problems is the encoder-decoder LSTM. An encoder-decoder LSTM is a model comprised of two sub-models: one called the encoder that reads the input sequences and compresses it to a fixed-length internal representation, and an output model called the decoder that interprets the internal representation and uses it to predict the output sequence. The encoder-decoder approach to sequence prediction has proven much more effective than outputting a vector directly and is the preferred approach. Generally, LSTMs have been found to not be very effective at auto-regression type problems. These are problems where forecasting the next time step is a function of recent time steps. For more on this issue, see the post: One-dimensional convolutional neural networks, or CNNs, have proven effective at automatically learning features from input sequences. A popular approach has been to combine CNNs with LSTMs, where the CNN is as an encoder to learn features from sub-sequences of input data which are provided as time steps to an LSTM. This architecture is called a CNN-LSTM. For more information on this architecture, see the post: A power variation on the CNN LSTM architecture is the ConvLSTM that uses the convolutional reading of input subsequences directly within an LSTM¡¯s units. This approach has proven very effective for time series classification and can be adapted for use in multi-step time series forecasting. In this tutorial, we will explore a suite of LSTM architectures for multi-step time series forecasting. Specifically, we will look at how to develop the following models: The models will be developed and demonstrated on the household power prediction problem. A model is considered skillful if it achieves performance better than a naive model, which is an overall RMSE of about 465 kilowatts across a seven day forecast. We will not focus on the tuning of these models to achieve optimal performance; instead, we will stop short at skillful models as compared to a naive forecast. The chosen structures and hyperparameters are chosen with a little trial and error. The scores should be taken as just an example rather than a study of the optimal model or configuration for the problem. Given the stochastic nature of the models, it is good practice to evaluate a given model multiple times and report the mean performance on a test dataset. In the interest of brevity and keeping the code simple, we will instead present single-runs of models in this tutorial. We cannot know which approach will be the most effective for a given multi-step forecasting problem. It is a good idea to explore a suite of methods in order to discover what works best on your specific dataset. We will start off by developing a simple or vanilla LSTM model that reads in a sequence of days of total daily power consumption and predicts a vector output of the next standard week of daily power consumption. This will provide the foundation for the more elaborate models developed in subsequent sections. The number of prior days used as input defines the one-dimensional (1D) subsequence of data that the LSTM will read and learn to extract features. Some ideas on the size and nature of this input include: There is no right answer; instead, each approach and more can be tested and the performance of the model can be used to choose the nature of the input that results in the best model performance. These choices define a few things: A good starting point would be to use the prior seven days. An LSTM model expects data to have the shape: One sample will be comprised of seven time steps with one feature for the seven days of total daily power consumed. The training dataset has 159 weeks of data, so the shape of the training dataset would be: This is a good start. The data in this format would use the prior standard week to predict the next standard week. A problem is that 159 instances is not a lot to train a neural network. A way to create a lot more training data is to change the problem during training to predict the next seven days given the prior seven days, regardless of the standard week. This only impacts the training data, and the test problem remains the same: predict the daily power consumption for the next standard week given the prior standard week. This will require a little preparation of the training data. The training data is provided in standard weeks with eight variables, specifically in the shape [159, 7, 8]. The first step is to flatten the data so that we have eight time series sequences. We then need to iterate over the time steps and divide the data into overlapping windows; each iteration moves along one time step and predicts the subsequent seven days. For example: We can do this by keeping track of start and end indexes for the inputs and outputs as we iterate across the length of the flattened data in terms of time steps. We can also do this in a way where the number of inputs and outputs are parameterized (e.g. n_input, n_out) so that you can experiment with different values or adapt it for your own problem. Below is a function named to_supervised() that takes a list of weeks (history) and the number of time steps to use as inputs and outputs and returns the data in the overlapping moving window format. When we run this function on the entire training dataset, we transform 159 samples into 1,099; specifically, the transformed dataset has the shapes X=[1099, 7, 1] and y=[1099, 7]. Next, we can define and fit the LSTM model on the training data. This multi-step time series forecasting problem is an autoregression. That means it is likely best modeled where that the next seven days is some function of observations at prior time steps. This and the relatively small amount of data means that a small model is required. We will develop a model with a single hidden LSTM layer with 200 units. The number of units in the hidden layer is unrelated to the number of time steps in the input sequences. The LSTM layer is followed by a fully connected layer with 200 nodes that will interpret the features learned by the LSTM layer. Finally, an output layer will directly predict a vector with seven elements, one for each day in the output sequence. We will use the mean squared error loss function as it is a good match for our chosen error metric of RMSE. We will use the efficient Adam implementation of stochastic gradient descent and fit the model for 70 epochs with a batch size of 16. The small batch size and the stochastic nature of the algorithm means that the same model will learn a slightly different mapping of inputs to outputs each time it is trained. This means results may vary when the model is evaluated. You can try running the model multiple times and calculate an average of model performance. The build_model() below prepares the training data, defines the model, and fits the model on the training data, returning the fit model ready for making predictions. Now that we know how to fit the model, we can look at how the model can be used to make a prediction. Generally, the model expects data to have the same three dimensional shape when making a prediction. In this case, the expected shape of an input pattern is one sample, seven days of one feature for the daily power consumed: Data must have this shape when making predictions for the test set and when a final model is being used to make predictions in the future. If you change the number if input days to 14, then the shape of the training data and the shape of new samples when making predictions must be changed accordingly to have 14 time steps. It is a modeling choice that you must carry forward when using the model. We are using walk-forward validation to evaluate the model as described in the previous section. This means that we have the observations available for the prior week in order to predict the coming week. These are collected into an array of standard weeks called history. In order to predict the next standard week, we need to retrieve the last days of observations. As with the training data, we must first flatten the history data to remove the weekly structure so that we end up with eight parallel time series. Next, we need to retrieve the last seven days of daily total power consumed (feature index 0). We will parameterize this as we did for the training data so that the number of prior days used as input by the model can be modified in the future. Next, we reshape the input into the expected three-dimensional structure. We then make a prediction using the fit model and the input data and retrieve the vector of seven days of output. The forecast() function below implements this and takes as arguments the model fit on the training dataset, the history of data observed so far, and the number of input time steps expected by the model. That¡¯s it; we now have everything we need to make multi-step time series forecasts with an LSTM model on the daily total power consumed univariate dataset. We can tie all of this together. The complete example is listed below. Running the example fits and evaluates the model, printing the overall RMSE across all seven days, and the per-day RMSE for each lead time. Your specific results may vary given the stochastic nature of the algorithm. You may want to try running the example a few times. We can see that in this case, the model was skillful as compared to a naive forecast, achieving an overall RMSE of about 399 kilowatts, less than 465 kilowatts achieved by a naive model. A plot of the daily RMSE is also created. The plot shows that perhaps Tuesdays and Fridays are easier days to forecast than the other days and that perhaps Saturday at the end of the standard week is the hardest day to forecast. Line Plot of RMSE per Day for Univariate LSTM with Vector Output and 7-day Inputs We can increase the number of prior days to use as input from seven to 14 by changing the n_input variable. Re-running the example with this change first prints a summary of performance of the model. Your specific results may vary; try running the example a few times. In this case, we can see a further drop in the overall RMSE to about 370 kilowatts, suggesting that further tuning of the input size and perhaps the number of nodes in the model may result in better performance. Comparing the per-day RMSE scores we see some are better and some are worse than using seven-day inputs. This may suggest benefit in using the two different sized inputs in some way, such as an ensemble of the two approaches or perhaps a single model (e.g. a multi-headed model) that reads the training data in different ways. Line Plot of RMSE per Day for Univariate LSTM with Vector Output and 14-day Inputs In this section, we can update the vanilla LSTM to use an encoder-decoder model. This means that the model will not output a vector sequence directly. Instead, the model will be comprised of two sub models, the encoder to read and encode the input sequence, and the decoder that will read the encoded input sequence and make a one-step prediction for each element in the output sequence. The difference is subtle, as in practice both approaches do in fact predict a sequence output. The important difference is that an LSTM model is used in the decoder, allowing it to both know what was predicted for the prior day in the sequence and accumulate internal state while outputting the sequence. Let¡¯s take a closer look at how this model is defined. As before, we define an LSTM hidden layer with 200 units. This is the decoder model that will read the input sequence and will output a 200 element vector (one output per unit) that captures features from the input sequence. We will use 14 days of total power consumption as input. We will use a simple encoder-decoder architecture that is easy to implement in Keras, that has a lot of similarity to the architecture of an LSTM autoencoder. First, the internal representation of the input sequence is repeated multiple times, once for each time step in the output sequence. This sequence of vectors will be presented to the LSTM decoder. We then define the decoder as an LSTM hidden layer with 200 units. Importantly, the decoder will output the entire sequence, not just the output at the end of the sequence as we did with the encoder. This means that each of the 200 units will output a value for each of the seven days, representing the basis for what to predict for each day in the output sequence. We will then use a fully connected layer to interpret each time step in the output sequence before the final output layer. Importantly, the output layer predicts a single step in the output sequence, not all seven days at a time, This means that we will use the same layers applied to each step in the output sequence. It means that the same fully connected layer and output layer will be used to process each time step provided by the decoder. To achieve this, we will wrap the interpretation layer and the output layer in a TimeDistributed wrapper that allows the wrapped layers to be used for each time step from the decoder. This allows the LSTM decoder to figure out the context required for each step in the output sequence and the wrapped dense layers to interpret each time step separately, yet reusing the same weights to perform the interpretation. An alternative would be to flatten all of the structure created by the LSTM decoder and to output the vector directly. You can try this as an extension to see how it compares. The network therefore outputs a three-dimensional vector with the same structure as the input, with the dimensions [samples, timesteps, features]. There is a single feature, the daily total power consumed, and there are always seven features. A single one-week prediction will therefore have the size: [1, 7, 1]. Therefore, when training the model, we must restructure the output data (y) to have the three-dimensional structure instead of the two-dimensional structure of [samples, features] used in the previous section. We can tie all of this together into the updated build_model() function listed below. The complete example with the encoder-decoder model is listed below. Running the example fits the model and summarizes the performance on the test dataset. Your specific results may vary given the stochastic nature of the algorithm. You may want to try running the example a few times. We can see that in this case, the model is skillful, achieving an overall RMSE score of about 372 kilowatts. A line plot of the per-day RMSE is also created showing a similar pattern in error as was seen in the previous section. Line Plot of RMSE per Day for Univariate Encoder-Decoder LSTM with 14-day Inputs In this section, we will update the Encoder-Decoder LSTM developed in the previous section to use each of the eight time series variables to predict the next standard week of daily total power consumption. We will do this by providing each one-dimensional time series to the model as a separate sequence of input. The LSTM will in turn create an internal representation of each input sequence that will together be interpreted by the decoder. Using multivariate inputs is helpful for those problems where the output sequence is some function of the observations at prior time steps from multiple different features, not just (or including) the feature being forecasted. It is unclear whether this is the case in the power consumption problem, but we can explore it nonetheless. First, we must update the preparation of the training data to include all of the eight features, not just the one total daily power consumed. It requires a single line change: The complete to_supervised() function with this change is listed below. We also must update the function used to make forecasts with the fit model to use all eight features from the prior time steps. Again, another small change: The complete forecast()<U+00A0>function with this change is listed below: The same model architecture and configuration is used directly, although we will increase the number of training epochs from 20 to 50 given the 8-fold increase in the amount of input data. The complete example is listed below. Running the example fits the model and summarizes the performance on the test dataset. Experimentation found that this model appears less stable than the univariate case and may be related to the differing scales of the input eight variables. Your specific results may vary given the stochastic nature of the algorithm. You may want to try running the example a few times. We can see that in this case, the model is skillful, achieving an overall RMSE score of about 376 kilowatts. A line plot of the per-day RMSE is also created. Line Plot of RMSE per Day for Multivariate Encoder-Decoder LSTM with 14-day Inputs A convolutional neural network, or CNN, can be used as the encoder in an encoder-decoder architecture. The CNN does not directly support sequence input; instead, a 1D CNN is capable of reading across sequence input and automatically learning the salient features. These can then be interpreted by an LSTM decoder as per normal. We refer to hybrid models that use a CNN and LSTM as CNN-LSTM models, and in this case we are using them together in an encoder-decoder architecture. The CNN expects the input data to have the same 3D structure as the LSTM model, although multiple features are read as different channels that ultimately have the same effect. We will simplify the example and focus on the CNN-LSTM with univariate input, but it can just as easily be updated to use multivariate input, which is left as an exercise. As before, we will use input sequences comprised of 14 days of daily total power consumption. We will define a simple but effective CNN architecture for the encoder that is comprised of two convolutional layers followed by a max pooling layer, the results of which are then flattened. The first convolutional layer reads across the input sequence and projects the results onto feature maps. The second performs the same operation on the feature maps created by the first layer, attempting to amplify any salient features. We will use 64 feature maps per convolutional layer and read the input sequences with a kernel size of three time steps. The max pooling layer simplifies the feature maps by keeping 1/4 of the values with the largest (max) signal. The distilled feature maps after the pooling layer are then flattened into one long vector that can then be used as input to the decoding process. The decoder is the same as was defined in previous sections. The only other change is to set the number of training epochs to 20. The build_model() function with these changes is listed below. We are now ready to try the encoder-decoder architecture with a CNN encoder. The complete code listing is provided below. Running the example fits the model and summarizes the performance on the test dataset. A little experimentation showed that using two convolutional layers made the model more stable than using just a single layer. Your specific results may vary given the stochastic nature of the algorithm. You may want to try running the example a few times. We can see that in this case the model is skillful, achieving an overall RMSE score of about 372 kilowatts. A line plot of the per-day RMSE is also created. Line Plot of RMSE per Day for Univariate Encoder-Decoder CNN LSTM with 14-day Inputs A further extension of the CNN-LSTM approach is to perform the convolutions of the CNN (e.g. how the CNN reads the input sequence data) as part of the LSTM for each time step. This combination is called a Convolutional LSTM, or ConvLSTM for short, and like the CNN-LSTM is also used for spatio-temporal data. Unlike an LSTM that reads the data in directly in order to calculate internal state and state transitions, and unlike the CNN-LSTM that is interpreting the output from CNN models, the ConvLSTM is using convolutions directly as part of reading input into the LSTM units themselves. For more information for how the equations for the ConvLSTM are calculated within the LSTM unit, see the paper: The Keras library provides the ConvLSTM2D class that supports the ConvLSTM model for 2D data. It can be configured for 1D multivariate time series forecasting. The ConvLSTM2D class, by default, expects input data to have the shape: Where each time step of data is defined as an image of (rows * columns) data points. We are working with a one-dimensional sequence of total power consumption, which we can interpret as one row with 14 columns, if we assume that we are using two weeks of data as input. For the ConvLSTM, this would be a single read: that is, the LSTM would read one time step of 14 days and perform a convolution across those time steps. This is not ideal. Instead, we can split the 14 days into two subsequences with a length of seven days. The ConvLSTM can then read across the two time steps and perform the CNN process on the seven days of data within each. For this chosen framing of the problem, the input for the ConvLSTM2D would therefore be: Or: You can explore other configurations, such as providing 21 days of input split into three subsequences of seven days, and/or providing all eight features or channels as input. We can now prepare the data for the ConvLSTM2D model. First, we must reshape the training dataset into the expected structure of [samples, timesteps, rows, cols, channels]. We can then define the encoder as a ConvLSTM hidden layer followed by a flatten layer ready for decoding. We will also parameterize the number of subsequences (n_steps) and the length of each subsequence (n_length) and pass them as arguments. The rest of the model and training is the same. The build_model() function with these changes is listed below. This model expects five-dimensional data as input. Therefore, we must also update the preparation of a single sample in the forecast() function when making a prediction. The forecast() function with this change and with the parameterized subsequences is provided below. We now have all of the elements for evaluating an encoder-decoder architecture for multi-step time series forecasting where a ConvLSTM is used as the encoder. The complete code example is listed below. Running the example fits the model and summarizes the performance on the test dataset. A little experimentation showed that using two convolutional layers made the model more stable than using just a single layer. We can see that in this case the model is skillful, achieving an overall RMSE score of about 367 kilowatts. A line plot of the per-day RMSE is also created. Line Plot of RMSE per Day for Univariate Encoder-Decoder ConvLSTM with 14-day Inputs This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop long short-term memory recurrent neural networks for multi-step time series forecasting of household power consumption. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hi Jason, Thanks for another great article.  I¡¯ve got a question about your thoughts about Attention based networks and how do they compere to LSTMs. I heard many voices in favor of the first ones, but I would like to know how this looks in real situations and not competitions-world <U+0001F609> Thanks,
Konrad Attention-based models can offer a lot of benefit on challenging sequence prediction problems. I have not used attention for time series forecasting though, sorry. Id on¡¯t have good off the cuff advice. Ok, sure, thanks for reply! <U+0001F642> # model.add(LSTM(200, activation=¡¯relu¡¯, input_shape=(n_timesteps, n_features)))
# model.add(Dense(100, activation=¡¯relu¡¯)) how do we choose LSTM unit and dense unit? for example, here 200 units for LSTM and 100 units for Dense have been used.  is there any formula out there? should we guess?  it would be great if you could explain!  Thanks in advance. Trial and error. I explain more here:https://machinelearningmastery.com/faq/single-faq/how-many-layers-and-nodes-do-i-need-in-my-neural-network How to calculate the accuracy of the Convolutional LSTM model of the electricity consumption dataset. Can you please provide the code for that? It is a regression problem, we cannot calculate accuracy for a regression problem. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): model(28), feature(16), step(15), input(13), unit(11), network(10), result(10), kilowatt(9), observation(9), time(8)"
"6","mastery",2018-10-08,"How to Develop Convolutional Neural Networks for Multi-Step Time Series Forecasting","https://machinelearningmastery.com/how-to-develop-convolutional-neural-networks-for-multi-step-time-series-forecasting/","Given the rise of smart electricity meters and the wide adoption of electricity generation technology like solar panels, there is a wealth of electricity usage data available. This data represents a multivariate time series of power-related variables that in turn could be used to model and even forecast future electricity consumption. Unlike other machine learning algorithms, convolutional neural networks are capable of automatically learning features from sequence data, support multiple-variate data, and can directly output a vector for multi-step forecasting. As such, one-dimensional CNNs have been demonstrated to perform well and even achieve state-of-the-art results on challenging sequence prediction problems. In this tutorial, you will discover how to develop 1D convolutional neural networks for multi-step time series forecasting. After completing this tutorial, you will know: Let¡¯s get started. How to Develop Convolutional Neural Networks for Multi-Step Time Series ForecastingPhoto by Banalities, some rights reserved. This tutorial is divided into seven parts; they are: The ¡®Household Power Consumption¡® dataset is a multivariate time series dataset that describes the electricity consumption for a single household over four years. The data was collected between December 2006 and November 2010 and observations of power consumption within the household were collected every minute. It is a multivariate series comprised of seven variables (besides the date and time); they are: Active and reactive energy refer to the technical details of alternative current. A fourth sub-metering variable can be created by subtracting the sum of three defined sub-metering variables from the total active energy as follows: The dataset can be downloaded from the UCI Machine Learning repository as a single 20 megabyte .zip file: Download the dataset and unzip it into your current working directory. You will now have the file ¡°household_power_consumption.txt¡± that is about 127 megabytes in size and contains all of the observations. We can use the read_csv() function to load the data and combine the first two columns into a single date-time column that we can use as an index. Next, we can mark all missing values indicated with a ¡®?¡® character with a NaN value, which is a float. This will allow us to work with the data as one array of floating point values rather than mixed types (less efficient.) We also need to fill in the missing values now that they have been marked. A very simple approach would be to copy the observation from the same time the day before. We can implement this in a function named fill_missing() that will take the NumPy array of the data and copy values from exactly 24 hours ago. We can apply this function directly to the data within the DataFrame. Now we can create a new column that contains the remainder of the sub-metering, using the calculation from the previous section. We can now save the cleaned-up version of the dataset to a new file; in this case we will just change the file extension to .csv and save the dataset as ¡®household_power_consumption.csv¡®. Tying all of this together, the complete example of loading, cleaning-up, and saving the dataset is listed below. Running the example creates the new file ¡®household_power_consumption.csv¡® that we can use as the starting point for our modeling project. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this section, we will consider how we can develop and evaluate predictive models for the household power dataset. This section is divided into four parts; they are: There are many ways to harness and explore the household power consumption dataset. In this tutorial, we will use the data to explore a very specific question; that is: Given recent power consumption, what is the expected power consumption for the week ahead? This requires that a predictive model forecast the total active power for each day over the next seven days. Technically, this framing of the problem is referred to as a multi-step time series forecasting problem, given the multiple forecast steps. A model that makes use of multiple input variables may be referred to as a multivariate multi-step time series forecasting model. A model of this type could be helpful within the household in planning expenditures. It could also be helpful on the supply side for planning electricity demand for a specific household. This framing of the dataset also suggests that it would be useful to downsample the per-minute observations of power consumption to daily totals. This is not required, but makes sense, given that we are interested in total power per day. We can achieve this easily using the resample() function on the pandas DataFrame. Calling this function with the argument ¡®D¡® allows the loaded data indexed by date-time to be grouped by day (see all offset aliases). We can then calculate the sum of all observations for each day and create a new dataset of daily power consumption data for each of the eight variables. The complete example is listed below. Running the example creates a new daily total power consumption dataset and saves the result into a separate file named ¡®household_power_consumption_days.csv¡®. We can use this as the dataset for fitting and evaluating predictive models for the chosen framing of the problem. A forecast will be comprised of seven values, one for each day of the week ahead. It is common with multi-step forecasting problems to evaluate each forecasted time step separately. This is helpful for a few reasons: The units of the total power are kilowatts and it would be useful to have an error metric that was also in the same units. Both Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) fit this bill, although RMSE is more commonly used and will be adopted in this tutorial. Unlike MAE, RMSE is more punishing of forecast errors. The performance metric for this problem will be the RMSE for each lead time from day 1 to day 7. As a short-cut, it may be useful to summarize the performance of a model using a single score in order to aide in model selection. One possible score that could be used would be the RMSE across all forecast days. The function evaluate_forecasts() below will implement this behavior and return the performance of a model based on multiple seven-day forecasts. Running the function will first return the overall RMSE regardless of day, then an array of RMSE scores for each day. We will use the first three years of data for training predictive models and the final year for evaluating models. The data in a given dataset will be divided into standard weeks. These are weeks that begin on a Sunday and end on a Saturday. This is a realistic and useful way for using the chosen framing of the model, where the power consumption for the week ahead can be predicted. It is also helpful with modeling, where models can be used to predict a specific day (e.g. Wednesday) or the entire sequence. We will split the data into standard weeks, working backwards from the test dataset. The final year of the data is in 2010 and the first Sunday for 2010 was January 3rd. The data ends in mid November 2010 and the closest final Saturday in the data is November 20th. This gives 46 weeks of test data. The first and last rows of daily data for the test dataset are provided below for confirmation. The daily data starts in late 2006. The first Sunday in the dataset is December 17th, which is the second row of data. Organizing the data into standard weeks gives 159 full standard weeks for training a predictive model. The function split_dataset() below splits the daily data into train and test sets and organizes each into standard weeks. Specific row offsets are used to split the data using knowledge of the dataset. The split datasets are then organized into weekly data using the NumPy split() function. We can test this function out by loading the daily dataset and printing the first and last rows of data from both the train and test sets to confirm they match the expectations above. The complete code example is listed below. Running the example shows that indeed the train dataset has 159 weeks of data, whereas the test dataset has 46 weeks. We can see that the total active power for the train and test dataset for the first and last rows match the data for the specific dates that we defined as the bounds on the standard weeks for each set. Models will be evaluated using a scheme called walk-forward validation. This is where a model is required to make a one week prediction, then the actual data for that week is made available to the model so that it can be used as the basis for making a prediction on the subsequent week. This is both realistic for how the model may be used in practice and beneficial to the models, allowing them to make use of the best available data. We can demonstrate this below with separation of input data and output/predicted data. The walk-forward validation approach to evaluating predictive models on this dataset is provided below, named evaluate_model(). The train and test datasets in standard-week format are provided to the function as arguments. An additional argument, n_input, is provided that is used to define the number of prior observations that the model will use as input in order to make a prediction. Two new functions are called: one to build a model from the training data called build_model() and another that uses the model to make forecasts for each new standard week, called forecast(). These will be covered in subsequent sections. We are working with neural networks and as such they are generally slow to train but fast to evaluate. This means that the preferred usage of the models is to build them once on historical data and to use them to forecast each step of the walk-forward validation. The models are static (i.e. not updated) during their evaluation. This is different to other models that are faster to train, where a model may be re-fit or updated each step of the walk-forward validation as new data is made available. With sufficient resources, it is possible to use neural networks this way, but we will not in this tutorial. The complete evaluate_model() function is listed below. Once we have the evaluation for a model, we can summarize the performance. The function below, named summarize_scores(), will display the performance of a model as a single line for easy comparison with other models. We now have all of the elements to begin evaluating predictive models on the dataset. Convolutional Neural Network models, or CNNs for short, are a type of deep neural network that was developed for use with image data, such as handwriting recognition. They are proven very effective on challenging computer vision problems when trained at scale for tasks such as identifying and localizing objects in images and automatically describing the content of images. They are a model that are comprised of two main types of elements: convolutional layers and pooling layers. Convolutional layers read an input, such as a 2D image or a 1D signal using a kernel that reads in small segments at a time and steps across the entire input field. Each read results in an interpretation of the input that is projected onto a filter map and represents an interpretation of the input. Pooling layers take the feature map projections and distill them to the most essential elements, such as using a signal averaging or signal maximizing process. The convolution and pooling layers can be repeated at depth, providing multiple layers of abstraction of the input signals. The output of these networks is often one or more fully-connected layers that interpret what has been read and maps this internal representation to a class value. For more information on convolutional neural networks, you can see the post: Convolutional neural networks can be used for multi-step time series forecasting. The key benefits of the approach are the automatic feature learning and the ability of the model to output a multi-step vector directly. CNNs can be used in either a recursive or direct forecast strategy, where the model makes one-step predictions and outputs are fed as inputs for subsequent predictions, and where one model is developed for each time step to be predicted. Alternately, CNNs can be used to predict the entire output sequence as a one-step prediction of the entire vector. This is a general benefit of feed-forward neural networks. An important secondary benefit of using CNNs is that they can support multiple 1D inputs in order to make a prediction. This is useful if the multi-step output sequence is a function of more than one input sequence. This can be achieved using two different model configurations. In this tutorial, we will explore how to develop three different types of CNN models for multi-step time series forecasting; they are: The models will be developed and demonstrated on the household power prediction problem. A model is considered skillful if it achieves performance better than a naive model, which is an overall RMSE of about 465 kilowatts across a seven day forecast. We will not focus on the tuning of these models to achieve optimal performance; instead we will sill stop short at skillful models as compared to a naive forecast. The chosen structures and hyperparameters are chosen with a little trial and error. In this section, we will develop a convolutional neural network for multi-step time series forecasting using only the univariate sequence of daily power consumption. Specifically, the framing of the problem is: Given some number of prior days of total daily power consumption, predict the next standard week of daily power consumption. The number of prior days used as input defines the one-dimensional (1D) subsequence of data that the CNN will read and learn to extract features. Some ideas on the size and nature of this input include: There is no right answer; instead, each approach and more can be tested and the performance of the model can be used to choose the nature of the input that results in the best model performance. These choices define a few things about the implementation, such as: A good starting point would be to use the prior seven days. A 1D CNN model expects data to have the shape of: One sample will be comprised of seven time steps with one feature for the seven days of total daily power consumed. The training dataset has 159 weeks of data, so the shape of the training dataset would be: This is a good start. The data in this format would use the prior standard week to predict the next standard week. A problem is that 159 instances is not a lot for a neural network. A way to create a lot more training data is to change the problem during training to predict the next seven days given the prior seven days, regardless of the standard week. This only impacts the training data, the test problem remains the same: predict the daily power consumption for the next standard week given the prior standard week. This will require a little preparation of the training data. The training data is provided in standard weeks with eight variables, specifically in the shape [159, 7, 8]. The first step is to flatten the data so that we have eight time series sequences. We then need to iterate over the time steps and divide the data into overlapping windows; each iteration moves along one time step and predicts the subsequent seven days. For example: We can do this by keeping track of start and end indexes for the inputs and outputs as we iterate across the length of the flattened data in terms of time steps. We can also do this in a way where the number of inputs and outputs are parameterized (e.g. n_input, n_out) so that you can experiment with different values or adapt it for your own problem. Below is a function named to_supervised() that takes a list of weeks (history) and the number of time steps to use as inputs and outputs and returns the data in the overlapping moving window format. When we run this function on the entire training dataset, we transform 159 samples into 1,099; specifically, the transformed dataset has the shapes X=[1099, 7, 1] and y=[1099, 7]. Next, we can define and fit the CNN model on the training data. This multi-step time series forecasting problem is an autoregression. That means it is likely best modeled where that the next seven days is some function of observations at prior time steps. This and the relatively small amount of data means that a small model is required. We will use a model with one convolution layer with 16 filters and a kernel size of 3. This means that the input sequence of seven days will be read with a convolutional operation three time steps at a time and this operation will be performed 16 times. A pooling layer will reduce these feature maps by 1/4 their size before the internal representation is flattened to one long vector. This is then interpreted by a fully connected layer before the output layer predicts the next seven days in the sequence. We will use the mean squared error loss function as it is a good match for our chosen error metric of RMSE. We will use the efficient Adam implementation of stochastic gradient descent and fit the model for 20 epochs with a batch size of 4. The small batch size and the stochastic nature of the algorithm means that the same model will learn a slightly different mapping of inputs to outputs each time it is trained. This means results may vary when the model is evaluated. You can try running the model multiple times and calculating an average of model performance. The build_model() below prepares the training data, defines the model, and fits the model on the training data, returning the fit model ready for making predictions. Now that we know how to fit the model, we can look at how the model can be used to make a prediction. Generally, the model expects data to have the same three dimensional shape when making a prediction. In this case, the expected shape of an input pattern is one sample, seven days of one feature for the daily power consumed: Data must have this shape when making predictions for the test set and when a final model is being used to make predictions in the future. If you change the number of input days to 14, then the shape of the training data and the shape of new samples when making predictions must be changed accordingly to have 14 time steps. It is a modeling choice that you must carry forward when using the model. We are using walk-forward validation to evaluate the model as described in the previous section. This means that we have the observations available for the prior week in order to predict the coming week. These are collected into an array of standard weeks, called history. In order to predict the next standard week, we need to retrieve the last days of observations. As with the training data, we must first flatten the history data to remove the weekly structure so that we end up with eight parallel time series. Next, we need to retrieve the last seven days of daily total power consumed (feature number 0). We will parameterize as we did for the training data so that the number of prior days used as input by the model can be modified in the future. Next, we reshape the input into the expected three-dimensional structure. We then make a prediction using the fit model and the input data and retrieve the vector of seven days of output. The forecast() function below implements this and takes as arguments the model fit on the training dataset, the history of data observed so far, and the number of inputs time steps expected by the model. That¡¯s it; we now have everything we need to make multi-step time series forecasts with a CNN model on the daily total power consumed univariate dataset. We can tie all of this together. The complete example is listed below. Running the example fits and evaluates the model, printing the overall RMSE across all seven days, and the per-day RMSE for each lead time. Your specific results may vary given the stochastic nature of the algorithm. You may want to try running the example a few times. We can see that in this case, the model was skillful as compared to a naive forecast, achieving an overall RMSE of about 404 kilowatts, less than 465 kilowatts achieved by a naive model. A plot of the daily RMSE is also created. The plot shows that perhaps Tuesdays and Fridays are easier days to forecast than the other days and that perhaps Saturday at the end of the standard week is the hardest day to forecast. Line Plot of RMSE per Day for Univariate CNN with 7-day Inputs We can increase the number of prior days to use as input from seven to 14 by changing the n_input variable. Re-running the example with this change first prints a summary of the performance of the model. Your specific results may vary; try running the example a few times. In this case, we can see a further drop in the overall RMSE, suggesting that further tuning of the input size and perhaps the kernel size of the model may result in better performance. Comparing the per-day RMSE scores, we see some are better and some are worse than using seventh inputs. This may suggest a benefit in using the two different sized inputs in some way, such as an ensemble of the two approaches or perhaps a single model (e.g. a multi-headed model) that reads the training data in different ways. Line Plot of RMSE per Day for Univariate CNN with 14-day Inputs In this section, we will update the CNN developed in the previous section to use each of the eight time series variables to predict the next standard week of daily total power consumption. We will do this by providing each one-dimensional time series to the model as a separate channel of input. The CNN will then use a separate kernel and read each input sequence onto a separate set of filter maps, essentially learning features from each input time series variable. This is helpful for those problems where the output sequence is some function of the observations at prior time steps from multiple different features, not just (or including) the feature being forecasted. It is unclear whether this is the case in the power consumption problem, but we can explore it nonetheless. First, we must update the preparation of the training data to include all of the eight features, not just the one total daily power consumed. It requires a single line: The complete to_supervised() function with this change is listed below. We also must update the function used to make forecasts with the fit model to use all eight features from the prior time steps. Again, another small change: The complete forecast() with this change is listed below: We will use 14 days of prior observations across eight of the input variables as we did in the final section of the prior section that resulted in slightly better performance. Finally, the model used in the previous section does not perform well on this new framing of the problem. The increase in the amount of data requires a larger and more sophisticated model that is trained for longer. With a little trial and error, one model that performs well uses two convolutional layers with 32 filter maps followed by pooling, then another convolutional layer with 16 feature maps and pooling. The fully connected layer that interprets the features is increased to 100 nodes and the model is fit for 70 epochs with a batch size of 16 samples. The updated build_model() function that defines and fits the model on the training dataset is listed below. We now have all of the elements required to develop a multi-channel CNN for multivariate input data to make multi-step time series forecasts. The complete example is listed below. Running the example fits and evaluates the model, printing the overall RMSE across all seven days, and the per-day RMSE for each lead time. Your specific results may vary given the stochastic nature of the algorithm. You may want to try running the example a few times. We can see that in this case, the use of all eight input variables does result in another small drop in the overall RMSE score. For the daily RMSE scores, we do see that some are better and some are worse than the univariate CNN from the previous section. The final day, Saturday, remains a challenging day to forecast, and Friday an easy day to forecast. There may be some benefit in designing models to focus specifically on reducing the error of the harder to forecast days. It may be interesting to see if the variance across daily scores could be further reduced with a tuned model or perhaps an ensemble of multiple different models. It may also be interesting to compare the performance for a model that uses seven or even 21 days of input data to see if further gains can be made. Line Plot of RMSE per Day for a Multichannel CNN with 14-day Inputs We can further extend the CNN model to have a separate sub-CNN model or head for each input variable, which we can refer to as a multi-headed CNN model. This requires a modification to the preparation of the model, and in turn, modification to the preparation of the training and test datasets. Starting with the model, we must define a separate CNN model for each of the eight input variables. The configuration of the model, including the number of layers and their hyperparameters, were also modified to better suit the new approach. The new configuration is not optimal and was found with a little trial and error. The multi-headed model is specified using the more flexible functional API for defining Keras models. We can loop over each variable and create a sub-model that takes a one-dimensional sequence of 14 days of data and outputs a flat vector containing a summary of the learned features from the sequence. Each of these vectors can be merged via concatenation to make one very long vector that is then interpreted by some fully connected layers before a prediction is made. As we build up the submodels, we keep track of the input layers and flatten layers in lists. This is so that we can specify the inputs in the definition of the model object and use the list of flatten layers in the merge layer. When the model is used, it will require eight arrays as input: one for each of the submodels. This is required when training the model, when evaluating the model, and when making predictions with a final model. We can achieve this by creating a list of 3D arrays, where each 3D array contains [samples, timesteps, 1], with one feature. We can prepare the training dataset in this format as follows: The updated build_model() function with these changes is listed below. When the model is built, a diagram of the structure of the model is created and saved to file. Note: the call to plot_model() requires that pygraphviz and pydot are installed. If this is a problem, you can comment out this line. The structure of the network looks as follows. Structure of the Multi Headed Convolutional Neural Network Next, we can update the preparation of input samples when making a prediction for the test dataset. We must perform the same change, where an input array of [1, 14, 8] must be transformed into a list of eight 3D arrays each with [1, 14, 1]. The forecast() function with this change is listed below. That¡¯s it. We can tie all of this together; the complete example is listed below. Running the example fits and evaluates the model, printing the overall RMSE across all seven days, and the per-day RMSE for each lead time. Your specific results may vary given the stochastic nature of the algorithm. You may want to try running the example a few times. We can see that in this case, the overall RMSE is skillful compared to a naive forecast, but with the chosen configuration may not perform better than the multi-channel model in the previous section. We can also see a different, more pronounced profile for the daily RMSE scores where perhaps Mon-Tue and Thu-Fri are easier for the model to predict than the other forecast days. These results may be useful when combined with another forecast model. It may be interesting to explore alternate methods in the architecture for merging the output of each sub-model. Line Plot of RMSE per Day for a Multi-head CNN with 14-day Inputs This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop 1D convolutional neural networks for multi-step time series forecasting. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hi Jason, thanks for the tutorial. One naive question: for last complete code, how to make prediction fixed / reproducible ? I tried ¡°from numpy.random import seed¡± ¡°seed(1)¡±, but the scores are still varied when running twice. This is a common question that I answer here:https://machinelearningmastery.com/faq/single-faq/what-value-should-i-set-for-the-random-number-seed Great article <U+2013> thank you for the code examples.  I really enjoy looking at your work since I¡¯m trying to learn ML/DL.  Question <U+2013> what if I wanted to forecast out all of the variables separately rather than one total daily power consumption?   Thank you in advance. This would be a multi-step multi-variate problem. I show how in my book. I would recommend treating it like a seq2seq problem and forecast n variables for each step in the output sequence. An encoder-decoder model would be appropriate with a CNN or LSTM input model. Thank you for the response Jason.  Which one of your books are you referring to? This book:https://machinelearningmastery.com/deep-learning-for-time-series-forecasting/ Hi. When you increase the number of training example by overlapping your data, do you not run the risk of overfitting your model? You are essentially giving the model the same data multiple times. It may, this is why we are so reliant on the model validation method. Please teach us capsnet. Thanks for the suggestion.  Why do you want to use capsule networks? They seem fringe to me. Thank you <U+2013> I have purchased your ebook. Comment  Name (required)  Email (will not be published) (required)  Website","Keyword(freq): model(21), input(14), layer(13), step(12), variable(12), network(11), observation(10), result(10), feature(8), prediction(7)"
"7","vidhya",2018-10-11,"A Step-by-Step Introduction to the Basic Object Detection Algorithms (Part 1)","https://www.analyticsvidhya.com/blog/2018/10/a-step-by-step-introduction-to-the-basic-object-detection-algorithms-part-1/","

FLASH SALE: Course On Introduction To Data Science | Use COUPON CODE: FLASH25 for 25% Discount| Work on 4 real-life projects |
 Buy Now 

 How much time have you spent looking for lost room keys in an untidy and messy house? It happens to the best of us and till date remains an incredibly frustrating experience. But what if a simple computer algorithm could locate your keys in a matter of milliseconds? That is the power of object detection algorithms. While this was a simple example, the applications of object detection span multiple and diverse industries, from round-the-clock surveillance to real-time vehicle detection in smart cities. In short, these are powerful deep learning algorithms. In this article specifically, we will dive deeper and look at various algorithms that can be used for object detection. We will start with the algorithms belonging to RCNN family, i.e. RCNN, Fast RCNN and Faster RCNN. In the upcoming article of this series, we will cover more advanced algorithms like YOLO, SSD, etc. I encourage you to go through this<U+00A0>previous article on object detection, where we cover the basics of this wonderful technique and show you an implementation in Python using the ImageAI library. Let¡¯s get started! The below image is a popular example of illustrating how an object detection algorithm works. Each object in the image, from a person to a kite, have been located and identified with a certain level of precision. Let¡¯s start with the simplest deep learning approach, and a widely used one, for detecting objects in images <U+2013> Convolutional Neural Networks or CNNs. If your understanding of CNNs is a little rusty, I recommend going through<U+00A0>this article<U+00A0>first. But I¡¯ll briefly summarize the inner workings of<U+00A0> a CNN for you. Take a look at the below image: We pass an image to the network, and it is then sent through various convolutions and pooling layers. Finally, we get the output in the form of the object¡¯s class. Fairly straightforward, isn¡¯t it? For each input image, we get a corresponding class as an output. Can we use this technique to detect various objects in an image? Yes, we can! Let¡¯s look at how we can solve a general object detection problem using a CNN. 1. First, we take an image as input: 2. Then we divide the image into various regions: 3. We will then consider each region as a separate image.
4. Pass all these regions (images) to the CNN and classify them into various classes.
5. Once we have divided each region into its corresponding class, we can combine all these regions to get the original image with the detected objects: The problem with using this approach is that the objects in the image can have different aspect ratios and spatial locations. For instance, in some cases the object might be covering most of the image, while in others the object might only be covering a small percentage of the image. The shapes of the objects might also be different (happens a lot in real-life use cases). As a result of these factors, we would require a very large number of regions resulting in a huge amount of computational time. So to solve this problem and reduce the number of regions, we can use region-based CNN, which selects the regions using a proposal method. Let¡¯s understand what this region-based CNN can do for us. Instead of working on a massive number of regions, the RCNN<U+00A0>algorithm proposes a bunch of boxes in the image and checks if any of these boxes contain any object. RCNN uses selective search to extract these boxes from an image (these boxes are called regions). Let¡¯s first understand what selective search is and how it identifies the different regions. There are basically four regions that form an object: varying scales, colors, textures, and enclosure. Selective search identifies these patterns in the image and based on that, proposes various regions. Here is a brief overview of how selective search works: Below is a succint summary of the steps followed in RCNN to detect objects: You might get a better idea of the above steps with a visual example (Images for the example shown below are taken from this paper) . So let¡¯s take one! And this, in a nutshell, is how an RCNN helps us to detect objects. So far, we¡¯ve seen how RCNN can be helpful<U+00A0>for object detection. But this technique comes with its own limitations. Training an RCNN model is expensive and slow thanks to the below steps: All these processes combine to make RCNN very slow. It takes around 40-50 seconds to make predictions for each new image, which essentially makes the model cumbersome and practically impossible to build when faced with a gigantic dataset. Here¡¯s the good news <U+2013> we have another object detection technique which fixes most of the limitations we saw in RCNN. What else can we do to reduce the computation time a RCNN algorithm typically takes? Instead of running a CNN 2,000 times per image, we can run it just once per image and get all the regions of interest (regions containing some object). Ross Girshick, the author of<U+00A0>RCNN, came up with this idea of running the CNN just once per image and then finding a way to share that computation across the 2,000 regions. In Fast RCNN, we feed the input image to the CNN, which in turn generates the convolutional feature maps. Using these maps, the regions of proposals are extracted. We then use a RoI pooling layer to reshape all the proposed regions into a fixed size, so that it can be fed into a fully connected network. Let¡¯s break this down into steps to simplify the concept: So, instead of using three different models (like in<U+00A0>RCNN), Fast RCNN uses a single model which extracts features from the regions, divides them into different classes, and returns the boundary boxes for the identified classes simultaneously. To break this down even further, I¡¯ll visualize each step to add a practical angle to the explanation. This is how Fast RCNN resolves two major issues of RCNN, i.e., passing one instead of 2,000 regions per image to the ConvNet, and using one instead of three different models for extracting features, classification and generating bounding boxes. But even Fast RCNN<U+00A0>has certain problem areas. It also uses selective search as a proposal method to find the Regions of Interest, which is a slow and time consuming<U+00A0>process. It takes around 2 seconds per image to detect objects, which is much better compared to RCNN. But when we consider large real-life datasets, then even a Fast RCNN doesn¡¯t look so fast anymore. But there¡¯s yet another object detection algorithm that trump Fast RCNN. And something tells me you won¡¯t be surprised by it¡¯s name. Faster RCNN is the modified version of Fast RCNN. The major difference between them is that Fast RCNN uses selective search for generating Regions of Interest, while Faster RCNN uses ¡°Region Proposal Network¡±, aka RPN. RPN takes image feature maps as an input and generates a set of object proposals, each with an objectness score as output. The<U+00A0> below steps are typically followed in a Faster RCNN<U+00A0>approach: Let me briefly explain how this Region Proposal Network (RPN) actually works. To begin with, Faster RCNN takes the feature maps from CNN and passes them on to the Region Proposal Network. RPN uses a sliding window over these feature maps, and at each window, it generates k Anchor boxes of different shapes and sizes: Anchor boxes are fixed sized boundary boxes that are placed throughout the image and have different shapes and sizes. For each anchor, RPN predicts two things: We now have bounding boxes of different shapes and sizes which are passed on to the RoI pooling layer. Now it might be possible that after the RPN step, there are proposals with no classes assigned to them. We can take each proposal and crop it so that each proposal contains an object. This is what the RoI pooling layer does. It extracts fixed sized feature maps for each anchor: Then these feature maps are passed to a fully connected layer which has a softmax and a linear regression layer. It finally classifies the object and predicts the bounding boxes for the identified objects. All of the object detection algorithms we have discussed so far use regions to identify the objects. The network does not look at the complete image in one go, but focuses on parts of the image sequentially. This creates two complications: The below table is a nice summary of all the algorithms we have covered in this article. I suggest keeping this handy next time you¡¯re working on an object detection challenge! Object detection is a fascinating field, and is rightly seeing a ton of traction in commercial, as well as research applications. Thanks to advances in modern hardware and computational resources, breakthroughs in this space have been quick and ground-breaking. This article is just the beginning of our object detection journey. In the next article of this series, we will encounter modern object detection algorithms such as YOLO and RetinaNet. So stay tuned! I always appreciate any feedback or suggestions on my articles, so please feel free to connect with me in the comments section below. Very interesting paper. Good job!!! Thank You Jacques! Excellent Article..Really its helpfull for beginers..Thanks a lot..!!","Keyword(freq): region(21), box(11), object(11), algorithm(8), map(7), step(5), shape(4), image(3), proposal(3), size(3)"
"8","vidhya",2018-10-08,"An Introduction to Random Forest using the fastai Library (Machine Learning for Programmers <U+2013> Part 1)","https://www.analyticsvidhya.com/blog/2018/10/comprehensive-overview-machine-learning-part-1/","

FLASH SALE: Course On Introduction To Data Science | Use COUPON CODE: FLASH25 for 25% Discount| Work on 4 real-life projects |
Buy Now 

 Programming is a crucial prerequisite for anyone wanting to learn machine learning. Sure quite a few autoML tools are out there, but most are still at a very nascent stage and well beyond an individual¡¯s budget. The sweet spot for a data scientist lies in combining programming with machine learning algorithms. Fast.ai is led by the amazing partnership of Jeremy Howard and Rachel Thomas. So when they recently released their machine learning course, I couldn¡¯t wait to get started. What I personally liked about this course is the top-down approach to teaching. You first learn how to code an algorithm in Python, and then move to the theory aspect. While not a unique approach, it certainly has it¡¯s advantages. While going these videos, I decided to curate my learning in the form of a series of articles for our awesome community! So in this first post, I have provided a comprehensive summary (including the code) of the first two videos where Jeremy Howard teaches us how to build a random forest model using the fastai library, and how tuning the different hyperparameters can significantly alter our model¡¯s accuracy. You need to have a bit of experience in Python to follow along with the code.<U+00A0>So if you¡¯re a beginner in machine learning and have not used Python and Jupyter Notebooks before, I recommend checking out the below two resources first: The video lectures are available on YouTube and the course has been divided into twelve lectures as per the below structure: This course assumes that you have Jupyter Notebook installed on your machine. In case you don¡¯t (and don¡¯t prefer installing it either), you can choose any of the following (these have a nominal fee attached to them): All the Notebooks associated with each lecture are available on fast.ai¡¯s GitHub repository. You can clone or download the entire repository in one go. You can locate the full installation steps under the to-install section. Ready to get started? Then check out the<U+00A0>Jupyter Notebook and the below video for the first lesson. In this lecture, we will learn how to build a random forest model in Python. Since a top-down approach is followed in this course, we will go ahead and code first while simultaneously understanding how the code work. We¡¯ll then look into the inner workings of the random forest algorithm. Let¡¯s deep dive into what this lecture covers. The above two commands will automatically modify the notebook when the source code is updated. Thus, using ext_autoreload<U+00A0>will automatically and dynamically make the changes in your notebook. Using %matplotlib inline, we can visualize the plots inside the notebook. Using import* will import everything in the fastai library. Other necessary libraries have also been imported for reading the dataframe summary, creating random forest models and metrics for calculating the RMSE (evaluation metric). The dataset we¡¯ll be using is the ¡®Blue Book for Bulldozers¡¯. The problem statement for this challenge is described below: The goal is to predict the sale price of a particular piece of heavy equipment at an auction, based on its usage, equipment type, and configuration. <U+00A0>The data is sourced from auction result postings and includes information on usage and equipment configurations. Fast Iron is creating a ¡°blue book for bulldozers¡±, for customers to value what their heavy equipment fleet is worth at an auction. The evaluation metric is RMSLE (root mean squared log error). Don¡¯t worry if you haven¡¯t heard of it before, we¡¯ll understand and deal with it during the code walk-through. Assuming you have successfully downloaded the dataset, let¡¯s move on to coding! This command is used to set the location of our dataset. We currently have the downloaded dataset stored in a folder named bulldozers within the data folder. To check what are the files inside the PATH, you can type: Or, The dataset provided is in a .csv format. This is a structured dataset, with columns representing a range of things, such as ID, Date, state, product group, etc. For dealing with structured data, pandas is the most important library. We already imported pandas as pd when we used the import* command earlier. We will now use the read_csv function of pandas to read the data : Let us look at the first few rows of the data: Since the dataset is large, this command does not show us the complete column-wise data. Instead, we will see some dots for the data that isn¡¯t being displayed (as shown in the screenshot): To fix this, we will define the following function, where we set max.rows and max.columns to 1000. We can now print the head of the dataset using this newly minted function. We have taken the transpose to make it visually appealing (we see column names as the index). Remember the evaluation metric is RMSLE <U+2013> which is basically the RMSE between the log values of the result. So we will transform the target variable by taking it¡¯s log values. This is where the popular library<U+00A0>numpy comes to the rescue. The concept of how a Random Forest model works from scratch will be discussed in detail in the later sections of the course, but here is a brief introduction in Jeremy Howard¡¯s words: Sounds like a smashing technique, right? RandomForestRegressor and RandomForestClassifier<U+00A0>functions are used in Python for regression and classification problems respectively. Since we¡¯re dealing with a regression challenge, we will stick to the RandomForestRegressor. The m.fit function takes two inputs: The target variable here is<U+00A0>df_raw.SalePrice. The independent variables are all the variables except SalePrice. Here, we are using<U+00A0>df_raw.drop to drop the SalePrice column (axis = 1 represents column). This would throw up an error like the one below: This suggests that the model could not deal with the value ¡®Conventional¡¯. Most machine learning models (including random forest) cannot directly use categorical columns. We need to convert these columns into numbers first. So naturally the next step is to convert all the categorical columns into continuous variables. Let¡¯s take each categorical column individually. First, consider the saledate column which is of datetime format. From the date column, we can extract numerical values such as <U+2013> year, month, day of month, day of the week, holiday or not, weekend or weekday, was it raining?, etc. We¡¯ll leverage the<U+00A0>add_datepart<U+00A0>function from the fastai library to create these features for us. The function creates the following features: Let¡¯s run the function and check the columns: The next step is to convert the categorical variables into numbers. We can use the train_cats function from fastai for this: While converting categorical to numeric columns, we have to take the following two issues into consideration: Although this won¡¯t make much of a difference in our current case since random forest works on splitting the dataset (we will understand how random forest works in detail in the shortly), it¡¯s still good to know this for other algorithms. The next step is to look at the number of missing values in the dataset and understand how to deal with them. This is a pretty widespread challenge in both machine learning competitions and real-life industry problems. We use .isnull().sum() to get the total number of missing values. This is divided by the length of the dataset to determine the ratio of missing values. The dataset is now ready to be used for creating a model. Data cleaning is always a tedious and time consuming process. Hence, ensure to save the transformed dataset so that the next time we load the data, we will not have to perform the above tasks again. We will save it in a feather format, as this let¡¯s us access the data efficiently: We have to impute the missing values and store the data as dependent and independent part. This is done by using the fastai function proc_df. The function performs the following tasks: We have dealt with the categorical columns and the date values. We have also taken care of the missing values. Now we can finally power up and build the random forest model we have been inching towards. The n_jobs is set to -1 to use all the available cores on the machine. This gives us a score (r^2) of 0.98, which is excellent. The caveat here is that we have trained the model on the training set, and checked the result on the same. There¡¯s a high chance that this model might not perform as well on unseen data (test set, in our case). The only way to find out is to create a validation set and check the performance of the model on it. So let¡¯s create a validation set that contains 12,000 data points (and the train set will contain the rest). Here, we will train the model on our new set (which is a sample of the original set) and check the performance across both <U+2013> train and validation sets. In order to compare the score against the train and test sets, the below function returns the RMSE value and score for both datasets. The result of the above code is shown below. The train set has a score of 0.98, while the validation set has a score of 0.88. A bit of a drop-off, but the model still performed well overall. Now that you know how to code a random forest model in Python, it¡¯s equally important to understand how it actually works underneath all that code. Random forest is often cited as a black box model, and it¡¯s time to put that misconception to bed. We observed in the first lesson that the model performs extremely well on the training data (the points it has seen before) but dips when tested on the validation set (the data points model was not trained on). Let us first understand how we created the validation set and why it¡¯s so crucial. Creating a good validation set that closely resembles the test set is one of the most important tasks in machine learning. The validation score is representative of how our model performs on real-world data, or on the test data. Keep in mind that if there¡¯s a<U+00A0>time component involved, then the most recent rows should be included in the validation set. So, our validation set will be of the same size as the test set (last 12,000 rows from the training data). The data points from 0 to (length <U+2013> 12000) are stored as the train set (x_train, y_train). A model is built using the train set and its performance is measured on both the train and validation sets as before. From the above code, we get the results: It¡¯s clear that the model is overfitting on the training set. Also, it takes a smidge over 1 minute to train. Can we reduce the training time? Yes, we can! To do this, we will further take a subset of the original dataset: A subset of 30,000 samples has been created from which we take 20,000 for training the Random Forest model. Random Forest is a group of trees which are called estimators. The number of trees in a random forest model is defined by the parameter n_estimator. We will first look at a single tree (set n_estimator = 1) with a maximum depth of 3. Plotting the tree: The tree is a set of binary decisions. Looking at the first box, the first split is on coupler-system value: less than/equal to 0.5 or greater than 0.5. After the split, we get 3,185 rows with coupler_system>0.5 and remaining 16,815 with <0.5. Similarly, next split is on enclosure and Year_made. For the first box, a model is created using only the average value (10.189). This means that all the rows have a predicted value of 10.189 and the MSE (Mean Squared Error) for these predictions is 0.459. Instead, if we make a split and separate the rows based on coupler_system <0.5, the MSE is reduced to 0.414 for samples satisfying the condition (true) and 0.109 for the remaining samples. So how do we decide which variable to split on? The idea is to split the data into two groups which are as different from each other as possible. This can be done by checking each possible split point for each variable, and then figuring out which one gives the lower MSE. To do this, we can take a weighted average of the two MSE values after the split. The splitting stops when it either reaches the pre-specified<U+00A0>max_depth<U+00A0>value or when each leaf node has only one value. We have a basic model <U+2013> a single tree, but this is not a very good model. We need something a bit more complex that builds upon this structure. For creating a forest, we will use a statistical technique called bagging. In the bagging technique, we create multiple models, each giving predictions which are not correlated to the other. Then we average the predictions from these models. Random Forest is a bagging technique. If all the trees created are similar to each other and give similar predictions, then averaging these predictions will not improve the model performance. Instead, we can create multiple trees on a different subset of data, so that even if these trees overfit, they will do so on a different set of points. These samples are taken with replacement. In simple words, we create multiple poor performing models and average them to create one good model. The individual models must be as predictive as possible, but together should be uncorrelated. We will now increase the number of estimators in our random forest and see the results. If we do not give a value to the n_estimator parameter, it is taken as 10 by default. We will get predictions from each of the 10 trees. Further, np.stack will be used to concatenate the predictions one over the other. The dimensions of the predictions is (10, 12000) . This means we have 10 predictions for each row in the validation set. Now for comparing our model¡¯s results against the validation set, here is the row of predictions, the mean of the predictions and the actual value from validation set. The actual value is 9.17 but none of our predictions comes close to this value. On taking the average of all our predictions we get 9.07, which is a better prediction than any of the individual trees. It¡¯s always a good idea to visualize your model as much as possible. Here is a plot that shows the variation in r^2 value as the number of trees increases. As expected, the r^2 becomes better as the number of trees increases. You can experiment with the n_estimator value and see how the r^2 value changes with each iteration. You¡¯ll notice that after a certain number of trees, the r^2 value plateaus. Creating a separate validation set for a small dataset can potentially be a problem since it will result in an even smaller training set. In such cases, we can use the data points (or samples) which the tree was not trained on. For this, we set the parameter oob_score =True. The oob_score is 0.84 which is close to that of the validation set. Let us look at some other interesting techniques by which we can improve our model. Earlier, we created a subset of 30,000 rows and the train set was randomly chosen from this subset. As an alternative, we can create a different subset each time so that the model is trained on a larger part of the data. We use set_rf_sample to specify the sample size. Let us check if the performance of the model has improved or not. We get a validation score of 0.876. So far, we have worked on a subset of one sample. We can fit this model on the entire dataset (but it will take a long time to run depending on how good your computational resources are!). This can be treated as the stopping criteria for the tree. The tree stops growing (or splitting) when the number of samples in the leaf node is less than specified. Here we have specified the min_sample_leaf<U+00A0>as 3. This means that the minimum number of samples in the node should be 3 for each split. We see that the r^2 has improved for the validation set and reduced on the test set, concluding that the model does not overfit on the training data. Another important parameter in random forest is max_features. We have discussed previously that the individual trees must be as uncorrelated as possible. For the same, random forest uses a subset of rows to train each tree. Additionally, we can also use a subset of columns (features) instead of using all the features. This is achieved by tweaking the<U+00A0>max_features<U+00A0>parameter. Setting max_features has slightly improved the validation score. Here the max_features is set to 0.5 which means using 50% of the features for each split. Keep in mind that this parameter can also take values like log2 or sqrt. Jeremy Howard mentioned a few tips and tricks for navigating Jupyter Notebooks which newcomers will find quite useful. Below are some of the highlights: The curse of dimensionality is the idea that the more dimensions we have, the more points sit on the edge of that space. So if the number of columns is more, it creates more and more empty space. What that means, in theory, is that the distance between points is much less meaningful. This should not be true because the points still are different distances away from each other. Even though they are on the edges, we can still determine how far away they are from each other. The evaluation metric of our dataset is RMSLE. The formula for this is We first take the mean of squared differences of log values. We take a square root of the result obtained. This is equivalent to calculating the root mean squared error (rmse) of log of the values. Here is the mathematical formula for R-square: The value of R-square can be anything less than 1. If the r square is negative, it means that your model is worse than predicting mean. In scikit-learn, we have another algorithm ExtraTreeClassifier which is extremely randomized tree model. Unlike Random forest, instead of trying each split point for every variable, it randomly tries a few split points for a few variables. This article was a pretty comprehensive summary of the first two videos from fast.ai¡¯s machine learning course. During the first lesson we learnt to code a simple random forest model on the bulldozer dataset. Random forest (and most ml algorithms) do not work with categorical variables. We faced a similar problem during the random forest implementation and we saw how can we use the date column and other categorical columns in the dataset for creating a model. In the second video, the concept of creating a validation set was introduced. We then used this validation set to check the performance of the model and tuned some basic hyper-parameters to improve the model. My favorite part from this video was plotting and visualizing the tree we built. I am sure you would have learnt a lot through these videos. I will shortly post another article covering the next two videos from the course. Good Article Aishwarya, nice explanation. Thank you <U+0001F642> Hi..Thanks for putting this into words..really helpful.
I am facing issue with df.to_feather(¡®tmp/bulldozers-raw¡¯).
Not able to install feather library. Any suggestions/pointers?","Keyword(freq): prediction(13), value(13), column(11), tree(11), point(10), row(9), model(8), sample(7), variable(6), feature(5)"
"9","vidhya",2018-10-08,"Simplifying Data Preparation and Machine Learning Tasks using RapidMiner","https://www.analyticsvidhya.com/blog/2018/10/rapidminer-data-preparation-machine-learning/","

FLASH SALE: Course On Introduction To Data Science | Use COUPON CODE: FLASH25 for 25% Discount| Work on 4 real-life projects |
Buy Now 

 It¡¯s a well-known fact that we spend too much time on data preparation and not as much time as we want on building cool machine learning models. In fact, a Harvard Business Review publication confirmed what we always knew:<U+00A0>analytics teams spend 80%<U+00A0>of their time preparing data. And they are typically slowed down by clunky data preparation tools coupled with a scarcity of data science experts. But not for much longer, folks! RapidMiner recently released a really nice functionality for data preparation, RapidMiner Turbo Prep. You will soon know why we picked this name <U+0001F642>, but the basic idea is that Turbo Prep provides a new data preparation experience that is fast and fun to use with a drag and drop interface. Let¡¯s walk through some of the possibilities of this feature, as well as demonstrate how it integrates with<U+00A0>RapidMiner Auto Model, our automated machine learning product. These two features truly make data prep and machine learning fast, fun, and simple. If you would like to follow along, make sure you have<U+00A0>RapidMiner Studio 9.0 downloaded.<U+00A0>All free users have access to Auto Model and Turbo Prep for 30 days. First, we¡¯re going to start by loading some data. Data can be added from all repository-based sources or be imported from your local machine. RapidMiner Turbo Prep start screen In this example, we¡¯re using a dataset of all the domestic flights leaving from New England in 2007, roughly 230,000 rows. You can find this dataset inside Studio¡¯s pre-installed repository. Click ¡®Load Data¡¯ / ¡®Community Samples¡¯ / ¡®Community Data Sets¡¯ / ¡®Transportation¡¯. Loading sample data sets Once you load the data it can be seen immediately in a data-centric view, along with some data quality indicators. At the top of the columns, the distributions and quality measurements of the data are displayed. These indicate whether the columns will be helpful for machine learning and modeling. Say, for example, the majority of the data in a column is missing, this could confuse a machine learning model, so it is often better to remove it all together. If the column acts as an ID, that means practically all of the values only occur once in the data set, so this not useful for identifying patterns, and also should be removed. Data centric view of RapidMiner Turbo Prep As a first step, in order to look at the data in aggregate, we are going to create a pivot table. To generate this pivot table, first, we will look at the airport codes, indicated by ¡®Origin¡¯, with the airport name ¡®OriginName¡¯, and calculate the average delay at these locations. We can see the result immediately by dragging ¡®DepDelay¡¯ into the ¡®Aggregates¡¯ area, which calculates the average. In this case, the biggest delays are happening at the Nantucket airport, which is a very small airport; there is a high average delay of more than 51 minutes. In order to take the number of flights into account, we will also add in ¡®Origin count¡¯ and sort to show the largest airport by flight. In this case, Boston Logan Airport is the largest with almost 130,000 flights. Pivot table in RapidMiner Turbo Prep This pivot table helped us quickly determine that we should focus on Boston Logan, so we will exit out of this view and go back to the original data we started with. Now, to only show ¡®BOS¡¯ flight data: select the ¡®Origin¡¯ column, right click, and select ¡®Transformations¡¯, then ¡®Filter¡¯. Immediately, there will be a preview of the data, so you know whether the results are correct. All the statistics and quality measurements are updated as well. Applying a filter Next, we¡¯re going to bring in some additional data about the weather in New England for the same year. This data set can be found in the same ¡®Transportation¡¯ folder as the flight data. We know from personal experience, that weather can create delays, so we want to add this data in to see if the model picks up on it. In a longer scenario, we might take a look at the flight data alone at first and discover that the model is 60% accurate. Then add in the weather information and see how the accuracy of the model improves. But for this demonstration, we will go straight to adding in the weather. In this data, there is a single ¡®Date¡¯ column but in our flight data there were two columns, one for the day and one for the month, so we¡¯ll need to transform the weather data to match. Single ¡®Date¡¯ Column in weather data Start the transformation by copying the ¡®Date¡¯ column so there are two duplicate columns next to each other. Then rename the columns to ¡®W_Day¡¯ and ¡®W_Month¡¯ for consistency. Copied and renamed ¡®Date¡¯ columns in weather data Then we need to split the data from these columns. To do so, click on the ¡®W_Day¡¯ column and select ¡®Change Type¡¯ which will display ¡®Change to number¡¯ with the option to ¡®Extract¡¯. In this case we need to extract the day relative to the month and click ¡®Apply¡¯. In the case of the ¡®W_Month¡¯ column, we need to follow the same steps, except we need to extract the month relative to the year and click ¡®Apply¡¯. Once the results look good, we commit the transformation. Extracting the day from the month Extracting the month from the year Now,<U+00A0>we need to merge the two data sets together.<U+00A0>Turbo Prep uses<U+00A0>smart algorithms<U+00A0>to intelligently identify data matches.<U+00A0>Two data sets are a good match if they have two columns that match with each other.<U+00A0>And two columns match well with each other if they contain similar values.<U+00A0>In this example, we see a pretty high match of 94%. % match of the two data sets Now<U+00A0>if we would like to join on the airport code, we select merge type ¡®Inner Join¡¯ and ¡®AirportCode¡¯ from the dropdown,<U+00A0>and it ranks the best matching columns. The best match is the<U+202F>¡®Origin¡¯<U+202F>code column in the other data set, which is a good<U+00A0>sign. Next, we pick the month and the month pops up to the top showing it¡¯s the best match.<U+202F>Last, we select day and ¡®DayofMonth¡¯,<U+00A0>which<U+00A0>is at the top of the list as the best match.<U+00A0>This is helpful to make sure that the joins and merges deliver the correct results.<U+00A0>Clicking ¡®Update preview¡¯ will show us<U+00A0>the three<U+00A0>joined keys<U+00A0>in purple, all of the weather information<U+00A0>in blue, and all of the original flight information for Boston Logan<U+00A0>in green. Merged data view Next, we<U+00A0>will<U+00A0>generate a new column<U+00A0>based<U+00A0>on<U+00A0>existing information<U+00A0>in our data sets. The data in the ¡®DepDelay¡¯ column<U+00A0>indicates the number of minutes the flight was delayed.<U+00A0>If a flight is a minute late, we would not (for this purpose)<U+00A0>actually<U+00A0>consider that to be delayed so this column, as is,<U+00A0>isn¡¯t all that important to us.<U+00A0>What we<U+00A0>want<U+00A0>to do is<U+00A0>use this column to<U+00A0>define<U+00A0>what a delay is.<U+00A0>For this example, we will consider any flights more than 15 minutes<U+00A0>late<U+00A0>as a delay.<U+00A0>To<U+00A0>generate<U+00A0>this<U+00A0>new column,<U+00A0>we<U+00A0>will click ¡®Generate¡¯ and<U+00A0>will start by<U+00A0>creating a<U+00A0>new column called ¡®Delay Class¡¯. Next, we can<U+00A0>either<U+00A0>drag in<U+00A0>or type out,<U+00A0>the formula of ¡®if()¡¯. The drag in, or type out ¡®DepDelay¡¯ where<U+00A0>a delay<U+00A0>greater<U+00A0>than fifteen minutes<U+00A0>is<U+00A0>true,<U+00A0>and<U+00A0>the rest<U+00A0>is<U+00A0>false. Ultimately, the formula will read, ¡®if([DepDelay]>15,TRUE,FALSE)¡¯. Then,<U+00A0>we want to update the preview to see the amount of false versus<U+00A0>true. In our case, the formula seems to work, so we commit the Generate and the new column is added. Generating a ¡®Delay Class¡¯ column The last step before Modeling,<U+00A0>here,<U+00A0>is cleansing our data. When we first saw our data, we could see a couple of data quality issues indicated, for example,<U+00A0>in the ¡®Cancellation¡¯ column, so we know that needs to be addressed. We could go through the data column by column,<U+00A0>or we could use the ¡®Auto Cleansing¡¯ feature.<U+00A0>Clicking on that feature will pop up a dialogue box, prompting us to identify what we would like to predict. RapidMiner Turbo Prep auto cleansing option Defining a target in auto cleanse Based on that<U+00A0>selection, RapidMiner suggests and selects the columns that should be removed. These are suggested because there is too much data missing or because the data is too stable, for example.<U+00A0>By simply clicking ¡®Next¡¯ those columns are removed. There are more ways to improve the data quality, but that step is the only one we will use for this example,<U+00A0>leaving<U+00A0>the rest to the default settings. Then,<U+00A0>we click ¡®Commit Cleanse¡¯. Removing columns with quality issues in auto cleanse We made quite a few changes and we can review them all by clicking on ¡®History¡¯, which shows all of the individual steps we took.<U+00A0>If we want to, we can<U+00A0>click on one of the steps and decide to roll back the changes before that step or create a copy of the rollback step. History view Possibly the most exciting aspect of Turbo Prep is that we<U+00A0>can<U+00A0>see the full data preparation process by clicking ¡®Process¡¯, leading to a fully annotated view in<U+00A0>RapidMiner<U+00A0>Studio.<U+00A0>There are no black boxes in RapidMiner.<U+00A0>We<U+00A0>can see every step and can make edits and changes as necessary.<U+00A0>Whenever we see a model that we like, we can click on it and can open the process. The process is generated with annotations, so we get a full explanation of what happens.<U+00A0>We can save this, we can apply it on new data sets, say the flight data from 2008,<U+00A0>and<U+00A0>we can share this process with our colleagues, or we can deploy it and run it<U+00A0>frequently. Process view in RapidMiner Studio The results can also be exported into the RapidMiner repositories,<U+00A0>into<U+00A0>various file formats, or<U+00A0>it can be handed over to RapidMiner Auto Model for immediate model creation.<U+00A0>In this case, we are going to explore building<U+00A0>some quick<U+00A0>models<U+00A0>using<U+00A0>RapidMiner Auto Model<U+00A0>by<U+00A0>simply<U+00A0>clicking on the<U+00A0>¡®Auto Model¡¯<U+00A0>tab. RapidMiner Auto Model From here, we<U+00A0>are<U+00A0>able to<U+00A0>build some clustering, segmentation, or outlier predictions. In this case, we want to predict<U+00A0>the ¡®Delay Class¡¯<U+00A0>column.<U+00A0>To do that, we just click on the ¡®Delay Class¡¯ and<U+00A0>¡®Predict¡¯ is already selected so we continue on and click ¡®Next¡¯. Predicting the ¡®Delay Class¡¯ In the ¡®Prepare Target¡¯ view, we can choose to map values or change the class of highest<U+00A0>interest,<U+00A0>but we are most interested in predicting delays, so we will keep the<U+00A0>default<U+00A0>settings here.<U+00A0> Prepare target view in RapidMiner Auto Model On the next<U+00A0>screen, we see<U+00A0>those quality measurements<U+00A0>are visible again, and<U+00A0>we see that<U+00A0>there are no red columns in this overview.<U+00A0>That¡¯s<U+00A0>because we did the auto cleansing already in Turbo Prep.<U+00A0>But we do still have a couple of suspicious columns marked in yellow. It is<U+00A0>important<U+00A0>that Auto Model is pointing out the ¡®DepDelay¡¯ as a suspicious column because this is the column that we used to create our predictions. If you recall,<U+00A0>when the<U+00A0>¡®DepDelay¡¯ is greater than 15 minutes<U+00A0>late<U+00A0>then this is<U+00A0>a delay, otherwise, it is not.<U+00A0>If we kept<U+00A0>this in,<U+00A0>all of<U+00A0>the models would focus on that one column and that is not what we want to base our predictions on, so we have to remove the column.<U+00A0>In this case, we are also going to remove the other two suspicious columns<U+00A0>by clicking ¡®Deselect Yellow¡¯<U+00A0>but those could stay in. This<U+00A0>is an important feature of Turbo Prep and Auto Model,<U+00A0>while we automate as much as we can, we still give the option to overwrite the recommendations. Removing suspicious columns in yellow With all three suspicious columns deselected, we click ¡®Next¡¯ and move on to the ¡®Model Types¡¯ view.<U+00A0>In this view, we see a couple of models selected already<U+00A0>(suggested by Auto Model), Naive Bayes and GLM<U+00A0>and<U+00A0>we can choose to see Logistic Regression as well here. Selecting the model types In a few seconds, we see the<U+00A0>Naive<U+00A0>Bayes model and can start inspecting it<U+00A0>by clicking on ¡®Model¡¯ underneath ¡®Naive Bayes¡¯ in the Results window.<U+00A0>Here we have a visual way to inspect the model, so, for example, the<U+00A0>¡®ActualLapsedTime¡¯ attribute<U+00A0>isn¡¯t super helpful, but<U+00A0>we can dropdown<U+00A0>and select ¡®Min Humidity¡¯ instead and start to see that the two classes differ a bit. Actual Lapsed Time Min Humidity There¡¯s another way to see this information<U+00A0>as well,<U+00A0>through<U+00A0>Auto Model, by clicking on ¡®Simulator¡¯ underneath ¡®Model¡¯ in the Results window. Here<U+00A0>we can experiment with the model a bit.<U+00A0>Right off the bat, we see that for the average inputs for our model, it¡¯s more likely<U+00A0>that the flight will be delayed. And<U+00A0>then<U+00A0>we can make some changes. Visibility seems to be pretty important, indicated by the length of the gray bar beneath the class name,<U+00A0>so<U+00A0>let¡¯s<U+00A0>change the visibility a little bit<U+00A0>by<U+00A0>reducing<U+00A0>it, which<U+00A0>makes it even more likely that the flight is delayed. Naive Bayes simulator with average inputs Naive Bayes simulator with decreased visibility In ¡®Overview¡¯ we can see how well the different models performed, here we see that GLM and Logistic Regression performed better than Naive Bayes.<U+00A0>We could also<U+00A0>look at the ROC Comparison, or the individual Model Performance and Lift Chart. Auto Model results overview Finally, you can see the data itself, under ¡®General¡¯ and the most important influence factors<U+00A0>by clicking on<U+00A0>¡®Weights¡¯.<U+00A0>Here the most influential factor is if the incoming aircraft is delayed, which makes sense. We may want<U+00A0>to consider taking that out because it might not be something that we can influence but we will keep it in for now. Important influence factors And just<U+00A0>like Turbo Prep, Auto Model processes<U+00A0>can be opened<U+00A0>in RapidMiner Studio,<U+00A0>showing<U+00A0>the<U+00A0>full<U+00A0>process<U+00A0>with annotations.<U+00A0>With Auto Model, every step is explained with its importance and why certain predictions are made during model creation. We can see exactly how the full model was created; there are no black boxes! Auto Model process Through this demonstration,<U+00A0>we¡¯ve shown<U+00A0>that<U+00A0>Turbo Prep<U+00A0>is an incredibly exciting and useful new capability, radically simplifying<U+00A0>and accelerating<U+00A0>the time-consuming data preparation task.<U+00A0>We demonstrated that it makes it easy to<U+00A0>quickly extract, join, filter, group, pivot, transform and cleanse data.<U+00A0>You can also connect to<U+00A0>a variety of sources like relational databases, spreadsheets, applications, social media, and more.<U+00A0> You can also create repeatable data prep steps, making it faster to reuse processes. Data can also be saved as Excel or CSV or sent to data visualization products like Qlik.<U+00A0> We also<U+00A0>demonstrated that once we¡¯re<U+00A0>ready to build predictive models with<U+00A0>the<U+00A0>newly<U+00A0>transformed data<U+00A0>it¡¯s simple to<U+00A0>jump into<U+00A0>Auto Model with<U+00A0>just<U+00A0>one click.<U+00A0>RapidMiner<U+00A0>Auto Model, unlike any other<U+00A0>tool<U+00A0>available in the market,<U+00A0>automates machine learning<U+00A0>by<U+00A0>leveraging a decade of data science<U+00A0>wisdom<U+00A0>so you<U+00A0>can focus on<U+00A0>quickly<U+00A0>unlocking valuable insights from your data.<U+00A0>And best of all,<U+202F>there are no black boxes, we can always see exactly what happened in the background and we can replicate it.<U+00A0><U+00A0> If you haven¡¯t tried<U+00A0>these two features yet,<U+00A0>we¡¯re offering a 30-day trial of Studio Large to all free users<U+00A0>so<U+00A0>download it now.<U+00A0><U+00A0><U+00A0> RapidMiner brings artificial intelligence to the enterprise through an open and extensible data science platform. Built for analytics teams, RapidMiner unifies the entire data science lifecycle from data prep to machine learning to predictive model deployment. 400,000 analytics professionals use RapidMiner products to drive revenue, reduce costs, and avoid risks. For more information visit www.rapidminer.com.","Keyword(freq): column(17), result(7), set(7), baye(5), model(5), change(4), flight(4), prediction(4), step(4), analytics(3)"
