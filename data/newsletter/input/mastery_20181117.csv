"site","date","headline","url_address","text"
"mastery",2018-11-16,"How to Grid Search Deep Learning Models for Time Series Forecasting","https://machinelearningmastery.com/how-to-grid-search-deep-learning-models-for-time-series-forecasting/","Grid searching is generally not an operation that we can perform with deep learning methods. This is because deep learning methods often require large amounts of data and large models, together resulting in models that take hours, days, or weeks to train. In those cases where the datasets are smaller, such as univariate time series, it may be possible to use a grid search to tune the hyperparameters of a deep learning model. In this tutorial, you will discover how to develop a framework to grid search hyperparameters for deep learning models. After completing this tutorial, you will know: Let¡¯s get started. How to Grid Search Deep Learning Models for Time Series ForecastingPhoto by Hannes Flo, some rights reserved. This tutorial is divided into five parts; they are: The ¡®monthly airline passenger¡® dataset summarizes the monthly total number of international passengers in thousands on for an airline from 1949 to 1960. Download the dataset directly from here: Save the file with the filename ¡®monthly-airline-passengers.csv¡® in your current working directory. We can load this dataset as a Pandas series using the function read_csv(). Once loaded, we can summarize the shape of the dataset in order to determine the number of observations. We can then create a line plot of the series to get an idea of the structure of the series. We can tie all of this together; the complete example is listed below. Running the example first prints the shape of the dataset. The dataset is monthly and has 12 years, or 144 observations. In our testing, we will use the last year, or 12 observations, as the test set. A line plot is created. The dataset has an obvious trend and seasonal component. The period of the seasonal component is 12 months. Line Plot of Monthly International Airline Passengers In this tutorial, we will introduce the tools for grid searching, but we will not optimize the model hyperparameters for this problem. Instead, we will demonstrate how to grid search the deep learning model hyperparameters generally and find models with some skill compared to a naive model. From prior experiments, a naive model can achieve a root mean squared error, or RMSE, of 50.70 (remember the units are thousands of passengers) by persisting the value from 12 months ago (relative index -12). The performance of this naive model provides a bound on a model that is considered skillful for this problem. Any model that achieves a predictive performance of lower than 50.70 on the last 12 months has skill. It should be noted that a tuned ETS model can achieve an RMSE of 17.09 and a tuned SARIMA can achieve an RMSE of 13.89. These provide a lower bound on the expectations of a well-tuned deep learning model for this problem. Now that we have defined our problem and expectations of model skill, we can look at defining the grid search test harness. In this section, we will develop a grid search test harness that can be used to evaluate a range of hyperparameters for different neural network models, such as MLPs, CNNs, and LSTMs. This section is divided into the following parts: The first step is to split the loaded series into train and test sets. We will use the first 11 years (132 observations) for training and the last 12 for the test set. The train_test_split() function below will split the series taking the raw observations and the number of observations to use in the test set as arguments. Next, we need to be able to frame the univariate series of observations as a supervised learning problem so that we can train neural network models. A supervised learning framing of a series means that the data needs to be split into multiple examples that the model learns from and generalizes across. Each sample must have both an input component and an output component. The input component will be some number of prior observations, such as three years, or 36 time steps. The output component will be the total sales in the next month because we are interested in developing a model to make one-step forecasts. We can implement this using the shift() function on the pandas DataFrame. It allows us to shift a column down (forward in time) or back (backward in time). We can take the series as a column of data, then create multiple copies of the column, shifted forward or backward in time in order to create the samples with the input and output elements we require. When a series is shifted down, NaN values are introduced because we don¡¯t have values beyond the start of the series. For example, the series defined as a column: This column can be shifted and inserted as a column beforehand: We can see that on the second row, the value 1 is provided as input as an observation at the prior time step, and 2 is the next value in the series that can be predicted, or learned by the model to be predicted when 1 is presented as input. Rows with NaN values can be removed. The series_to_supervised() function below implements this behavior, allowing you to specify the number of lag observations to use in the input and the number to use in the output for each sample. It will also remove rows that have NaN values as they cannot be used to train or test a model. Time series forecasting models can be evaluated on a test set using walk-forward validation. Walk-forward validation is an approach where the model makes a forecast for each observation in the test dataset one at a time. After each forecast is made for a time step in the test dataset, the true observation for the forecast is added to the test dataset and made available to the model. Simpler models can be refit with the observation prior to making the subsequent prediction. More complex models, such as neural networks, are not refit given the much greater computational cost. Nevertheless, the true observation for the time step can then be used as part of the input for making the prediction on the next time step. First, the dataset is split into train and test sets. We will call the train_test_split() function to perform this split and pass in the pre-specified number of observations to use as the test data. A model will be fit once on the training dataset for a given configuration. We will define a generic model_fit() function to perform this operation that can be filled in for the given type of neural network that we may be interested in later. The function takes the training dataset and the model configuration and returns the fit model ready for making predictions. Each time step of the test dataset is enumerated. A prediction is made using the fit model. Again, we will define a generic function named model_predict() that takes the fit model, the history, and the model configuration and makes a single one-step prediction. The prediction is added to a list of predictions and the true observation from the test set is added to a list of observations that was seeded with all observations from the training dataset. This list is built up during each step in the walk-forward validation, allowing the model to make a one-step prediction using the most recent history. All of the predictions can then be compared to the true values in the test set and an error measure calculated. We will calculate the root mean squared error, or RMSE, between predictions and the true values. RMSE is calculated as the square root of the average of the squared differences between the forecasts and the actual values. The measure_rmse() implements this below using the mean_squared_error() scikit-learn function to first calculate the mean squared error, or MSE, before calculating the square root. The complete walk_forward_validation() function that ties all of this together is listed below. It takes the dataset, the number of observations to use as the test set, and the configuration for the model, and returns the RMSE for the model performance on the test set. Neural network models are stochastic. This means that, given the same model configuration and the same training dataset, a different internal set of weights will result each time the model is trained that will, in turn, have a different performance. This is a benefit, allowing the model to be adaptive and find high performing configurations to complex problems. It is also a problem when evaluating the performance of a model and in choosing a final model to use to make predictions. To address model evaluation, we will evaluate a model configuration multiple times via walk-forward validation and report the error as the average error across each evaluation. This is not always possible for large neural networks and may only make sense for small networks that can be fit in minutes or hours. The repeat_evaluate() function below implements this and allows the number of repeats to be specified as an optional parameter that defaults to 10 and returns the mean RMSE score from all repeats. We now have all the pieces of the framework. All that is left is a function to drive the search. We can define a grid_search() function that takes the dataset, a list of configurations to search, and the number of observations to use as the test set and perform the search. Once mean scores are calculated for each config, the list of configurations is sorted in ascending order so that the best scores are listed first. The complete function is listed below. Now that we have defined the elements of the test harness, we can tie them all together and define a simple persistence model. We do not need to fit a model so the model_fit() function will be implemented to simply return None. We will use the config to define a list of index offsets in the prior observations relative to the time to be forecasted that will be used as the prediction. For example, 12 will use the observation 12 months ago (-12) relative to the time to be forecasted. The model_predict() function can be implemented to use this configuration to persist the value at the negative relative offset. The complete example of using the framework with a simple persistence model is listed below. Running the example prints the RMSE of the model evaluated using walk-forward validation on the final 12 months of data. Each model configuration is evaluated 10 times, although, because the model has no stochastic element, the score is the same each time. At the end of the run, the configurations and RMSE for the top three performing model configurations are reported. We can see, as we might have expected, that persisting the value from one year ago (relative offset -12) resulted in the best performance for the persistence model. Now that we have a robust test harness for grid searching model hyperparameters, we can use it to evaluate a suite of neural network models. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course There are many aspects of the MLP that we may wish to tune. We will define a very simple model with one hidden layer and define five hyperparameters to tune. They are: Modern neural networks can handle raw data with little pre-processing, such as scaling and differencing. Nevertheless, when it comes to time series data, sometimes differencing the series can make a problem easier to model. Recall that differencing is the transform of the data such that a value of a prior observation is subtracted from the current observation, removing trend or seasonality structure. We will add support for differencing to the grid search test harness, just in case it adds value to your specific problem. It does add value for the internal airline passengers dataset. The difference() function below will calculate the difference of a given order for the dataset. Differencing will be optional, where an order of 0 suggests no differencing, whereas an order 1 or order 12 will require that the data be differenced prior to fitting the model and that the predictions of the model will need the differencing reversed prior to returning the forecast. We can now define the elements required to fit the MLP model in the test harness. First, we must unpack the list of hyperparameters. Next, we must prepare the data, including the differencing, transforming the data to a supervised format and separating out the input and output aspects of the data samples. We can now define and fit the model with the provided configuration. The complete implementation of the model_fit() function is listed below. The five chosen hyperparameters are by no means the only or best hyperparameters of the model to tune. You may modify the function to tune other parameters, such as the addition and size of more hidden layers and much more. Once the model is fit, we can use it to make forecasts. If the data was differenced, the difference must be inverted for the prediction of the model. This involves adding the value at the relative offset from the history back to the value predicted by the model. It also means that the history must be differenced so that the input data used to make the prediction has the expected form. Once prepared, we can use the history data to create a single sample as input to the model for making a one-step prediction. The shape of one sample must be [1, n_input] where n_input is the chosen number of lag observations to use. Finally, a prediction can be made. The complete implementation of the model_predict() function is listed below. Next, we must define the range of values to try for each hyperparameter. We can define a model_configs() function that creates a list of the different combinations of parameters to try. We will define a small subset of configurations to try as an example, including a differencing of 12 months, which we expect will be required. You are encouraged to experiment with standalone models, review learning curve diagnostic plots, and use information about the domain to set ranges of values of the hyperparameters to grid search. You are also encouraged to repeat the grid search to narrow in on ranges of values that appear to show better performance. An implementation of the model_configs() function is listed below. We now have all of the pieces needed to grid search MLP models for a univariate time series forecasting problem. The complete example is listed below. Running the example, we can see that there are a total of eight configurations to be evaluated by the framework. Each config will be evaluated 10 times; that means 10 models will be created and evaluated using walk-forward validation to calculate an RMSE score before an average of those 10 scores is reported and used to score the configuration. The scores are then sorted and the top 3 configurations with the lowest RMSE are reported at the end. A skillful model configuration was found as compared to a naive model that reported an RMSE of 50.70. We can see that the best RMSE of 18.98 was achieved with a configuration of [12, 100, 100, 1, 12], which we know can be interpreted as: A truncated example output of the grid search is listed below. Your specific scores may vary given the stochastic nature of the algorithm. We can now adapt the framework to grid search CNN models. Much the same set of hyperparameters can be searched as with the MLP model, except the number of nodes in the hidden layer can be replaced by the number of filter maps and kernel size in the convolutional layers. The chosen set of hyperparameters to grid search in the CNN model are as follows: Some additional hyperparameters that you may wish to investigate are the use of two convolutional layers before a pooling layer, the repetition of the convolutional and pooling layer pattern, the use of dropout, and more. We will define a very simple CNN model with one convolutional layer and one max pooling layer. The data must be prepared in much the same way as for the MLP. Unlike the MLP that expects the input data to have the shape [samples, features], the 1D CNN model expects the data to have the shape [samples, timesteps, features] where features maps onto channels and in this case 1 for the one variable we measure each month. The complete implementation of the model_fit() function is listed below. Making a prediction with a fit CNN model is very much like making a prediction with a fit MLP. Again, the only difference is that the one sample worth of input data must have a three-dimensional shape. The complete implementation of the model_predict() function is listed below. Finally, we can define a list of configurations for the model to evaluate. As before, we can do this by defining lists of hyperparameter values to try that are combined into a list. We will try a small number of configurations to ensure the example executes in a reasonable amount of time. The complete model_configs() function is listed below. We now have all of the elements needed to grid search the hyperparameters of a convolutional neural network for univariate time series forecasting. The complete example is listed below. Running the example, we can see that only eight distinct configurations are evaluated. We can see that a configuration of [12, 64, 5, 100, 1, 12] achieved an RMSE of 18.89, which is skillful as compared to a naive forecast model that achieved 50.70. We can unpack this configuration as: A truncated example output of the grid search is listed below. Your specific scores may vary given the stochastic nature of the algorithm. We can now adopt the framework for grid searching the hyperparameters of an LSTM model. The hyperparameters for the LSTM model will be the same five as the MLP; they are: We will define a simple LSTM model with a single hidden LSTM layer and the number of nodes specifying the number of units in this layer. It may be interesting to explore tuning additional configurations such as the use of a bidirectional input layer, stacked LSTM layers, and even hybrid models with CNN or ConvLSTM input models. As with the CNN model, the LSTM model expects input data to have a three-dimensional shape for the samples, time steps, and features. The complete implementation of the model_fit() function is listed below. Also like the CNN, the single input sample used to make a prediction must also be reshaped into the expected three-dimensional structure. The complete model_predict() function is listed below. We can now define the function used to create the list of model configurations to evaluate. The LSTM model is quite a bit slower to train than MLP and CNN models; as such, you may want to evaluate fewer configurations per run. We will define a very simple set of two configurations to explore: stochastic and batch gradient descent. We now have everything we need to grid search hyperparameters for the LSTM model for univariate time series forecasting. The complete example is listed below. Running the example, we can see that only two distinct configurations are evaluated. We can see that a configuration of [12, 100, 50, 1, 12] achieved an RMSE of 21.24, which is skillful as compared to a naive forecast model that achieved 50.70. The model requires a lot more tuning and may do much better with a hybrid configuration, such as having a CNN model as input. We can unpack this configuration as: A truncated example output of the grid search is listed below. Your specific scores may vary given the stochastic nature of the algorithm. This section lists some ideas for extending the tutorial that you may wish to explore. If you explore any of these extensions, I¡¯d love to know. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered how to develop a framework to grid search hyperparameters for deep learning models. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Thank you for providing such wonderful information. I¡¯m glad it helped. Just wanted to say thank you for your most excellent tutorials. They have helped me tremendously. Comment  Name (required)  Email (will not be published) (required)  Website"
"mastery",2018-11-14,"How to Develop LSTM Models for Time Series Forecasting","https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/","Long Short-Term Memory networks, or LSTMs for short, can be applied to time series forecasting. There are many types of LSTM models that can be used for each specific type of time series forecasting problem. In this tutorial, you will discover how to develop a suite of LSTM models for a range of standard time series forecasting problems. The objective of this tutorial is to provide standalone examples of each model on each type of time series problem as a template that you can copy and adapt for your specific time series forecasting problem. After completing this tutorial, you will know: This is a large and important post; you may want to bookmark it for future reference. Let¡¯s get started. How to Develop LSTM Models for Time Series ForecastingPhoto by N i c o l a, some rights reserved. In this tutorial, we will explore how to develop a suite of different types of LSTM models for time series forecasting. The models are demonstrated on small contrived time series problems intended to give the flavor of the type of time series problem being addressed. The chosen configuration of the models is arbitrary and not optimized for each problem; that was not the goal. This tutorial is divided into four parts; they are: LSTMs can be used to model univariate time series forecasting problems. These are problems comprised of a single series of observations and a model is required to learn from the series of past observations to predict the next value in the sequence. We will demonstrate a number of variations of the LSTM model for univariate time series forecasting. This section is divided into six parts; they are: Each of these models are demonstrated for one-step univariate time series forecasting, but can easily be adapted and used as the input part of a model for other types of time series forecasting problems. Before a univariate series can be modeled, it must be prepared. The LSTM model will learn a function that maps a sequence of past observations as input to an output observation. As such, the sequence of observations must be transformed into multiple examples from which the LSTM can learn. Consider a given univariate sequence: We can divide the sequence into multiple input/output patterns called samples, where three time steps are used as input and one time step is used as output for the one-step prediction that is being learned. The split_sequence() function below implements this behavior and will split a given univariate sequence into multiple samples where each sample has a specified number of time steps and the output is a single time step. We can demonstrate this function on our small contrived dataset above. The complete example is listed below. Running the example splits the univariate series into six samples where each sample has three input time steps and one output time step. Now that we know how to prepare a univariate series for modeling, let¡¯s look at developing LSTM models that can learn the mapping of inputs to outputs, starting with a Vanilla LSTM. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course A Vanilla LSTM is an LSTM model that has a single hidden layer of LSTM units, and an output layer used to make a prediction. We can define a Vanilla LSTM for univariate time series forecasting as follows. Key in the definition is the shape of the input; that is what the model expects as input for each sample in terms of the number of time steps and the number of features. We are working with a univariate series, so the number of features is one, for one variable. The number of time steps as input is the number we chose when preparing our dataset as an argument to the split_sequence() function. The shape of the input for each sample is specified in the input_shape argument on the definition of first hidden layer. We almost always have multiple samples, therefore, the model will expect the input component of training data to have the dimensions or shape: Our split_sequence() function in the previous section outputs the X with the shape [samples, timesteps], so we easily reshape it to have an additional dimension for the one feature. In this case, we define a model with 50 LSTM units in the hidden layer and an output layer that predicts a single numerical value. The model is fit using the efficient Adam version of stochastic gradient descent and optimized using the mean squared error, or ¡®mse¡® loss function. Once the model is defined, we can fit it on the training dataset. After the model is fit, we can use it to make a prediction. We can predict the next value in the sequence by providing the input: And expecting the model to predict something like: The model expects the input shape to be three-dimensional with [samples, timesteps, features], therefore, we must reshape the single input sample before making the prediction. We can tie all of this together and demonstrate how to develop a Vanilla LSTM for univariate time series forecasting and make a single prediction. Running the example prepares the data, fits the model, and makes a prediction. Your results may vary given the stochastic nature of the algorithm; try running the example a few times. We can see that the model predicts the next value in the sequence. Multiple hidden LSTM layers can be stacked one on top of another in what is referred to as a Stacked LSTM model. An LSTM layer requires a three-dimensional input and LSTMs by default will produce a two-dimensional output as an interpretation from the end of the sequence. We can address this by having the LSTM output a value for each time step in the input data by setting the return_sequences=True argument on the layer. This allows us to have 3D output from hidden LSTM layer as input to the next. We can therefore define a Stacked LSTM as follows. We can tie this together; the complete code example is listed below. Running the example predicts the next value in the sequence, which we expect would be 100. On some sequence prediction problems, it can be beneficial to allow the LSTM model to learn the input sequence both forward and backwards and concatenate both interpretations. This is called a Bidirectional LSTM. We can implement a Bidirectional LSTM for univariate time series forecasting by wrapping the first hidden layer in a wrapper layer called Bidirectional. An example of defining a Bidirectional LSTM to read input both forward and backward is as follows. The complete example of the Bidirectional LSTM for univariate time series forecasting is listed below. Running the example predicts the next value in the sequence, which we expect would be 100. A convolutional neural network, or CNN for short, is a type of neural network developed for working with two-dimensional image data. The CNN can be very effective at automatically extracting and learning features from one-dimensional sequence data such as univariate time series data. A CNN model can be used in a hybrid model with an LSTM backend where the CNN is used to interpret subsequences of input that together are provided as a sequence to an LSTM model to interpret. This hybrid model is called a CNN-LSTM. The first step is to split the input sequences into subsequences that can be processed by the CNN model. For example, we can first split our univariate time series data into input/output samples with four steps as input and one as output. Each sample can then be split into two sub-samples, each with two time steps. The CNN can interpret each subsequence of two time steps and provide a time series of interpretations of the subsequences to the LSTM model to process as input. We can parameterize this and define the number of subsequences as n_seq and the number of time steps per subsequence as n_steps. The input data can then be reshaped to have the required structure: For example: We want to reuse the same CNN model when reading in each sub-sequence of data separately. This can be achieved by wrapping the entire CNN model in a TimeDistributed wrapper that will apply the entire model once per input, in this case, once per input subsequence. The CNN model first has a convolutional layer for reading across the subsequence that requires a number of filters and a kernel size to be specified. The number of filters is the number of reads or interpretations of the input sequence. The kernel size is the number of time steps included of each ¡®read¡¯ operation of the input sequence. The convolution layer is followed by a max pooling layer that distills the filter maps down to 1/4 of their size that includes the most salient features. These structures are then flattened down to a single one-dimensional vector to be used as a single input time step to the LSTM layer. Next, we can define the LSTM part of the model that interprets the CNN model¡¯s read of the input sequence and makes a prediction. We can tie all of this together; the complete example of a CNN-LSTM model for univariate time series forecasting is listed below. Running the example predicts the next value in the sequence, which we expect would be 100. A type of LSTM related to the CNN-LSTM is the ConvLSTM, where the convolutional reading of input is built directly into each LSTM unit. The ConvLSTM was developed for reading two-dimensional spatial-temporal data, but can be adapted for use with univariate time series forecasting. The layer expects input as a sequence of two-dimensional images, therefore the shape of input data must be: For our purposes, we can split each sample into subsequences where timesteps will become the number of subsequences, or n_seq, and columns will be the number of time steps for each subsequence, or n_steps. The number of rows is fixed at 1 as we are working with one-dimensional data. We can now reshape the prepared samples into the required structure. We can define the ConvLSTM as a single layer in terms of the number of filters and a two-dimensional kernel size in terms of (rows, columns). As we are working with a one-dimensional series, the number of rows is always fixed to 1 in the kernel. The output of the model must then be flattened before it can be interpreted and a prediction made. The complete example of a ConvLSTM for one-step univariate time series forecasting is listed below. Running the example predicts the next value in the sequence, which we expect would be 100. Now that we have looked at LSTM models for univariate data, let¡¯s turn our attention to multivariate data. Multivariate time series data means data where there is more than one observation for each time step. There are two main models that we may require with multivariate time series data; they are: Let¡¯s take a look at each in turn. A problem may have two or more parallel input time series and an output time series that is dependent on the input time series. The input time series are parallel because each series has an observation at the same time steps. We can demonstrate this with a simple example of two parallel input time series where the output series is the simple addition of the input series. We can reshape these three arrays of data as a single dataset where each row is a time step, and each column is a separate time series. This is a standard way of storing parallel time series in a CSV file. The complete example is listed below. Running the example prints the dataset with one row per time step and one column for each of the two input and one output parallel time series. As with the univariate time series, we must structure these data into samples with input and output elements. An LSTM model needs sufficient context to learn a mapping from an input sequence to an output value. LSTMs can support parallel input time series as separate variables or features. Therefore, we need to split the data into samples maintaining the order of observations across the two input sequences. If we chose three input time steps, then the first sample would look as follows: Input: Output: That is, the first three time steps of each parallel series are provided as input to the model and the model associates this with the value in the output series at the third time step, in this case, 65. We can see that, in transforming the time series into input/output samples to train the model, that we will have to discard some values from the output time series where we do not have values in the input time series at prior time steps. In turn, the choice of the size of the number of input time steps will have an important effect on how much of the training data is used. We can define a function named split_sequences() that will take a dataset as we have defined it with rows for time steps and columns for parallel series and return input/output samples. We can test this function on our dataset using three time steps for each input time series as input. The complete example is listed below. Running the example first prints the shape of the X and y components. We can see that the X component has a three-dimensional structure. The first dimension is the number of samples, in this case 7. The second dimension is the number of time steps per sample, in this case 3, the value specified to the function. Finally, the last dimension specifies the number of parallel time series or the number of variables, in this case 2 for the two parallel series. This is the exact three-dimensional structure expected by an LSTM as input. The data is ready to use without further reshaping. We can then see that the input and output for each sample is printed, showing the three time steps for each of the two input series and the associated output for each sample. We are now ready to fit an LSTM model on this data. Any of the varieties of LSTMs in the previous section can be used, such as a Vanilla, Stacked, Bidirectional, CNN, or ConvLSTM model. We will use a Vanilla LSTM where the number of time steps and parallel series (features) are specified for the input layer via the input_shape argument. When making a prediction, the model expects three time steps for two input time series. We can predict the next value in the output series providing the input values of: The shape of the one sample with three time steps and two variables must be [1, 3, 2]. We would expect the next value in the sequence to be 100 + 105, or 205. The complete example is listed below. Running the example prepares the data, fits the model, and makes a prediction. An alternate time series problem is the case where there are multiple parallel time series and a value must be predicted for each. For example, given the data from the previous section: We may want to predict the value for each of the three time series for the next time step. This might be referred to as multivariate forecasting. Again, the data must be split into input/output samples in order to train a model. The first sample of this dataset would be: Input: Output: The split_sequences() function below will split multiple parallel time series with rows for time steps and one series per column into the required input/output shape. We can demonstrate this on the contrived problem; the complete example is listed below. Running the example first prints the shape of the prepared X and y components. The shape of X is three-dimensional, including the number of samples (6), the number of time steps chosen per sample (3), and the number of parallel time series or features (3). The shape of y is two-dimensional as we might expect for the number of samples (6) and the number of time variables per sample to be predicted (3). The data is ready to use in an LSTM model that expects three-dimensional input and two-dimensional output shapes for the X and y components of each sample. Then, each of the samples is printed showing the input and output components of each sample. We are now ready to fit an LSTM model on this data. Any of the varieties of LSTMs in the previous section can be used, such as a Vanilla, Stacked, Bidirectional, CNN, or ConvLSTM model. We will use a Stacked LSTM where the number of time steps and parallel series (features) are specified for the input layer via the input_shape argument. The number of parallel series is also used in the specification of the number of values to predict by the model in the output layer; again, this is three. We can predict the next value in each of the three parallel series by providing an input of three time steps for each series. The shape of the input for making a single prediction must be 1 sample, 3 time steps, and 3 features, or [1, 3, 3] We would expect the vector output to be: We can tie all of this together and demonstrate a Stacked LSTM for multivariate output time series forecasting below. Running the example prepares the data, fits the model, and makes a prediction. A time series forecasting problem that requires a prediction of multiple time steps into the future can be referred to as multi-step time series forecasting. Specifically, these are problems where the forecast horizon or interval is more than one time step. There are two main types of LSTM models that can be used for multi-step forecasting; they are: Before we look at these models, let¡¯s first look at the preparation of data for multi-step forecasting. As with one-step forecasting, a time series used for multi-step time series forecasting must be split into samples with input and output components. Both the input and output components will be comprised of multiple time steps and may or may not have the same number of steps. For example, given the univariate time series: We could use the last three time steps as input and forecast the next two time steps. The first sample would look as follows: Input: Output: The split_sequence() function below implements this behavior and will split a given univariate time series into samples with a specified number of input and output time steps. We can demonstrate this function on the small contrived dataset. The complete example is listed below. Running the example splits the univariate series into input and output time steps and prints the input and output components of each. Now that we know how to prepare data for multi-step forecasting, let¡¯s look at some LSTM models that can learn this mapping. Like other types of neural network models, the LSTM can output a vector directly that can be interpreted as a multi-step forecast. This approach was seen in the previous section were one time step of each output time series was forecasted as a vector. As with the LSTMs for univariate data in a prior section, the prepared samples must first be reshaped. The LSTM expects data to have a three-dimensional structure of [samples, timesteps, features], and in this case, we only have one feature so the reshape is straightforward. With the number of input and output steps specified in the n_steps_in and n_steps_out variables, we can define a multi-step time-series forecasting model. Any of the presented LSTM model types could be used, such as Vanilla, Stacked, Bidirectional, CNN-LSTM, or ConvLSTM. Below defines a Stacked LSTM for multi-step forecasting. The model can make a prediction for a single sample. We can predict the next two steps beyond the end of the dataset by providing the input: We would expect the predicted output to be: As expected by the model, the shape of the single sample of input data when making the prediction must be [1, 3, 1] for the 1 sample, 3 time steps of the input, and the single feature. Tying all of this together, the Stacked LSTM for multi-step forecasting with a univariate time series is listed below. Running the example forecasts and prints the next two time steps in the sequence. A model specifically developed for forecasting variable length output sequences is called the Encoder-Decoder LSTM. The model was designed for prediction problems where there are both input and output sequences, so-called sequence-to-sequence, or seq2seq problems, such as translating text from one language to another. This model can be used for multi-step time series forecasting. As its name suggests, the model is comprised of two sub-models: the encoder and the decoder. The encoder is a model responsible for reading and interpreting the input sequence. The output of the encoder is a fixed length vector that represents the model¡¯s interpretation of the sequence. The encoder is traditionally a Vanilla LSTM model, although other encoder models can be used such as Stacked, Bidirectional, and CNN models. The decoder uses the output of the encoder as an input. First, the fixed-length output of the encoder is repeated, once for each required time step in the output sequence. This sequence is then provided to an LSTM decoder model. The model must output a value for each value in the output time step, which can be interpreted by a single output model. We can use the same output layer or layers to make each one-step prediction in the output sequence. This can be achieved by wrapping the output part of the model in a TimeDistributed wrapper. The full definition for an Encoder-Decoder model for multi-step time series forecasting is listed below. As with other LSTM models, the input data must be reshaped into the expected three-dimensional shape of [samples, timesteps, features]. In the case of the Encoder-Decoder model, the output, or y part, of the training dataset must also have this shape. This is because the model will predict a given number of time steps with a given number of features for each input sample. The complete example of an Encoder-Decoder LSTM for multi-step time series forecasting is listed below. Running the example forecasts and prints the next two time steps in the sequence. In the previous sections, we have looked at univariate, multivariate, and multi-step time series forecasting. It is possible to mix and match the different types of LSTM models presented so far for the different problems. This too applies to time series forecasting problems that involve multivariate and multi-step forecasting, but it may be a little more challenging. In this section, we will provide short examples of data preparation and modeling for multivariate multi-step time series forecasting as a template to ease this challenge, specifically: Perhaps the biggest stumbling block is in the preparation of data, so this is where we will focus our attention. There are those multivariate time series forecasting problems where the output series is separate but dependent upon the input time series, and multiple time steps are required for the output series. For example, consider our multivariate time series from a prior section: We may use three prior time steps of each of the two input time series to predict two time steps of the output time series. Input: Output: The split_sequences() function below implements this behavior. We can demonstrate this on our contrived dataset. The complete example is listed below. Running the example first prints the shape of the prepared training data. We can see that the shape of the input portion of the samples is three-dimensional, comprised of six samples, with three time steps, and two variables for the 2 input time series. The output portion of the samples is two-dimensional for the six samples and the two time steps for each sample to be predicted. The prepared samples are then printed to confirm that the data was prepared as we specified. We can now develop an LSTM model for multi-step predictions. A vector output or an encoder-decoder model could be used. In this case, we will demonstrate a vector output with a Stacked LSTM. The complete example is listed below. Running the example fits the model and predicts the next two time steps of the output sequence beyond the dataset. We would expect the next two steps to be: [185, 205] It is a challenging framing of the problem with very little data, and the arbitrarily configured version of the model gets close. A problem with parallel time series may require the prediction of multiple time steps of each time series. For example, consider our multivariate time series from a prior section: We may use the last three time steps from each of the three time series as input to the model and predict the next time steps of each of the three time series as output. The first sample in the training dataset would be the following. Input: Output: The split_sequences() function below implements this behavior. We can demonstrate this function on the small contrived dataset. The complete example is listed below. Running the example first prints the shape of the prepared training dataset. We can see that both the input (X) and output (Y) elements of the dataset are three dimensional for the number of samples, time steps, and variables or parallel time series respectively. The input and output elements of each series are then printed side by side so that we can confirm that the data was prepared as we expected. We can use either the Vector Output or Encoder-Decoder LSTM to model this problem. In this case, we will use the Encoder-Decoder model. The complete example is listed below. Running the example fits the model and predicts the values for each of the three time steps for the next two time steps beyond the end of the dataset. We would expect the values for these series and time steps to be as follows: We can see that the model forecast gets reasonably close to the expected values. In this tutorial, you discovered how to develop a suite of LSTM models for a range of standard time series forecasting problems. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. This tutorial is so helpful to me. Thank you very much!
It will be more helpful in the real projects if the dataset is split into batches. Hope you will mention this in the future. Keras will split the dataset into batches. I think this blog ( https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/) may answer my question. I will do more research. Thanks a lot. Great! Thanks Jason for this good tutorial. I have a question. When we have two different time series, 1 and 2. Time series 1 will influence time series 2 and our goal is to predict the future value of time series 2. How can we use LSTM for this case? I call this a dependent time series problem. I given an example of how to model it on this post:https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/ The link is the link of the current page, Do you mean that? Yes, I give an example above. Comment  Name (required)  Email (will not be published) (required)  Website"
"mastery",2018-11-12,"How to Develop Convolutional Neural Network Models for Time Series Forecasting","https://machinelearningmastery.com/how-to-develop-convolutional-neural-network-models-for-time-series-forecasting/","Convolutional Neural Network models, or CNNs for short, can be applied to time series forecasting. There are many types of CNN models that can be used for each specific type of time series forecasting problem. In this tutorial, you will discover how to develop a suite of CNN models for a range of standard time series forecasting problems. The objective of this tutorial is to provide standalone examples of each model on each type of time series problem as a template that you can copy and adapt for your specific time series forecasting problem. After completing this tutorial, you will know: This is a large and important post; you may want to bookmark it for future reference. Let¡¯s get started. How to Develop Convolutional Neural Network Models for Time Series ForecastingPhoto by Bureau of Land Management, some rights reserved. In this tutorial, we will explore how to develop a suite of different types of CNN models for time series forecasting. The models are demonstrated on small contrived time series problems intended to give the flavor of the type of time series problem being addressed. The chosen configuration of the models is arbitrary and not optimized for each problem; that was not the goal. This tutorial is divided into four parts; they are: Although traditionally developed for two-dimensional image data, CNNs can be used to model univariate time series forecasting problems. Univariate time series are datasets comprised of a single series of observations with a temporal ordering and a model is required to learn from the series of past observations to predict the next value in the sequence. This section is divided into two parts; they are: Before a univariate series can be modeled, it must be prepared. The CNN model will learn a function that maps a sequence of past observations as input to an output observation. As such, the sequence of observations must be transformed into multiple examples from which the model can learn. Consider a given univariate sequence: We can divide the sequence into multiple input/output patterns called samples, where three time steps are used as input and one time step is used as output for the one-step prediction that is being learned. The split_sequence() function below implements this behavior and will split a given univariate sequence into multiple samples where each sample has a specified number of time steps and the output is a single time step. We can demonstrate this function on our small contrived dataset above. The complete example is listed below. Running the example splits the univariate series into six samples where each sample has three input time steps and one output time step. Now that we know how to prepare a univariate series for modeling, let¡¯s look at developing a CNN model that can learn the mapping of inputs to outputs. Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course A one-dimensional CNN is a CNN model that has a convolutional hidden layer that operates over a 1D sequence. This is followed by perhaps a second convolutional layer in some cases, such as very long input sequences, and then a pooling layer whose job it is to distill the output of the convolutional layer to the most salient elements. The convolutional and pooling layers are followed by a dense fully connected layer that interprets the features extracted by the convolutional part of the model. A flatten layer is used between the convolutional layers and the dense layer to reduce the feature maps to a single one-dimensional vector. We can define a 1D CNN Model for univariate time series forecasting as follows. Key in the definition is the shape of the input; that is what the model expects as input for each sample in terms of the number of time steps and the number of features. We are working with a univariate series, so the number of features is one, for one variable. The number of time steps as input is the number we chose when preparing our dataset as an argument to the split_sequence() function. The input shape for each sample is specified in the input_shape argument on the definition of the first hidden layer. We almost always have multiple samples, therefore, the model will expect the input component of training data to have the dimensions or shape: Our split_sequence() function in the previous section outputs the X with the shape [samples, timesteps], so we can easily reshape it to have an additional dimension for the one feature. The CNN does not actually view the data as having time steps, instead, it is treated as a sequence over which convolutional read operations can be performed, like a one-dimensional image. In this example, we define a convolutional layer with 64 filter maps and a kernel size of 2. This is followed by a max pooling layer and a dense layer to interpret the input feature. An output layer is specified that predicts a single numerical value. The model is fit using the efficient Adam version of stochastic gradient descent and optimized using the mean squared error, or ¡®mse¡®, loss function. Once the model is defined, we can fit it on the training dataset. After the model is fit, we can use it to make a prediction. We can predict the next value in the sequence by providing the input: And expecting the model to predict something like: The model expects the input shape to be three-dimensional with [samples, timesteps, features], therefore, we must reshape the single input sample before making the prediction. We can tie all of this together and demonstrate how to develop a 1D CNN model for univariate time series forecasting and make a single prediction. Running the example prepares the data, fits the model, and makes a prediction. Your results may vary given the stochastic nature of the algorithm; try running the example a few times. We can see that the model predicts the next value in the sequence. Multivariate time series data means data where there is more than one observation for each time step. There are two main models that we may require with multivariate time series data; they are: Let¡¯s take a look at each in turn. A problem may have two or more parallel input time series and an output time series that is dependent on the input time series. The input time series are parallel because each series has observations at the same time steps. We can demonstrate this with a simple example of two parallel input time series where the output series is the simple addition of the input series. We can reshape these three arrays of data as a single dataset where each row is a time step and each column is a separate time series. This is a standard way of storing parallel time series in a CSV file. The complete example is listed below. Running the example prints the dataset with one row per time step and one column for each of the two input and one output parallel time series. As with the univariate time series, we must structure these data into samples with input and output samples. A 1D CNN model needs sufficient context to learn a mapping from an input sequence to an output value. CNNs can support parallel input time series as separate channels, like red, green, and blue components of an image. Therefore, we need to split the data into samples maintaining the order of observations across the two input sequences. If we chose three input time steps, then the first sample would look as follows: Input: Output: That is, the first three time steps of each parallel series are provided as input to the model and the model associates this with the value in the output series at the third time step, in this case, 65. We can see that, in transforming the time series into input/output samples to train the model, that we will have to discard some values from the output time series where we do not have values in the input time series at prior time steps. In turn, the choice of the size of the number of input time steps will have an important effect on how much of the training data is used. We can define a function named split_sequences() that will take a dataset as we have defined it with rows for time steps and columns for parallel series and return input/output samples. We can test this function on our dataset using three time steps for each input time series as input. The complete example is listed below. Running the example first prints the shape of the X and y components. We can see that the X component has a three-dimensional structure. The first dimension is the number of samples, in this case 7. The second dimension is the number of time steps per sample, in this case 3, the value specified to the function. Finally, the last dimension specifies the number of parallel time series or the number of variables, in this case 2 for the two parallel series. This is the exact three-dimensional structure expected by a 1D CNN as input. The data is ready to use without further reshaping. We can then see that the input and output for each sample is printed, showing the three time steps for each of the two input series and the associated output for each sample. We are now ready to fit a 1D CNN model on this data, specifying the expected number of time steps and features to expect for each input sample, in this case three and two respectively. When making a prediction, the model expects three time steps for two input time series. We can predict the next value in the output series providing the input values of: The shape of the one sample with three time steps and two variables must be [1, 3, 2]. We would expect the next value in the sequence to be 100 + 105 or 205. The complete example is listed below. Running the example prepares the data, fits the model, and makes a prediction. There is another, more elaborate way to model the problem. Each input series can be handled by a separate CNN and the output of each of these submodels can be combined before a prediction is made for the output sequence. We can refer to this as a multi-headed CNN model. It may offer more flexibility or better performance depending on the specifics of the problem that is being modeled. For example, it allows you to configure each sub-model differently for each input series, such as the number of filter maps and the kernel size. This type of model can be defined in Keras using the Keras functional API. First, we can define the first input model as a 1D CNN with an input layer that expects vectors with n_steps and 1 feature. We can define the second input submodel in the same way. Now that both input submodels have been defined, we can merge the output from each model into one long vector which can be interpreted before making a prediction for the output sequence. We can then tie the inputs and outputs together. The image below provides a schematic for how this model looks, including the shape of the inputs and outputs of each layer. Plot of Multi-Headed 1D CNN for Multivariate Time Series Forecasting This model requires input to be provided as a list of two elements where each element in the list contains data for one of the submodels. In order to achieve this, we can split the 3D input data into two separate arrays of input data; that is from one array with the shape [7, 3, 2] to two 3D arrays with [7, 3, 1] These data can then be provided in order to fit the model. Similarly, we must prepare the data for a single sample as two separate two-dimensional arrays when making a single one-step prediction. We can tie all of this together; the complete example is listed below. Running the example prepares the data, fits the model, and makes a prediction. An alternate time series problem is the case where there are multiple parallel time series and a value must be predicted for each. For example, given the data from the previous section: We may want to predict the value for each of the three time series for the next time step. This might be referred to as multivariate forecasting. Again, the data must be split into input/output samples in order to train a model. The first sample of this dataset would be: Input: Output: The split_sequences() function below will split multiple parallel time series with rows for time steps and one series per column into the required input/output shape. We can demonstrate this on the contrived problem; the complete example is listed below. Running the example first prints the shape of the prepared X and y components. The shape of X is three-dimensional, including the number of samples (6), the number of time steps chosen per sample (3), and the number of parallel time series or features (3). The shape of y is two-dimensional as we might expect for the number of samples (6) and the number of time variables per sample to be predicted (3). The data is ready to use in a 1D CNN model that expects three-dimensional input and two-dimensional output shapes for the X and y components of each sample. Then, each of the samples is printed showing the input and output components of each sample. We are now ready to fit a 1D CNN model on this data. In this model, the number of time steps and parallel series (features) are specified for the input layer via the input_shape argument. The number of parallel series is also used in the specification of the number of values to predict by the model in the output layer; again, this is three. We can predict the next value in each of the three parallel series by providing an input of three time steps for each series. The shape of the input for making a single prediction must be 1 sample, 3 time steps, and 3 features, or [1, 3, 3]. We would expect the vector output to be: We can tie all of this together and demonstrate a 1D CNN for multivariate output time series forecasting below. Running the example prepares the data, fits the model and makes a prediction. As with multiple input series, there is another more elaborate way to model the problem. Each output series can be handled by a separate output CNN model. We can refer to this as a multi-output CNN model. It may offer more flexibility or better performance depending on the specifics of the problem that is being modeled. This type of model can be defined in Keras using the Keras functional API. First, we can define the first input model as a 1D CNN model. We can then define one output layer for each of the three series that we wish to forecast, where each output submodel will forecast a single time step. We can then tie the input and output layers together into a single model. To make the model architecture clear, the schematic below clearly shows the three separate output layers of the model and the input and output shapes of each layer. Plot of Multi-Output 1D CNN for Multivariate Time Series Forecasting When training the model, it will require three separate output arrays per sample. We can achieve this by converting the output training data that has the shape [7, 3] to three arrays with the shape [7, 1]. These arrays can be provided to the model during training. Tying all of this together, the complete example is listed below. Running the example prepares the data, fits the model, and makes a prediction. In practice, there is little difference to the 1D CNN model in predicting a vector output that represents different output variables (as in the previous example), or a vector output that represents multiple time steps of one variable. Nevertheless, there are subtle and important differences in the way the training data is prepared. In this section, we will demonstrate the case of developing a multi-step forecast model using a vector model. Before we look at the specifics of the model, let¡¯s first look at the preparation of data for multi-step forecasting. As with one-step forecasting, a time series used for multi-step time series forecasting must be split into samples with input and output components. Both the input and output components will be comprised of multiple time steps and may or may not have the same number of steps. For example, given the univariate time series: We could use the last three time steps as input and forecast the next two time steps. The first sample would look as follows: Input: Output: The split_sequence() function below implements this behavior and will split a given univariate time series into samples with a specified number of input and output time steps. We can demonstrate this function on the small contrived dataset. The complete example is listed below. Running the example splits the univariate series into input and output time steps and prints the input and output components of each. Now that we know how to prepare data for multi-step forecasting, let¡¯s look at a 1D CNN model that can learn this mapping. The 1D CNN can output a vector directly that can be interpreted as a multi-step forecast. This approach was seen in the previous section were one time step of each output time series was forecasted as a vector. As with the 1D CNN models for univariate data in a prior section, the prepared samples must first be reshaped. The CNN expects data to have a three-dimensional structure of [samples, timesteps, features], and in this case, we only have one feature so the reshape is straightforward. With the number of input and output steps specified in the n_steps_in and n_steps_out variables, we can define a multi-step time-series forecasting model. The model can make a prediction for a single sample. We can predict the next two steps beyond the end of the dataset by providing the input: We would expect the predicted output to be: As expected by the model, the shape of the single sample of input data when making the prediction must be [1, 3, 1] for the 1 sample, 3 time steps of the input, and the single feature. Tying all of this together, the 1D CNN for multi-step forecasting with a univariate time series is listed below. Running the example forecasts and prints the next two time steps in the sequence. In the previous sections, we have looked at univariate, multivariate, and multi-step time series forecasting. It is possible to mix and match the different types of 1D CNN models presented so far for the different problems. This too applies to time series forecasting problems that involve multivariate and multi-step forecasting, but it may be a little more challenging. In this section, we will explore short examples of data preparation and modeling for multivariate multi-step time series forecasting as a template to ease this challenge, specifically: Perhaps the biggest stumbling block is in the preparation of data, so this is where we will focus our attention. There are those multivariate time series forecasting problems where the output series is separate but dependent upon the input time series, and multiple time steps are required for the output series. For example, consider our multivariate time series from a prior section: We may use three prior time steps of each of the two input time series to predict two time steps of the output time series. Input: Output: The split_sequences() function below implements this behavior. We can demonstrate this on our contrived dataset. The complete example is listed below. Running the example first prints the shape of the prepared training data. We can see that the shape of the input portion of the samples is three-dimensional, comprised of six samples, with three time steps and two variables for the two input time series. The output portion of the samples is two-dimensional for the six samples and the two time steps for each sample to be predicted. The prepared samples are then printed to confirm that the data was prepared as we specified. We can now develop a 1D CNN model for multi-step predictions. In this case, we will demonstrate a vector output model. The complete example is listed below. Running the example fits the model and predicts the next two time steps of the output sequence beyond the dataset. We would expect the next two steps to be [185, 205]. It is a challenging framing of the problem with very little data, and the arbitrarily configured version of the model gets close. A problem with parallel time series may require the prediction of multiple time steps of each time series. For example, consider our multivariate time series from a prior section: We may use the last three time steps from each of the three time series as input to the model, and predict the next time steps of each of the three time series as output. The first sample in the training dataset would be the following. Input: Output: The split_sequences() function below implements this behavior. We can demonstrate this function on the small contrived dataset. The complete example is listed below. Running the example first prints the shape of the prepared training dataset. We can see that both the input (X) and output (Y) elements of the dataset are three dimensional for the number of samples, time steps, and variables or parallel time series respectively. The input and output elements of each series are then printed side by side so that we can confirm that the data was prepared as we expected. We can now develop a 1D CNN model for this dataset. We will use a vector-output model in this case. As such, we must flatten the three-dimensional structure of the output portion of each sample in order to train the model. This means, instead of predicting two steps for each series, the model is trained on and expected to predict a vector of six numbers directly. The complete example is listed below. Running the example fits the model and predicts the values for each of the three time steps for the next two time steps beyond the end of the dataset. We would expect the values for these series and time steps to be as follows: We can see that the model forecast gets reasonably close to the expected values. In this tutorial, you discovered how to develop a suite of CNN models for a range of standard time series forecasting problems. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ¡¦with just a few lines of python code Discover how in my new Ebook:Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like: CNNs, LSTMs,Multivariate Forecasting, Multi-Step Forecasting and much more¡¦ Skip the Academics. Just Results. Click to learn more. Hi Jason, Good post (as always)! I got a non related question. Recently I have been developed almost exclusively in javascript (both front react and backend with node js). It has been long time i have done asny solid coding in python, hence my skillset is rusty.  Now, I wonder, how do you see the applying of programming languages for ML apps.
Tensorflow is running now both inn a browser tf.js as well on the backend with node js (just like python?). That sounds like a great thing <U+2013> one language for everything. There are also courses on the topic, getting more tractionhttps://www.udemy.com/machine-learning-with-javascript/ Is javascript enough for machine learning apps? or python should be used? Can you please elaborate? thanks and regards
JSman Hmmm, maybe for small apps. I cannot imagine being able to convince my team that a JS solution would make more sense, unless the existing system was all JS or it as a front-end demo or something. Or maybe if the model was fit using something fast and used to make predictions in JS. Really, you want to use the same tech stack as the rest of the existing system/enterprise. Hi Jason, A very high quality article for me to learn more about deep learning. It really help me a lots.Please keep sharing the knowledge. Thank you! Cheer Thanks, I¡¯m glad to hear that. Nice site. Just a comment. IMO, It¡¯s a bit pretentious and weak to put the title PhD after your name (¡± I¡¯m Jason Brownlee PhD¡¦¡±). You don¡¯t need to validate yourself through a useless degree. You have already earned the respect of all of us through your wonderful work. A mention of your credentials at a bio page would have sufficed. Just my two cents. Thanks for the feedback. Testing showed me that ¡°phd¡± splashed around helps with creditability for first time visitors. Thanks Jason for your new clear, detailed and very well explained explanation (as always)!. I¡¯m glad it helped. I index an image by a low-level feature (color) as form of a digital vector can i can exploit the current topic for an image clasifier Maybe. Thanks Jason for a very detailed explanation of CNN, and the many ways we can approach a time forecasting problem with CNNs. I¡¯m happy it helped. Hi Jason, I have become a fan, after reading this post of yours. I have been trying to use 1D CNNs for one of my network anomaly applications, but somehow couldn¡¯t get them to work effectively. This post has all that I need to get my network up and running. Thanks. I¡¯m happy to hear that! Comment  Name (required)  Email (will not be published) (required)  Website"
