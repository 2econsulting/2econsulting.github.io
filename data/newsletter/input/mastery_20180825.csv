"site","date","headline","url_address","text"
"mastery",2018-08-24,"How to Model Volatility with ARCH and GARCH for Time Series Forecasting in Python","https://machinelearningmastery.com/develop-arch-and-garch-models-for-time-series-forecasting-in-python/","The ARCH or<U+00A0>Autoregressive Conditional Heteroskedasticity method provides a way to model a change in variance in a time series that is time dependent, such as increasing or decreasing volatility. An extension of this approach named GARCH or Generalized Autoregressive Conditional Heteroskedasticity allows the method to support changes in the time dependent volatility, such as increasing and decreasing volatility in the same series. In this tutorial, you will discover the ARCH and GARCH models for predicting the variance of a time series. After completing this tutorial, you will know: Let¡¯s get started. How to Develop ARCH and GARCH Models for Time Series Forecasting in PythonPhoto by Murray Foubister, some rights reserved. This tutorial is divided into five parts; they are: Autoregressive models can be developed for univariate time series data that is stationary (AR), has a trend (ARIMA), and has a seasonal component (SARIMA). One aspect of a univariate time series that these autoregressive models do not model is a change in the variance over time. Classically, a time series with modest changes in variance can sometimes be adjusted using a power transform, such as by taking the Log or using a Box-Cox transform. There are some time series where the variance changes consistently over time. In the context of a time series in the financial domain, this would be called increasing and decreasing volatility. In time series where the variance is increasing in a systematic way, such as an increasing trend, this property of the series is called heteroskedasticity. It¡¯s a fancy word from statistics that means changing or unequal variance across the series. If the change in variance can be correlated over time, then it can be modeled using an autoregressive process, such as ARCH. Autoregressive Conditional Heteroskedasticity, or ARCH, is a method that explicitly models the change in variance over time in a time series. Specifically, an ARCH method models the variance at a time step as a function of the residual errors from a mean process (e.g. a zero mean). The ARCH process introduced by Engle (1982) explicitly recognizes the difference between the unconditional and the conditional variance allowing the latter to change over time as a function of past errors. <U+2014> Generalized autoregressive conditional heteroskedasticity, 1986. A lag parameter must be specified to define the number of prior residual errors to include in the model. Using the notation of the GARCH model (discussed later), we can refer to this parameter as ¡°q¡°. Originally, this parameter was called ¡°p¡°, and is also called ¡°p¡± in the arch Python package used later in this tutorial. A generally accepted notation for an ARCH model is to specify the ARCH() function with the q parameter ARCH(q); for example, ARCH(1) would be a first order ARCH model. The approach expects the series is stationary, other than the change in variance, meaning it does not have a trend or seasonal component. An ARCH model is used to predict the variance at future time steps. [ARCH] are mean zero, serially uncorrelated processes with nonconstant variances conditional on the past, but constant unconditional variances. For such processes, the recent past gives information about the one-period forecast variance. <U+2013> Autoregressive Conditional Heteroscedasticity with Estimates of the Variance of United Kingdom Inflation, 1982. In practice, this can be used to model the expected variance on the residuals after another autoregressive model has been used, such as an ARMA or similar. The model should only be applied to a prewhitened residual series {e_t} that is uncorrelated and contains no trends or seasonal changes, such as might be obtained after fitting a satisfactory SARIMA model. <U+2014> Page 148, Introductory Time Series with R, 2009. Generalized Autoregressive Conditional Heteroskedasticity, or GARCH, is an extension of the ARCH model that incorporates a moving average component together with the autoregressive component. Specifically, the model includes lag variance terms (e.g. the observations if modeling the white noise residual errors of another process), together with lag residual errors from a mean process. The introduction of a moving average component allows the model to both model the conditional change in variance over time as well as changes in the time-dependent variance. Examples include conditional increases and decreases in variance. As such, the model introduces a new parameter ¡°p¡± that describes the number of lag variance terms: A generally accepted notation for a GARCH model is to specify the GARCH() function with the p and q parameters GARCH(p, q); for example GARCH(1, 1) would be a first order GARCH model. A GARCH model subsumes ARCH models, where a GARCH(0, q) is equivalent to an ARCH(q) model. For p = 0 the process reduces to the ARCH(q) process, and for p = q = 0 E(t) is simply white noise. In the ARCH(q) process the conditional variance is specified as a linear function of past sample variances only, whereas the GARCH(p, q) process allows lagged conditional variances to enter as well. This corresponds to some sort of adaptive learning mechanism. <U+2014> Generalized autoregressive conditional heteroskedasticity, 1986. As with ARCH, GARCH predicts the future variance and expects that the series is stationary, other than the change in variance, meaning it does not have a trend or seasonal component. The configuration for an ARCH model is best understood in the context of ACF and PACF plots of the variance of the time series. This can be achieved by subtracting the mean from each observation in the series and squaring the result, or just squaring the observation if you¡¯re already working with white noise residuals from another model. If a correlogram appears to be white noise [¡¦], then volatility ca be detected by looking at the correlogram of the squared values since the squared values are equivalent to the variance (provided the series is adjusted to have a mean of zero). <U+2014> Pages 146-147, Introductory Time Series with R, 2009. The ACF and PACF plots can then be interpreted to estimate values for p and q, in a similar way as is done for the ARMA model. For more information on how to do this, see the post: In this section, we will look at how we can develop ARCH and GARCH models in Python using the arch library. First, let¡¯s prepare a dataset we can use for these examples. We can create a dataset with a controlled model of variance. The simplest case would be a series of random noise where the mean is zero and the variance starts at 0.0 and steadily increases. We can achieve this in Python using the gauss() function that generates a Gaussian random number with the specified mean and standard deviation. We can plot the dataset to get an idea of how the linear change in variance looks. The complete example is listed below. Running the example creates and plots the dataset. We can see the clear change in variance over the course of the series. Line Plot of Dataset with Increasing Variance We know there is an autocorrelation in the variance of the contrived dataset. Nevertheless, we can look at an autocorrelation plot to confirm this expectation. The complete example is listed below. Running the example creates an autocorrelation plot of the squared observations. We see significant positive correlation in variance out to perhaps 15 lag time steps. This might make a reasonable value for the parameter in the ARCH model. Autocorrelation Plot of Data with Increasing Variance Developing an ARCH model involves three steps: Before fitting and forecasting, we can split the dataset into a train and test set so that we can fit the model on the train and evaluate its performance on the test set. A model can be defined by calling the arch_model() function. We can specify a model for the mean of the series: in this case mean=¡¯Zero¡¯ is an appropriate model. We can then specify the model for the variance: in this case vol=¡¯ARCH¡¯. We can also specify the lag parameter for the ARCH model: in this case p=15. Note, in the arch library, the names of p and q parameters for ARCH/GARCH have been reversed. The model can be fit on the data by calling the fit() function. There are many options on this function, although the defaults are good enough for getting started. This will return a fit model. Finally, we can make a prediction by calling the forecast() function on the fit model. We can specify the horizon for the forecast. In this case, we will predict the variance for the last 10 time steps of the dataset, and withhold them from the training of the model. We can tie all of this together; the complete example is listed below. Running the example defines and fits the model then predicts the variance for the last 10 time steps of the dataset. A line plot is created comparing the series of expected variance to the predicted variance. Although the model was not tuned, the predicted variance looks reasonable. Line Plot of Expected Variance to Predicted Variance using ARCH We can fit a GARCH model just as easily using the arch library. The arch_model() function can specify a GARCH instead of ARCH model vol=¡¯GARCH¡¯ as well as the lag parameters for both. The dataset may not be a good fit for a GARCH model given the linearly increasing variance, nevertheless, the complete example is listed below. A plot of the expected and predicted variance is listed below. Line Plot of Expected Variance to Predicted Variance using GARCH This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered the ARCH and GARCH models for predicting the variance of a time series. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ...with just a few lines of python code Discover how in my new Ebook:Introduction to Time Series Forecasting With Python It covers self-study tutorials and end-to-end projects on topics like:Loading data, visualization, modeling, algorithm tuning, and much more... Skip the Academics. Just Results. Click to learn more. Hi Jason, You mentioned the need for PACF but you haven¡¯t plotted it, isn¡¯t PACF needed to determine q? Best,
Elie K Yes, you can use ACF and PACF, learn more here:https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/ Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning. Read More"
"mastery",2018-08-22,"4 Common Machine Learning Data Transforms for Time Series Forecasting","https://machinelearningmastery.com/machine-learning-data-transforms-for-time-series-forecasting/","Time series data often requires some preparation prior to being modeled with machine learning algorithms. For example, differencing operations can be used to remove trend and seasonal structure from the sequence in order to simplify the prediction problem. Some algorithms, such as neural networks, prefer data to be standardized and/or normalized prior to modeling. Any transform operations applied to the series also require a similar inverse transform to be applied on the predictions. This is required so that the resulting calculated performance measures are in the same scale as the output variable and can be compared to classical forecasting methods. In this post, you will discover how to perform and invert four common data transforms for time series data in machine learning. After reading this post, you will know: Let¡¯s get started. 4 Common Machine Learning Data Transforms for Time Series ForecastingPhoto by Wolfgang Staudt, some rights reserved. This tutorial is divided into three parts; they are: Given a univariate time series dataset, there are four transforms that are popular when using machine learning methods to model and make predictions. They are: Let¡¯s take a quick look at each in turn and how to perform these transforms in Python. We will also review how to reverse the transform operation as this is required when we want to evaluate the predictions in their original scale so that performance measures can be compared directly. Are there other transforms you like to use on your time series data for modeling with machine learning methods?
Let me know in the comments below. A power transform removes a shift from a data distribution to make the distribution more-normal (Gaussian). On a time series dataset, this can have the effect of removing a change in variance over time. Popular examples are the log transform (positive values) or generalized versions such as the Box-Cox transform (positive values) or the Yeo-Johnson transform (positive and negative values). For example, we can implement the Box-Cox transform in Python using the boxcox() function from the SciPy library. By default, the method will numerically optimize the lambda value for the transform and return the optimal value. The transform can be inverted but requires a custom function listed below named invert_boxcox() that takes a transformed value and the lambda value that was used to perform the transform. A complete example of applying the power transform to a dataset and reversing the transform is listed below. Running the example prints the original dataset, the results of the power transform, and the original values (or close to it) after the transform is inverted. A difference transform is a simple way for removing a systematic structure from the time series. For example, a trend can be removed by subtracting the previous value from each value in the series. This is called first order differencing. The process can be repeated (e.g. difference the differenced series) to remove second order trends, and so on. A seasonal structure can be removed in a similar way by subtracting the observation from the prior season, e.g. 12 time steps ago for monthly data with a yearly seasonal structure. A single differenced value in a series can be calculated with a custom function named difference() listed below. The function takes the time series and the interval for the difference calculation, e.g. 1 for a trend difference or 12 for a seasonal difference. Again, this operation can be inverted with a custom function that adds the original value back to the differenced value named invert_difference() that takes the original series and the interval. We can demonstrate this function below. Running the example prints the original dataset, the results of the difference transform, and the original values after the transform is inverted. Note, the first ¡°interval¡± values will be lost from the sequence after the transform. This is because they do not have a value at ¡°interval¡± prior time steps, therefore cannot be differenced. Standardization is a transform for data with a Gaussian distribution. It subtracts the mean and divides the result by the standard deviation of the data sample. This has the effect of transforming the data to have mean of zero, or centered, with a standard deviation of 1. This resulting distribution is called a standard Gaussian distribution, or a standard normal, hence the name of the transform. We can perform standardization using the StandardScaler object in Python from the scikit-learn library. This class allows the transform to be fit on a training dataset by calling fit(), applied to one or more datasets (e.g. train and test) by calling transform() and also provides a function to reverse the transform by calling inverse_transform(). A complete example is applied below. Running the example prints the original dataset, the results of the standardize transform, and the original values after the transform is inverted. Note the expectation that data is provided as a column with multiple rows. Normalization is a rescaling of data from the original range to a new range between 0 and 1. As with standardization, this can be implemented using a transform object from the scikit-learn library, specifically the MinMaxScaler class. In addition to normalization, this class can be used to rescale data to any range you wish by specifying the preferred range in the constructor of the object. It can be used in the same way to fit, transform, and inverse the transform. A complete example is listed below. Running the example prints the original dataset, the results of the normalize transform, and the original values after the transform is inverted. We have mentioned the importance of being able to invert a transform on the predictions of a model in order to calculate a model performance statistic that is directly comparable to other methods. Additionally, another concern is the problem of data leakage. Three of the above data transforms estimate coefficients from a provided dataset that are then used to transform the data. Specifically: These coefficients must be estimated on the training dataset only. Once estimated, the transform can be applied using the coefficients to the training and the test dataset before evaluating your model. If the coefficients are estimated using the entire dataset prior to splitting into train and test sets, then there is a small leakage of information from the test set to the training dataset. This can result in estimates of model skill that are optimistically biased. As such, you may want to enhance the estimates of the coefficients with domain knowledge, such as expected min/max values for all time in the future. Generally, differencing does not suffer the same problems. In most cases, such as one-step forecasting, the lag observations are available to perform the difference calculation. If not, the lag predictions can be used wherever needed as a proxy for the true observations in difference calculations. You may want to experiment with applying multiple data transforms to a time series prior to modeling. This is quite common, e.g. to apply a power transform to remove an increasing variance, to apply seasonal differencing to remove seasonality, and to apply one-step differencing to remove a trend. The order that the transform operations are applied is important. Intuitively, we can think through how the transforms may interact. As such, a suggested ordering for data transforms is as follows: Obviously, you would only use the transforms required for your specific dataset. Importantly, when the transform operations are inverted, the order of the inverse transform operations must be reversed. Specifically, the inverse operations must be performed in the following order: This section provides more resources on the topic if you are looking to go deeper. In this post, you discovered how to perform and invert four common data transforms for time series data in machine learning. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning. Read More"
"mastery",2018-08-20,"A Gentle Introduction to Exponential Smoothing for Time Series Forecasting in Python","https://machinelearningmastery.com/exponential-smoothing-for-time-series-forecasting-in-python/","It is a powerful forecasting method that may be used as an alternative to the popular Box-Jenkins ARIMA family of methods. In this tutorial, you will discover the exponential smoothing method for univariate time series forecasting. After completing this tutorial, you will know: Let¡¯s get started. A Gentle Introduction to Exponential Smoothing for Time Series Forecasting in PythonPhoto by Wolfgang Staudt, some rights reserved. This tutorial is divided into 4 parts; they are: Exponential smoothing is a time series forecasting method for univariate data. Time series methods like the Box-Jenkins ARIMA family of methods develop a model where the prediction is a weighted linear sum of recent past observations or lags. Exponential smoothing forecasting methods are similar in that a prediction is a weighted sum of past observations, but the model explicitly uses an exponentially decreasing weight for past observations. Specifically, past observations are weighted with a geometrically decreasing ratio. Forecasts produced using exponential smoothing methods are weighted averages of past observations, with the weights decaying exponentially as the observations get older. In other words, the more recent the observation the higher the associated weight. <U+2014> Page 171, Forecasting: principles and practice, 2013. Exponential smoothing methods may be considered as peers and an alternative to the popular Box-Jenkins ARIMA class of methods for time series forecasting. Collectively, the methods are sometimes referred to as ETS models, referring to the explicit modeling of Error, Trend and Seasonality. There are three main types of exponential smoothing time series forecasting methods. A simple method that assumes no systematic structure, an extension that explicitly handles trends, and the most advanced approach that add support for seasonality. Single Exponential Smoothing, SES for short, also called Simple Exponential Smoothing, is a time series forecasting method for univariate data without a trend or seasonality. It requires a single parameter, called alpha (a), also called the smoothing factor or smoothing coefficient. This parameter controls the rate at which the influence of the observations at prior time steps decay exponentially. Alpha is often set to a value between 0 and 1. Large values mean that the model pays attention mainly to the most recent past observations, whereas smaller values mean more of the history is taken into account when making a prediction. A value close to 1 indicates fast learning (that is, only the most recent values influence the forecasts), whereas a value close to 0 indicates slow learning (past observations have a large influence on forecasts). <U+2014> Page 89, Practical Time Series Forecasting with R, 2016. Hyperparameters: Double Exponential Smoothing is an extension to Exponential Smoothing that explicitly adds support for trends in the univariate time series. In addition to the alpha parameter for controlling smoothing factor for the level, an additional smoothing factor is added to control the decay of the influence of the change in trend called beta (b). The method supports trends that change in different ways: an additive and a multiplicative, depending on whether the trend is linear or exponential respectively. Double Exponential Smoothing with an additive trend is classically referred to as Holt¡¯s linear trend model, named for the developer of the method Charles Holt. For longer range (multi-step) forecasts, the trend may continue on unrealistically. As such, it can be useful to dampen the trend over time. Dampening means reducing the size of the trend over future time steps down to a straight line (no trend). The forecasts generated by Holt¡¯s linear method display a constant trend (increasing or decreasing) indecently into the future. Even more extreme are the forecasts generated by the exponential trend method [¡¦] Motivated by this observation [¡¦] introduced a parameter that ¡°dampens¡± the trend to a flat line some time in the future. <U+2014> Page 183, Forecasting: principles and practice, 2013. As with modeling the trend itself, we can use the same principles in dampening the trend, specifically additively or multiplicatively for a linear or exponential dampening effect. A damping coefficient Phi (p) is used to control the rate of dampening. Hyperparameters: Triple Exponential Smoothing is an extension of Exponential Smoothing that explicitly adds support for seasonality to the univariate time series. This method is sometimes called Holt-Winters Exponential Smoothing, named for two contributors to the method: Charles Holt and Peter Winters. In addition to the alpha and beta smoothing factors, a new parameter is added called gamma (g) that controls the influence on the seasonal component. As with the trend, the seasonality may be modeled as either an additive or multiplicative process for a linear or exponential change in the seasonality. Triple exponential smoothing is the most advanced variation of exponential smoothing and through configuration, it can also develop double and single exponential smoothing models. Being an adaptive method, Holt-Winter¡¯s exponential smoothing allows the level, trend and seasonality patterns to change over time. <U+2014> Page 95, Practical Time Series Forecasting with R, 2016. Additionally, to ensure that the seasonality is modeled correctly, the number of time steps in a seasonal period (Period) must be specified. For example, if the series was monthly data and the seasonal period repeated each year, then the Period=12. Hyperparameters: All of the model hyperparameters can be specified explicitly. This can be challenging for experts and beginners alike. Instead, it is common to use numerical optimization to search for and fund the smoothing coefficients (alpha, beta, gamma, and phi) for the model that result in the lowest error. [¡¦] a more robust and objective way to obtain values for the unknown parameters included in any exponential smoothing method is to estimate them from the observed data. [¡¦] the unknown parameters and the initial values for any exponential smoothing method can be estimated by minimizing the SSE [sum of the squared errors]. <U+2014> Page 177, Forecasting: principles and practice, 2013. The parameters that specify the type of change in the trend and seasonality, such as weather they are additive or multiplicative and whether they should be dampened, must be specified explicitly. This section looks at how to implement exponential smoothing in Python. The implementations of Exponential Smoothing in Python are provided in the Statsmodels Python library. The implementations are based on the description of the method in Rob Hyndman and George Athana¡©sopou¡©los¡¯ excellent book ¡°Forecasting: Principles and Practice,¡± 2013 and their R implementations in their ¡°forecast¡± package. Single Exponential Smoothing or simple smoothing can be implemented in Python via the SimpleExpSmoothing Statsmodels class. First, an instance of the SimpleExpSmoothing class must be instantiated and passed the training data. The fit() function is then called providing the fit configuration, specifically the alpha value called smoothing_level. If this is not provided or set to None, the model will automatically optimize the value. This fit() function returns an instance of the HoltWintersResults class that contains the learned coefficients. The forecast() or the predict() function on the result object can be called to make a forecast. For example: Single, Double and Triple Exponential Smoothing can be implemented in Python using the ExponentialSmoothing Statsmodels class. First, an instance of the ExponentialSmoothing class must be instantiated, specifying both the training data and some configuration for the model. Specifically, you must specify the following configuration parameters: The model can then be fit on the training data by calling the fit() function. This function allows you to either specify the smoothing coefficients of the exponential smoothing model or have them optimized. By default, they are optimized (e.g. optimized=True). These coefficients include: Additionally, the fit function can perform basic data preparation prior to modeling; specifically: The fit() function will return an instance of the HoltWintersResults class that contains the learned coefficients. The forecast() or the predict() function on the result object can be called to make a forecast. This section provides more resources on the topic if you are looking to go deeper. In this tutorial, you discovered the exponential smoothing method for univariate time series forecasting. Specifically, you learned: Do you have any questions?
Ask your questions in the comments below and I will do my best to answer. ...with just a few lines of python code Discover how in my new Ebook:Introduction to Time Series Forecasting With Python It covers self-study tutorials and end-to-end projects on topics like:Loading data, visualization, modeling, algorithm tuning, and much more... Skip the Academics. Just Results. Click to learn more. Hi Jason,  Thank you very much for your post. This is very helpful resources. I would like to know how to install ¡°statsmodels.tsa.holtwinters¡± as I see that it is throwing error when I ran the command :
from statsmodels.tsa.holtwinters import ExponentialSmoothing It seems that statsmodels package do not have that command.
Could you please help me in working that command? ThanK you,
Sandeep It really depends on your platform, for example:  Alternately, try this tutorial:https://machinelearningmastery.com/setup-python-environment-machine-learning-deep-learning-anaconda/ Hello Jason, I am working on a forecasting project with a big dataset which includes 15 columns and around 9000 rows. The problem is I have to forecast the result for the next two years base on 14 columns of independent data, and the result should be binary(0,1).
I saw many forecasting problems online, but most of them forecast base on just one column of independent data with no binary result.
Is there any way to guide me or refer me any references to solve the problem? Thank you in advance,
Ehsan Yes, a neural network can easily forecast multiple variables, perhaps start with an MLP. Hi Jason, Hyndman has published a new edition of ¡®Forecasting, principles and practice¡¯. It is available free of charge at: https://otexts.org/fpp2/ . Best,
Elie Thanks. Thanks for this <U+2013> clear, and gentle, with nice follow up resources! You¡¯re welcome! Thanks for really nice and helpful matter on exponential smoothing. Thanks! Comment  Name (required)  Email (will not be published) (required)  Website Hi, I'm Jason Brownlee, Ph.D.

My goal is to make developers like YOU awesome at applied machine learning. Read More"
