<div class="about row">

    <div class="col-md-8">

        <h4>Feature Engineering</h4> 
        <li>정규화 옵션 : log1p, z-score, min-max(mlp)</li>
        <li>결측값 처리 옵션 : 0, -9999, NA </li>
        <li>WoE Features if there are too many splits</li>
        <li>Target Mean Encoding</li>
        <li>2 or 3 Interaction Features</li>
        <li>Feature 6: Clustering features of original dataset</li>
		<li>Feature 7: Number of non-zeros elements in each row</li>
        <li>post-processing</li>
        
        <br/>

        <h4>Feature Selection</h4> 
        <li><a href="https://www.kaggle.com/ogrellier/feature-scoring-vs-zeros">https://www.kaggle.com/ogrellier/feature-scoring-vs-zeros</a></li>
        <li>For each LGB model, I choose top 50 features and random choose 50 features from left features</li>

        <br/>

        <h4>Feature Extraction</h4>
        <li> .. </li> 
        
        <br/>

        <h4>Modeling</h4> 
        <li>Light GBM, CatBoost, XGBoost, Extra Trees</li> 
        <li>Tuning : Cartesian + Random Grid Search</li> 
        <li>Ensemble : 2-layer or 3-layer</li>
        <li>train데이터셋의 target range를 기준으로 예측값 cut off</li>
        <li>light gbm tuning guideline : <a href="https://github.com/Microsoft/LightGBM/issues/695">https://github.com/Microsoft/LightGBM/issues/695</a></li>
        <li>tuning options guideline: <a href="https://sites.google.com/view/lauraepp/parameters">https://sites.google.com/view/lauraepp/parameters</a></li>
        <li>3 LGBM models with the same parameters, but different seeds inside of LGBM (UPD: and different seeds in KFold splitting, which was even more important)</li>
        <li>binary_logloss</li>
        <br/>

    </div>

</div>